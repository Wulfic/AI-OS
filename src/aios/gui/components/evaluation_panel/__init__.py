"""Evaluation panel for running standardized model benchmarks.

This module provides a comprehensive UI for evaluating AI models using
lm-evaluation-harness. Supports preset benchmark groups, real-time progress
tracking, results visualization, and evaluation history.
"""

from .panel_main import EvaluationPanel

__all__ = ["EvaluationPanel"]
