logo_path: artifacts/dreams/images/logo.png
data_figpath: artifacts/dreams/images/data.png
result_table_figpath: artifacts/dreams/images/result_table.png
cm_figpath: artifacts/dreams/images/cm.png
acc_figpath: artifacts/dreams/images/acc.png
loss_figpath: artifacts/dreams/images/loss.png
uncertainty_figpath: artifacts/dreams/images/uncertainty.png
describe_overview: Auto-generated model card. Ingested 17 loss points; last_loss=0.0849.
dataset_name: synthetic/toy
num_target_class: 5
ground_truth: n/a
split_ratio: 80/20
preprocess_steps: none
describe_dataset: Tiny synthetic samples generated by AI-OS training loop.
model_output:
- action id (0-4)
- reward estimate
model_input:
- state vector (toy)
learning_approach: reinforcement learning (toy)
model_type: MLP/Tabular (toy)
learning_rate: n/a
batch_size: 32
additional_info: Auto-generated by AI-OS vendor-dreams scaffold-config.
model_details: Simple proof-of-concept training loop; replace with your model details.
performance_comments: 'Last observed loss: 0.0849. Lower is better in this toy setup.'
limitation_details:
- Toy environment; metrics do not reflect real-world performance.
- No validation/test split rigor.
- Small synthetic dataset and trivial model.
uncertainty_describe: Uncertainty estimation not computed (placeholder).
references: https://github.com/Wulfic/AI-OS
_aios_metrics:
  losses:
  - 0.15380127727985382
  - 0.13000252842903137
  - 0.13503871858119965
  - 0.09500616043806076
  - 0.08489043265581131
  count: 17
