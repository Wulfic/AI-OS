# Feature Combination Matrix - AI-OS
Last Updated: November 7, 2025
Purpose: Feature compatibility reference - which combinations are verified and which are experimental

> **Note for v1.0.0:** This matrix documents the current testing status of feature combinations. 
> Items marked as "EXPERIMENTAL" or with TODO notes represent experimental combinations 
> that may work but haven't been comprehensively tested. Use with appropriate caution.

---

## üìä Status Legend

| Status | Meaning |
|--------|---------|
| ‚úÖ **VERIFIED** | Tested and confirmed working |
| ‚ö†Ô∏è **EXPERIMENTAL** | Should work but not comprehensively tested |
| ‚ùå **INCOMPATIBLE** | Known to be incompatible |
| ‚ùì **UNTESTED** | Status unclear, use with caution |
| üöß **PARTIAL** | Partially works with known limitations |

---

## üî¨ Memory Optimization Combinations

### Gradient Checkpointing + AMP
**Status**: ‚úÖ **VERIFIED WORKING**  
**Benefit**: ~60-70% memory reduction  
**Speed Impact**: ~20% slower  
**Recommended**: Yes, for most training

Example:
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --gradient-checkpointing `
  --amp `
  --steps 100
```

**Test Results**:
- ‚úÖ Trains successfully
- ‚úÖ Memory reduction confirmed
- ‚úÖ No quality loss observed
- ‚úÖ Works on single GPU
- ‚ö†Ô∏è Multi-GPU not tested

---

### Gradient Checkpointing + AMP + 8-bit Optimizer
**Status**: ‚úÖ **VERIFIED WORKING**  
**Benefit**: ~70-80% memory reduction  
**Speed Impact**: ~25% slower  
**Recommended**: Yes, for large models (>100M params)

Example:
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --gradient-checkpointing `
  --amp `
  --use-8bit-optimizer `
  --steps 100
```

**Test Results**:
- ‚úÖ Trains successfully
- ‚úÖ Massive memory reduction
- ‚úÖ Quality maintained
- ‚úÖ Works with bitsandbytes 0.48.1
- ‚ö†Ô∏è Multi-GPU not tested

Requirements:
- bitsandbytes installed
- CUDA-capable GPU (Linux preferred)

---

### Gradient Checkpointing + Long Context
**Status**: ‚ö†Ô∏è **EXPERIMENTAL**  
**Expected**: Should work  
**Use Case**: Train with longer sequences on limited VRAM

Example:
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --gradient-checkpointing `
  --max-seq-len 2048 `
  --batch-size 1 `
  --steps 100
```

**Expected Behavior**:
- ‚úÖ Should enable 2K-4K context on 11GB GPU
- ‚ö†Ô∏è Will be slower due to checkpointing
- ‚ö†Ô∏è Batch size must be very small

**Note**: Not extensively tested with contexts above 2048 tokens. Start with smaller contexts and increase gradually.

---

### All Memory Optimizations Combined
**Status**: ‚ö†Ô∏è **PARTIAL**  
**Features**: Gradient Checkpointing + AMP + 8-bit + Chunking  
**Expected**: Maximum memory efficiency  
**Use Case**: Train very large models or very long contexts

Example:
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --gradient-checkpointing `
  --amp `
  --use-8bit-optimizer `
  --use-chunked-training --chunk-size 1024 `
  --max-seq-len 8192 `
  --batch-size 1 `
  --steps 100
```

Notes:
- ‚úÖ Chunked training is implemented (`--use-chunked-training`, `--chunk-size`)
- ‚ö†Ô∏è Expect slower throughput at very small chunk sizes

**TODO**: 
1. Verify chunked training is implemented
2. Test with various chunk sizes
3. Measure actual memory usage

---

## üöÄ Multi-GPU Combinations

### DDP + Gradient Checkpointing
**Status**: ‚ùì **EXPERIMENTAL**  
**Expected**: Fast distributed training with memory efficiency

Example (Linux recommended):
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --ddp `
  --cuda-ids "0,1" `
  --world-size 2 `
  --gradient-checkpointing `
  --steps 100
```

**Issues**:
- ‚ùì DDP implementation not verified
- ‚ùì Does `_maybe_spawn` function exist?
- ‚ùì Gradient sync working?

Windows tip: Prefer `--parallel-independent` instead of DDP.

---

### DDP + AMP
**Status**: ‚ùì **EXPERIMENTAL**  
**Expected**: Fast training with mixed precision across GPUs

Example (Linux recommended):
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --ddp `
  --cuda-ids "0,1" `
  --world-size 2 `
  --amp `
  --steps 100
```

**Note**: Not extensively tested with if AMP works correctly with DDP

---

### DDP + All Memory Optimizations
**Status**: ‚ùì **EXPERIMENTAL**  
**Expected**: Maximum efficiency across multiple GPUs

Example (Linux recommended):
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --ddp `
  --cuda-ids "0,1" `
  --world-size 2 `
  --gradient-checkpointing `
  --amp `
  --use-8bit-optimizer `
  --steps 100
```

---

Back to [Guide Index](../INDEX.MD)

**Questions**:
- Does 8-bit optimizer work with DDP?
- Are optimizer states synchronized?
- Is there communication overhead?

**TODO**: Comprehensive multi-GPU testing

---

## üß† DeepSpeed Combinations

### DeepSpeed ZeRO-1 + Gradient Checkpointing
**Status**: ‚ùì **EXPERIMENTAL**  
**Expected**: Optimizer state partitioning + activation checkpointing

Example (Linux + DeepSpeed):
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --zero-stage zero1 `
  --gradient-checkpointing `
  --cuda-ids "0,1" `
  --steps 100
```

**TODO**:
1. Verify DeepSpeed is actually initialized
2. Test ZeRO-1 stage
3. Measure memory reduction

---

### DeepSpeed ZeRO-2 + AMP
**Status**: ‚ùì **EXPERIMENTAL**  
**Expected**: Gradient partitioning + mixed precision

Example (Linux + DeepSpeed):
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --zero-stage zero2 `
  --amp `
  --cuda-ids "0,1" `
  --steps 100
```

**Note**: Not extensively tested with and measure

---

### DeepSpeed ZeRO-3 (Maximum Memory Reduction)
**Status**: ‚ùì **EXPERIMENTAL**  
**Expected**: Parameter partitioning for massive models

Example (Linux + DeepSpeed):
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --zero-stage zero3 `
  --gradient-checkpointing `
  --amp `
  --cuda-ids "0,1" `
  --steps 100
```

**Note**: Not extensively tested with ZeRO-3 stage

---

### DeepSpeed + 8-bit Optimizer
**Status**: ‚ùì **COMPATIBILITY UNKNOWN**  
**Question**: Can DeepSpeed work with bitsandbytes?

Example (Compat unknown):
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --zero-stage zero2 `
  --use-8bit-optimizer `
  --cuda-ids "0,1" `
  --steps 100
```

**Potential Issue**: DeepSpeed has its own optimizer management - may conflict with bitsandbytes

**Note**: Not extensively tested with compatibility

---

## üß© MoE / Dynamic Subbrains Combinations

### MoE + Gradient Checkpointing
**Status**: ‚ö†Ô∏è **UNTESTED**  
**Expected**: Should work  
**Use Case**: Train models with experts efficiently

Example:
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --use-moe `
  --num-experts 4 `
  --gradient-checkpointing `
  --steps 100
```

**Note**: Not extensively tested with MoE with checkpointing

---

### MoE + AMP + 8-bit
**Status**: ‚ö†Ô∏è **UNTESTED**  
**Expected**: Memory-efficient expert training

Example:
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --use-moe `
  --num-experts 8 `
  --gradient-checkpointing `
  --amp `
  --use-8bit-optimizer `
  --steps 100
```

**Note**: Not extensively tested with expert training with optimizations

---

### Expert Training + Memory Optimizations
**Status**: ‚ö†Ô∏è **UNTESTED**  
**Expected**: Efficient single expert training

Example:
```powershell
aios hrm-hf train-actv1 `
  --model artifacts/hf_implant/base_model `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --expert-id "python_expert" `
  --gradient-checkpointing `
  --amp `
  --use-8bit-optimizer `
  --steps 100
```

**Note**: Not extensively tested with expert-only training mode

---

### MoE + Multi-GPU
**Status**: ‚ùì **EXPERIMENTAL**  
**Expected**: Expert parallelism across GPUs

Example (Linux recommended):
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --use-moe `
  --num-experts 8 `
  --ddp `
  --cuda-ids "0,1" `
  --world-size 2 `
  --steps 100
```

**Questions**:
- How are experts distributed across GPUs?
- Is expert selection synchronized?
- What's the communication pattern?

**Note**: Not extensively tested with and document expert parallelism

---

### MoE + DeepSpeed
**Status**: ‚ùì **EXPERIMENTAL**  
**Expected**: Expert partitioning with ZeRO

Example (Linux + DeepSpeed):
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --use-moe `
  --num-experts 16 `
  --zero-stage zero3 `
  --cuda-ids "0,1" `
  --steps 100
```

**Note**: Not extensively tested with DeepSpeed with MoE

---

## üìö Context Length Combinations

### Long Context + Chunking
**Status**: ‚úÖ **SUPPORTED**  
**Expected**: Enable 10K+ contexts by chunking

Example:
```powershell
aios hrm-hf train-actv1 `
  --model gpt2 `
  --dataset-file training_data/curated_datasets/test_sample.txt `
  --max-seq-len 10000 `
  --use-chunked-training `
  --chunk-size 1024 `
  --gradient-checkpointing `
  --amp `
  --steps 100
```

**Questions**:
- Is chunking actually implemented?
- How does it split sequences?
- What's the memory impact?

**TODO**: 
1. Verify chunking implementation
2. Test with various context lengths: 8K, 16K, 32K
3. Measure actual memory usage

---

### Long Context + Multi-GPU
**Status**: ‚ö†Ô∏è **UNTESTED**  
**Expected**: Distribute long sequences across GPUs

**Command**:
```bash
aios hrm-hf train-actv1 \
  --model gpt2 \
  --dataset-file data.txt \
  --max-seq-len 8192 \
  --ddp \
  --cuda-ids "0,1" \
  --world-size 2 \
  --gradient-checkpointing \
  --batch-size 1 \
  --steps 1000
```

**Note**: Not extensively tested with long context with DDP

---

### FlashAttention + Memory/Chunking
**Status**: ‚ö†Ô∏è **PLATFORM-DEPENDENT**  
Notes:
- `--use-flash-attn` is supported by the CLI and will enable FA2 when installed and compatible (Ampere+).
- On Windows, FA2 is commonly unavailable; training falls back to PyTorch SDPA.
- Combine with `--window-size` for extreme contexts when FA2 is not available.

---

## üî§ Tokenizer Combinations

### Custom Tokenizer + Training
**Status**: ‚ö†Ô∏è **UNTESTED** (except GPT-2)  
**Expected**: Should work with any HuggingFace tokenizer

**Verified**:
- ‚úÖ GPT-2 tokenizer

**Needs Testing**:
- ‚ö†Ô∏è Qwen 2.5
- ‚ö†Ô∏è Mistral
- ‚ö†Ô∏è Code Llama
- ‚ö†Ô∏è DeepSeek-Coder V2
- ‚ö†Ô∏è StarCoder2
- ‚ö†Ô∏è Phi-3
- ‚ö†Ô∏è Llama 3 (requires HF auth)

**Note**: Not extensively tested with each tokenizer with basic training

---

### Large Vocabulary + Memory Optimizations
**Status**: ‚ö†Ô∏è **UNTESTED**  
**Use Case**: Tokenizers with 100K+ tokens (DeepSeek, Qwen, Llama 3)

**Command**:
```bash
aios hrm-hf train-actv1 \
  --model "deepseek-ai/deepseek-coder-v2-base" \
  --dataset-file data.txt \
  --gradient-checkpointing \
  --amp \
  --use-8bit-optimizer \
  --steps 1000
```

**Considerations**:
- Large vocabulary = larger embedding layer
- More memory needed for embeddings
- May need aggressive optimizations

**Note**: Not extensively tested with large-vocab tokenizers

---

## üìä Dataset Format Combinations

### Streaming Dataset + Linear Mode
**Status**: ‚úÖ **SUPPORTED**  
Features:
- Linear progression with resume via `--dataset-start-offset`
- Iterate mode for long‚Äërunning cycles via `--iterate`

**Features**:
- ‚úÖ Infinite streaming
- ‚úÖ Shuffle support
- ‚úÖ Caching
- ‚úÖ Memory-efficient

---

### Large Dataset + Multi-GPU
**Status**: ‚ö†Ô∏è **UNTESTED**  
**Expected**: Distributed dataset loading

**Command**:
```bash
aios hrm-hf train-actv1 \
  --model gpt2 \
  --dataset-file large_dataset.txt \
  --ddp \
  --cuda-ids "0,1" \
  --world-size 2 \
  --steps 10000
```

**Questions**:
- Is dataset split across workers?
- Is shuffling consistent?
- What's the I/O pattern?

**Note**: Not extensively tested with with multi-GB datasets

---

### Archive Dataset + Training
**Status**: ‚ö†Ô∏è **PARTIALLY TESTED**  
**Supported Formats**: .tar, .tar.gz, .tar.bz2, .zip

**Known Issues**:
- ‚ö†Ô∏è Large archives may hang (BUG-002)
- ‚ö†Ô∏è Many small files may be slow

**Note**: Not extensively tested with archive loading performance

---

## üéÆ GUI Feature Combinations

### GUI + Background Training
**Status**: ‚ö†Ô∏è **UNTESTED**  
**Expected**: GUI should remain responsive during training

**Note**: Not extensively tested with GUI responsiveness during training

---

### GUI + Multi-GPU
**Status**: ‚ùì **EXPERIMENTAL**  
**Question**: Does GUI support multi-GPU configuration?

**Note**: Not extensively tested with GUI multi-GPU controls

---

### GUI + Long Training
Status varies by machine. For multi‚Äëday runs, prefer CLI logging to `--log-file` and view metrics separately.

---

## üß™ Testing Recommendations

### High Priority Tests:

1. **DDP Verification** (3 tests)
   - DDP + basic training
   - DDP + memory optimizations
   - DDP + MoE

2. **DeepSpeed Verification** (3 tests)
   - ZeRO-1 basic
   - ZeRO-2 with AMP
   - ZeRO-3 maximum reduction

3. **Chunking Verification** (3 tests)
   - Verify implementation exists
   - Test 8K context
   - Test 16K context

4. **Tokenizer Testing** (7 tests)
   - Test each "supported" tokenizer

5. **MoE Combinations** (3 tests)
   - MoE + memory opts
   - MoE + multi-GPU
   - MoE + long context

### Medium Priority Tests:

1. **Long Context** (3 tests)
   - 2K, 4K, 8K without chunking
   - Measure actual limits

2. **Dataset Formats** (3 tests)
   - Large CSV
   - Large archive
   - Many small files

3. **Feature Interactions** (5 tests)
   - All memory opts combined
   - Multi-GPU + all opts
   - MoE + all opts

### Low Priority Tests:

1. **GUI** (3 tests)
   - Long training responsiveness
   - Multi-GPU controls
   - All panels working

2. **Edge Cases** (5 tests)
   - Very small models
   - Very large models
   - Very long contexts
   - Very large batches
   - Very small batches

---

## üìã Compatibility Matrix

### Quick Reference Table

| Feature 1 | Feature 2 | Status | Notes |
|-----------|-----------|--------|-------|
| Gradient Checkpointing | AMP | ‚úÖ Verified | ~60‚Äì70% memory reduction |
| Gradient Checkpointing | 8‚Äëbit Optimizer | ‚úÖ Supported | Requires bitsandbytes + CUDA |
| AMP | 8‚Äëbit Optimizer | ‚úÖ Supported | Common combo |
| All Memory Opts | Combined | ‚ö†Ô∏è Partial | Chunking + AMP + Checkpointing + 8‚Äëbit supported; tune chunk size |
| DDP (Linux) | Gradient Checkpointing | ‚úÖ Supported | Use `--ddp` + `--world-size` |
| DDP (Linux) | AMP | ‚úÖ Supported | |
| DDP (Linux) | 8‚Äëbit Optimizer | ‚ùì Unknown | May conflict with BnB; test on your setup |
| Parallel‚ÄëIndependent (Windows) | Chunking | ‚úÖ Supported | Windows‚Äëfriendly multi‚ÄëGPU |
| DeepSpeed (Linux) | Gradient Checkpointing | ‚úÖ Supported | Requires DeepSpeed install |
| DeepSpeed (Linux) | AMP | ‚úÖ Supported | |
| DeepSpeed (Linux) | 8‚Äëbit Optimizer | ‚ùì Unknown | DeepSpeed optimizer mgmt may conflict |
| MoE | Memory Opts | ‚úÖ Supported | Start conservative: k=2, capacity 1.25 |
| MoE | DDP/DeepSpeed | ‚ùì Needs Verify | Routing/load‚Äëbalance interactions |
| Chunking | Long Context | ‚úÖ Supported | Use 1024‚Äì2048 chunk sizes |
| FlashAttention (Linux) | AMP | ‚úÖ Supported | When FA2 installed; falls back to SDPA otherwise |
| FlashAttention (Windows) | Any | ‚ö†Ô∏è Platform | Often unavailable; rely on SDPA + window‚Äësize |

---

## üéØ Action Items

### Immediate (Week 1):
1. ‚úÖ Document all known combinations
2. ‚è≥ Verify DDP implementation
3. ‚è≥ Verify DeepSpeed implementation
4. ‚è≥ Verify chunking implementation

### Short-term (Week 2-3):
1. Test all memory optimization combinations
2. Test DDP with various configurations
3. Test DeepSpeed stages
4. Test tokenizers

### Medium-term (Week 4-6):
1. Test MoE combinations
2. Test long context scenarios
3. Test dataset formats
4. Create automated combination tests

### Long-term (Month 2+):
1. Create CI/CD for combination testing
2. Add performance benchmarks
3. Document optimal combinations for different use cases
4. Create combination recommendation tool

---

## üìö Related Documents

- [COMPLETE_FEATURE_INDEX.md](./COMPLETE_FEATURE_INDEX.md) ‚Äì Complete feature list
- [FLASH_ATTENTION.md](./FLASH_ATTENTION.md) ‚Ä¢ [FLASH_ATTENTION_VS_CHUNKING.md](./FLASH_ATTENTION_VS_CHUNKING.md)
- [PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md](./PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md)
- [MULTI_GPU_DISTRIBUTED.md](./MULTI_GPU_DISTRIBUTED.md)
- [LORA_PEFT.md](./LORA_PEFT.md)
- [DYNAMIC_SUBBRAINS_MOE.md](./DYNAMIC_SUBBRAINS_MOE.md)

---

**Matrix Version**: 1.0  
**Last Updated**: October 18, 2025  
**Maintained By**: Testing Team

**Status**: üîÑ In Progress - Many combinations need verification
