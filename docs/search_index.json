[{"path": "index.md", "content": "# AI-OS v1.0.14\n\n[![CI](https://github.com/Wulfic/AI-OS/actions/workflows/ci.yml/badge.svg)](https://github.com/Wulfic/AI-OS/actions/workflows/ci.yml)\n[![Link Check](https://github.com/Wulfic/AI-OS/actions/workflows/link-check.yml/badge.svg)](https://github.com/Wulfic/AI-OS/actions/workflows/link-check.yml)\n[![Version](https://img.shields.io/badge/version-1.0.14-blue.svg)](https://github.com/Wulfic/AI-OS/releases/tag/Official)\n[![License](https://img.shields.io/badge/license-ANSL--v1.0-blue.svg)](LICENSE.md)\n[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/S6S31MS4TK)\n\n# Fancy Documentation: https://wulfic.github.io/AI-OS/\n\nHRM-sMoE LLM training toolkit with a clean GUI, CLI, and installers. Future-facing architecture for OS-integrated autonomous assistance.\n\nLicense: AI-OS Non\u2011Selling Attribution License (ANSL) v1.0. You may use, copy, modify, and redistribute this code, but you may not sell the Software or derivative works. All redistributions must retain attribution and link back to the original repository. See [LICENSE](LICENSE.md) and [NOTICE](https://github.com/Wulfic/AI-OS/blob/main/NOTICE).\n\n## What is AI-OS?\n\n- Today: Train Hierarchical Reasoning Models (HRM) with Sparse Mixture\u2011of\u2011Experts (MoE) on consumer GPUs. Optimizations enable long context training on almost any sized card! Designed to be a streamlined, pipeline, from data prep to model training/inference to evaluation for users of all skill levels. No need to be an ML expert to get started!\n- Tomorrow: Deeper OS integration and autonomous, idle\u2011time learning.\n- Bleeding edge technologies, pushing the boundaries of consumer hardware.\n- Tons more features! Check out the Planned_Features section\n\nKey features in v1.0.14:\n- Windows and Ubuntu Support!\n- Long-context training strategies and memory optimizations \n- Multi\u2011GPU training (DDP/Parallel/Sharding)\n- GUI + CLI(TUI coming soon!)\n- End to End pipeline for HRM based models\n- Integration with huggingface\n- (Optional) Base model with extremely basic english comprehension\n\n## Authors Note\nWhile the minimum specs listed below will allow you to train and run models, please bare in mind, that training could take weeks on smaller/older cards.\nThe Optional model that ships alongside this program was trained on 2x RTX 2080ti(11GB) cards. The model is a utilizing the qwen tokenizer, sMoE enabled 268M Params with 8 experts, context length of 1024 resulting in about ~6000 characters for output. Trained 1/10th the Tiny Stories Dataset in ~6 hours.\n\nTLDR; Have a fast ass GPU with lots of VRAM if you're impatient.\n\n## Minimum Recommended Specs!\n- GPU: 6GB VRAM **NOTE** (Nvidia:Full Support|AMD:UNTESTED|INTEL(ARC):UNTESTED|CPU:Partial Support)\n- System Memory: 16GB\n- CPU: 4 Core\n\n### Prerequisites\n- ~8-10GB Storage for the program, however having at a least a 1 TB SSD/NVME is ideal!\n- Windows 11 or Ubuntu 24\n- Internet connection(Faster is better)\n*Note*: If you have a metered connection(limited monthly bandwidth), be mindful of the datasets you choose.\n\n## Quick Install Info\n### Installers\nFind the official installers also in the [Releases section](https://github.com/Wulfic/AI-OS/releases).\nSee the [Installers Documentation](https://github.com/Wulfic/AI-OS/blob/main/installers/README.md) for Windows (.exe) and Ubuntu (.deb) info.\nIf you broke your install or are missing dependencies in terminal use: \n```bash\naios doctor --repair\n```\n### APT\n```bash\nsudo add-apt-repository ppa:wulfic/ppa-aios\nsudo apt update\nsudo apt install ai-os -y\n```\n### Ubuntu\n```bash\n# From repo root\nsudo chmod a+x ./installers/scripts/install_aios_on_ubuntu.sh\nsudo ./installers/scripts/install_aios_on_ubuntu.sh install --yes\n```\n### Windows (PowerShell)\n```powershell\n# From repo root(might need to run as admin)\nSet-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass\n./installers/scripts/install_aios_on_windows.ps1 -Action install -Yes\n```\n\n## Administrator Privileges\nAI-OS requires Administrator privileges on Windows to function correctly. This is necessary for:\n- Write access to `%ProgramData%` for shared artifacts and models.\n- GPU scheduling and performance tuning.\n- System-level integration features.\n\nThe installer creates shortcuts that automatically request elevation. If you launch AI-OS manually, please ensure you run it as Administrator.\n\n## Usage\nUse the shortcut to start the program or via terminal.\n\nGUI (recommended):\n```\naios gui\n```\nInteractive CLI:\n```\naios\n```\n## Documentation\n\n- Start here: [Guide Index](guide/INDEX.MD)\n- Feature Reference: [Complete Feature Index](guide/features/COMPLETE_FEATURE_INDEX.md)\n- Attributions and upstream projects: [REFERENCES.md](REFERENCES.md)\n- One-page Quick Start: [Quick Start Guide](guide/QUICK_START.md)\n\n## References and third\u2011party integrations\n\nSee [REFERENCES.md](REFERENCES.md) for a complete list of libraries and upstream projects used (PyTorch, Transformers, DeepSpeed, bitsandbytes, FlashAttention, and more) with links and licenses.\n\n## Acknowledgments\n\nThanks to the open\u2011source ML community and upstream projects that made this possible. See [REFERENCES.md](REFERENCES.md).\n\n## License\n\nAI\u2011OS Non\u2011Selling Attribution License (ANSL) v1.0 \u2014 see [LICENSE](LICENSE.md) and [NOTICE](https://github.com/Wulfic/AI-OS/blob/main/NOTICE).\n", "tags": ["cli", "datasets", "evaluation", "experts", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "AI-OS v1.0.14"}, {"line": 8, "text": "Fancy Documentation: https://wulfic.github.io/AI-OS/"}, {"line": 14, "text": "What is AI-OS?"}, {"line": 30, "text": "Authors Note"}, {"line": 36, "text": "Minimum Recommended Specs!"}, {"line": 41, "text": "Prerequisites"}, {"line": 47, "text": "Quick Install Info"}, {"line": 48, "text": "Installers"}, {"line": 55, "text": "APT"}, {"line": 61, "text": "Ubuntu"}, {"line": 63, "text": "From repo root"}, {"line": 67, "text": "Windows (PowerShell)"}, {"line": 69, "text": "From repo root(might need to run as admin)"}, {"line": 74, "text": "Administrator Privileges"}, {"line": 82, "text": "Usage"}, {"line": 93, "text": "Documentation"}, {"line": 100, "text": "References and third\u2011party integrations"}, {"line": 104, "text": "Acknowledgments"}, {"line": 108, "text": "License"}]}, {"path": "LICENSE.md", "content": "AI-OS Non\u2011Selling Attribution License (ANSL) v1.0\n\nCopyright (c) 2025 WulNet Designs LLC\n\nThis is a source-available license that allows use, copying, modification, and\nredistribution, with attribution, while prohibiting selling the Software or\nderivative works. It is not an OSI-approved open source license.\n\n1. Definitions\n\t1.1. \"Software\" means the AI-OS codebase and accompanying files in this\n\t\t  repository and any derivative works thereof.\n\t1.2. \"You\" and \"Your\" mean the individual or legal entity exercising\n\t\t  permissions granted by this license.\n\t1.3. \"Sell\" means to charge a fee, price, subscription, or other\n\t\t  consideration specifically for the Software or for access to the\n\t\t  Software or any derivative works, including offering it as a paid\n\t\t  hosted service. Charging for ancillary services (e.g., consulting,\n\t\t  training, custom integration) where the fee is not for the Software\n\t\t  itself is not considered Selling.\n\t1.4. \"User-Created Content\" means any machine learning models, neural\n\t\t  network weights, datasets, fine-tuned models, trained adapters,\n\t\t  embeddings, or other outputs generated or created by You through\n\t\t  Your use of the Software. User-Created Content is expressly not\n\t\t  considered a \"derivative work\" of the Software for purposes of\n\t\t  this license.\n\n2. Grant of Rights\n\tSubject to the Conditions below, You are hereby granted a perpetual,\n\tworldwide, non-exclusive, non-transferable, royalty-free license to:\n\t- use, copy, modify, and create derivative works of the Software;\n\t- distribute the Software and derivative works in any medium.\n\n3. User-Created Content Rights\n\tYou retain full and exclusive ownership of all User-Created Content.\n\tThe licensor claims no rights, title, or interest in any User-Created\n\tContent. You may use, copy, modify, distribute, sell, license,\n\tcommercialize, or otherwise dispose of Your User-Created Content in any\n\tmanner You choose, without restriction and without any obligation to the\n\tlicensor. The restrictions in this license regarding Selling and\n\tattribution do not apply to User-Created Content.\n\n4. Conditions\n\tYour rights above are conditioned on all of the following:\n\t3.1. Attribution & Link Back. You must preserve all copyright notices and\n\t\t  include a prominent notice of attribution in all copies and derivative\n\t\t  works that states: \"This work is based on AI-OS (https://github.com/Wulfic/AI-OS)\"\n\t\t  and include a working hyperlink to the original repository.\n\t3.2. Preserve License and NOTICE. You must include this license text and the\n\t\t  NOTICE file with all copies and distributions of the Software and any\n\t\t  derivative works, and you must not remove or alter them.\n\t3.3. Modifications Disclosure. If You modify the Software, You must include\n\t\t  a clear, prominent statement in Your distribution describing that You\n\t\t  have modified the Software and the date of the modification.\n\t3.4. No Selling. You may not Sell the Software or derivative works, nor\n\t\t  charge for access to the Software or derivative works, including as a\n\t\t  paid hosted service. This restriction does not prohibit charging for\n\t\t  services that do not themselves consist of providing the Software (e.g.,\n\t\t  consulting, installation, or training), provided the Software remains\n\t\t  freely available under this license.\n\n5. Trademark and Brand Protection\n\t5.1. Reserved Marks. The following are trademarks and/or service marks of\n\t\t  WulNet Designs LLC (collectively, the \"Marks\"):\n\t\t  (a) \"AI-OS\" (name);\n\t\t  (b) \"WulNet Designs\" and \"Wulfnet Designs\" (company names);\n\t\t  (c) The WulNet Designs wolf logo and any substantially similar wolf\n\t\t      head designs associated with the Software;\n\t\t  (d) Any other logos, icons, or visual branding included in the\n\t\t      Software's official distribution.\n\t5.2. No Trademark License. This license does not grant permission to use\n\t\t  the Marks, except as expressly permitted in Section 5.3.\n\t5.3. Permitted Use. You may use the Marks solely for:\n\t\t  (a) Factual attribution statements describing the origin of the\n\t\t      Software (e.g., \"Based on AI-OS by WulNet Designs LLC\");\n\t\t  (b) Unmodified reproduction of the Marks in the LICENSE and NOTICE\n\t\t      files as required by Section 4.\n\t5.4. Prohibited Use. Without prior written permission from WulNet Designs\n\t\t  LLC, You may not:\n\t\t  (a) Use any of the Marks as part of Your own product name, service\n\t\t      name, company name, domain name, or social media handle;\n\t\t  (b) Display the Marks in a manner that suggests sponsorship,\n\t\t      endorsement, or affiliation with WulNet Designs LLC;\n\t\t  (c) Use the wolf logo or any confusingly similar design in connection\n\t\t      with derivative works or forks of the Software;\n\t\t  (d) Modify, distort, or alter the Marks in any way;\n\t\t  (e) Use the Marks in any manner that could tarnish, disparage, or\n\t\t      reflect negatively on WulNet Designs LLC or the Software.\n\t5.5. Derivative Works Branding. If You distribute a modified version of\n\t\t  the Software, You must:\n\t\t  (a) Remove or replace all instances of the Marks (except in the\n\t\t      LICENSE, NOTICE, and attribution statements);\n\t\t  (b) Use a distinct name and branding that does not incorporate or\n\t\t      imitate the Marks;\n\t\t  (c) Include a clear statement that Your version is a modified\n\t\t      derivative and is not affiliated with or endorsed by WulNet\n\t\t      Designs LLC.\n\t5.6. Enforcement. WulNet Designs LLC reserves the right to revoke the\n\t\t  permissions granted in Section 5.3 at any time if, in its sole\n\t\t  discretion, Your use of the Marks is deemed harmful to its reputation\n\t\t  or interests. Unauthorized use of the Marks may constitute trademark\n\t\t  infringement and unfair competition under applicable law.\n\n6. Disclaimer of Warranty\n\tTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\tIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\tFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\tAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\tLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n\tFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n\tDEALINGS IN THE SOFTWARE.\n\n7. Limitation of Liability\n\tTO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT SHALL THE\n\tAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY INDIRECT, INCIDENTAL,\n\tSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n\tPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n\tOR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n\tWHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n\tOTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THE SOFTWARE, EVEN IF\n\tADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n8. Termination\n\tAny breach of this license automatically terminates the rights granted\n\therein. Upon termination, You must cease all distribution of the Software\n\tand derivative works, but Your obligations under Sections 4, 6, and 7 shall\n\tsurvive.\n\n9. Severability\n\tIf any provision of this license is held to be unenforceable, such\n\tprovision shall be reformed only to the extent necessary to make it\n\tenforceable, and the remaining provisions shall remain in full force and\n\teffect.\n\n", "tags": ["datasets", "training"], "headings": []}, {"path": "README.md", "content": "# AI-OS v1.0.14\n\n[![CI](https://github.com/Wulfic/AI-OS/actions/workflows/ci.yml/badge.svg)](https://github.com/Wulfic/AI-OS/actions/workflows/ci.yml)\n[![Link Check](https://github.com/Wulfic/AI-OS/actions/workflows/link-check.yml/badge.svg)](https://github.com/Wulfic/AI-OS/actions/workflows/link-check.yml)\n[![Version](https://img.shields.io/badge/version-1.0.14-blue.svg)](https://github.com/Wulfic/AI-OS/releases/tag/Official)\n[![License](https://img.shields.io/badge/license-ANSL--v1.0-blue.svg)](LICENSE.md)\n[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/S6S31MS4TK)\n\n# Fancy Documentation: https://wulfic.github.io/AI-OS/\n\nHRM-sMoE LLM training toolkit with a clean GUI, CLI, and installers. Future-facing architecture for OS-integrated autonomous assistance.\n\nLicense: AI-OS Non\u2011Selling Attribution License (ANSL) v1.0. You may use, copy, modify, and redistribute this code, but you may not sell the Software or derivative works. All redistributions must retain attribution and link back to the original repository. See [LICENSE](LICENSE.md) and [NOTICE](https://github.com/Wulfic/AI-OS/blob/main/NOTICE).\n\n## What is AI-OS?\n\n- Today: Train Hierarchical Reasoning Models (HRM) with Sparse Mixture\u2011of\u2011Experts (MoE) on consumer GPUs. Optimizations enable long context training on almost any sized card! Designed to be a streamlined, pipeline, from data prep to model training/inference to evaluation for users of all skill levels. No need to be an ML expert to get started!\n- Tomorrow: Deeper OS integration and autonomous, idle\u2011time learning.\n- Bleeding edge technologies, pushing the boundaries of consumer hardware.\n- Tons more features! Check out the Planned_Features section\n\nKey features in v1.0.14:\n- Windows and Ubuntu Support!\n- Long-context training strategies and memory optimizations \n- Multi\u2011GPU training (DDP/Parallel/Sharding)\n- GUI + CLI(TUI coming soon!)\n- End to End pipeline for HRM based models\n- Integration with huggingface\n- (Optional) Base model with extremely basic english comprehension\n\n## Authors Note\nWhile the minimum specs listed below will allow you to train and run models, please bare in mind, that training could take weeks on smaller/older cards.\nThe Optional model that ships alongside this program was trained on 2x RTX 2080ti(11GB) cards. The model is a utilizing the qwen tokenizer, sMoE enabled 268M Params with 8 experts, context length of 1024 resulting in about ~6000 characters for output. Trained 1/10th the Tiny Stories Dataset in ~6 hours.\n\nTLDR; Have a fast ass GPU with lots of VRAM if you're impatient.\n\n## Minimum Recommended Specs!\n- GPU: 6GB VRAM **NOTE** (Nvidia:Full Support|AMD:UNTESTED|INTEL(ARC):UNTESTED|CPU:Partial Support)\n- System Memory: 16GB\n- CPU: 4 Core\n\n### Prerequisites\n- ~8-10GB Storage for the program, however having at a least a 1 TB SSD/NVME is ideal!\n- Windows 11 or Ubuntu 24\n- Internet connection(Faster is better)\n*Note*: If you have a metered connection(limited monthly bandwidth), be mindful of the datasets you choose.\n\n## Quick Install Info\n### Installers\nFind the official installers also in the [Releases section](https://github.com/Wulfic/AI-OS/releases).\nSee the [Installers Documentation](https://github.com/Wulfic/AI-OS/blob/main/installers/README.md) for Windows (.exe) and Ubuntu (.deb) info.\nIf you broke your install or are missing dependencies in terminal use: \n```bash\naios doctor --repair\n```\n### APT\n```bash\nsudo add-apt-repository ppa:wulfic/ppa-aios\nsudo apt update\nsudo apt install ai-os -y\n```\n### Ubuntu\n```bash\n# From repo root\nsudo chmod a+x ./installers/scripts/install_aios_on_ubuntu.sh\nsudo ./installers/scripts/install_aios_on_ubuntu.sh install --yes\n```\n### Windows (PowerShell)\n```powershell\n# From repo root(might need to run as admin)\nSet-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass\n./installers/scripts/install_aios_on_windows.ps1 -Action install -Yes\n```\n\n## Administrator Privileges\nAI-OS requires Administrator privileges on Windows to function correctly. This is necessary for:\n- Write access to `%ProgramData%` for shared artifacts and models.\n- GPU scheduling and performance tuning.\n- System-level integration features.\n\nThe installer creates shortcuts that automatically request elevation. If you launch AI-OS manually, please ensure you run it as Administrator.\n\n## Usage\nUse the shortcut to start the program or via terminal.\n\nGUI (recommended):\n```\naios gui\n```\nInteractive CLI:\n```\naios\n```\n## Documentation\n\n- Start here: [Guide Index](guide/INDEX.MD)\n- User Guide Overview: [Guide README](guide/README.md)\n- Attributions and upstream projects: [REFERENCES.md](REFERENCES.md)\n- One-page Quick Start: [Quick Start Guide](guide/QUICK_START.md)\n\n## References and third\u2011party integrations\n\nSee [REFERENCES.md](REFERENCES.md) for a complete list of libraries and upstream projects used (PyTorch, Transformers, DeepSpeed, bitsandbytes, FlashAttention, and more) with links and licenses.\n\n## Acknowledgments\n\nThanks to the open\u2011source ML community and upstream projects that made this possible. See [REFERENCES.md](REFERENCES.md).\n\n## License\n\nAI\u2011OS Non\u2011Selling Attribution License (ANSL) v1.0 \u2014 see [LICENSE](LICENSE.md) and [NOTICE](https://github.com/Wulfic/AI-OS/blob/main/NOTICE).\n", "tags": ["cli", "datasets", "evaluation", "experts", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "AI-OS v1.0.14"}, {"line": 8, "text": "Fancy Documentation: https://wulfic.github.io/AI-OS/"}, {"line": 14, "text": "What is AI-OS?"}, {"line": 30, "text": "Authors Note"}, {"line": 36, "text": "Minimum Recommended Specs!"}, {"line": 41, "text": "Prerequisites"}, {"line": 47, "text": "Quick Install Info"}, {"line": 48, "text": "Installers"}, {"line": 55, "text": "APT"}, {"line": 61, "text": "Ubuntu"}, {"line": 63, "text": "From repo root"}, {"line": 67, "text": "Windows (PowerShell)"}, {"line": 69, "text": "From repo root(might need to run as admin)"}, {"line": 74, "text": "Administrator Privileges"}, {"line": 82, "text": "Usage"}, {"line": 93, "text": "Documentation"}, {"line": 100, "text": "References and third\u2011party integrations"}, {"line": 104, "text": "Acknowledgments"}, {"line": 108, "text": "License"}]}, {"path": "REFERENCES.md", "content": "# References and Third-Party Integrations\n\n**Last Updated**: December 12, 2025  \n**Purpose**: Attribution and acknowledgment of upstream projects and libraries\n\n---\n\nThis document lists the third-party libraries, tools, and upstream projects that AI-OS depends on or is inspired by. We are grateful to the open-source community for making these tools available.\n\n---\n\n## Core Dependencies\n\n### Machine Learning & Deep Learning\n\n| Library | License | Description | Links |\n|---------|---------|-------------|-------|\n| **PyTorch** | BSD-3-Clause | Deep learning framework | [Website](https://pytorch.org/) \u00b7 [GitHub](https://github.com/pytorch/pytorch) |\n| **Transformers** | Apache-2.0 | State-of-the-art NLP models | [Website](https://huggingface.co/transformers/) \u00b7 [GitHub](https://github.com/huggingface/transformers) |\n| **Accelerate** | Apache-2.0 | Distributed training utilities | [GitHub](https://github.com/huggingface/accelerate) |\n| **Datasets** | Apache-2.0 | Dataset loading and processing | [GitHub](https://github.com/huggingface/datasets) |\n| **PEFT** | Apache-2.0 | Parameter-efficient fine-tuning | [GitHub](https://github.com/huggingface/peft) |\n| **Safetensors** | Apache-2.0 | Safe tensor serialization | [GitHub](https://github.com/huggingface/safetensors) |\n| **Hugging Face Hub** | Apache-2.0 | Model hub integration | [GitHub](https://github.com/huggingface/huggingface_hub) |\n\n### Memory Optimization & Performance\n\n| Library | License | Description | Links |\n|---------|---------|-------------|-------|\n| **DeepSpeed** | Apache-2.0 | Deep learning optimization library | [Website](https://www.deepspeed.ai/) \u00b7 [GitHub](https://github.com/microsoft/DeepSpeed) |\n| **bitsandbytes** | MIT | 8-bit optimizers and quantization | [GitHub](https://github.com/TimDettmers/bitsandbytes) |\n| **FlashAttention** | BSD-3-Clause | Fast and memory-efficient attention | [GitHub](https://github.com/Dao-AILab/flash-attention) |\n\n### Tokenization\n\n| Library | License | Description | Links |\n|---------|---------|-------------|-------|\n| **SentencePiece** | Apache-2.0 | Unsupervised text tokenizer | [GitHub](https://github.com/google/sentencepiece) |\n| **Protobuf** | BSD-3-Clause | Protocol buffers for tokenizers | [GitHub](https://github.com/protocolbuffers/protobuf) |\n\n### Evaluation\n\n| Library | License | Description | Links |\n|---------|---------|-------------|-------|\n| **lm-eval** | MIT | Language model evaluation harness | [GitHub](https://github.com/EleutherAI/lm-evaluation-harness) |\n| **math-verify** | MIT | Mathematical verification utilities | [PyPI](https://pypi.org/project/math-verify/) |\n\n---\n\n## Application Dependencies\n\n### Web & Networking\n\n| Library | License | Description | Links |\n|---------|---------|-------------|-------|\n| **aiohttp** | Apache-2.0 | Async HTTP client/server | [GitHub](https://github.com/aio-libs/aiohttp) |\n| **httpx** | BSD-3-Clause | Modern HTTP client | [GitHub](https://github.com/encode/httpx) |\n| **Playwright** | Apache-2.0 | Browser automation | [GitHub](https://github.com/microsoft/playwright-python) |\n| **BeautifulSoup4** | MIT | HTML/XML parsing | [Website](https://www.crummy.com/software/BeautifulSoup/) |\n| **lxml** | BSD-3-Clause | XML and HTML processing | [GitHub](https://github.com/lxml/lxml) |\n| **Trafilatura** | Apache-2.0 | Web scraping and text extraction | [GitHub](https://github.com/adbar/trafilatura) |\n\n### Data & Validation\n\n| Library | License | Description | Links |\n|---------|---------|-------------|-------|\n| **Pydantic** | MIT | Data validation using Python types | [GitHub](https://github.com/pydantic/pydantic) |\n| **orjson** | Apache-2.0 / MIT | Fast JSON library | [GitHub](https://github.com/ijl/orjson) |\n| **PyYAML** | MIT | YAML parser and emitter | [GitHub](https://github.com/yaml/pyyaml) |\n| **NumPy** | BSD-3-Clause | Numerical computing | [Website](https://numpy.org/) \u00b7 [GitHub](https://github.com/numpy/numpy) |\n\n### CLI & UI\n\n| Library | License | Description | Links |\n|---------|---------|-------------|-------|\n| **Typer** | MIT | CLI application framework | [GitHub](https://github.com/tiangolo/typer) |\n| **Rich** | MIT | Rich text and formatting | [GitHub](https://github.com/Textualize/rich) |\n| **Pillow** | HPND | Python Imaging Library | [GitHub](https://github.com/python-pillow/Pillow) |\n| **pystray** | LGPL-3.0 | System tray icon library | [GitHub](https://github.com/moses-palmer/pystray) |\n| **matplotlib** | PSF | Plotting library | [GitHub](https://github.com/matplotlib/matplotlib) |\n| **Markdown** | BSD-3-Clause | Markdown parser | [GitHub](https://github.com/Python-Markdown/markdown) |\n| **tkinterweb** | MIT | Web browser widget for Tkinter | [GitHub](https://github.com/AKEric/tkinterweb) |\n| **rapidfuzz** | MIT | Fast string matching | [GitHub](https://github.com/maxbachmann/RapidFuzz) |\n\n### System & Utilities\n\n| Library | License | Description | Links |\n|---------|---------|-------------|-------|\n| **psutil** | BSD-3-Clause | Process and system utilities | [GitHub](https://github.com/giampaolo/psutil) |\n| **watchdog** | Apache-2.0 | Filesystem event monitoring | [GitHub](https://github.com/gorakhargosh/watchdog) |\n| **tenacity** | Apache-2.0 | Retry library | [GitHub](https://github.com/jd/tenacity) |\n| **tqdm** | MIT / MPL-2.0 | Progress bars | [GitHub](https://github.com/tqdm/tqdm) |\n| **dbus-next** | MIT | D-Bus for Python (Linux) | [GitHub](https://github.com/altdesktop/python-dbus-next) |\n\n---\n\n## Development Dependencies\n\n| Library | License | Description | Links |\n|---------|---------|-------------|-------|\n| **pytest** | MIT | Testing framework | [GitHub](https://github.com/pytest-dev/pytest) |\n| **pytest-asyncio** | Apache-2.0 | Async test support | [GitHub](https://github.com/pytest-dev/pytest-asyncio) |\n| **Ruff** | MIT | Fast Python linter | [GitHub](https://github.com/astral-sh/ruff) |\n| **Black** | MIT | Code formatter | [GitHub](https://github.com/psf/black) |\n\n---\n\n## Documentation\n\n| Tool | License | Description | Links |\n|------|---------|-------------|-------|\n| **MkDocs** | BSD-2-Clause | Static site generator | [GitHub](https://github.com/mkdocs/mkdocs) |\n| **Material for MkDocs** | MIT | MkDocs theme | [GitHub](https://github.com/squidfunk/mkdocs-material) |\n\n---\n\n## Research References\n\nAI-OS implements concepts from the following research:\n\n### Hierarchical Reasoning Models (HRM)\n- **Paper**: [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734)\n- Implements hierarchical reasoning with mixture-of-experts architecture\n\n### Mixture of Experts (MoE)\n- **Switch Transformers**: [Paper](https://arxiv.org/abs/2101.03961) - Scaling to trillion parameter models\n- **GShard**: [Paper](https://arxiv.org/abs/2006.16668) - Scaling giant models with conditional computation\n\n### Memory Optimization Techniques\n- **Gradient Checkpointing**: Trade compute for memory during backpropagation\n- **Mixed Precision Training**: FP16/BF16 training with loss scaling\n- **ZeRO Optimization**: Memory-efficient distributed training (DeepSpeed)\n\n---\n\n## Acknowledgments\n\nSpecial thanks to:\n- The **PyTorch** team for the foundational deep learning framework\n- **Hugging Face** for the transformers ecosystem and model hub\n- **Microsoft DeepSpeed** team for training optimizations\n- The **EleutherAI** team for the evaluation harness\n- All contributors to the open-source ML community\n\n---\n\n## License Compliance\n\nAll dependencies are used in compliance with their respective licenses. AI-OS is licensed under the **AI-OS Non\u2011Selling Attribution License (ANSL) v1.0** - see [LICENSE](LICENSE.md) for details.\n\nFor questions about licensing or attribution, please open an issue on [GitHub](https://github.com/Wulfic/AI-OS/issues).\n\n---\n\n[Back to Home](index.md) | [Back to Guide](guide/INDEX.MD)\n", "tags": ["cli", "datasets", "evaluation", "training"], "headings": [{"line": 0, "text": "References and Third-Party Integrations"}, {"line": 11, "text": "Core Dependencies"}, {"line": 13, "text": "Machine Learning & Deep Learning"}, {"line": 25, "text": "Memory Optimization & Performance"}, {"line": 33, "text": "Tokenization"}, {"line": 40, "text": "Evaluation"}, {"line": 49, "text": "Application Dependencies"}, {"line": 51, "text": "Web & Networking"}, {"line": 62, "text": "Data & Validation"}, {"line": 71, "text": "CLI & UI"}, {"line": 84, "text": "System & Utilities"}, {"line": 96, "text": "Development Dependencies"}, {"line": 107, "text": "Documentation"}, {"line": 116, "text": "Research References"}, {"line": 120, "text": "Hierarchical Reasoning Models (HRM)"}, {"line": 124, "text": "Mixture of Experts (MoE)"}, {"line": 128, "text": "Memory Optimization Techniques"}, {"line": 135, "text": "Acknowledgments"}, {"line": 146, "text": "License Compliance"}]}, {"path": "guide/INDEX.MD", "content": "# AI-OS User Guide - Index\n**Last Updated**: December 12, 2025  \n**Purpose**: Primary navigation hub for all AI-OS documentation  \n**Audience**: Users, developers, and contributors\n\n---\n\n## \ud83d\udcd6 Welcome to the AI-OS Guide\n\nThis guide provides comprehensive documentation for AI-OS, including feature documentation, testing resources, bug tracking, and API references. Start here to navigate all available documentation.\n\nNote on scope: This guide indexes and verifies ONLY files within the `docs/guide/` folder (and its `api/` subfolder). References to other docs are provided for convenience but are out of scope for this guide\u2019s consistency checks.\n\n---\n\n## \ud83d\ude80 Quick Start\n\n### New Users\n1. **Start here**: [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md) - Learn what AI-OS can do\n2. **Then review**: [Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md) - Compatibility and caveats\n3. **API quick reference**: [Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md)\n\n### Developers\n1. **Feature planning**: [Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md)\n2. **API reference**: [Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md)\n3. **Advanced features**: [Advanced Training Features](features/ADVANCED_FEATURES.md)\n\n### Contributors\n1. **Run diagnostics (optional)**: See `scripts/README.md`\n2. **Open issues**: https://github.com/Wulfic/AI-OS/issues\n3. **Verify features**: [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)\n\n---\n\n## \ud83d\udcda Core Documentation\n\n### Navigation Hubs\nUse these to explore features, combinations, and references:\n- [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)\n- [Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md)\n- [Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md)\n\n---\n\n### [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)\n**Feature Documentation** | **1,402 lines** | **100+ features**\n\nComprehensive mapping of EVERY feature in AI-OS with implementation status.\n\n**Contents**:\n- \u2705 CLI Commands (15+)\n- \u2705 Core Training Features (10+)\n- \u2705 Memory Optimization (6 features)\n- \u2705 Model Architecture (6 components)\n- \u2705 Dataset System (5 features)\n- \u2705 Tokenizers (15 types)\n- \u2705 Dynamic Subbrains / MoE (7 components)\n- \u2705 GUI Features (8 panels)\n- \u26a0\ufe0f Multi-GPU & Distributed (needs verification)\n- \u2705 Tools & Integrations (10+)\n\n**Use this when**: You want to know what features exist and where they're implemented\n\n---\n\n### Issues and Bug Tracking\nUse GitHub Issues to report and track problems:\n- https://github.com/Wulfic/AI-OS/issues\n\n---\n\n### [Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md)\n**Compatibility Guide** | **719 lines** | **30+ combinations**\n\nDocuments which features work together and known incompatibilities.\n\n**Verified Combinations**:\n- \u2705 Gradient Checkpointing + AMP (~60-70% memory reduction)\n- \u2705 Gradient Checkpointing + AMP + 8-bit (~70-80% memory reduction)\n- \u2705 Streaming Dataset + Shuffling\n\n**Needs Testing**:\n- \u2753 DDP + anything (unverified)\n- \u2753 DeepSpeed + anything (unverified)\n- \u2753 Chunking + anything (unclear if implemented)\n\n**Use this when**: Planning training configurations or debugging feature interactions\n\n---\n\n### Advanced Features and Notes\nSee advanced flags, platform notes, and troubleshooting:\n- [Advanced Training Features](features/ADVANCED_FEATURES.md)\n- [Multi-GPU GUI Guide](features/MULTIGPU_GUI_GUIDE.md)\n\n---\n\n## \ud83d\udd0c API References\n\n### [Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md)\n**CLI Reference** | **363 lines**\n\nQuick reference for CLI training API changes and usage.\n\n**Contents**:\n- What works vs what's removed\n- CLI parameter reference\n- Python API examples\n- Migration guides\n\n**Use this when**: Writing training scripts or using the CLI\n\n---\n\n### [GUI Tooltips Quick Reference](api/TOOLTIPS_QUICK_REFERENCE.md)\n**GUI Documentation** | **174 lines**\n\nComplete reference for all GUI tooltips and interface elements.\n\n**Contents**:\n- Preset configurations\n- Brain naming conventions\n- Custom architecture fields\n- Resource settings\n- Training controls\n\n**Use this when**: Using the GUI or debugging UI issues\n\n---\n\n## \u2705 Verification checklist (maintainers)\n\nUse this quick checklist when adding or updating docs in this folder:\n- Each file begins with: Title, and \u201cLast Updated: <Month DD, YYYY>\u201d, \u201cPurpose\u201d, optional \u201cStatus\u201d\n- Links within `docs/guide/` use relative paths without `docs/` prefix\n- Links to parent docs (outside guide) use `../` correctly\n- Each core doc ends with a \u201cBack to Guide Index\u201d link\n- Dates reflect the latest meaningful change\n- Terminology: use consistent emoji status markers (\u2705 \u26a0\ufe0f \u274c \u2753) and section headers\n- If new API docs are added, update `docs/guide/api/README.md`\n\n### Link and anchor conventions (contributors)\n- Use kebab-case fragment IDs for custom anchors: `### Example Section {#example-section}`\n- Prefer relative links within this folder (no leading `docs/`), e.g., `[Core Training](features/CORE_TRAINING.md)`\n- When linking to feature docs from feature docs, use `./Other_Feature.md` for siblings and `../INDEX.MD` for guide root\n- End each feature doc with: \u201cBack to Feature Index\u201d and \u201cBack to Guide Index\u201d links\n- Keep file names as-is; normalize link text and ensure accurate casing in paths on case-sensitive systems\n\n## \ud83e\uddea Testing & Diagnostics\n\n### Diagnostics\nSee `scripts/README.md` for available diagnostic scripts and how to run them.\n\n---\n\n## \ud83d\udcca Project Health Metrics\n\n### Implementation Status\n- \u2705 **Fully Implemented**: 58% of features\n- \u26a0\ufe0f **Partially Implemented**: 23% of features\n- \u274c **Not Implemented**: 19% of features\n\n### Test Coverage\n- \u2705 **System Tests Passing**: 77.6% (52/67 tests)\n- \u26a0\ufe0f **Unit Tests**: Minimal coverage (needs expansion)\n- \u274c **Integration Tests**: Not implemented\n\n### Bug Tracking\nBugs are tracked in GitHub Issues:\n- **Open Issues**: [github.com/Wulfic/AI-OS/issues](https://github.com/Wulfic/AI-OS/issues)\n\n### Documentation Quality\n- \u2705 **Feature Documentation**: 100+ features documented\n- \u2705 **Accuracy**: Verified against source code\n- \u2705 **Bug Tracking**: Comprehensive via GitHub Issues\n- \u2705 **Testing**: 67 automated tests\n\n---\n\n## \ud83c\udfaf Common Workflows\n\n### Testing Changes\n1. Run diagnostics: `python scripts/comprehensive_diagnostics.py`\n2. Review failures in `artifacts/diagnostics/diagnostics_latest.txt`\n3. Fix issues identified\n4. Re-run tests to verify\n5. Update documentation if behavior changed\n\n### Reporting Bugs\n1. Check if bug already tracked in GitHub Issues\n2. If new, add to appropriate severity section\n3. Include: Description, Impact, Reproduction, Workaround, Fix Required\n4. Update bug statistics table\n5. Add to action plan if high priority\n\n### Verifying Features\n1. Find feature in [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)\n2. Check implementation status and file location\n3. Look for related tests in diagnostic script\n4. Test manually if automated tests don't exist\n5. Update status (\u2705/\u26a0\ufe0f/\u274c) based on results\n6. Document findings\n\n### Planning Training\n1. Review available features in [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)\n2. Check compatibility in [Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md)\n3. Check for known issues in GitHub Issues\n4. Refer to [Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md)\n5. Start training with verified feature combinations\n\n---\n\n## \ud83d\udd0d Known Critical Issues\n\n### 1. FlashAttention availability and verification\n**Severity**: \ud83d\udd34 CRITICAL  \n**Details**: See [Advanced Training Features](features/ADVANCED_FEATURES.md) for platform notes\n\n- Documentation previously overstated FlashAttention as \"fully integrated\"\n- Current code path attempts FA2 under a user-controlled flag (`use_flash_attn`) and falls back to PyTorch SDPA if unavailable\n- Availability depends on GPU and whether `flash_attn` is installed\n**Impact**: Environment-dependent behavior; performance claims must be qualified\n\n**Action**: Verify on supported hardware, document behavior and fallback clearly\n\n**Action**: Update documentation to remove false claims\n\n---\n\n### 2. Many Features Unverified\n**Severity**: \ud83d\udfe0 HIGH  \n**Details**: Refer to feature docs and open issues for verification notes\n\n**Unverified Features**:\n- DDP (Multi-GPU) - config exists but not tested\n- DeepSpeed ZeRO - config exists but not tested\n- Chunked training - unclear if implemented\n- Most tokenizers - only GPT-2 verified\n- Feature combinations - most untested\n\n**Impact**: Unknown if features work as claimed\n\n**Action**: Systematic testing of all features (2-3 week effort)\n\n---\n\n### 3. Limited Test Coverage\n**Severity**: \ud83d\udfe0 HIGH  \n**Details**: See repository README and CI for coverage and health\n\n- Only 3 test files found in entire project\n- 67 system-level diagnostic tests (77.6% passing)\n- No unit tests for core functionality\n- No integration tests for feature combinations\n- No performance benchmarks\n\n**Impact**: Can't verify changes don't break things\n\n**Action**: Add comprehensive test suite (3-4 week effort)\n\n---\n\n## \ud83d\udee0\ufe0f Contributing\n\n### Before Making Changes\n1. \u2705 Run comprehensive diagnostics\n2. \u2705 Review related features in Complete Feature Index\n3. \u2705 Check for related bugs in Bug Tracker\n4. \u2705 Verify feature combinations won't break\n\n### After Making Changes\n1. \u2705 Run comprehensive diagnostics again\n2. \u2705 Update feature status if behavior changed\n3. \u2705 Add/update bug tracker entries if issues found\n4. \u2705 Update feature combination matrix if compatibility changed\n5. \u2705 Update this guide if new features added\n\n---\n\n## \ud83d\udcd6 Additional Documentation\n\n### In Other Locations\nThese documents are NOT in the guide folder but may be referenced:\n\n- **Main README**: `../../README.md` (project root)\n- **Quick Start**: `QUICK_START.md` (this folder)\n- **Docs README**: `../README.md`\n- **Tests**: `../../tests/` folder\n\n---\n\n## \ud83c\udd98 Getting Help\n\n### For Users\n- Check [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md) for feature documentation\n- Refer to [GUI Tooltips Quick Reference](api/TOOLTIPS_QUICK_REFERENCE.md) for UI help\n\n### For Developers\n- Use [Advanced Training Features](features/ADVANCED_FEATURES.md) for platform notes\n- Reference [Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md) for API usage\n\n### For Contributors\n- See diagnostics in `scripts/README.md`\n- Review feature status: [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)\n- Verify combinations: [Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md)\n\n---\n\n## \ud83d\udcdd Document Status\n\n| Document | Purpose | Last Updated |\n|----------|---------|--------------|\n| INDEX.MD (this file) | Guide navigation hub | Oct 18, 2025 |\n| features/COMPLETE_FEATURE_INDEX.md | Feature docs index | Oct 18, 2025 |\n| features/FEATURE_COMBINATION_MATRIX.md | Compatibility guide | Oct 18, 2025 |\n| features/ADVANCED_FEATURES.md | Advanced flags and notes | Oct 18, 2025 |\n| api/TRAINING_API_QUICK_REFERENCE.md | CLI quick reference | Oct 18, 2025 |\n| api/TOOLTIPS_QUICK_REFERENCE.md | GUI tooltips reference | Oct 18, 2025 |\n\n---\n\n## \ud83c\udf93 Learning Path\n\n### Beginner: Just Starting with AI-OS\n1. Review [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md) basics\n2. Check [GUI Tooltips Quick Reference](api/TOOLTIPS_QUICK_REFERENCE.md) for UI\n\n### Intermediate: Using AI-OS for Training\n1. Start with [Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md)\n2. Plan configurations using [Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md)\n3. Reference [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md) for advanced features\n\n### Advanced: Contributing to AI-OS\n1. See diagnostics in `scripts/README.md`\n2. Reference feature docs and API as needed\n3. Keep documentation and links accurate\n\n---\n\n## \ud83d\udcde Support\n\nFor questions, issues, or contributions:\n- **Issues**: https://github.com/Wulfic/AI-OS/issues\n- **Features**: Review [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)\n- **Documentation**: Use this Guide Index and feature docs\n\n---\n\n**Last Updated**: January 31, 2025  \n**Next Review**: February 28, 2025  \n**Maintainer**: AI-OS Documentation Team\n", "tags": ["cli", "datasets", "gui", "training"], "headings": [{"line": 0, "text": "AI-OS User Guide - Index"}, {"line": 7, "text": "\ud83d\udcd6 Welcome to the AI-OS Guide"}, {"line": 15, "text": "\ud83d\ude80 Quick Start"}, {"line": 17, "text": "New Users"}, {"line": 22, "text": "Developers"}, {"line": 27, "text": "Contributors"}, {"line": 34, "text": "\ud83d\udcda Core Documentation"}, {"line": 36, "text": "Navigation Hubs"}, {"line": 44, "text": "[Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)"}, {"line": 65, "text": "Issues and Bug Tracking"}, {"line": 71, "text": "[Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md)"}, {"line": 90, "text": "Advanced Features and Notes"}, {"line": 97, "text": "\ud83d\udd0c API References"}, {"line": 99, "text": "[Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md)"}, {"line": 114, "text": "[GUI Tooltips Quick Reference](api/TOOLTIPS_QUICK_REFERENCE.md)"}, {"line": 130, "text": "\u2705 Verification checklist (maintainers)"}, {"line": 141, "text": "Link and anchor conventions (contributors)"}, {"line": 148, "text": "\ud83e\uddea Testing & Diagnostics"}, {"line": 150, "text": "Diagnostics"}, {"line": 155, "text": "\ud83d\udcca Project Health Metrics"}, {"line": 157, "text": "Implementation Status"}, {"line": 162, "text": "Test Coverage"}, {"line": 167, "text": "Bug Tracking"}, {"line": 171, "text": "Documentation Quality"}, {"line": 179, "text": "\ud83c\udfaf Common Workflows"}, {"line": 181, "text": "Testing Changes"}, {"line": 188, "text": "Reporting Bugs"}, {"line": 195, "text": "Verifying Features"}, {"line": 203, "text": "Planning Training"}, {"line": 212, "text": "\ud83d\udd0d Known Critical Issues"}, {"line": 214, "text": "1. FlashAttention availability and verification"}, {"line": 229, "text": "2. Many Features Unverified"}, {"line": 246, "text": "3. Limited Test Coverage"}, {"line": 262, "text": "\ud83d\udee0\ufe0f Contributing"}, {"line": 264, "text": "Before Making Changes"}, {"line": 270, "text": "After Making Changes"}, {"line": 279, "text": "\ud83d\udcd6 Additional Documentation"}, {"line": 281, "text": "In Other Locations"}, {"line": 291, "text": "\ud83c\udd98 Getting Help"}, {"line": 293, "text": "For Users"}, {"line": 297, "text": "For Developers"}, {"line": 301, "text": "For Contributors"}, {"line": 308, "text": "\ud83d\udcdd Document Status"}, {"line": 321, "text": "\ud83c\udf93 Learning Path"}, {"line": 323, "text": "Beginner: Just Starting with AI-OS"}, {"line": 327, "text": "Intermediate: Using AI-OS for Training"}, {"line": 332, "text": "Advanced: Contributing to AI-OS"}, {"line": 339, "text": "\ud83d\udcde Support"}]}, {"path": "guide/QUICK_START.md", "content": "# Quick Start\n\nThis single page gets you from zero to the GUI.\n\n## Windows (PowerShell)\n\n```powershell\n# From repo root\nSet-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass\n./installers/scripts/install_aios_on_windows.ps1 -Action install -Yes\naios gui\n```\n\nDeveloper setup:\n\n```powershell\npython -m venv .venv\n. .\\.venv\\Scripts\\Activate.ps1\npip install -e .\naios gui\n```\n\n## Ubuntu\n\n```bash\nchmod a+x ./installers/scripts/install_aios_on_ubuntu.sh\n./installers/scripts/install_aios_on_ubuntu.sh install --yes\naios gui\n```\n\n## CLI training (example)\n\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --steps 1000\n```\n\nTrouble? See docs/README.md for guides and maintenance notes.\n\n### Multi-GPU quick facts\n\n- Set preferred inference GPUs from the **Resources** panel. Linux builds honour multi-GPU selections for evaluation fan-out; Windows keeps chat/eval on the first GPU and shows a warning badge automatically.\n- You can always verify the active selection from the evaluation log lines that start with `[eval] Device selection:`.\n", "tags": ["cli", "datasets", "evaluation", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Quick Start"}, {"line": 4, "text": "Windows (PowerShell)"}, {"line": 7, "text": "From repo root"}, {"line": 22, "text": "Ubuntu"}, {"line": 30, "text": "CLI training (example)"}, {"line": 41, "text": "Multi-GPU quick facts"}]}, {"path": "guide/README.md", "content": "# AI-OS User Guide\n\n**Last Updated**: December 12, 2025  \n**Purpose**: Overview page for the AI-OS User Guide section\n\n---\n\n## Welcome to the AI-OS User Guide\n\nThis section contains comprehensive documentation for using AI-OS, including feature documentation, API references, and tutorials.\n\n## Quick Navigation\n\n### Getting Started\n- [Quick Start Guide](QUICK_START.md) - Get up and running quickly\n- [Guide Index](INDEX.MD) - Complete navigation hub for all documentation\n\n### Features\n- [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md) - Every feature documented\n- [Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md) - What works with what\n- [Core Training](features/CORE_TRAINING.md) - Training fundamentals\n- [Advanced Features](features/ADVANCED_FEATURES.md) - Power user options\n\n### API Reference\n- [Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md) - CLI and API reference\n- [GUI Tooltips Reference](api/TOOLTIPS_QUICK_REFERENCE.md) - GUI element documentation\n\n## Documentation Structure\n\n```\ndocs/guide/\n\u251c\u2500\u2500 INDEX.MD               # Main navigation hub\n\u251c\u2500\u2500 QUICK_START.md         # Getting started guide\n\u251c\u2500\u2500 README.md              # This file\n\u251c\u2500\u2500 api/                   # API documentation\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 TRAINING_API_QUICK_REFERENCE.md\n\u2502   \u2514\u2500\u2500 TOOLTIPS_QUICK_REFERENCE.md\n\u2514\u2500\u2500 features/              # Feature documentation\n    \u251c\u2500\u2500 COMPLETE_FEATURE_INDEX.md\n    \u251c\u2500\u2500 FEATURE_COMBINATION_MATRIX.md\n    \u251c\u2500\u2500 CORE_TRAINING.md\n    \u251c\u2500\u2500 ADVANCED_FEATURES.md\n    \u2514\u2500\u2500 ... (20+ feature docs)\n```\n\n## Need Help?\n\n- **Issues**: [GitHub Issues](https://github.com/Wulfic/AI-OS/issues)\n- **Discussions**: [GitHub Discussions](https://github.com/Wulfic/AI-OS/discussions)\n\n---\n\n[Back to Home](../index.md) | [Guide Index](INDEX.MD)\n", "tags": ["cli", "gui", "training"], "headings": [{"line": 0, "text": "AI-OS User Guide"}, {"line": 7, "text": "Welcome to the AI-OS User Guide"}, {"line": 11, "text": "Quick Navigation"}, {"line": 13, "text": "Getting Started"}, {"line": 17, "text": "Features"}, {"line": 23, "text": "API Reference"}, {"line": 27, "text": "Documentation Structure"}, {"line": 46, "text": "Need Help?"}]}, {"path": "guide/api/README.md", "content": "# API References - Guide\n**Last Updated**: December 12, 2025  \n**Purpose**: Index of API reference documents within `docs/guide/api/`\n\n---\n\n## Available References\n\n- [TRAINING_API_QUICK_REFERENCE.md](TRAINING_API_QUICK_REFERENCE.md)\n  - CLI training usage, parameters, examples, and migration notes.\n\n- [TOOLTIPS_QUICK_REFERENCE.md](TOOLTIPS_QUICK_REFERENCE.md)\n  - GUI tooltips and control descriptions for the training interface.\n\n---\n\nBack to [Guide Index](../INDEX.MD)\n", "tags": ["cli", "gui", "training"], "headings": [{"line": 0, "text": "API References - Guide"}, {"line": 6, "text": "Available References"}]}, {"path": "guide/api/TOOLTIPS_QUICK_REFERENCE.md", "content": "# Quick Reference: All Tooltip Texts\n\n## Preset Section\n\n### Preset Header\n```\nQuick architecture presets with pre-configured parameters.\nSelect Custom for full control over all parameters.\n```\n\n### 1M Preset\n```\n~1M parameters: Tiny model (hidden=256, 2+2 layers)\nFast training, minimal VRAM (~0.5 GB)\n```\n\n### 5M Preset\n```\n~5M parameters: Small model (hidden=512, 2+2 layers)\nGood for testing and quick experiments (~1.5 GB)\n```\n\n### 10M Preset\n```\n~10M parameters: Medium model (hidden=768, 2+2 layers)\nBalanced size/performance (~2.5 GB)\n```\n\n### 20M Preset\n```\n~20M parameters: Large model (hidden=1024, 2+2 layers)\nGood quality, moderate VRAM (~4 GB)\n```\n\n### 50M Preset\n```\n~50M parameters: Very large (hidden=1536, 2+2 layers)\nHigh quality, needs more VRAM (~7 GB)\n```\n\n### Custom Preset\n```\nCustom architecture: Configure all parameters manually.\nReveals advanced options for hidden size, layers, heads, etc.\n```\n\n## Brain Name Field\n```\nUnique name for this brain/model.\nWill be saved to: artifacts/brains/actv1/{name}/\nUse descriptive names like: large_context_v1, fast_inference, etc.\n```\n\n## Custom Architecture Fields\n\n### Hidden Size\n```\nModel width / embedding dimension.\nLarger = more expressive but more VRAM.\nMust be divisible by num_heads.\nExamples: 256, 512, 768, 1024, 1536, 2048\n```\n\n### H Layers\n```\nNumber of Hierarchical reasoning layers.\nHigher-level abstract processing.\nMore layers = deeper reasoning but slower.\nTypical: 2-8 layers\n```\n\n### L Layers\n```\nNumber of Local processing layers.\nLower-level detail processing.\nMore layers = better detail but slower.\nTypical: 2-8 layers\n```\n\n### Num Heads\n```\nNumber of attention heads per layer.\nMore heads = more parallel attention patterns.\nMust evenly divide hidden_size.\nExamples: 4, 8, 12, 16, 24, 32\n```\n\n### Expansion\n```\nFeed-forward network expansion factor.\nFFN size = hidden_size \u00d7 expansion.\nHigher = more capacity but more VRAM.\nTypical: 2.0-4.0\n```\n\n### H Cycles\n```\nNumber of processing cycles per H layer.\nMore cycles = more refinement per layer.\nTypical: 1-3 cycles\n```\n\n### L Cycles\n```\nNumber of processing cycles per L layer.\nMore cycles = more refinement per layer.\nTypical: 1-3 cycles\n```\n\n### Position Encoding\n```\nPosition encoding method:\n\n\u2022 rope (Rotary): Best for long contexts,\n  relative positions, no learned params.\n  RECOMMENDED for most use cases.\n\n\u2022 learned: Absolute positions,\n  trained embeddings, fixed max length.\n```\n\n## Visual Map\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        Create New HRM Student                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                      \u2502\n\u2502  Choose architecture preset: \u2190 \"Quick presets...\"   \u2502\n\u2502  \u25cb 1M        \u2190 \"~1M params: Tiny model...\"          \u2502\n\u2502  \u25cb 5M        \u2190 \"~5M params: Small model...\"         \u2502\n\u2502  \u25cb 10M       \u2190 \"~10M params: Medium model...\"       \u2502\n\u2502  \u25cb 20M       \u2190 \"~20M params: Large model...\"        \u2502\n\u2502  \u25cb 50M       \u2190 \"~50M params: Very large...\"         \u2502\n\u2502  \u25cf Custom    \u2190 \"Custom architecture: Configure...\"   \u2502\n\u2502                                                      \u2502\n\u2502  Brain name: [new_brain] \u2190 \"Unique name...\"         \u2502\n\u2502                                                      \u2502\n\u2502  \u250c\u2500 Custom Architecture \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  Hidden size:  [512] \u2190 \"Model width...\"       \u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  H layers:     [2]   \u2190 \"Hierarchical...\"      \u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  L layers:     [2]   \u2190 \"Local processing...\"  \u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  Num heads:    [8]   \u2190 \"Attention heads...\"   \u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  Expansion:    [2.0] \u2190 \"FFN expansion...\"     \u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  H cycles:     [2]   \u2190 \"Processing cycles...\" \u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  L cycles:     [2]   \u2190 \"Processing cycles...\" \u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  Pos encoding: [rope\u25bc] \u2190 \"rope/learned/sincos\"\u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  Note: DeepSpeed ZeRO can be selected in      \u2502  \u2502\n\u2502  \u2502        the main training panel                 \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                      \u2502\n\u2502  [Create]  [Cancel]                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Hover Behavior\n- Tooltips appear after **0.5 second** hover delay\n- Tooltips stay visible while hovering\n- Tooltips disappear when mouse moves away\n- Multi-line tooltips are properly formatted\n- All interactive elements have tooltips\n", "tags": ["gui", "hrm", "training"], "headings": [{"line": 0, "text": "Quick Reference: All Tooltip Texts"}, {"line": 2, "text": "Preset Section"}, {"line": 4, "text": "Preset Header"}, {"line": 10, "text": "1M Preset"}, {"line": 16, "text": "5M Preset"}, {"line": 22, "text": "10M Preset"}, {"line": 28, "text": "20M Preset"}, {"line": 34, "text": "50M Preset"}, {"line": 40, "text": "Custom Preset"}, {"line": 46, "text": "Brain Name Field"}, {"line": 53, "text": "Custom Architecture Fields"}, {"line": 55, "text": "Hidden Size"}, {"line": 63, "text": "H Layers"}, {"line": 71, "text": "L Layers"}, {"line": 79, "text": "Num Heads"}, {"line": 87, "text": "Expansion"}, {"line": 95, "text": "H Cycles"}, {"line": 102, "text": "L Cycles"}, {"line": 109, "text": "Position Encoding"}, {"line": 121, "text": "Visual Map"}, {"line": 164, "text": "Hover Behavior"}]}, {"path": "guide/api/TRAINING_API_QUICK_REFERENCE.md", "content": "# Quick Reference: New Training API\n\n## For CLI Users\n\n### \u2705 What Still Works (No Changes)\n\nAll standard training parameters work exactly as before:\n\n```bash\n# Basic training\naios hrm-hf train-actv1 \\\n    --model artifacts/hf_implant/base_model \\\n    --dataset-file training_data/my_data.txt \\\n    --max-seq-len 128 \\\n    --batch-size 8 \\\n    --steps 200\n\n# With optimizations\naios hrm-hf train-actv1 \\\n    --model base_model \\\n    --dataset-file data.txt \\\n    --optimize \\\n    --gradient-checkpointing \\\n    --amp \\\n    --zero-stage zero2\n\n# Multi-GPU DDP\naios hrm-hf train-actv1 \\\n    --model base_model \\\n    --dataset-file data.txt \\\n    --ddp \\\n    --cuda-ids 0,1 \\\n    --world-size 2\n```\n\n\n---\n\n## For Python API Users\n\n```python\nfrom aios.core.hrm_training import TrainingConfig\nfrom aios.cli.hrm_hf.train_actv1_impl import train_actv1_impl\n\n# Create config object\nconfig = TrainingConfig(\n    model=\"artifacts/hf_implant/base_model\",\n    dataset_file=\"training_data/my_data.txt\",\n    max_seq_len=128,\n    batch_size=8,\n    steps=200,\n    lr=2e-4,\n    device=\"auto\",\n    # ... other parameters ...\n)\n\n# Validate (optional but recommended)\nconfig.validate()\n\n# Train\ntrain_actv1_impl(config=config)\n```\n\n**Error you'll see**:\n```\nTypeError: train_actv1_impl() missing 1 required positional argument: 'config'\n```\n\n---\n\n## Common Migration Examples\n\n### Example 1: Basic Training Script\n\n**Before**:\n```python\nfrom aios.cli.hrm_hf.train_actv1_impl import train_actv1_impl\n\ntrain_actv1_impl(\n    model=\"base_model\",\n    dataset_file=\"data.txt\",\n    max_seq_len=256,\n    batch_size=16,\n    steps=500,\n)\n```\n\n**After**:\n```python\nfrom aios.core.hrm_training import TrainingConfig\nfrom aios.cli.hrm_hf.train_actv1_impl import train_actv1_impl\n\nconfig = TrainingConfig(\n    model=\"base_model\",\n    dataset_file=\"data.txt\",\n    max_seq_len=256,\n    batch_size=16,\n    steps=500,\n)\ntrain_actv1_impl(config=config)\n```\n\n### Example 2: Programmatic Configuration\n\n**Before**:\n```python\n# Build kwargs dynamically\nkwargs = {\n    \"model\": \"base_model\",\n    \"dataset_file\": \"data.txt\",\n}\n\nif use_gpu:\n    kwargs[\"device\"] = \"cuda\"\n    kwargs[\"ddp\"] = True\n\ntrain_actv1_impl(**kwargs)\n```\n\n**After**:\n```python\n# Build config dynamically (better type safety!)\nconfig = TrainingConfig(\n    model=\"base_model\",\n    dataset_file=\"data.txt\",\n)\n\nif use_gpu:\n    config.device = \"cuda\"\n    config.ddp = True\n\ntrain_actv1_impl(config=config)\n```\n\n### Example 3: Config Persistence\n\n**New feature**: Save and load configurations!\n\n```python\nfrom aios.core.hrm_training import TrainingConfig\n\n# Create config\nconfig = TrainingConfig(\n    model=\"base_model\",\n    dataset_file=\"data.txt\",\n    max_seq_len=1024,\n    batch_size=4,\n    steps=1000,\n    optimize=True,\n    zero_stage=\"zero2\",\n)\n\n# Save to JSON\nimport json\nwith open(\"my_training_config.json\", \"w\") as f:\n    json.dump(config.to_dict(), f, indent=2)\n\n# Load later\nwith open(\"my_training_config.json\", \"r\") as f:\n    config_dict = json.load(f)\n\nconfig = TrainingConfig.from_dict(config_dict)\ntrain_actv1_impl(config=config)\n```\n\n### Example 4: Config Validation\n\n```python\nfrom aios.core.hrm_training import TrainingConfig\n\n# Create invalid config\nconfig = TrainingConfig(\n    model=\"base_model\",\n    dataset_file=None,  # \u274c Required!\n    max_seq_len=128,\n)\n\ntry:\n    config.validate()\nexcept ValueError as e:\n    print(f\"Config error: {e}\")\n    # Output: \"Config error: dataset_file is required for training\"\n```\n\n---\n\n## TrainingConfig Parameters\n\nAll parameters available in TrainingConfig:\n\n### Core Training:\n- `model` (str) - HF model name or path\n- `dataset_file` (str) - Training data file\n- `max_seq_len` (int) - Sequence length, default: 128\n- `batch_size` (int) - Batch size, default: 8\n- `steps` (int) - Training steps, default: 200\n- `lr` (float) - Learning rate, default: 2e-4\n- `device` (str) - Device: auto|cpu|cuda|xpu|mps|dml, default: \"auto\"\n- `halt_max_steps` (int) - Max ACT segments, default: 2\n- `save_dir` (str) - Output directory, default: \"training_data/actv1\"\n\n### Data Processing:\n- `ascii_only` (bool) - Filter to ASCII-only lines, default: False\n- `eval_file` (str|None) - Held-out eval file, default: None\n- `eval_batches` (int) - Eval batches, default: 10\n- `sys_mem_cap_pct` (int|None) - Memory cap %, default: None\n\n### Training Control:\n- `stop_file` (str|None) - Stop signal file, default: None\n- `log_file` (str|None) - Metrics log file, default: None\n- `student_init` (str|None) - Resume from checkpoint, default: None\n- `iterate` (bool) - Loop training indefinitely, default: False\n\n### Output Bundle:\n- `brain_name` (str|None) - Brain bundle name, default: None\n- `bundle_dir` (str) - Bundle directory, default: \"artifacts/brains/actv1\"\n\n### Model Architecture:\n- `h_layers` (int) - High-level layers, default: 2\n- `l_layers` (int) - Low-level layers, default: 2\n- `hidden_size` (int) - Hidden dimension, default: 512\n- `expansion` (float) - FFN expansion, default: 2.0\n- `num_heads` (int) - Attention heads, default: 8\n- `h_cycles` (int) - High-level cycles, default: 2\n- `l_cycles` (int) - Low-level cycles, default: 2\n- `pos_encodings` (str) - Position encoding: rope|learned, default: \"rope\"\n\n### Optimization:\n- `optimize` (bool) - Auto-optimize for VRAM, default: False\n- `gradient_checkpointing` (bool) - Enable grad checkpointing, default: True\n- `use_amp` (bool) - Use mixed precision, default: True\n- `use_cpu_offload` (bool) - Offload to CPU, default: False\n- `zero_stage` (str) - DeepSpeed ZeRO: none|zero1|zero2|zero3, default: \"none\"\n\n### Multi-GPU:\n- `cuda_ids` (str|None) - CUDA device IDs (e.g., \"0,1\"), default: None\n- `ddp` (bool) - Enable DDP, default: False\n- `world_size` (int|None) - Number of GPUs, default: None\n- `strict` (bool) - No device fallbacks, default: False\n\n### Advanced:\n- `kl` (float) - KL divergence scaling factor, default: 0.0\n- `kl_temp` (float) - KL temperature annealing, default: 1.0\n\n---\n\n## Need Help?\n\n### Check Configuration:\n```python\nfrom aios.core.hrm_training import TrainingConfig\n\nconfig = TrainingConfig(model=\"test\", dataset_file=\"data.txt\")\nprint(config)  # Shows formatted summary\n```\n\n### Validate Before Training:\n```python\ntry:\n    config.validate()\n    print(\"\u2713 Config is valid!\")\nexcept ValueError as e:\n    print(f\"\u2717 Config error: {e}\")\n```\n\n### Get CLI Args:\n```python\ncli_args = config.to_cli_args()\nprint(\" \".join(cli_args))  # See what CLI command this config represents\n```\n\n---\n\n**Questions?** Check the full docs:\n- [Advanced Features](../features/ADVANCED_FEATURES.md) - Training configuration options\n- [Feature Combination Matrix](../features/FEATURE_COMBINATION_MATRIX.md) - What works together\n- `src/aios/core/hrm_training/training_config.py` - Source code\n", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Quick Reference: New Training API"}, {"line": 2, "text": "For CLI Users"}, {"line": 4, "text": "\u2705 What Still Works (No Changes)"}, {"line": 9, "text": "Basic training"}, {"line": 17, "text": "With optimizations"}, {"line": 26, "text": "Multi-GPU DDP"}, {"line": 38, "text": "For Python API Users"}, {"line": 44, "text": "Create config object"}, {"line": 53, "text": "... other parameters ..."}, {"line": 56, "text": "Validate (optional but recommended)"}, {"line": 59, "text": "Train"}, {"line": 70, "text": "Common Migration Examples"}, {"line": 72, "text": "Example 1: Basic Training Script"}, {"line": 102, "text": "Example 2: Programmatic Configuration"}, {"line": 106, "text": "Build kwargs dynamically"}, {"line": 121, "text": "Build config dynamically (better type safety!)"}, {"line": 134, "text": "Example 3: Config Persistence"}, {"line": 141, "text": "Create config"}, {"line": 152, "text": "Save to JSON"}, {"line": 157, "text": "Load later"}, {"line": 165, "text": "Example 4: Config Validation"}, {"line": 170, "text": "Create invalid config"}, {"line": 181, "text": "Output: \"Config error: dataset_file is required for training\""}, {"line": 186, "text": "TrainingConfig Parameters"}, {"line": 190, "text": "Core Training:"}, {"line": 201, "text": "Data Processing:"}, {"line": 207, "text": "Training Control:"}, {"line": 213, "text": "Output Bundle:"}, {"line": 217, "text": "Model Architecture:"}, {"line": 227, "text": "Optimization:"}, {"line": 234, "text": "Multi-GPU:"}, {"line": 240, "text": "Advanced:"}, {"line": 246, "text": "Need Help?"}, {"line": 248, "text": "Check Configuration:"}, {"line": 256, "text": "Validate Before Training:"}, {"line": 265, "text": "Get CLI Args:"}]}, {"path": "guide/features/ADVANCED_FEATURES.md", "content": "# Advanced Training Features \u2014 AI-OS\nLast Updated: December 12, 2025\nPurpose: Practical guide to advanced HRM-HF training flags, best\u2011known\u2011good combinations, platform notes, and troubleshooting.\nStatus: Implemented\n\n## Overview\nThis guide consolidates all advanced knobs for the HRM-HF trainer (`aios hrm-hf train-actv1`) into one place. It groups options by function (attention/positional, memory & performance, precision/quantization, dataset streaming & chunking, distributed, MoE, PEFT, hot\u2011reload inference) and calls out compatibility constraints on Windows vs Linux.\n\nSource of truth (CLI): `src/aios/cli/hrm_hf_cli.py` \u2192 command `train-actv1`\nConfiguration model: `src/aios/core/hrm_training/training_config.py`\n\nSee also:\n- Memory & VRAM: [MEMORY_OPTIMIZATION.md](./MEMORY_OPTIMIZATION.md)\n- FlashAttention: [FLASH_ATTENTION.md](./FLASH_ATTENTION.md) and [FLASH_ATTENTION_VS_CHUNKING.md](./FLASH_ATTENTION_VS_CHUNKING.md)\n- Multi\u2011GPU & streaming: [MULTI_GPU_DISTRIBUTED.md](./MULTI_GPU_DISTRIBUTED.md), [PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md](./PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md)\n- MoE: [DYNAMIC_SUBBRAINS_MOE.md](./DYNAMIC_SUBBRAINS_MOE.md)\n- PEFT/LoRA: [LORA_PEFT.md](./LORA_PEFT.md)\n- Core training entry: [CORE_TRAINING.md](./CORE_TRAINING.md)\n\n## Prerequisites\n- Windows PowerShell (pwsh), repo venv activated\n- GPU and drivers installed. CUDA recommended on NVIDIA. DirectML supported for inference; training support varies.\n- For 8\u2011bit optimizer and INT8/INT4 quantization: `bitsandbytes` with CUDA GPU. Windows support is limited\u2014prefer Linux for 8/4\u2011bit.\n- For FlashAttention 2: Ampere+ GPU; dedicated FA2 build required on most setups. Not typically available on Windows\u2014falls back to SDPA.\n- For DeepSpeed ZeRO: DeepSpeed installation (Linux recommended). ZeRO often not supported or unstable on Windows.\n\n## Commands (CLI syntax)\n\n### a) Direct CLI\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 200 --batch-size 8\n```\n\n### b) Module invocation (exact venv)\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 200 --batch-size 8\n```\n\n## Key option groups (with notes)\n\n### Attention & positional\n- `--use-flash-attn/--no-flash-attn`: Enable FlashAttention 2. Requires Ampere+ and proper install. On Windows, usually unavailable; will fall back to SDPA.\n- `--window-size <int|None>`: Sliding\u2011window attention. Use 256\u2013512 for extreme contexts (50\u2013100k tokens) to reduce memory. Works with or without FlashAttention.\n- `--pos-encodings rope|learned`: Choose positional encoding. Default: rope.\n\n### Memory & performance\n- `--gradient-checkpointing/--no-gradient-checkpointing`: \u2193VRAM by ~30\u201350% at ~20% speed cost. Default: enabled.\n- `--amp/--no-amp`: Mixed precision activations (FP16/BF16). Big savings with minimal quality impact. Default: enabled.\n- `--cpu-offload/--no-cpu-offload`: Offload carry states to CPU between chunks for ultra\u2011long contexts (>500k). Slower; requires sufficient system RAM.\n- `--use-8bit-optimizer`: Use bitsandbytes 8\u2011bit optimizer (~75% optimizer memory reduction). Requires CUDA + bitsandbytes.\n- `--dataset-chunk-size <int>`: Samples per training cycle in iterate mode. Smaller uses less memory, larger is faster.\n\n### Precision & quantization\n- `--model-dtype fp32|fp16|bf16`: Weight precision when loading full\u2011precision models. Separate from AMP.\n- `--load-in-8bit`: INT8 weight loading (75% memory reduction). Requires bitsandbytes + CUDA.\n- `--load-in-4bit`: INT4 (QLoRA\u2011style) loading (\u224887.5% memory reduction). Strongly pair with PEFT.\nNotes:\n- When `--load-in-8bit` or `--load-in-4bit` is set, the base weights load quantized; AMP still controls activation precision. On Windows, bitsandbytes support is limited\u2014prefer Linux.\n\n### Dataset streaming & chunked training\n- `--use-chunked-training`: Split sequences into chunks to fit memory for long contexts.\n- `--chunk-size <tokens>`: 1024\u20134096 typical. Smaller = less VRAM, slower.\n- `--linear-dataset/--no-linear-dataset`: Linear order (default) enables progress tracking and resume.\n- `--dataset-start-offset <int>`: Resume index for linear mode.\n- `--iterate`: Repeat generate\u2192train cycles until stopped.\n\n### Distributed & multi\u2011GPU\n- `--ddp`: Enable torch.distributed (CUDA only). Best on Linux.\n- `--world-size <int>`: Number of processes/GPUs for DDP.\n- `--cuda-ids \"0,1\"`: Pin devices explicitly.\n- `--parallel-independent`: Windows\u2011friendly multi\u2011GPU alternative. Trains separate data blocks on different GPUs sequentially, then merges checkpoints. Bypasses DDP.\n- `--zero-stage none|zero1|zero2|zero3`: DeepSpeed ZeRO. Requires DeepSpeed; Linux recommended.\n- `--strict`: Disallow device fallbacks; fail fast on mismatches.\n\n### MoE (Mixture of Experts)\n- `--use-moe/--no-moe` (default: enabled)\n- `--num-experts <int>`: Total experts (capacity vs VRAM trade\u2011off).\n- `--num-experts-per-tok <int>`: Top\u2011k experts per token; lower = faster/less memory.\n- `--moe-capacity-factor <float>`: Load\u2011balancing headroom.\n- `--auto-adjust-lr/--no-auto-adjust-lr`: Auto reduce LR for MoE stability.\nTips: Start with 8 experts, top\u2011k=2, capacity 1.25; raise gradually.\n\n### PEFT (parameter\u2011efficient fine\u2011tuning)\n- `--use-peft/--no-peft` and `--peft-method lora|adalora|ia3|loha|lokr`\n- `--lora-r`, `--lora-alpha`, `--lora-dropout`, `--lora-target-modules \"q_proj,v_proj\"`\nBest practice: With `--load-in-4bit`, enable PEFT and tune adapters only to keep memory low.\n\n### Inference hot\u2011reload during training\n- `--inference-device cuda:N`: Use a dedicated GPU for inference while training on another.\n- `--hot-reload-steps <int>`: Frequency to reload inference model from checkpoints.\n\n### Auto optimization\n- `--optimize`: Auto\u2011find a stable combination for context length (up to ~100k) and batch size based on VRAM. May override `--max-seq-len` and `--batch-size`.\n\n## Try it: minimal safe examples\n\n### 1) Quick dry\u2011run (single GPU)\nRuns 1 step with tiny batch to validate pipeline and logging.\n```powershell\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t--steps 1 --batch-size 2 `\n\t--halt-max-steps 1 `\n\t--eval-batches 1 `\n\t--log-file artifacts/brains/actv1/metrics.jsonl\n```\nVS Code Task: Run \u201cRun brief HRM CLI dry-run\u201d.\n\nExpected outputs:\n- Metrics log appended at `artifacts/brains/actv1/metrics.jsonl`\n- Checkpoints under `training_data/actv1` (default `--save-dir`)\n\n### 2) Windows multi\u2011GPU without DDP\nUse parallel\u2011independent with chunked training to reduce VRAM pressure.\n```powershell\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t--parallel-independent `\n\t--use-chunked-training --chunk-size 2048 `\n\t--amp --gradient-checkpointing `\n\t--steps 200 --batch-size 4\n```\n\n### 3) QLoRA\u2011style PEFT on a single GPU (very low VRAM)\n```powershell\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t--load-in-4bit --use-peft --peft-method lora `\n\t--lora-r 16 --lora-alpha 32 --lora-dropout 0.05 `\n\t--lora-target-modules \"q_proj,v_proj\" `\n\t--amp --gradient-checkpointing `\n\t--steps 200 --batch-size 8\n```\nNote: Requires bitsandbytes + CUDA; prefer Linux.\n\n### 4) MoE enabled with conservative routing\n```powershell\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t--use-moe --num-experts 8 --num-experts-per-tok 2 `\n\t--moe-capacity-factor 1.25 --auto-adjust-lr `\n\t--amp --gradient-checkpointing `\n\t--steps 200 --batch-size 8\n```\n\n## Compatibility, constraints, and tips\n\n- FlashAttention 2:\n\t- Works best on Linux with an Ampere+ GPU and proper FA2 install.\n\t- On Windows, expect fallback to SDPA; performance gain may be limited.\n- DeepSpeed ZeRO:\n\t- Requires DeepSpeed; Linux recommended. ZeRO not typically supported on Windows.\n- DDP:\n\t- Best on Linux. On Windows, prefer `--parallel-independent`.\n- Quantization:\n\t- `--load-in-4bit` pairs best with `--use-peft`. Keep base LM frozen; train adapters.\n\t- If bitsandbytes isn\u2019t available, omit `--load-in-8bit/--load-in-4bit` and consider `--use-8bit-optimizer` only.\n- Heads and shapes:\n\t- Ensure `--num-heads` divides `--hidden-size`. The validator will error otherwise.\n- Chunked training:\n\t- For very long contexts, combine `--use-chunked-training`, `--chunk-size 1024\u20132048`, `--gradient-checkpointing`, and optionally `--cpu-offload`.\n- Logging & resume:\n\t- Use `--log-file` for JSONL metrics; pair with `--linear-dataset` and `--dataset-start-offset` to resume deterministically.\n\n## Troubleshooting\n\n- \u201cConfiguration error \u2026\u201d on launch:\n\t- Check incompatible shapes (e.g., `num_heads` must divide `hidden_size`).\n\t- Remove `--use-flash-attn` if FA2 isn\u2019t installed; it will fall back, but explicit removal helps isolate issues.\n\t- If using ZeRO on Windows, remove `--zero-stage`.\n- \u201cbitsandbytes not found\u201d or CUDA errors:\n\t- Remove `--load-in-8bit/--load-in-4bit` and/or `--use-8bit-optimizer`, or switch to Linux with CUDA.\n- DDP hang on Windows:\n\t- Switch to `--parallel-independent`. Verify `--cuda-ids` and drivers.\n- OOM during long\u2011context training:\n\t- Lower `--chunk-size`, enable `--gradient-checkpointing`, ensure `--amp`, and reduce `--batch-size`.\n\n## References\n- CLI entry: `src/aios/cli/hrm_hf_cli.py` (train\u2011actv1)\n- Config: `src/aios/core/hrm_training/training_config.py`\n- Related docs:\n\t- [FLASH_ATTENTION.md](./FLASH_ATTENTION.md)\n\t- [FLASH_ATTENTION_VS_CHUNKING.md](./FLASH_ATTENTION_VS_CHUNKING.md)\n\t- [PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md](./PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md)\n\t- [MULTI_GPU_DISTRIBUTED.md](./MULTI_GPU_DISTRIBUTED.md)\n\t- [LORA_PEFT.md](./LORA_PEFT.md)\n\t- [DYNAMIC_SUBBRAINS_MOE.md](./DYNAMIC_SUBBRAINS_MOE.md)\n\t- [CORE_TRAINING.md](./CORE_TRAINING.md)\n\t- [CLI_COMMANDS.md](./CLI_COMMANDS.md)\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](./COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Advanced Training Features \u2014 AI-OS"}, {"line": 5, "text": "Overview"}, {"line": 19, "text": "Prerequisites"}, {"line": 26, "text": "Commands (CLI syntax)"}, {"line": 28, "text": "a) Direct CLI"}, {"line": 33, "text": "b) Module invocation (exact venv)"}, {"line": 38, "text": "Key option groups (with notes)"}, {"line": 40, "text": "Attention & positional"}, {"line": 45, "text": "Memory & performance"}, {"line": 52, "text": "Precision & quantization"}, {"line": 59, "text": "Dataset streaming & chunked training"}, {"line": 66, "text": "Distributed & multi\u2011GPU"}, {"line": 74, "text": "MoE (Mixture of Experts)"}, {"line": 82, "text": "PEFT (parameter\u2011efficient fine\u2011tuning)"}, {"line": 87, "text": "Inference hot\u2011reload during training"}, {"line": 91, "text": "Auto optimization"}, {"line": 94, "text": "Try it: minimal safe examples"}, {"line": 96, "text": "1) Quick dry\u2011run (single GPU)"}, {"line": 112, "text": "2) Windows multi\u2011GPU without DDP"}, {"line": 123, "text": "3) QLoRA\u2011style PEFT on a single GPU (very low VRAM)"}, {"line": 135, "text": "4) MoE enabled with conservative routing"}, {"line": 145, "text": "Compatibility, constraints, and tips"}, {"line": 164, "text": "Troubleshooting"}, {"line": 177, "text": "References"}]}, {"path": "guide/features/CLI_COMMANDS.md", "content": "# CLI Commands - AI-OS\nGenerated: December 12, 2025\nPurpose: Reference for the `aios` CLI and subcommands\nStatus: Implemented\n\n## Overview\n\n- Main entry point: `aios`\n- File: `src/aios/cli/aios.py`\n\nSub-commands overview:\n- hrm-hf \u2013 HuggingFace-based HRM training (see Core Training)\n- brains \u2013 Brain management\n- gui \u2013 Launch GUI (see GUI Features)\n- status \u2013 System status\n- datasets \u2013 Dataset management (see Datasets)\n- cache \u2013 Cache management\n- goals \u2013 Goals management\n- eval \u2013 Evaluation utilities (see Advanced Features \u2192 Evaluation)\n- artifacts \u2013 Artifacts management\n- cleanup \u2013 Cleanup utilities\n- crawl \u2013 Web crawling (see Tools & Integrations)\n- optimization \u2013 Optimization utilities (see Memory Optimization)\n- modelcard \u2013 Model card generation\n- agent \u2013 Agent commands\n- budgets \u2013 Budget management (see Advanced Features \u2192 Budgets)\n- core \u2013 Core commands\n- hf-cache \u2013 HuggingFace cache management\n- dml \u2013 DirectML utilities\n\n## HRM-HF Training\n\n- Command: `aios hrm-hf`\n- File: `src/aios/cli/hrm_hf_cli.py`\n- Subcommand: `train-actv1` \u2013 Train HRM models with ACT v1\n- File: `src/aios/cli/hrm_hf/train_actv1.py`\n- Deep dive: See Core Training and Memory Optimization docs\n\nKey parameters (selection):\n- Model: `--model <name_or_path>`\n- Brain naming: `--brain-name`, `--bundle-dir`\n- Training control: `--steps`, `--batch-size`, `--lr`, `--max-seq-len`, `--iterate`, `--stop-file`, `--resume`, `--stop-after-epoch`\n- Architecture: `--h-layers`, `--l-layers`, `--hidden-size`, `--expansion`, `--num-heads`, `--h-cycles`, `--l-cycles`, `--halt-max-steps`, `--window-size`, `--pos-encodings`\n- Memory optimization: `--gradient-checkpointing|--no-gradient-checkpointing`, `--amp|--no-amp`, `--use-8bit-optimizer`, `--use-chunked-training`, `--chunk-size`, `--cpu-offload`\n- Dataset: `--dataset-file`, `--ascii-only`, `--linear-dataset`, `--dataset-start-offset`, `--dataset-chunk-size`\n- Evaluation: `--eval-file`, `--eval-batches`, `--log-file`\n- Multi-GPU: `--ddp`, `--cuda-ids`, `--world-size`, `--parallel-independent`, `--strict`\n- DeepSpeed: `--zero-stage <none|zero1|zero2|zero3>`\n- MoE: `--use-moe`, `--num-experts`, `--num-experts-per-tok`, `--moe-capacity-factor`, `--auto-adjust-lr`\n- PEFT: `--use-peft`, `--peft-method`, `--lora-r`, `--lora-alpha`, `--lora-dropout`, `--lora-target-modules`\n- Precision/Quant: `--model-dtype fp32|fp16|bf16`, `--load-in-8bit`, `--load-in-4bit`\n- Inference hot\u2011reload: `--inference-device`, `--hot-reload-steps`\n\n## Brains Management\n\n- Command: `aios brains`\n- File: `src/aios/cli/brains.py`\n- Subcommands: list, load, info, delete, export, import\n- Related: Core Training \u2192 Brain Bundle System, GUI Features \u2192 Brains Panel\n\n## Datasets Management\n\n- Command: `aios datasets`\n- File: `src/aios/cli/datasets_cli.py`\n- Features: list/download, scan, metadata, verification\n- Related: Datasets doc\n\n## Goals Management\n\n- Command: `aios goals`\n- File: `src/aios/cli/goals_cli.py`\n- Create/list/activate goals, link to experts, goal-driven training.\n- Related: Dynamic Subbrains/MoE and Advanced Features \u2192 Orchestrator\n\n## Cache and HF Cache\n\n- `aios cache` \u2192 Clear/show stats\n- `aios hf-cache` \u2192 Location, move, clear, size reporting\n\n## Evaluation\n\n- Command: `aios eval`\n- File: `src/aios/cli/eval_cli.py`\n- Run evaluations, generate reports, compare models.\n- Related: Advanced Features \u2192 Evaluation\n\n## Crawling\n\n- Command: `aios crawl`\n- File: `src/aios/cli/crawl_cli.py`\n- Web crawling, dataset generation from web.\n- Related: Tools & Integrations\n\n## Optimization\n\n- Command: `aios optimization`\n- File: `src/aios/cli/optimization_cli.py`\n- Memory/VRAM estimation and parameter optimization.\n- Related: Memory Optimization\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "evaluation", "experts", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "CLI Commands - AI-OS"}, {"line": 5, "text": "Overview"}, {"line": 30, "text": "HRM-HF Training"}, {"line": 53, "text": "Brains Management"}, {"line": 60, "text": "Datasets Management"}, {"line": 67, "text": "Goals Management"}, {"line": 74, "text": "Cache and HF Cache"}, {"line": 79, "text": "Evaluation"}, {"line": 86, "text": "Crawling"}, {"line": 93, "text": "Optimization"}]}, {"path": "guide/features/COMPLETE_FEATURE_INDEX.md", "content": "# Complete Feature Index - AI-OS\nGenerated: December 12, 2025\nPurpose: Ultra-lean index linking to feature docs and deep dives\nStatus: Implemented\n\n---\n\n## Quick Links\n\n- Core docs\n    1. [CLI Commands](CLI_COMMANDS.md)\n    2. [Core Training](CORE_TRAINING.md)\n    3. [Memory Optimization](MEMORY_OPTIMIZATION.md)\n    4. [Model Architecture](MODEL_ARCHITECTURE.md)\n    5. [Datasets](DATASETS.md)\n    6. [Tokenizers](TOKENIZERS.md)\n    7. [Dynamic Subbrains / MoE](DYNAMIC_SUBBRAINS_MOE.md)\n    8. [GUI Features](GUI_FEATURES.md)\n    9. [Multi-GPU & Distributed](MULTI_GPU_DISTRIBUTED.md)\n    10. [Tools & Integrations](TOOLS_INTEGRATIONS.md)\n    11. [Advanced Features](ADVANCED_FEATURES.md)\n\n- Deep dives & matrices\n    - [Feature Combination Matrix](FEATURE_COMBINATION_MATRIX.md)\n    - [Flash Attention](FLASH_ATTENTION.md)\n    - [Flash Attention vs Chunking](FLASH_ATTENTION_VS_CHUNKING.md)\n    - [Parallel Training Block Chunk System](PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md)\n    - [Configurable Dataset Chunk Size](CONFIGURABLE_DATASET_CHUNK_SIZE.md)\n    - [LoRA / PEFT](LORA_PEFT.md)\n\n---\n\nBack to Guide Index: [../INDEX.MD](../INDEX.MD)\n\n", "tags": ["cli", "datasets", "gui", "training"], "headings": [{"line": 0, "text": "Complete Feature Index - AI-OS"}, {"line": 7, "text": "Quick Links"}]}, {"path": "guide/features/CONFIGURABLE_DATASET_CHUNK_SIZE.md", "content": "# Configurable Dataset Chunk Size (Sub-topic)\nLast Updated: December 12, 2025\n\nStatus: Implemented and used by CLI/GUI\n\nCanonical feature doc: `PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md`\n\n## Overview\n\nThe dataset chunk size controls how many samples are processed per training cycle when using iterate-style loading. Smaller chunks reduce peak memory usage; larger chunks improve throughput at the cost of higher memory.\n\n## How it works\n\n- CLI flag: --dataset-chunk-size <int> (default: 4000)\n- TrainingConfig field: dataset_chunk_size: int\n- GUI: \u201cChunk size\u201d field under Steps; the \u201cAuto\u201d button aligns steps to the chunk size\n\n## Recommended values\n\n- 8 GB VRAM: 2000\u20133000\n- 12\u201316 GB VRAM: 4000 (default)\n- 24 GB+ VRAM: 8000+\n\n## CLI examples (Windows PowerShell)\n\n- Low VRAM (8 GB)\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --dataset-chunk-size 2000 --steps 100\n```\n\n- Balanced (12 GB)\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --dataset-chunk-size 4000 --steps 100\n```\n\n- High VRAM (24 GB+)\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --dataset-chunk-size 8000 --steps 100\n```\n\n## Where it\u2019s wired in\n\n- src/aios/cli/hrm_hf_cli.py\n  --dataset-chunk-size option passed into config\n- src/aios/core/hrm_training/training_config/base_fields.py\n  dataset_chunk_size: int = 4000\n- src/aios/cli/hrm_hf/data.py, encoding.py, block_manager.py, train_actv1.py\n  use dataset_chunk_size to bound lines read and chunk within blocks\n- GUI\n  - Variable: panel.dataset_chunk_size_var (default \"4000\")\n  - Auto button in helpers.py sets steps to match chunk size\n\n## Try it\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --dataset-chunk-size 4000 --steps 1 --batch-size 2 --halt-max-steps 1 --eval-batches 1 --log-file artifacts/brains/actv1/metrics.jsonl\n```\nExpected: training parses and logs metrics; dataset loader will cap lines per cycle at 4000.\n\n## Notes\n\n- This setting affects data loading cadence and memory pressure, not model architecture.\n- If you also use gradient accumulation, consider total tokens per optimizer step when tuning throughput vs memory.\n", "tags": ["cli", "datasets", "evaluation", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Configurable Dataset Chunk Size (Sub-topic)"}, {"line": 7, "text": "Overview"}, {"line": 11, "text": "How it works"}, {"line": 17, "text": "Recommended values"}, {"line": 23, "text": "CLI examples (Windows PowerShell)"}, {"line": 40, "text": "Where it\u2019s wired in"}, {"line": 52, "text": "Try it"}, {"line": 58, "text": "Notes"}]}, {"path": "guide/features/CORE_TRAINING.md", "content": "# Core Training - AI-OS\nGenerated: December 12, 2025\nPurpose: Architecture and training engine for HRM\nStatus: Implemented\n\n## Key Files\n- `src/aios/core/hrm_training/training_config.py` \u2013 TrainingConfig (874+ lines)\n- `src/aios/cli/hrm_hf/train_actv1.py` \u2013 Training loop (~2000 lines)\n- `src/aios/core/hrm_engine.py` \u2013 Engine utilities\n\n## Overview\nThe core training flow is exposed via the `aios hrm-hf train-actv1` command. It loads a base model and tokenizer, applies HRM training logic, logs metrics, writes checkpoints, and maintains a brain bundle directory under `artifacts/brains/actv1/`.\n\n## Training Configuration\n- Single source of truth for parameters\n- Validation, type checking, CLI arg conversion, defaults, serialization\n\n## Training Loop Features\n- Gradient accumulation, loss/optimizer/scheduler\n- Checkpoints, metrics logging, OOM handling, graceful stop file\n\n## Brain Bundle System\nDirectory structure:\n```\nartifacts/brains/actv1/<brain-name>/\n\u251c\u2500 config.json\n\u251c\u2500 model.safetensors\n\u251c\u2500 tokenizer.json\n\u251c\u2500 metadata.json\n\u251c\u2500 training_args.json\n\u2514\u2500 checkpoints/\n```\nFeatures: auto-create, save model/tokenizer/config, resume\n\n## Commands (CLI syntax)\n\nYou can run training either via the CLI entry point or directly through Python's module interface. On Windows, prefer PowerShell examples below.\n\n### a) Direct CLI (named flags)\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1000 --batch-size 2 --halt-max-steps 1 --eval-batches 2 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n### b) Module invocation (explicit model flag)\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1000 --batch-size 2 --halt-max-steps 1 --eval-batches 2 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n### Key parameters (selection)\n- Model selection: `--model <name_or_path>`\n- Dataset: `--dataset-file <path>` (txt/jsonl); optional `--ascii-only`\n- Steps and batching: `--steps <int>`, `--batch-size <int>`\n- Halting: `--halt-max-steps <int>` (controls ACT halting behavior)\n- Evaluation: `--eval-file <path>`, `--eval-batches <int>`\n- Logging: `--log-file <path>` (JSONL)\n- Iteration control: `--iterate`, `--stop-file <path>`\n- Brain bundle: `--brain-name <str>`, `--bundle-dir <path>`\n- Architecture knobs: `--h-layers`, `--l-layers`, `--hidden-size`, `--expansion`, `--num-heads`, `--h-cycles`, `--l-cycles`, `--window-size`, `--pos-encodings`\n- Memory: `--gradient-checkpointing|--no-gradient-checkpointing`, `--amp|--no-amp`, `--use-8bit-optimizer`\n- Multi-GPU: `--ddp`, `--cuda-ids <list>`, `--world-size <int>`\n- DeepSpeed: `--zero-stage <none|zero1|zero2|zero3>` (uses configs in `config/`)\n- Experts: `--expert-id <id>` (train/freeze expert-specific components)\n\nNotes:\n- Paths are relative to repo root unless absolute. PowerShell accepts forward slashes (`/`) in Python paths.\n- For Windows shells, escape backslashes or quote paths with spaces.\n\n## Iterate Mode\n- `--iterate`: restart after completion with new shuffle; supports stop file\n\n## Evaluation During Training\n- `--eval-file`, `--eval-batches`\n- Periodic eval, validation loss, perplexity, history\n\n## Expert Training Mode\n- `--expert-id <id>`: train individual expert, freeze base, save under `artifacts/experts/<id>/`\n- Related: Dynamic Subbrains/MoE\n\n## Inputs\n- Dataset file(s): `training_data/curated_datasets/*.txt` (example set provided)\n- Optional eval file: `training_data/eval_test_dataset.txt`\n- Base model: HuggingFace hub id or local path (e.g., `gpt2` or `artifacts/hf_implant/base_model`)\n- Tokenizer: auto-resolved from model or `artifacts/hf_implant/tokenizers`\n\n## Outputs\n- Brain bundle under `artifacts/brains/actv1/<brain-name>/`\n- Metrics log (JSONL): default/explicit `artifacts/brains/actv1/metrics.jsonl`\n- Checkpoints under the bundle `checkpoints/`\n- Optional evaluation summaries in metrics/logs\n\n## Try it: quick dry-run examples\n\nThese mirror VS Code tasks configured in this repo and are safe to run. Ensure your venv is active.\n\n### Option 1: Direct CLI dry-run\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1 --batch-size 2 --halt-max-steps 1 --eval-batches 1 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n### Option 2: Module invocation\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1 --batch-size 2 --halt-max-steps 1 --eval-batches 1 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n### Option 3: Use VS Code Task\n- Run: Tasks \u2192 \"Run brief HRM CLI dry-run\" or \"Run HRM dry-run (module)\"\n- Expected outputs:\n\t- Metrics JSONL at `artifacts/brains/actv1/metrics.jsonl`\n\t- Brain bundle directories under `artifacts/brains/actv1/`\n\t- Console logs including training/eval step counts\n\n## Usage Notes\n- Use AMP and gradient checkpointing for memory savings\n- Use 8-bit optimizer for larger models when bitsandbytes is available\n\nRelated: Memory Optimization, Model Architecture, Datasets, Tokenizers\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)\n\n## Troubleshooting\n\n- OOM (out of memory): lower `--batch-size`, `--max-seq-len`, or `--dataset-chunk-size`; enable `--gradient-checkpointing` and `--amp`; consider `--use-8bit-optimizer` if bitsandbytes is installed.\n- FlashAttention: ensure `--use-flash-attn` and that your GPU supports it; otherwise it will fall back to SDPA.\n- Multi-GPU on Windows: prefer `--parallel-independent` with `--cuda-ids`; DDP often fails on Windows. If you need DDP, set `$env:AIOS_DDP_SPAWN = \"1\"` before running with `--ddp`.\n- Resume: when using parallel mode, `chunk_tracker_state.json` in the brain bundle enables resume; delete it if you want a fresh start.\n\nSee also:\n- Parallel Training Block/Chunk System\n- Multi-GPU & Distributed", "tags": ["cli", "datasets", "evaluation", "experts", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Core Training - AI-OS"}, {"line": 5, "text": "Key Files"}, {"line": 10, "text": "Overview"}, {"line": 13, "text": "Training Configuration"}, {"line": 17, "text": "Training Loop Features"}, {"line": 21, "text": "Brain Bundle System"}, {"line": 34, "text": "Commands (CLI syntax)"}, {"line": 38, "text": "a) Direct CLI (named flags)"}, {"line": 43, "text": "b) Module invocation (explicit model flag)"}, {"line": 48, "text": "Key parameters (selection)"}, {"line": 67, "text": "Iterate Mode"}, {"line": 70, "text": "Evaluation During Training"}, {"line": 74, "text": "Expert Training Mode"}, {"line": 78, "text": "Inputs"}, {"line": 84, "text": "Outputs"}, {"line": 90, "text": "Try it: quick dry-run examples"}, {"line": 94, "text": "Option 1: Direct CLI dry-run"}, {"line": 99, "text": "Option 2: Module invocation"}, {"line": 104, "text": "Option 3: Use VS Code Task"}, {"line": 111, "text": "Usage Notes"}, {"line": 119, "text": "Troubleshooting"}]}, {"path": "guide/features/DATASETS.md", "content": "# Datasets - AI-OS\nGenerated: December 12, 2025\nPurpose: Dataset system, readers, registry, and streaming\nStatus: Implemented\n\n## Files\n- Readers: `src/aios/data/datasets/*.py`\n- Registry: `src/aios/core/datasets/registry.py`\n- Catalog: `src/aios/core/datasets/catalog.py`\n- Streaming: `src/aios/data/streaming_cache.py`, `src/aios/data/stream_manager.py`\n\n## Supported Formats\n- Plain text, CSV, archives (tar/zip), directories, JSON, JSONL\n\n## Dataset Registry\n- Rich metadata (20+ fields), search, recommendations\n- Expert-dataset usage tracking, JSON persistence, local scanning\n\n## Streaming Dataset\n- Memory-efficient loading, infinite streaming, shuffling, caching\n\n## CLI\n\nThe datasets functionality is exposed as discrete commands under the main `aios` CLI:\n\n### Discovery and capacity\n- Show storage usage and cap:\n\t```powershell\n\taios datasets-stats\n\t```\n\tOutput example: `{ \"usage_gb\": 0.125, \"cap_gb\": 15.0 }`\n\n- Set storage capacity cap (persisted to ~/.config/aios/datasets.json):\n\t```powershell\n\taios datasets-set-cap 20\n\t```\n\tOutput example: `{ \"ok\": true, \"cap_gb\": 20.0 }`\n\n- List known datasets within size limit:\n\t```powershell\n\taios datasets-list-known --max-size-gb 10\n\t```\n\tOutput: JSON array of known items `{ name, url, approx_size_gb, notes }`\n\n### Building datasets (web-assisted)\nThese commands create datasets under the resolved base directory from `datasets_base_dir()`:\n`training_data/curated_datasets/<type>/<dataset_name>/`\n\n- Build text dataset by extracting main readable text from top sites:\n\t```powershell\n\taios datasets-build-text \"boats\" --max-docs 50 --per-site 10 --search-results 10 --min-chars 400 --progress\n\t```\n\tOutputs: `manifest.jsonl` with `{ path, label, url, title, chars, excerpt }` and text files.\n\tOptions: `--allow-ext txt,pdf,doc,docx,rtf,md,html,htm` to restrict downloads; `--file-prefix` to prefix filenames; `--store-dataset` to set dataset folder name; `--overwrite` to replace existing.\n\n- Build websites snapshot dataset (HTML pages):\n\t```powershell\n\taios datasets-build-websites \"boats\" --max-pages 30 --per-site 10 --search-results 10 --min-bytes 2000 --progress\n\t```\n\tOutputs: `pages/*.html` and `manifest.jsonl` with `{ path, url, title, bytes, links }`.\n\n- Build images dataset (perceptual dedup optional):\n\t```powershell\n\taios datasets-build-images \"boats\" --max-images 100 --per-site 20 --pages-per-site 5 --near-duplicate-threshold 8 --allow-ext jpg,png,webp --progress\n\t```\n\tOutputs: image files and `manifest.jsonl` with `{ path, label, source_url, page_url, title, alt }`.\n\n- Build generic raw files dataset (by extension allowlist):\n\t```powershell\n\taios datasets-build-raw \"boats\" --max-files 50 --per-site 10 --allow-ext pdf,csv,json,txt,zip --progress\n\t```\n\tOutputs: `files/*` and `manifest.jsonl` with `{ path, label, source_url, page_url, bytes }`.\n\n- Build videos dataset:\n\t```powershell\n\taios datasets-build-videos \"boats\" --max-videos 20 --per-site 5 --allow-ext mp4,webm --progress\n\t```\n\tOutputs: video files and `manifest.jsonl` with `{ path, label, source_url, page_url, bytes }`.\n\nNotes:\n- These commands respect a storage capacity cap and will stop early if the cap would be exceeded.\n- Networking is best-effort and may skip pages or files if unavailable or too small.\n- Use `--overwrite` to rebuild a dataset folder.\n\n### Base directory resolution\nDataset base directory is chosen in this order:\n1) `AIOS_DATASETS_DIR` environment variable\n2) Project root detection \u2192 `training_data/curated_datasets`\n3) Fallback: `~/.local/share/aios/datasets`\n\nUse this to find your outputs. Example (project root):\n`training_data/curated_datasets/text/<dataset_name>/manifest.jsonl`\n\n## Inputs\n- Web-sourced datasets via CLI builders as shown above\n- Local files in supported formats (txt/csv/json/jsonl/archives/directories)\n\n## Outputs\n- Organized dataset directories under the base dir\n- Manifest files (`manifest.jsonl`) describing items for each dataset type\n- Storage cap config at `~/.config/aios/datasets.json`\n\n## Try it: quick local example\nUse an existing small text file to validate training pipeline compatibility:\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1 --batch-size 2 --halt-max-steps 1 --eval-batches 1 --log-file artifacts/brains/actv1/metrics.jsonl\n```\nExpected: metrics JSONL created and a brain bundle directory under `artifacts/brains/actv1/`.\n\n## Troubleshooting\n- \"cap_exceeded\": Increase cap via `aios datasets-set-cap <GB>` or delete old datasets\n- Permission issues on Windows: run VS Code as a user with write access to dataset dir and HF cache dir\n- Empty manifests: Increase `--max-docs`/`--max-pages` or relax `--min-chars`/`--min-bytes`\n\nRelated: Tokenizers, Core Training, Dynamic Subbrains/MoE\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "experts", "gui", "training"], "headings": [{"line": 0, "text": "Datasets - AI-OS"}, {"line": 5, "text": "Files"}, {"line": 11, "text": "Supported Formats"}, {"line": 14, "text": "Dataset Registry"}, {"line": 18, "text": "Streaming Dataset"}, {"line": 21, "text": "CLI"}, {"line": 25, "text": "Discovery and capacity"}, {"line": 44, "text": "Building datasets (web-assisted)"}, {"line": 84, "text": "Base directory resolution"}, {"line": 93, "text": "Inputs"}, {"line": 97, "text": "Outputs"}, {"line": 102, "text": "Try it: quick local example"}, {"line": 109, "text": "Troubleshooting"}]}, {"path": "guide/features/DATASET_PREPROCESSING.md", "content": "# Dataset Preprocessing Guide\n\n## Overview\n\nThe dataset preprocessing utility converts downloaded datasets into an optimized block-based structure for efficient training with accurate progress tracking.\n\n## Why Preprocess?\n\n**Without Preprocessing:**\n- \u274c Slow or failed dataset size detection (especially on network drives)\n- \u274c No block/chunk progress tracking\n- \u274c Unpredictable performance on large datasets\n- \u274c Shows \"0/???\" for chunks and blocks\n\n**With Preprocessing:**\n- \u2705 Instant dataset size detection (reads metadata file)\n- \u2705 Accurate block and chunk progress tracking\n- \u2705 Consistent performance regardless of storage location\n- \u2705 Shows \"15/25\" for chunks, \"2/10\" for blocks\n- \u2705 Optimal for network drives and large datasets\n\n## When to Preprocess\n\nPreprocess datasets in these scenarios:\n- Downloaded to network drives (Z:, mapped drives, NAS)\n- Large datasets (>1GB, millions of samples)\n- Datasets with many small files\n- When training shows \"epoch tracking disabled\"\n\n## Usage\n\n### Command Line\n\n```bash\n# Basic preprocessing (100k samples per block)\naios hrm-hf preprocess-dataset Z:\\training_datasets\\tinystories\n\n# Custom block size\naios hrm-hf preprocess-dataset ~/datasets/my_corpus --block-size 50000\n\n# ASCII-only filtering\naios hrm-hf preprocess-dataset /data/multilingual --ascii-only\n\n# Overwrite existing preprocessed structure\naios hrm-hf preprocess-dataset ./datasets/corpus --overwrite\n```\n\n### Python API\n\n```python\nfrom aios.cli.datasets.preprocess_dataset import preprocess_dataset\n\n# Preprocess dataset\ntotal_samples, samples_per_block, total_blocks = preprocess_dataset(\n    dataset_path=\"Z:/training_datasets/tinystories\",\n    samples_per_block=100000,  # 100k samples per block\n    ascii_only=False,\n    overwrite=False\n)\n\nprint(f\"Preprocessed: {total_samples:,} samples in {total_blocks} blocks\")\n```\n\n## Structure Created\n\n```\ndataset_name/\n\u251c\u2500\u2500 dataset_info.json     # Metadata (instant size detection)\n\u251c\u2500\u2500 raw/                  # Original files (preserved)\n\u2502   \u251c\u2500\u2500 file1.txt\n\u2502   \u251c\u2500\u2500 file2.txt\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 block_0/              # First 100k samples\n\u2502   \u2514\u2500\u2500 samples.txt       # One sample per line\n\u251c\u2500\u2500 block_1/              # Next 100k samples\n\u2502   \u2514\u2500\u2500 samples.txt\n\u251c\u2500\u2500 block_2/\n\u2502   \u2514\u2500\u2500 samples.txt\n\u2514\u2500\u2500 ...\n```\n\n### Metadata File (dataset_info.json)\n\n```json\n{\n  \"dataset_name\": \"tinystories\",\n  \"total_samples\": 2456789,\n  \"samples_per_block\": 100000,\n  \"total_blocks\": 25,\n  \"ascii_only\": false,\n  \"preprocessed_by\": \"AI-OS dataset preprocessor\",\n  \"structure\": \"block_N/samples.txt format\"\n}\n```\n\n## Supported Input Formats\n\nThe preprocessor automatically detects and handles:\n\n### 1. HuggingFace Datasets\n- Saved with `dataset.save_to_disk()`\n- Contains `dataset_info.json`, `.arrow` files, or `data/` directory\n- Extracts text from columns: text, content, sentence, article, etc.\n\n### 2. Plain Text Files\n- `.txt`, `.csv`, `.json`, `.jsonl` files\n- Recursively scans subdirectories\n- One sample per line\n\n### 3. Mixed Directories\n- Combination of text files and HF dataset files\n- Automatically chooses best extraction method\n\n## Training with Preprocessed Datasets\n\nOnce preprocessed, training automatically detects the structure:\n\n```bash\n# Just point to the preprocessed directory\naios hrm-hf train-actv1 --dataset-file Z:\\training_datasets\\tinystories --steps 1000\n\n# Training output will show:\n# \u2713 Epoch tracking initialized\n# \u2713 Dataset: tinystories\n# \u2713 Total: 2,456,789 samples in 25 blocks\n# \u2713 Chunk: 15/25   Block: 2/25   Epoch: 0\n```\n\n## Parameters\n\n### --block-size (default: 100000)\nNumber of samples per block. Larger blocks = fewer files but more memory per block load.\n\n**Guidelines:**\n- **Small datasets (<100k samples)**: Use 10000-50000\n- **Medium datasets (100k-1M)**: Use 100000 (default)\n- **Large datasets (>1M)**: Use 100000-200000\n\n### --ascii-only\nFilter to ASCII-only text, removing non-ASCII characters and samples.\n\n**Use when:**\n- Training English-only models\n- Avoiding encoding issues\n- Reducing dataset size\n\n### --overwrite\nRebuild the preprocessed structure from scratch.\n\n**Use when:**\n- Updating after adding/removing raw files\n- Changing block size\n- Fixing corrupted structure\n\n## Performance\n\n### Before Preprocessing\n```\nDataset: Z:\\training_datasets\\tinystories (network drive)\nDetection: 45-120 seconds (or fails with timeout)\nProgress: \"Chunk: 0/???  Block: 0/???\"\n```\n\n### After Preprocessing\n```\nDataset: Z:\\training_datasets\\tinystories\nDetection: <1 second (reads metadata file)\nProgress: \"Chunk: 15/25  Block: 2/25  Epoch: 0\"\n```\n\n## Checking Status\n\n```python\nfrom aios.cli.datasets.preprocess_dataset import is_preprocessed, get_preprocessed_info\n\n# Check if dataset is preprocessed\nif is_preprocessed(\"Z:/training_datasets/tinystories\"):\n    print(\"Dataset is preprocessed!\")\n    \n    # Get metadata\n    info = get_preprocessed_info(\"Z:/training_datasets/tinystories\")\n    print(f\"Samples: {info['total_samples']:,}\")\n    print(f\"Blocks: {info['total_blocks']}\")\n```\n\n## Troubleshooting\n\n### \"No text samples found\"\n- Check that raw files contain readable text\n- Verify file extensions (.txt, .csv, .json, .jsonl)\n- Try without `--ascii-only` flag\n\n### \"Preprocessed structure exists\"\n- Use `--overwrite` to rebuild\n- Or delete `dataset_info.json` and `block_*` directories manually\n\n### \"Permission denied\"\n- Ensure write access to dataset directory\n- Try running with elevated privileges\n- Check network drive permissions\n\n### Slow preprocessing\n- Normal for large datasets (millions of samples)\n- Progress shown every 100 files\n- Consider preprocessing on local drive first, then moving\n\n## Best Practices\n\n1. **Preprocess once, train many times**\n   - Preprocessing is one-time cost\n   - Subsequent training runs are fast\n\n2. **Keep raw files**\n   - Original files moved to `raw/` subdirectory\n   - Can rebuild anytime with `--overwrite`\n\n3. **Use standard block size**\n   - 100k samples per block works well for most datasets\n   - Only adjust if specific memory constraints\n\n4. **Preprocess before long training runs**\n   - Ensures accurate progress tracking\n   - Prevents \"epoch tracking disabled\" issues\n\n5. **Version control metadata only**\n   - Add `block_*/` to .gitignore\n   - Keep `dataset_info.json` for reference\n   - Raw files can be re-downloaded\n\n## Integration with GUI\n\nThe GUI automatically detects preprocessed datasets:\n- Shows accurate total blocks in \"Training Progress\"\n- Displays chunk progress within current block\n- Updates blocks as \"X/Y\" instead of \"X\"\n\nNo GUI changes needed - just preprocess the dataset and start training!\n", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Dataset Preprocessing Guide"}, {"line": 2, "text": "Overview"}, {"line": 6, "text": "Why Preprocess?"}, {"line": 21, "text": "When to Preprocess"}, {"line": 29, "text": "Usage"}, {"line": 31, "text": "Command Line"}, {"line": 34, "text": "Basic preprocessing (100k samples per block)"}, {"line": 37, "text": "Custom block size"}, {"line": 40, "text": "ASCII-only filtering"}, {"line": 43, "text": "Overwrite existing preprocessed structure"}, {"line": 47, "text": "Python API"}, {"line": 52, "text": "Preprocess dataset"}, {"line": 63, "text": "Structure Created"}, {"line": 81, "text": "Metadata File (dataset_info.json)"}, {"line": 95, "text": "Supported Input Formats"}, {"line": 99, "text": "1. HuggingFace Datasets"}, {"line": 104, "text": "2. Plain Text Files"}, {"line": 109, "text": "3. Mixed Directories"}, {"line": 113, "text": "Training with Preprocessed Datasets"}, {"line": 118, "text": "Just point to the preprocessed directory"}, {"line": 121, "text": "Training output will show:"}, {"line": 122, "text": "\u2713 Epoch tracking initialized"}, {"line": 123, "text": "\u2713 Dataset: tinystories"}, {"line": 124, "text": "\u2713 Total: 2,456,789 samples in 25 blocks"}, {"line": 125, "text": "\u2713 Chunk: 15/25   Block: 2/25   Epoch: 0"}, {"line": 128, "text": "Parameters"}, {"line": 130, "text": "--block-size (default: 100000)"}, {"line": 138, "text": "--ascii-only"}, {"line": 146, "text": "--overwrite"}, {"line": 154, "text": "Performance"}, {"line": 156, "text": "Before Preprocessing"}, {"line": 163, "text": "After Preprocessing"}, {"line": 170, "text": "Checking Status"}, {"line": 175, "text": "Check if dataset is preprocessed"}, {"line": 179, "text": "Get metadata"}, {"line": 185, "text": "Troubleshooting"}, {"line": 187, "text": "\"No text samples found\""}, {"line": 192, "text": "\"Preprocessed structure exists\""}, {"line": 196, "text": "\"Permission denied\""}, {"line": 201, "text": "Slow preprocessing"}, {"line": 206, "text": "Best Practices"}, {"line": 229, "text": "Integration with GUI"}]}, {"path": "guide/features/DYNAMIC_SUBBRAINS_MOE.md", "content": "# Dynamic Subbrains (Mixture of Experts)\n\nPurpose: Sparse expert routing for efficiency and specialization. Includes expert metadata/registry, expert-only training, and goal-aware routing hooks.\n\nStatus: Implemented core MoE in ACTv1 + expert registry and expert-only training. Goal-aware biasing and full GUI management are WIP.\n\nKey files:\n- MoE layer and routing stats: `src/aios/core/hrm_models/moe_layer.py`\n- Goal-aware router (biasing by goals): `src/aios/core/hrm_models/goal_aware_router.py`\n- Expert metadata and registry: `src/aios/core/hrm_models/expert_metadata.py`\n- ACTv1 model uses MoE by default: `src/aios/core/hrm_models/impl/hrm_act_v1.py` and `src/aios/core/brains/hf_brain.py`\n- Training CLI flags (MoE and experts): `src/aios/cli/hrm_hf_cli.py`\n- Expert-only training implementation: `src/aios/cli/hrm_hf/expert_training.py`\n- Metrics logging (load balancing + expert usage): `src/aios/cli/hrm_hf/training_logic/train_epoch.py`\n- GUI Subbrains Manager (WIP): `src/aios/gui/components/subbrains_manager_panel/`\n\nSee also:\n- Core training: `CORE_TRAINING.md`\n- Memory optimization (8-bit optimizer, AMP): `MEMORY_OPTIMIZATION.md`\n- Multi-GPU/Windows-friendly parallel training: `MULTI_GPU_DISTRIBUTED.md`, `PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md`\n- Goals CLI (link goals to experts): `CLI_COMMANDS.md` (Goals section)\n\n## What you get\n- Sparse MoE with top-k expert routing per token for ~75% compute reduction while increasing capacity.\n- Automatic auxiliary load-balancing loss to prevent collapse.\n- Periodic expert-usage metrics in your log file (routing probabilities, token counts).\n- Expert-only training mode that produces standalone expert checkpoints and updates a persistent registry.\n- Goal-aware router module (WIP hookup) to bias expert selection by active goals.\n\n## Commands (PowerShell, Windows-first)\n\n1) Train ACTv1 with MoE (default enabled)\n- Flags come from `aios hrm-hf train-actv1`. MoE-related flags:\n\t- `--use-moe/--no-moe` (default: `--use-moe`)\n\t- `--num-experts <int>` (default: 8)\n\t- `--num-experts-per-tok <int>` (top-k, default: 2)\n\t- `--moe-capacity-factor <float>` (default: 1.25)\n\t- `--auto-adjust-lr/--no-auto-adjust-lr` (default: on; reduces LR for MoE stability)\n\nExample (small dry-run, logs expert usage):\n\n\t\taios hrm-hf train-actv1 `\n\t\t\t--model artifacts/hf_implant/base_model `\n\t\t\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t\t\t--steps 20 --batch-size 8 `\n\t\t\t--use-moe --num-experts 8 --num-experts-per-tok 2 --moe-capacity-factor 1.25 `\n\t\t\t--log-file artifacts/brains/actv1/metrics.jsonl\n\nDisable MoE (train dense FFN instead):\n\n\t\taios hrm-hf train-actv1 `\n\t\t\t--model artifacts/hf_implant/base_model `\n\t\t\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t\t\t--steps 20 --batch-size 8 `\n\t\t\t--no-moe `\n\t\t\t--log-file artifacts/brains/actv1/metrics.jsonl\n\nTips:\n- Lower `--num-experts-per-tok` to 1 to reduce active compute/memory on very constrained GPUs.\n- Keep `--auto-adjust-lr` enabled unless you know what you\u2019re doing; MoE routers can be unstable at higher LR.\n\n2) Train a standalone expert only (writes artifacts/experts and updates registry)\n- Trigger by passing `--expert-id <string>` to `train-actv1`.\n- Uses a lightweight FeedForward expert, saves as `.safetensors`, and writes/updates `artifacts/experts/registry.json`.\n\nExample (quick expert build):\n\n\t\taios hrm-hf train-actv1 `\n\t\t\t--model artifacts/hf_implant/base_model `\n\t\t\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t\t\t--steps 3 --batch-size 2 `\n\t\t\t--expert-id test-expert-004 `\n\t\t\t--default-goal \"Improve summarization quality\" `\n\t\t\t--log-file artifacts/experts/test-expert-004/metrics.jsonl\n\nOutputs:\n- `artifacts/experts/test-expert-004/expert.safetensors`\n- `artifacts/experts/registry.json` (created or updated with metadata including `expert_id`, `name`, `goals`, `checkpoint_path`, `is_active/is_frozen`, hierarchy fields)\n\n3) Link goals to experts (biasing signal for router)\n- Goals live in the directives DB and can be associated with an expert.\n- Commands are under `aios goals-*`.\n\nExamples:\n\n\t\t# Add a goal and link to an expert immediately\n\t\taios goals-add \"Improve summarization quality\" --expert-id test-expert-004\n\n\t\t# Link an existing goal to an expert\n\t\taios goals-link-expert 42 test-expert-004\n\n\t\t# List active goals\n\t\taios goals-list\n\n\t\t# List goals for an expert\n\t\taios goals-list-for-expert test-expert-004\n\nNotes:\n- The `GoalAwareRouter` module supports biasing toward experts linked to active goals. Integration into the default training/inference loop is in progress; track `src/aios/core/hrm_models/goal_aware_router.py`.\n\n## Inputs and Outputs\n\nInputs (training flags relevant to MoE/experts):\n- `--use-moe`, `--num-experts`, `--num-experts-per-tok`, `--moe-capacity-factor`, `--auto-adjust-lr`\n- Standard training knobs: `--max-seq-len`, `--batch-size`, `--steps`, `--lr`, `--amp`, `--gradient-checkpointing`, etc.\n- Expert-only mode: `--expert-id <id>` plus optional `--default-goal` to seed goal linkage.\n\nOutputs (files and metrics):\n- Brain training logs: your `--log-file` JSONL includes, when MoE is enabled:\n\t- `lb_loss`: load-balancing loss value (applied internally; coef ~0.05)\n\t- Periodic `expert_usage` events with:\n\t\t- `avg_routing_prob`: average probability per expert\n\t\t- `token_counts`: tokens routed to each expert\n\t\t- `total_tokens`: total tokens seen when sampled\n- Expert-only training:\n\t- `artifacts/experts/<expert-id>/expert.safetensors`\n\t- `artifacts/experts/registry.json` with entries like:\n\t\t- `expert_id`, `name`, `description`, `category`, `goals`, timestamps\n\t\t- `is_active`, `is_frozen`, `parent_expert_id`, `child_expert_ids`\n\t\t- `checkpoint_path`: e.g., `artifacts\\\\experts\\\\<expert-id>\\\\expert.safetensors`\n\t\t- `training_config`: hidden/intermediate sizes, steps, batch size, etc.\n\n## How routing works (high level)\n- Each MoE layer computes router logits over N experts and activates the top-k experts per token (`--num-experts-per-tok`).\n- An auxiliary load-balancing loss is added to spread traffic across experts and avoid collapse. Metrics include `lb_loss` and `moe_layers` count.\n- `expert_usage` entries in logs let you validate router health and specialization during training.\n\n## GUI: Subbrains Manager (WIP)\n- The panel shows the expert registry with counts and hierarchy and can refresh from disk:\n\t- Code: `src/aios/gui/components/subbrains_manager_panel/`\n\t- Data loader: `data_manager.py` reads `artifacts/experts/registry.json`\n- Actions like create/delete/freeze are currently placeholders that print \u201cCLI command needed\u201d. Use CLI for expert training and goals linking.\n- As features land, the panel will manage expert lifecycle and goal associations directly.\n\n## Troubleshooting\n- Training is unstable (NaNs/Inf) with MoE:\n\t- Keep `--auto-adjust-lr` enabled (default). It reduces LR for MoE automatically.\n\t- Lower base `--lr` and/or `--num-experts-per-tok`.\n\t- Ensure AMP/precision settings are stable (`--amp` by default; try `--model-dtype bf16` on supported GPUs).\n- VRAM pressure with many experts:\n\t- Reduce `--num-experts` and/or set `--num-experts-per-tok 1`.\n\t- Use `--amp`, `--gradient-checkpointing`, and `--use-8bit-optimizer` (requires bitsandbytes).\n- No expert usage metrics in log:\n\t- Ensure `--use-moe` is on.\n\t- `expert_usage` logs are periodic (every ~100 steps) and sampled from early layers; short runs may not emit them.\n- Can\u2019t find the expert registry:\n\t- Path: `artifacts/experts/registry.json`. It\u2019s created on first expert-only training.\n\n## Try it quickly\n- Minimal MoE run with metrics:\n\n\t\taios hrm-hf train-actv1 `\n\t\t\t--model artifacts/hf_implant/base_model `\n\t\t\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t\t\t--steps 30 --batch-size 4 `\n\t\t\t--use-moe --num-experts 8 --num-experts-per-tok 2 `\n\t\t\t--log-file artifacts/brains/actv1/metrics.jsonl\n\n- Train one tiny expert and link a goal:\n\n\t\taios hrm-hf train-actv1 `\n\t\t\t--model artifacts/hf_implant/base_model `\n\t\t\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t\t\t--steps 3 --batch-size 2 `\n\t\t\t--expert-id demo-expert-001 `\n\t\t\t--default-goal \"Focus on troubleshooting clarity\" `\n\t\t\t--log-file artifacts/experts/demo-expert-001/metrics.jsonl\n\n\t\taios goals-list-for-expert demo-expert-001\n\n## Notes and next steps\n- Goal-aware router module exists and exposes bias controls; full wiring to training/inference loops and GUI controls is in progress.\n- The GUI Subbrains Manager will gain create/delete/freeze operations backed by CLI endpoints.\n- We\u2019ll expose advanced router knobs (e.g., load-balance loss coef) once stabilized.\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "experts", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Dynamic Subbrains (Mixture of Experts)"}, {"line": 22, "text": "What you get"}, {"line": 29, "text": "Commands (PowerShell, Windows-first)"}, {"line": 85, "text": "\t\t# Add a goal and link to an expert immediately"}, {"line": 88, "text": "\t\t# Link an existing goal to an expert"}, {"line": 91, "text": "\t\t# List active goals"}, {"line": 94, "text": "\t\t# List goals for an expert"}, {"line": 100, "text": "Inputs and Outputs"}, {"line": 122, "text": "How routing works (high level)"}, {"line": 127, "text": "GUI: Subbrains Manager (WIP)"}, {"line": 134, "text": "Troubleshooting"}, {"line": 148, "text": "Try it quickly"}, {"line": 170, "text": "Notes and next steps"}]}, {"path": "guide/features/FEATURE_COMBINATION_MATRIX.md", "content": "# Feature Combination Matrix - AI-OS\nLast Updated: December 12, 2025\nPurpose: Feature compatibility reference - which combinations are verified and which are experimental\n\n> **Note for v1.0.0:** This matrix documents the current testing status of feature combinations. \n> Items marked as \"EXPERIMENTAL\" or with TODO notes represent experimental combinations \n> that may work but haven't been comprehensively tested. Use with appropriate caution.\n\n---\n\n## \ud83d\udcca Status Legend\n\n| Status | Meaning |\n|--------|---------|\n| \u2705 **VERIFIED** | Tested and confirmed working |\n| \u26a0\ufe0f **EXPERIMENTAL** | Should work but not comprehensively tested |\n| \u274c **INCOMPATIBLE** | Known to be incompatible |\n| \u2753 **UNTESTED** | Status unclear, use with caution |\n| \ud83d\udea7 **PARTIAL** | Partially works with known limitations |\n\n---\n\n## \ud83d\udd2c Memory Optimization Combinations\n\n### Gradient Checkpointing + AMP\n**Status**: \u2705 **VERIFIED WORKING**  \n**Benefit**: ~60-70% memory reduction  \n**Speed Impact**: ~20% slower  \n**Recommended**: Yes, for most training\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --gradient-checkpointing `\n  --amp `\n  --steps 100\n```\n\n**Test Results**:\n- \u2705 Trains successfully\n- \u2705 Memory reduction confirmed\n- \u2705 No quality loss observed\n- \u2705 Works on single GPU\n- \u26a0\ufe0f Multi-GPU not tested\n\n---\n\n### Gradient Checkpointing + AMP + 8-bit Optimizer\n**Status**: \u2705 **VERIFIED WORKING**  \n**Benefit**: ~70-80% memory reduction  \n**Speed Impact**: ~25% slower  \n**Recommended**: Yes, for large models (>100M params)\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --gradient-checkpointing `\n  --amp `\n  --use-8bit-optimizer `\n  --steps 100\n```\n\n**Test Results**:\n- \u2705 Trains successfully\n- \u2705 Massive memory reduction\n- \u2705 Quality maintained\n- \u2705 Works with bitsandbytes 0.48.1\n- \u26a0\ufe0f Multi-GPU not tested\n\nRequirements:\n- bitsandbytes installed\n- CUDA-capable GPU (Linux preferred)\n\n---\n\n### Gradient Checkpointing + Long Context\n**Status**: \u26a0\ufe0f **EXPERIMENTAL**  \n**Expected**: Should work  \n**Use Case**: Train with longer sequences on limited VRAM\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --gradient-checkpointing `\n  --max-seq-len 2048 `\n  --batch-size 1 `\n  --steps 100\n```\n\n**Expected Behavior**:\n- \u2705 Should enable 2K-4K context on 11GB GPU\n- \u26a0\ufe0f Will be slower due to checkpointing\n- \u26a0\ufe0f Batch size must be very small\n\n**Note**: Not extensively tested with contexts above 2048 tokens. Start with smaller contexts and increase gradually.\n\n---\n\n### All Memory Optimizations Combined\n**Status**: \u26a0\ufe0f **PARTIAL**  \n**Features**: Gradient Checkpointing + AMP + 8-bit + Chunking  \n**Expected**: Maximum memory efficiency  \n**Use Case**: Train very large models or very long contexts\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --gradient-checkpointing `\n  --amp `\n  --use-8bit-optimizer `\n  --use-chunked-training --chunk-size 1024 `\n  --max-seq-len 8192 `\n  --batch-size 1 `\n  --steps 100\n```\n\nNotes:\n- \u2705 Chunked training is implemented (`--use-chunked-training`, `--chunk-size`)\n- \u26a0\ufe0f Expect slower throughput at very small chunk sizes\n\n**TODO**: \n1. Verify chunked training is implemented\n2. Test with various chunk sizes\n3. Measure actual memory usage\n\n---\n\n## \ud83d\ude80 Multi-GPU Combinations\n\n### DDP + Gradient Checkpointing\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Fast distributed training with memory efficiency\n\nExample (Linux recommended):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --ddp `\n  --cuda-ids \"0,1\" `\n  --world-size 2 `\n  --gradient-checkpointing `\n  --steps 100\n```\n\n**Issues**:\n- \u2753 DDP implementation not verified\n- \u2753 Does `_maybe_spawn` function exist?\n- \u2753 Gradient sync working?\n\nWindows tip: Prefer `--parallel-independent` instead of DDP.\n\n---\n\n### DDP + AMP\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Fast training with mixed precision across GPUs\n\nExample (Linux recommended):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --ddp `\n  --cuda-ids \"0,1\" `\n  --world-size 2 `\n  --amp `\n  --steps 100\n```\n\n**Note**: Not extensively tested with if AMP works correctly with DDP\n\n---\n\n### DDP + All Memory Optimizations\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Maximum efficiency across multiple GPUs\n\nExample (Linux recommended):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --ddp `\n  --cuda-ids \"0,1\" `\n  --world-size 2 `\n  --gradient-checkpointing `\n  --amp `\n  --use-8bit-optimizer `\n  --steps 100\n```\n\n---\n\nBack to [Guide Index](../INDEX.MD)\n\n**Questions**:\n- Does 8-bit optimizer work with DDP?\n- Are optimizer states synchronized?\n- Is there communication overhead?\n\n**TODO**: Comprehensive multi-GPU testing\n\n---\n\n## \ud83e\udde0 DeepSpeed Combinations\n\n### DeepSpeed ZeRO-1 + Gradient Checkpointing\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Optimizer state partitioning + activation checkpointing\n\nExample (Linux + DeepSpeed):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --zero-stage zero1 `\n  --gradient-checkpointing `\n  --cuda-ids \"0,1\" `\n  --steps 100\n```\n\n**TODO**:\n1. Verify DeepSpeed is actually initialized\n2. Test ZeRO-1 stage\n3. Measure memory reduction\n\n---\n\n### DeepSpeed ZeRO-2 + AMP\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Gradient partitioning + mixed precision\n\nExample (Linux + DeepSpeed):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --zero-stage zero2 `\n  --amp `\n  --cuda-ids \"0,1\" `\n  --steps 100\n```\n\n**Note**: Not extensively tested with and measure\n\n---\n\n### DeepSpeed ZeRO-3 (Maximum Memory Reduction)\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Parameter partitioning for massive models\n\nExample (Linux + DeepSpeed):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --zero-stage zero3 `\n  --gradient-checkpointing `\n  --amp `\n  --cuda-ids \"0,1\" `\n  --steps 100\n```\n\n**Note**: Not extensively tested with ZeRO-3 stage\n\n---\n\n### DeepSpeed + 8-bit Optimizer\n**Status**: \u2753 **COMPATIBILITY UNKNOWN**  \n**Question**: Can DeepSpeed work with bitsandbytes?\n\nExample (Compat unknown):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --zero-stage zero2 `\n  --use-8bit-optimizer `\n  --cuda-ids \"0,1\" `\n  --steps 100\n```\n\n**Potential Issue**: DeepSpeed has its own optimizer management - may conflict with bitsandbytes\n\n**Note**: Not extensively tested with compatibility\n\n---\n\n## \ud83e\udde9 MoE / Dynamic Subbrains Combinations\n\n### MoE + Gradient Checkpointing\n**Status**: \u26a0\ufe0f **UNTESTED**  \n**Expected**: Should work  \n**Use Case**: Train models with experts efficiently\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --use-moe `\n  --num-experts 4 `\n  --gradient-checkpointing `\n  --steps 100\n```\n\n**Note**: Not extensively tested with MoE with checkpointing\n\n---\n\n### MoE + AMP + 8-bit\n**Status**: \u26a0\ufe0f **UNTESTED**  \n**Expected**: Memory-efficient expert training\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --use-moe `\n  --num-experts 8 `\n  --gradient-checkpointing `\n  --amp `\n  --use-8bit-optimizer `\n  --steps 100\n```\n\n**Note**: Not extensively tested with expert training with optimizations\n\n---\n\n### Expert Training + Memory Optimizations\n**Status**: \u26a0\ufe0f **UNTESTED**  \n**Expected**: Efficient single expert training\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/base_model `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --expert-id \"python_expert\" `\n  --gradient-checkpointing `\n  --amp `\n  --use-8bit-optimizer `\n  --steps 100\n```\n\n**Note**: Not extensively tested with expert-only training mode\n\n---\n\n### MoE + Multi-GPU\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Expert parallelism across GPUs\n\nExample (Linux recommended):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --use-moe `\n  --num-experts 8 `\n  --ddp `\n  --cuda-ids \"0,1\" `\n  --world-size 2 `\n  --steps 100\n```\n\n**Questions**:\n- How are experts distributed across GPUs?\n- Is expert selection synchronized?\n- What's the communication pattern?\n\n**Note**: Not extensively tested with and document expert parallelism\n\n---\n\n### MoE + DeepSpeed\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Expert partitioning with ZeRO\n\nExample (Linux + DeepSpeed):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --use-moe `\n  --num-experts 16 `\n  --zero-stage zero3 `\n  --cuda-ids \"0,1\" `\n  --steps 100\n```\n\n**Note**: Not extensively tested with DeepSpeed with MoE\n\n---\n\n## \ud83d\udcda Context Length Combinations\n\n### Long Context + Chunking\n**Status**: \u2705 **SUPPORTED**  \n**Expected**: Enable 10K+ contexts by chunking\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --max-seq-len 10000 `\n  --use-chunked-training `\n  --chunk-size 1024 `\n  --gradient-checkpointing `\n  --amp `\n  --steps 100\n```\n\n**Questions**:\n- Is chunking actually implemented?\n- How does it split sequences?\n- What's the memory impact?\n\n**TODO**: \n1. Verify chunking implementation\n2. Test with various context lengths: 8K, 16K, 32K\n3. Measure actual memory usage\n\n---\n\n### Long Context + Multi-GPU\n**Status**: \u26a0\ufe0f **UNTESTED**  \n**Expected**: Distribute long sequences across GPUs\n\n**Command**:\n```bash\naios hrm-hf train-actv1 \\\n  --model gpt2 \\\n  --dataset-file data.txt \\\n  --max-seq-len 8192 \\\n  --ddp \\\n  --cuda-ids \"0,1\" \\\n  --world-size 2 \\\n  --gradient-checkpointing \\\n  --batch-size 1 \\\n  --steps 1000\n```\n\n**Note**: Not extensively tested with long context with DDP\n\n---\n\n### FlashAttention + Memory/Chunking\n**Status**: \u26a0\ufe0f **PLATFORM-DEPENDENT**  \nNotes:\n- `--use-flash-attn` is supported by the CLI and will enable FA2 when installed and compatible (Ampere+).\n- On Windows, FA2 is commonly unavailable; training falls back to PyTorch SDPA.\n- Combine with `--window-size` for extreme contexts when FA2 is not available.\n\n---\n\n## \ud83d\udd24 Tokenizer Combinations\n\n### Custom Tokenizer + Training\n**Status**: \u26a0\ufe0f **UNTESTED** (except GPT-2)  \n**Expected**: Should work with any HuggingFace tokenizer\n\n**Verified**:\n- \u2705 GPT-2 tokenizer\n\n**Needs Testing**:\n- \u26a0\ufe0f Qwen 2.5\n- \u26a0\ufe0f Mistral\n- \u26a0\ufe0f Code Llama\n- \u26a0\ufe0f DeepSeek-Coder V2\n- \u26a0\ufe0f StarCoder2\n- \u26a0\ufe0f Phi-3\n- \u26a0\ufe0f Llama 3 (requires HF auth)\n\n**Note**: Not extensively tested with each tokenizer with basic training\n\n---\n\n### Large Vocabulary + Memory Optimizations\n**Status**: \u26a0\ufe0f **UNTESTED**  \n**Use Case**: Tokenizers with 100K+ tokens (DeepSeek, Qwen, Llama 3)\n\n**Command**:\n```bash\naios hrm-hf train-actv1 \\\n  --model \"deepseek-ai/deepseek-coder-v2-base\" \\\n  --dataset-file data.txt \\\n  --gradient-checkpointing \\\n  --amp \\\n  --use-8bit-optimizer \\\n  --steps 1000\n```\n\n**Considerations**:\n- Large vocabulary = larger embedding layer\n- More memory needed for embeddings\n- May need aggressive optimizations\n\n**Note**: Not extensively tested with large-vocab tokenizers\n\n---\n\n## \ud83d\udcca Dataset Format Combinations\n\n### Streaming Dataset + Linear Mode\n**Status**: \u2705 **SUPPORTED**  \nFeatures:\n- Linear progression with resume via `--dataset-start-offset`\n- Iterate mode for long\u2011running cycles via `--iterate`\n\n**Features**:\n- \u2705 Infinite streaming\n- \u2705 Shuffle support\n- \u2705 Caching\n- \u2705 Memory-efficient\n\n---\n\n### Large Dataset + Multi-GPU\n**Status**: \u26a0\ufe0f **UNTESTED**  \n**Expected**: Distributed dataset loading\n\n**Command**:\n```bash\naios hrm-hf train-actv1 \\\n  --model gpt2 \\\n  --dataset-file large_dataset.txt \\\n  --ddp \\\n  --cuda-ids \"0,1\" \\\n  --world-size 2 \\\n  --steps 10000\n```\n\n**Questions**:\n- Is dataset split across workers?\n- Is shuffling consistent?\n- What's the I/O pattern?\n\n**Note**: Not extensively tested with with multi-GB datasets\n\n---\n\n### Archive Dataset + Training\n**Status**: \u26a0\ufe0f **PARTIALLY TESTED**  \n**Supported Formats**: .tar, .tar.gz, .tar.bz2, .zip\n\n**Known Issues**:\n- \u26a0\ufe0f Large archives may hang (BUG-002)\n- \u26a0\ufe0f Many small files may be slow\n\n**Note**: Not extensively tested with archive loading performance\n\n---\n\n## \ud83c\udfae GUI Feature Combinations\n\n### GUI + Background Training\n**Status**: \u26a0\ufe0f **UNTESTED**  \n**Expected**: GUI should remain responsive during training\n\n**Note**: Not extensively tested with GUI responsiveness during training\n\n---\n\n### GUI + Multi-GPU\n**Status**: \u2753 **EXPERIMENTAL**  \n**Question**: Does GUI support multi-GPU configuration?\n\n**Note**: Not extensively tested with GUI multi-GPU controls\n\n---\n\n### GUI + Long Training\nStatus varies by machine. For multi\u2011day runs, prefer CLI logging to `--log-file` and view metrics separately.\n\n---\n\n## \ud83e\uddea Testing Recommendations\n\n### High Priority Tests:\n\n1. **DDP Verification** (3 tests)\n   - DDP + basic training\n   - DDP + memory optimizations\n   - DDP + MoE\n\n2. **DeepSpeed Verification** (3 tests)\n   - ZeRO-1 basic\n   - ZeRO-2 with AMP\n   - ZeRO-3 maximum reduction\n\n3. **Chunking Verification** (3 tests)\n   - Verify implementation exists\n   - Test 8K context\n   - Test 16K context\n\n4. **Tokenizer Testing** (7 tests)\n   - Test each \"supported\" tokenizer\n\n5. **MoE Combinations** (3 tests)\n   - MoE + memory opts\n   - MoE + multi-GPU\n   - MoE + long context\n\n### Medium Priority Tests:\n\n1. **Long Context** (3 tests)\n   - 2K, 4K, 8K without chunking\n   - Measure actual limits\n\n2. **Dataset Formats** (3 tests)\n   - Large CSV\n   - Large archive\n   - Many small files\n\n3. **Feature Interactions** (5 tests)\n   - All memory opts combined\n   - Multi-GPU + all opts\n   - MoE + all opts\n\n### Low Priority Tests:\n\n1. **GUI** (3 tests)\n   - Long training responsiveness\n   - Multi-GPU controls\n   - All panels working\n\n2. **Edge Cases** (5 tests)\n   - Very small models\n   - Very large models\n   - Very long contexts\n   - Very large batches\n   - Very small batches\n\n---\n\n## \ud83d\udccb Compatibility Matrix\n\n### Quick Reference Table\n\n| Feature 1 | Feature 2 | Status | Notes |\n|-----------|-----------|--------|-------|\n| Gradient Checkpointing | AMP | \u2705 Verified | ~60\u201370% memory reduction |\n| Gradient Checkpointing | 8\u2011bit Optimizer | \u2705 Supported | Requires bitsandbytes + CUDA |\n| AMP | 8\u2011bit Optimizer | \u2705 Supported | Common combo |\n| All Memory Opts | Combined | \u26a0\ufe0f Partial | Chunking + AMP + Checkpointing + 8\u2011bit supported; tune chunk size |\n| DDP (Linux) | Gradient Checkpointing | \u2705 Supported | Use `--ddp` + `--world-size` |\n| DDP (Linux) | AMP | \u2705 Supported | |\n| DDP (Linux) | 8\u2011bit Optimizer | \u2753 Unknown | May conflict with BnB; test on your setup |\n| Parallel\u2011Independent (Windows) | Chunking | \u2705 Supported | Windows\u2011friendly multi\u2011GPU |\n| DeepSpeed (Linux) | Gradient Checkpointing | \u2705 Supported | Requires DeepSpeed install |\n| DeepSpeed (Linux) | AMP | \u2705 Supported | |\n| DeepSpeed (Linux) | 8\u2011bit Optimizer | \u2753 Unknown | DeepSpeed optimizer mgmt may conflict |\n| MoE | Memory Opts | \u2705 Supported | Start conservative: k=2, capacity 1.25 |\n| MoE | DDP/DeepSpeed | \u2753 Needs Verify | Routing/load\u2011balance interactions |\n| Chunking | Long Context | \u2705 Supported | Use 1024\u20132048 chunk sizes |\n| FlashAttention (Linux) | AMP | \u2705 Supported | When FA2 installed; falls back to SDPA otherwise |\n| FlashAttention (Windows) | Any | \u26a0\ufe0f Platform | Often unavailable; rely on SDPA + window\u2011size |\n\n---\n\n## \ud83c\udfaf Action Items\n\n### Immediate (Week 1):\n1. \u2705 Document all known combinations\n2. \u23f3 Verify DDP implementation\n3. \u23f3 Verify DeepSpeed implementation\n4. \u23f3 Verify chunking implementation\n\n### Short-term (Week 2-3):\n1. Test all memory optimization combinations\n2. Test DDP with various configurations\n3. Test DeepSpeed stages\n4. Test tokenizers\n\n### Medium-term (Week 4-6):\n1. Test MoE combinations\n2. Test long context scenarios\n3. Test dataset formats\n4. Create automated combination tests\n\n### Long-term (Month 2+):\n1. Create CI/CD for combination testing\n2. Add performance benchmarks\n3. Document optimal combinations for different use cases\n4. Create combination recommendation tool\n\n---\n\n## \ud83d\udcda Related Documents\n\n- [COMPLETE_FEATURE_INDEX.md](./COMPLETE_FEATURE_INDEX.md) \u2013 Complete feature list\n- [FLASH_ATTENTION.md](./FLASH_ATTENTION.md) \u2022 [FLASH_ATTENTION_VS_CHUNKING.md](./FLASH_ATTENTION_VS_CHUNKING.md)\n- [PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md](./PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md)\n- [MULTI_GPU_DISTRIBUTED.md](./MULTI_GPU_DISTRIBUTED.md)\n- [LORA_PEFT.md](./LORA_PEFT.md)\n- [DYNAMIC_SUBBRAINS_MOE.md](./DYNAMIC_SUBBRAINS_MOE.md)\n\n---\n\n**Matrix Version**: 1.0  \n**Last Updated**: October 18, 2025  \n**Maintained By**: Testing Team\n\n**Status**: \ud83d\udd04 In Progress - Many combinations need verification\n", "tags": ["datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Feature Combination Matrix - AI-OS"}, {"line": 10, "text": "\ud83d\udcca Status Legend"}, {"line": 22, "text": "\ud83d\udd2c Memory Optimization Combinations"}, {"line": 24, "text": "Gradient Checkpointing + AMP"}, {"line": 49, "text": "Gradient Checkpointing + AMP + 8-bit Optimizer"}, {"line": 79, "text": "Gradient Checkpointing + Long Context"}, {"line": 104, "text": "All Memory Optimizations Combined"}, {"line": 135, "text": "\ud83d\ude80 Multi-GPU Combinations"}, {"line": 137, "text": "DDP + Gradient Checkpointing"}, {"line": 162, "text": "DDP + AMP"}, {"line": 182, "text": "DDP + All Memory Optimizations"}, {"line": 213, "text": "\ud83e\udde0 DeepSpeed Combinations"}, {"line": 215, "text": "DeepSpeed ZeRO-1 + Gradient Checkpointing"}, {"line": 237, "text": "DeepSpeed ZeRO-2 + AMP"}, {"line": 256, "text": "DeepSpeed ZeRO-3 (Maximum Memory Reduction)"}, {"line": 276, "text": "DeepSpeed + 8-bit Optimizer"}, {"line": 297, "text": "\ud83e\udde9 MoE / Dynamic Subbrains Combinations"}, {"line": 299, "text": "MoE + Gradient Checkpointing"}, {"line": 319, "text": "MoE + AMP + 8-bit"}, {"line": 340, "text": "Expert Training + Memory Optimizations"}, {"line": 360, "text": "MoE + Multi-GPU"}, {"line": 386, "text": "MoE + DeepSpeed"}, {"line": 406, "text": "\ud83d\udcda Context Length Combinations"}, {"line": 408, "text": "Long Context + Chunking"}, {"line": 437, "text": "Long Context + Multi-GPU"}, {"line": 459, "text": "FlashAttention + Memory/Chunking"}, {"line": 468, "text": "\ud83d\udd24 Tokenizer Combinations"}, {"line": 470, "text": "Custom Tokenizer + Training"}, {"line": 490, "text": "Large Vocabulary + Memory Optimizations"}, {"line": 514, "text": "\ud83d\udcca Dataset Format Combinations"}, {"line": 516, "text": "Streaming Dataset + Linear Mode"}, {"line": 530, "text": "Large Dataset + Multi-GPU"}, {"line": 554, "text": "Archive Dataset + Training"}, {"line": 566, "text": "\ud83c\udfae GUI Feature Combinations"}, {"line": 568, "text": "GUI + Background Training"}, {"line": 576, "text": "GUI + Multi-GPU"}, {"line": 584, "text": "GUI + Long Training"}, {"line": 589, "text": "\ud83e\uddea Testing Recommendations"}, {"line": 591, "text": "High Priority Tests:"}, {"line": 616, "text": "Medium Priority Tests:"}, {"line": 632, "text": "Low Priority Tests:"}, {"line": 648, "text": "\ud83d\udccb Compatibility Matrix"}, {"line": 650, "text": "Quick Reference Table"}, {"line": 673, "text": "\ud83c\udfaf Action Items"}, {"line": 675, "text": "Immediate (Week 1):"}, {"line": 681, "text": "Short-term (Week 2-3):"}, {"line": 687, "text": "Medium-term (Week 4-6):"}, {"line": 693, "text": "Long-term (Month 2+):"}, {"line": 701, "text": "\ud83d\udcda Related Documents"}]}, {"path": "guide/features/FLASH_ATTENTION.md", "content": "# Flash Attention 2: Window Size Guide\n\nThis feature can be toggled via the training CLI or GUI:\n- CLI: enable optimized kernels (if available) and optionally set a sliding window:\n    ```powershell\n    aios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 10 --batch-size 1 --amp --gradient-checkpointing --window-size 2048 --log-file artifacts/brains/actv1/metrics.jsonl\n    ```\n    Note: FA2 usage is environment-dependent; when unavailable, PyTorch SDPA is used as a fallback. Windowing works with FA2 or SDPA.\n- GUI: \u201cFlashAttn-2\u201d checkbox and \u201cWindow Size\u201d field (see GUI Features \u2192 Training panel optimizations)\n\nThis page complements the canonical attention-optimization feature doc:\n- Canonical: [FLASH_ATTENTION_VS_CHUNKING.md](FLASH_ATTENTION_VS_CHUNKING.md)\n\n## What is Window Size?\n\n**Window size** is NOT about enabling Flash Attention - it's about **limiting attention range** using a sliding window.\n\n### Sliding Window Attention\n\nInstead of each token attending to ALL previous tokens (full attention), it only attends to the N most recent tokens.\n\n```\nFull Attention (window_size = None or 0):\nToken 1000 can attend to: Token 1, 2, 3, ..., 999, 1000 (all 1000 tokens)\n\nSliding Window (window_size = 512):\nToken 1000 can attend to: Token 488, 489, ..., 999, 1000 (only 512 tokens)\n```\n\n## Why Use Sliding Window?\n\n### Benefits\n\u2705 **Reduced memory** - Less attention computation\n\u2705 **Faster training** - Fewer attention scores to compute\n\u2705 **Enables longer contexts** - Can fit more tokens in VRAM\n\u2705 **Local coherence** - Most relevant context is usually recent\n\n### Trade-offs\n\u274c **Limited long-range attention** - Can't see tokens outside window\n\u274c **May lose important context** - Earlier information might be needed\n\u274c **Not suitable for all tasks** - Some tasks need full context\n\n## Choosing the Right Window Size\n\n### Decision Matrix\n\n| Context Length | Recommended Window | Reasoning |\n|----------------|-------------------|-----------|\n| **< 2K tokens** | `None` (full) | No need for windowing, fits easily |\n| **2K-8K tokens** | `None` or `2048` | Full attention works fine |\n| **8K-16K tokens** | `2048-4096` | Balance memory and context |\n| **16K-32K tokens** | `1024-2048` | Need windowing for efficiency |\n| **32K-64K tokens** | `512-1024` | Aggressive windowing needed |\n| **64K-100K tokens** | `256-512` | Very aggressive windowing |\n| **100K+ tokens** | `256` | Maximum memory savings |\n\n### Rule of Thumb\n\n```python\nif context_length < 8192:\n    window_size = None  # Full attention\nelif context_length < 32768:\n    window_size = 2048  # Moderate window\nelse:\n    window_size = 512   # Aggressive window\n```\n\n## Window Size vs Context Length\n\n**IMPORTANT**: Window size is NOT the same as max sequence length!\n\n```\nmax_seq_len = 50000    # How many tokens to train on\nwindow_size = 512      # How far each token can \"see\" back\n\nExample with 50K tokens:\n\u251c\u2500 Token 1:    Sees tokens 1 (only itself)\n\u251c\u2500 Token 100:  Sees tokens 1-100 (all previous, window not limiting yet)\n\u251c\u2500 Token 1000: Sees tokens 488-1000 (512 token window)\n\u2514\u2500 Token 50000: Sees tokens 49488-50000 (512 token window)\n```\n\n## Practical Examples\n\n### Example 1: Short Story (4K tokens)\n```yaml\nmax_seq_len: 4096\nwindow_size: None  # Full attention - story is short enough\nuse_flash_attn: True  # Enable Flash Attention for speed\n```\n**Result**: Each word can see the ENTIRE story\n\n### Example 2: Long Document (32K tokens)\n```yaml\nmax_seq_len: 32768\nwindow_size: 2048  # Sliding window - see ~2K tokens back\nuse_flash_attn: True  # Enable Flash Attention for efficiency\n```\n**Result**: Each word sees ~2K tokens of recent context\n\n### Example 3: Extreme Context (100K tokens)\n```yaml\nmax_seq_len: 100000\nwindow_size: 512   # Very limited window - memory constrained\nuse_flash_attn: True  # Enable Flash Attention\nuse_chunked_training: True  # Also enable chunking\nchunk_size: 2048   # Process in 2048-token chunks\n```\n**Result**: Each word sees only ~500 tokens back, processed in chunks\n\n## Window Size for Different Tasks\n\n### Full Attention (window_size = None)\n**Best for**:\n- Short contexts (< 8K tokens)\n- Tasks requiring global understanding\n- Document classification\n- Sentiment analysis\n\n### Medium Window (1024-4096)\n**Best for**:\n- Long documents (8K-32K tokens)\n- Story writing\n- Technical documentation\n- Most training scenarios\n\n### Small Window (256-512)\n**Best for**:\n- Extreme contexts (50K+ tokens)\n- Memory-constrained scenarios\n- Stream-of-consciousness text\n- Chat logs\n\n## How Flash Attention Uses Window Size\n\n### With Flash Attention 2 Enabled\n\n```python\nif window_size is not None:\n    # Flash Attention uses efficient sliding window\n    window = (window_size - 1, 0)  # Look back window_size-1 tokens\n    output = flash_attn_func(q, k, v, causal=True, window_size=window)\nelse:\n    # Flash Attention with full attention\n    output = flash_attn_func(q, k, v, causal=True)\n```\n\n### Without Flash Attention (Fallback)\n\n```python\nif window_size is not None:\n    # PyTorch SDPA with manual mask (less efficient)\n    mask = create_sliding_window_mask(window_size)\n    output = scaled_dot_product_attention(q, k, v, attn_mask=mask)\nelse:\n    # PyTorch SDPA with full attention\n    output = scaled_dot_product_attention(q, k, v, is_causal=True)\n```\n\n**Flash Attention is MORE EFFICIENT at sliding windows** - another reason to use it!\n\n## Common Misconceptions\n\n### \u274c WRONG: \"Window size is how many tokens I can train on\"\n\u2705 CORRECT: Window size is how far back each token can attend. You can train on 100K tokens with a 512 window.\n\n### \u274c WRONG: \"Larger window always better\"\n\u2705 CORRECT: Larger window uses more memory. Choose based on what your task needs and memory allows.\n\n### \u274c WRONG: \"Window size enables Flash Attention\"\n\u2705 CORRECT: Window size is a parameter TO Flash Attention. The checkbox enables it, window size configures it.\n\n### \u274c WRONG: \"I need window_size = max_seq_len\"\n\u2705 CORRECT: That's just full attention. Use `window_size = None` instead.\n\n## Testing Window Sizes\n\n### Start Conservative\n1. Begin with **no window** (full attention) for short contexts\n2. If OOM, enable window at **max_seq_len / 4**\n3. Gradually reduce window if still OOM\n4. Monitor training quality - smaller windows may reduce accuracy\n\n### Monitor Impact\n```python\n# Log attention range\neffective_context = min(window_size or max_seq_len, max_seq_len)\nprint(f\"Each token attends to {effective_context} previous tokens\")\n```\n\n## GUI Settings\n\n### Flash Attention Checkbox\n- **Checked**: Use Flash Attention 2 (if available)\n- **Unchecked**: Use PyTorch SDPA fallback\n\n### Window Size Entry\n- **Empty or 0**: Full attention (no window)\n- **256-8192**: Sliding window size in tokens\n- **Default: 512**: Good balance for long contexts\n\n### Recommended Combinations\n\n```\nShort context (< 8K):\n\u2611 FlashAttn-2  Window: [    ] (empty/full attention)\n\nMedium context (8K-32K):\n\u2611 FlashAttn-2  Window: [2048]\n\nLong context (32K-64K):\n\u2611 FlashAttn-2  Window: [1024]\n\u2611 Context Chunking  Chunk Size: [4096]\n\nExtreme context (100K+):\n\u2611 FlashAttn-2  Window: [512]\n\u2611 Context Chunking  Chunk Size: [2048]\n```\n\n## Performance Impact\n\n### Memory Usage (50K token sequence)\n\n| Configuration | VRAM Usage | Speed |\n|--------------|------------|-------|\n| Full Attn, No Flash | ~20GB \u274c | Baseline |\n| Full Attn + Flash | ~4GB \u2705 | +30% faster |\n| Window 2048 + Flash | ~2GB \u2705 | +50% faster |\n| Window 512 + Flash | ~1GB \u2705 | +80% faster |\n\n### Accuracy Impact\n\n```\nWindow Size vs Task Performance:\n- Full attention: 100% baseline accuracy\n- Window 4096: 99-100% (minimal impact)\n- Window 2048: 95-99% (slight impact on long-range tasks)\n- Window 512: 90-95% (noticeable for tasks needing full context)\n- Window 256: 85-90% (significant for most tasks)\n```\n\n## Summary\n\n| Parameter | Purpose | Values | Default |\n|-----------|---------|--------|---------|\n| **use_flash_attn** | Enable Flash Attention | True/False | Should be True (GUI checkbox) |\n| **window_size** | Sliding window size | None or 256-8192 | 512 |\n| **max_seq_len** | Total sequence length | Any | 2048 |\n\n**Key Insight**: Window size is about **local vs global attention**, not about enabling Flash Attention. The checkbox enables Flash Attention, the window size configures how it attends.\n\n**Recommendation**: \n- Enable Flash Attention checkbox (for speed)\n- Set window_size based on your context length and memory\n- Use \"Optimize Settings\" button to find optimal values\n", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Flash Attention 2: Window Size Guide"}, {"line": 13, "text": "What is Window Size?"}, {"line": 17, "text": "Sliding Window Attention"}, {"line": 29, "text": "Why Use Sliding Window?"}, {"line": 31, "text": "Benefits"}, {"line": 37, "text": "Trade-offs"}, {"line": 42, "text": "Choosing the Right Window Size"}, {"line": 44, "text": "Decision Matrix"}, {"line": 56, "text": "Rule of Thumb"}, {"line": 67, "text": "Window Size vs Context Length"}, {"line": 82, "text": "Practical Examples"}, {"line": 84, "text": "Example 1: Short Story (4K tokens)"}, {"line": 92, "text": "Example 2: Long Document (32K tokens)"}, {"line": 100, "text": "Example 3: Extreme Context (100K tokens)"}, {"line": 110, "text": "Window Size for Different Tasks"}, {"line": 112, "text": "Full Attention (window_size = None)"}, {"line": 119, "text": "Medium Window (1024-4096)"}, {"line": 126, "text": "Small Window (256-512)"}, {"line": 133, "text": "How Flash Attention Uses Window Size"}, {"line": 135, "text": "With Flash Attention 2 Enabled"}, {"line": 139, "text": "Flash Attention uses efficient sliding window"}, {"line": 143, "text": "Flash Attention with full attention"}, {"line": 147, "text": "Without Flash Attention (Fallback)"}, {"line": 151, "text": "PyTorch SDPA with manual mask (less efficient)"}, {"line": 155, "text": "PyTorch SDPA with full attention"}, {"line": 161, "text": "Common Misconceptions"}, {"line": 163, "text": "\u274c WRONG: \"Window size is how many tokens I can train on\""}, {"line": 166, "text": "\u274c WRONG: \"Larger window always better\""}, {"line": 169, "text": "\u274c WRONG: \"Window size enables Flash Attention\""}, {"line": 172, "text": "\u274c WRONG: \"I need window_size = max_seq_len\""}, {"line": 175, "text": "Testing Window Sizes"}, {"line": 177, "text": "Start Conservative"}, {"line": 183, "text": "Monitor Impact"}, {"line": 185, "text": "Log attention range"}, {"line": 190, "text": "GUI Settings"}, {"line": 192, "text": "Flash Attention Checkbox"}, {"line": 196, "text": "Window Size Entry"}, {"line": 201, "text": "Recommended Combinations"}, {"line": 219, "text": "Performance Impact"}, {"line": 221, "text": "Memory Usage (50K token sequence)"}, {"line": 230, "text": "Accuracy Impact"}, {"line": 241, "text": "Summary"}]}, {"path": "guide/features/FLASH_ATTENTION_VS_CHUNKING.md", "content": "# Flash Attention 2 vs Context Chunking: Technical Deep Dive\n\nNote: Canonical source of truth for attention optimization in AI-OS. Window sizing guidance is a sub-topic; see `FLASH_ATTENTION.md`.\n\n## Executive Summary\n\n**Flash Attention 2** and **Context Chunking** solve different problems in the memory hierarchy:\n- **Flash Attention 2**: Optimizes the *attention computation algorithm itself* (compute-level optimization)\n- **Context Chunking**: Manages *sequence length* when even optimized attention can't fit in VRAM (data-level optimization)\n\nThey're complementary because Flash Attention 2 makes each chunk more efficient, and chunking makes Flash Attention 2 viable for extreme contexts.\n\n---\n\n## The Memory Problem: Understanding the Layers\n\n### Standard Attention Memory Usage\n\nFor a sequence of length `N` with `d` dimensions:\n\n```\nStandard Attention Memory = O(N\u00b2)\n```\n\n**Example: 50K token sequence**\n- Attention matrix: 50,000 \u00d7 50,000 = 2.5 billion entries\n- At fp16 (2 bytes): **5GB just for attention scores**\n- Plus gradients, activations, KV cache: **~20GB total**\n\nThis is why long contexts OOM (Out Of Memory).\n\n---\n\n## Solution 1: Flash Attention 2 (Algorithm Optimization)\n\n### What It Does\n\nFlash Attention 2 **changes how attention is computed** to avoid materializing the full N\u00d7N matrix in VRAM.\n\n### Key Innovation: Tiling + Recomputation\n\nInstead of computing the full attention matrix:\n1. **Tiles attention into blocks** (e.g., 128\u00d7128 chunks)\n2. **Streams through HBM** (High Bandwidth Memory) efficiently\n3. **Recomputes values** instead of storing intermediate results\n4. **Fuses operations** to minimize memory reads/writes\n\n```python\n# Standard Attention (simplified)\nQ, K, V = input.chunk(3)\nscores = Q @ K.T  # N\u00d7N matrix materialized in VRAM \u274c\nattn = softmax(scores)\noutput = attn @ V\n\n# Flash Attention 2 (simplified concept)\noutput = flash_attn_func(Q, K, V)  # Never materializes N\u00d7N \u2705\n# Internally uses tiling: processes 128\u00d7128 blocks at a time\n```\n\n### Memory Reduction\n\n```\nFlash Attention 2 Memory = O(N)\n```\n\n**Same 50K example:**\n- No full attention matrix stored\n- Memory: **~2-4GB** instead of 20GB\n- Can handle **2-3x longer contexts** with same VRAM\n\n### What It DOESN'T Solve\n\nFlash Attention 2 still needs to:\n- Store full input sequence (50K tokens \u00d7 embedding dimension)\n- Store full gradients during backprop\n- Store model activations for each token\n\n**Limit: ~16K-32K tokens on consumer GPUs (24GB VRAM)**\n\n---\n\n## Solution 2: Context Chunking (Sequence Management)\n\n### What It Does\n\nContext Chunking **splits the sequence into smaller pieces** and processes them separately.\n\n### How It Works\n\n```python\n# Without Chunking (standard)\ninput_tokens = [1, 2, 3, 4, ..., 50000]  # All at once\noutput = model(input_tokens)  # OOM! \u274c\n\n# With Chunking\nchunk_1 = [1, 2, ..., 2048]      # Process first 2048 tokens\noutput_1, hidden_state_1 = model(chunk_1)\n\nchunk_2 = [2049, 2050, ..., 4096]  # Process next 2048 tokens\noutput_2, hidden_state_2 = model(chunk_2, carry=hidden_state_1)\n\n# Continue for all chunks...\n```\n\n### Key Features\n\n1. **Recurrent State Passing**: Hidden states carry context between chunks\n2. **Gradient Accumulation**: Gradients accumulated across chunks\n3. **CPU Offloading**: Can move carry states to CPU for extreme contexts (100K+)\n\n### Memory Reduction\n\n```\nChunked Training Memory = O(chunk_size)\n```\n\n**Same 50K example with 2048 chunk size:**\n- Only 2048 tokens in VRAM at once\n- Rest stored in RAM or on disk\n- Memory: **~1-2GB** (very conservative)\n\n### What It DOESN'T Solve\n\n- **Slower training**: Processing chunks sequentially has overhead (~10-20%)\n- **Reduced parallelism**: Can't process all tokens in parallel\n- **Potential context loss**: Chunks may have limited view of full context\n\n---\n\n## Why They're Complementary\n\n### Scenario 1: Medium Context (8K-16K tokens)\n\n**Flash Attention 2 ONLY:**\n```\nSequence: 16K tokens\nFlash Attention: 16K fits comfortably in VRAM \u2705\nChunking: NOT NEEDED\nResult: Fast, efficient training\n```\n\n### Scenario 2: Large Context (32K-64K tokens)\n\n**Flash Attention 2 + Chunking:**\n```\nSequence: 64K tokens\nFlash Attention: Each chunk processed efficiently\nChunking: 64K \u00f7 2048 = 32 chunks\nResult: Feasible on 24GB GPU\n```\n\nWithout Flash Attention:\n- Standard attention on 2048-token chunks would still use more memory\n- Training would be even slower\n\n### Scenario 3: Extreme Context (100K+ tokens)\n\n**Flash Attention 2 + Aggressive Chunking + CPU Offload:**\n```\nSequence: 100K tokens\nFlash Attention: Optimizes each 512-token chunk\nChunking: 100K \u00f7 512 = 195 chunks\nCPU Offload: Carry states stored in RAM\nResult: Possible on consumer hardware (slow but works)\n```\n\n---\n\n## Visual Comparison\n\n### Memory Usage Hierarchy\n\n```\nSame 50K Token Sequence:\n\nStandard Attention (NO CHUNKING)\n\u251c\u2500 Attention Matrix: 5GB\n\u251c\u2500 Activations: 8GB\n\u251c\u2500 Gradients: 7GB\n\u2514\u2500 Total: ~20GB \u274c OOM on 24GB GPU\n\nFlash Attention 2 (NO CHUNKING)\n\u251c\u2500 No Attention Matrix: 0GB (tiled)\n\u251c\u2500 Activations: 2GB (optimized)\n\u251c\u2500 Gradients: 2GB (optimized)\n\u2514\u2500 Total: ~4GB \u2705 Fits, but limited headroom\n\nStandard Attention + CHUNKING (2048 chunks)\n\u251c\u2500 Attention Matrix: 200MB (per chunk)\n\u251c\u2500 Activations: 400MB (per chunk)\n\u251c\u2500 Gradients: 300MB (per chunk)\n\u2514\u2500 Total: ~1GB \u2705 Fits, but SLOW\n\nFlash Attention 2 + CHUNKING (2048 chunks)\n\u251c\u2500 No Attention Matrix: 0GB\n\u251c\u2500 Activations: 80MB (per chunk, optimized)\n\u251c\u2500 Gradients: 60MB (per chunk, optimized)\n\u2514\u2500 Total: ~200MB \u2705 Fits, FASTER than standard chunking\n```\n\n---\n\n## Practical Decision Matrix\n\n| Context Length | GPU VRAM | Recommendation | Why |\n|----------------|----------|----------------|-----|\n| **< 4K tokens** | Any | Flash Attention 2 | No chunking needed, maximum speed |\n| **4K-16K tokens** | 24GB+ | Flash Attention 2 | Fits comfortably without chunking |\n| **4K-16K tokens** | 12GB | Flash Attention 2 + Optional Chunking | Test first; chunk if needed |\n| **16K-32K tokens** | 24GB+ | Flash Attention 2 + Light Chunking (4096) | Balance speed and memory |\n| **32K-64K tokens** | 24GB | Flash Attention 2 + Chunking (2048) | Flash Attention makes chunks efficient |\n| **64K-100K tokens** | 24GB | Flash Attention 2 + Aggressive Chunking (512-1024) | Extreme context requires both |\n| **100K+ tokens** | 24GB | Flash Attention 2 + Ultra-Aggressive Chunking (256-512) + CPU Offload | Maximum memory savings |\n\n---\n\n## Code Example: How They Work Together\n\n```python\n# In aios/core/hrm_models/impl/layers.py\nclass HRMAttention:\n    def forward(self, hidden_states):\n        # Flash Attention 2 optimizes THIS computation\n        try:\n            from flash_attn import flash_attn_func\n            # Even with chunking, each chunk uses Flash Attention\n            attn_output = flash_attn_func(q, k, v, causal=True)\n        except:\n            # Fallback to standard attention (slower)\n            attn_output = F.scaled_dot_product_attention(q, k, v)\n        \n        return attn_output\n\n# In aios/core/hrm_models/chunked_training.py\ndef chunked_segment_rollout(model, batch, chunk_size=2048):\n    full_sequence = batch['input_ids']  # e.g., 50K tokens\n    \n    # Split into chunks\n    for chunk_start in range(0, len(full_sequence), chunk_size):\n        chunk = full_sequence[chunk_start:chunk_start + chunk_size]\n        \n        # Each chunk STILL uses Flash Attention inside the model!\n        output, carry_state = model(chunk, carry_state=prev_carry)\n        \n        # Flash Attention makes THIS step 2-3x faster\n        loss.backward()  # Gradient computation\n        \n    return total_loss\n```\n\n---\n\n## Key Insight: Different Problems, Different Solutions\n\n### Flash Attention 2 Solves:\n\u274c **Problem**: Attention computation is memory-inefficient (O(N\u00b2))  \n\u2705 **Solution**: Smarter algorithm that avoids materializing full matrix (O(N))  \n\ud83d\udcca **Impact**: 2-3x longer contexts with same VRAM  \n\u26a1 **Speed**: Actually FASTER than standard attention  \n\n### Context Chunking Solves:\n\u274c **Problem**: Even optimized attention can't fit 100K tokens in VRAM  \n\u2705 **Solution**: Process sequence in smaller pieces  \n\ud83d\udcca **Impact**: Unlimited context length (constrained by time, not memory)  \n\u26a1 **Speed**: 10-20% slower due to sequential processing  \n\n---\n\n## Analogy: Moving a Mountain\n\n**Problem**: Move 100 tons of rocks from point A to B\n\n**Flash Attention 2 = Better Truck**\n- Upgraded from 1-ton truck to 3-ton truck\n- Each trip carries 3x more rocks\n- Same number of trips needed, but faster per trip\n- **Benefit**: Can move 3x more rocks in same time\n\n**Context Chunking = Multiple Trips**\n- Split 100 tons into 20 trips of 5 tons each\n- Make multiple trips back and forth\n- **Benefit**: Can move ANY amount (not limited by truck size)\n\n**Both Together = Best Solution**\n- Use the 3-ton truck (Flash Attention)\n- Make fewer trips (Chunking with larger chunks)\n- Result: Move 100 tons efficiently\n\n---\n\n## When to Use What\n\n### \u2705 Use Flash Attention 2 Alone\n- Context \u2264 16K tokens on 24GB GPU\n- You want maximum training speed\n- You have compatible CUDA GPU\n\n### \u2705 Use Chunking Alone (Rare)\n- Very old GPU without Flash Attention support\n- CPU-only training (Flash Attention requires CUDA)\n- Debugging/testing with small contexts\n\n### \u2705 Use Both Together (Common for Large Contexts)\n- Context > 16K tokens\n- Training on consumer GPUs (8-24GB VRAM)\n- Want balance of memory efficiency and speed\n- Extreme contexts (50K-100K+ tokens)\n\n### \u274c Use Neither (Default for Short Contexts)\n- Context \u2264 2K tokens\n- Plenty of VRAM available\n- Standard attention works fine\n\n---\n\n## Summary Table\n\n| Feature | Flash Attention 2 | Context Chunking |\n|---------|------------------|------------------|\n| **Optimization Level** | Algorithm/Compute | Data/Sequence |\n| **Memory Complexity** | O(N) from O(N\u00b2) | O(chunk_size) |\n| **Speed Impact** | Faster (+20-30%) | Slower (-10-20%) |\n| **Max Context Gain** | 2-3x | Unlimited |\n| **Hardware Requirement** | CUDA GPU | Any |\n| **When Needed** | Always beneficial | Only for very long contexts |\n| **Typical Use** | 4K-32K contexts | 32K-100K+ contexts |\n\n---\n\n## Bottom Line\n\n**Flash Attention 2** makes attention computation efficient.  \n**Context Chunking** makes extremely long sequences feasible.\n\nTogether, they enable training with **50K-100K token contexts on consumer GPUs** that would otherwise require data center hardware.\n\n**Your system now gives users full control** - they can enable chunking when needed, and Flash Attention 2 automatically optimizes whatever they choose to do. The \"Optimize Settings\" button helps find the sweet spot between memory and speed.\n\n---\n\n## How to switch between approaches (CLI)\n\n- Full attention (no window, no explicit chunking flag):\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 50 --batch-size 2 --amp --gradient-checkpointing --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n- Sliding window attention (works with FA2 or SDPA):\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 50 --batch-size 2 --amp --gradient-checkpointing --window-size 2048 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n- Long-context with dataset chunk cadence:\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 50 --batch-size 1 --amp --gradient-checkpointing --dataset-chunk-size 4000 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nNotes:\n- Windowed attention reduces attention range; dataset chunk size controls data loading/encoding cadence and memory pressure.\n- The training loop may also use internal chunking for long sequences; see Configurable Dataset Chunk Size and Parallel Training Block/Chunk System.\n\n## Measuring impact (Optimization CLI)\n\nUse the optimization CLI to gather throughput/VRAM metrics and write results under `artifacts/optimization/`.\n\n```powershell\naios optimize --model artifacts/hf_implant/base_model --batch-sizes \"1,2,4,8\" --test-duration 10 --output-dir artifacts/optimization\n```\n\nOutputs include JSON like `artifacts/optimization/results_<session>.json` and GPU metrics JSONL; compare runs with/without windowing or different dataset chunk sizes.\n", "tags": ["gui"], "headings": [{"line": 0, "text": "Flash Attention 2 vs Context Chunking: Technical Deep Dive"}, {"line": 4, "text": "Executive Summary"}, {"line": 14, "text": "The Memory Problem: Understanding the Layers"}, {"line": 16, "text": "Standard Attention Memory Usage"}, {"line": 33, "text": "Solution 1: Flash Attention 2 (Algorithm Optimization)"}, {"line": 35, "text": "What It Does"}, {"line": 39, "text": "Key Innovation: Tiling + Recomputation"}, {"line": 48, "text": "Standard Attention (simplified)"}, {"line": 54, "text": "Flash Attention 2 (simplified concept)"}, {"line": 56, "text": "Internally uses tiling: processes 128\u00d7128 blocks at a time"}, {"line": 59, "text": "Memory Reduction"}, {"line": 70, "text": "What It DOESN'T Solve"}, {"line": 81, "text": "Solution 2: Context Chunking (Sequence Management)"}, {"line": 83, "text": "What It Does"}, {"line": 87, "text": "How It Works"}, {"line": 90, "text": "Without Chunking (standard)"}, {"line": 94, "text": "With Chunking"}, {"line": 101, "text": "Continue for all chunks..."}, {"line": 104, "text": "Key Features"}, {"line": 110, "text": "Memory Reduction"}, {"line": 121, "text": "What It DOESN'T Solve"}, {"line": 129, "text": "Why They're Complementary"}, {"line": 131, "text": "Scenario 1: Medium Context (8K-16K tokens)"}, {"line": 141, "text": "Scenario 2: Large Context (32K-64K tokens)"}, {"line": 155, "text": "Scenario 3: Extreme Context (100K+ tokens)"}, {"line": 168, "text": "Visual Comparison"}, {"line": 170, "text": "Memory Usage Hierarchy"}, {"line": 202, "text": "Practical Decision Matrix"}, {"line": 216, "text": "Code Example: How They Work Together"}, {"line": 219, "text": "In aios/core/hrm_models/impl/layers.py"}, {"line": 222, "text": "Flash Attention 2 optimizes THIS computation"}, {"line": 225, "text": "Even with chunking, each chunk uses Flash Attention"}, {"line": 228, "text": "Fallback to standard attention (slower)"}, {"line": 233, "text": "In aios/core/hrm_models/chunked_training.py"}, {"line": 237, "text": "Split into chunks"}, {"line": 241, "text": "Each chunk STILL uses Flash Attention inside the model!"}, {"line": 244, "text": "Flash Attention makes THIS step 2-3x faster"}, {"line": 252, "text": "Key Insight: Different Problems, Different Solutions"}, {"line": 254, "text": "Flash Attention 2 Solves:"}, {"line": 260, "text": "Context Chunking Solves:"}, {"line": 268, "text": "Analogy: Moving a Mountain"}, {"line": 290, "text": "When to Use What"}, {"line": 292, "text": "\u2705 Use Flash Attention 2 Alone"}, {"line": 297, "text": "\u2705 Use Chunking Alone (Rare)"}, {"line": 302, "text": "\u2705 Use Both Together (Common for Large Contexts)"}, {"line": 308, "text": "\u274c Use Neither (Default for Short Contexts)"}, {"line": 315, "text": "Summary Table"}, {"line": 329, "text": "Bottom Line"}, {"line": 340, "text": "How to switch between approaches (CLI)"}, {"line": 361, "text": "Measuring impact (Optimization CLI)"}]}, {"path": "guide/features/GUI_FEATURES.md", "content": "# GUI Features (Desktop)\n\n**Last Updated**: December 12, 2025  \n**Purpose**: Visual management for training, brains, chat, datasets, resources, and early Subbrains (experts) administration.\n\n> **Note**: Screenshots are planned for a future documentation update. For now, launch `aios gui` to explore the interface.\n\n**Status**: Implemented core panels. Subbrains Manager is WIP (view and refresh registry, actions are placeholders).\n\n## Key Modules\n\n- App bootstrap and panel wiring: `src/aios/gui/app/app_main.py`, `src/aios/gui/app/panel_setup.py`, `src/aios/gui/app/logging_setup.py`\n- Services: `src/aios/gui/services/__init__.py`, `router.py`, `log_router.py`\n- Panels:\n  - HRM Training: `src/aios/gui/components/hrm_training_panel/`\n  - Brains: `src/aios/gui/components/brains_panel/`\n  - Rich Chat: `src/aios/gui/components/rich_chat_panel/`\n  - Datasets: `src/aios/gui/components/datasets_panel/`\n  - Subbrains Manager (WIP): `src/aios/gui/components/subbrains_manager_panel/`\n  - Resources: `src/aios/gui/components/resources_panel/`\n  - Settings/Status: `src/aios/gui/components/*`\n\n## Related Documentation\n\n- Core training: [CORE_TRAINING.md](CORE_TRAINING.md)\n- Dynamic Subbrains (MoE): [DYNAMIC_SUBBRAINS_MOE.md](DYNAMIC_SUBBRAINS_MOE.md)\n- Memory optimization: [MEMORY_OPTIMIZATION.md](MEMORY_OPTIMIZATION.md)\n- Multi-GPU and parallel chunk system: [MULTI_GPU_DISTRIBUTED.md](MULTI_GPU_DISTRIBUTED.md), [PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md](PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md)\n- GUI-specific multi-GPU behaviour: [MULTIGPU_GUI_GUIDE.md](MULTIGPU_GUI_GUIDE.md)\n\n---\n\n## Panels and Flows\n\n### HRM Training Panel\n- Create/train ACTv1 brains with presets, VRAM estimator, logs, and stop/resume.\n- MoE awareness in estimator (shows total vs active params).\n- **Location**: `src/aios/gui/components/hrm_training_panel/`\n\n**Tips (Windows)**:\n- Prefer \"Parallel Independent\" mode for multi-GPU on Windows (DDP is CUDA-only and more finicky). See [MULTI_GPU_DISTRIBUTED.md](MULTI_GPU_DISTRIBUTED.md).\n- Use Optimize to search max context/batch fitting VRAM.\n\n### Brains Panel\n- List brains (bundles), load/unload, view details, export/import.\n- Scans `artifacts/brains` and temporary \"router-generated\" brains.\n- **Location**: `src/aios/gui/components/brains_panel/`\n\n### Rich Chat Panel\n- Chat with loaded brain, Markdown and code highlighting.\n- Routing output formatter is shared service (`router.py`).\n- **Location**: `src/aios/gui/components/rich_chat_panel/`\n\n### Datasets Panel\n- Discover/download datasets, verify metadata, manage cache.\n- **Location**: `src/aios/gui/components/datasets_panel/`\n\n### Resources Panel\n- CPU/GPU utilization targets, throttle modes, real-time stats for training.\n- Multi-GPU usage caveats and troubleshooting: see [MULTIGPU_GUI_GUIDE.md](MULTIGPU_GUI_GUIDE.md)\n- **Location**: `src/aios/gui/components/resources_panel/`\n\n### Subbrains Manager (WIP)\n- View expert registry with counts (total/active/frozen), hierarchy, average routing weight, activations.\n- Refresh reads `artifacts/experts/registry.json`. Actions like create/delete/freeze and goal linking currently print \"CLI command needed\". Use CLI instead (see [DYNAMIC_SUBBRAINS_MOE.md](DYNAMIC_SUBBRAINS_MOE.md)).\n- **Location**: `src/aios/gui/components/subbrains_manager_panel/`\n\n### Debug Panel\n- Centralized debug output with categorized logs via LogRouter; helpful during training/inference.\n- **Location**: `src/aios/gui/app/logging_setup.py` registers the log router; debug UI integrated in app.\n\n### Evaluation Panel\n- Run evaluation suites and view results; compare runs.\n- **Location**: `src/aios/gui/components/evaluation_panel/` (if separated) or integrated evaluation view.\n\n### Tools & MCP Panel\n- Manage Tools and MCP servers; configure and observe status.\n- **Location**: `src/aios/gui/components/tools_panel/`, `src/aios/gui/components/mcp_panel/` (module names may vary by integration status)\n\n### Settings and Themes\n- Configure themes, fonts, logging level, cache, and other app settings.\n- **Location**: `src/aios/gui/components/settings_panel/`\n- **Available themes**: Light Mode, Dark Mode, Matrix Mode, Halloween Mode, Barbie Mode\n\n---\n\n## Running the GUI\n\nFrom a configured environment, start the app:\n\n```powershell\naios gui\n```\n\nIf you need to pin a specific Python interpreter, ensure `.venv` is activated first.\n\n---\n\n[Back to Feature Index](COMPLETE_FEATURE_INDEX.md) | [Back to Guide Index](../INDEX.MD)\n", "tags": ["datasets", "experts", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "GUI Features (Desktop)"}, {"line": 9, "text": "Key Modules"}, {"line": 22, "text": "Related Documentation"}, {"line": 32, "text": "Panels and Flows"}, {"line": 34, "text": "HRM Training Panel"}, {"line": 43, "text": "Brains Panel"}, {"line": 48, "text": "Rich Chat Panel"}, {"line": 53, "text": "Datasets Panel"}, {"line": 57, "text": "Resources Panel"}, {"line": 62, "text": "Subbrains Manager (WIP)"}, {"line": 67, "text": "Debug Panel"}, {"line": 71, "text": "Evaluation Panel"}, {"line": 75, "text": "Tools & MCP Panel"}, {"line": 79, "text": "Settings and Themes"}, {"line": 86, "text": "Running the GUI"}]}, {"path": "guide/features/LORA_PEFT.md", "content": "# LoRA/PEFT Comprehensive Analysis for AI-OS\n\nNote: Canonical source of truth for LoRA/PEFT in AI-OS. Other LoRA/PEFT docs in this folder have been consolidated into this page.\n\nQuick links:\n- Quick start presets: see Configuration Presets\n- Parameter impact overview: see PEFT Methods Comparison and Target Modules Explained\n- Troubleshooting and validation: see Testing & Validation\n\n**Date:** October 19, 2025  \n**System:** AI-OS HRM ACTv1 Training  \n**PEFT Library:** Hugging Face PEFT (v0.11.1+)\n\n---\n\n## Table of Contents\n1. [Overview](#overview)\n2. [What is PEFT?](#what-is-peft)\n3. [Implementation Details](#implementation-details)\n4. [Parameter Breakdown](#parameter-breakdown)\n5. [PEFT Methods Comparison](#peft-methods-comparison)\n6. [Target Modules Explained](#target-modules-explained)\n7. [Configuration Presets](#configuration-presets)\n8. [Memory & Performance Impact](#memory-performance-impact)\n9. [Best Practices](#best-practices)\n10. [Testing & Validation](#testing-validation)\n11. [Commands (CLI)](#commands-cli)\n12. [Inputs & Outputs](#inputs-outputs)\n13. [Try it (PowerShell)](#try-it-powershell)\n\n---\n\n## Overview\n\nParameter-Efficient Fine-Tuning (PEFT) in AI-OS allows training with **95-99% fewer trainable parameters** by using adapter techniques like LoRA. Instead of updating all 87M+ model parameters, PEFT adds small adapter layers (~500K-8M params) that achieve comparable or better results.\n\n**Key Benefits:**\n- \u2705 **Memory Reduction:** 40-60% less VRAM usage\n- \u2705 **Speed:** Faster training and convergence\n- \u2705 **Quality:** Comparable or better results than full fine-tuning\n- \u2705 **Flexibility:** Easy to merge adapters or switch between them\n- \u2705 **Compatibility:** Works with all other optimizations (gradient checkpointing, AMP, etc.)\n\n---\n\n## What is PEFT?\n\nPEFT techniques modify only a small subset of model parameters while keeping the base model frozen. This is achieved through:\n\n1. **Adapter Layers:** Small neural network modules inserted into the model\n2. **Low-Rank Decomposition:** Decomposing weight updates into smaller matrices\n3. **Selective Training:** Only training specific components (e.g., attention layers)\n\n### Why Use PEFT?\n\n| Scenario | Full Fine-Tuning | PEFT (LoRA) |\n|----------|------------------|-------------|\n| Parameters to train | 87M (100%) | 500K-8M (1-5%) |\n| VRAM Required (GPT-2 size) | 12-16 GB | 6-10 GB |\n| Training Speed | Baseline | 1.5-2\u00d7 faster |\n| Convergence | Requires more data | Often better with less data |\n| Risk of Catastrophic Forgetting | High | Low |\n| Storage per fine-tune | Full model (~350 MB) | Adapter only (~10-30 MB) |\n\n---\n\n## Implementation Details\n\n### Code Location\n- **Main Implementation:** `src/aios/cli/hrm_hf/model_precision.py` (`apply_peft()` function)\n- **Configuration:** `src/aios/core/hrm_training/training_config/advanced_fields.py`\n- **GUI Controls:** `src/aios/gui/components/hrm_training_panel/`\n\n### How It Works\n\n```python\n# From model_precision.py\ndef apply_peft(model, config, log_fn):\n    if not config.use_peft:\n        return model\n    \n    # 1. Parse target modules\n    target_modules_list = [m.strip() for m in config.lora_target_modules.split(',')]\n    \n    # 2. Create PEFT config\n    if config.peft_method == \"lora\":\n        peft_config = LoraConfig(\n            r=config.lora_r,                    # Rank\n            lora_alpha=config.lora_alpha,      # Scaling\n            lora_dropout=config.lora_dropout,  # Regularization\n            target_modules=target_modules_list,\n            task_type=TaskType.CAUSAL_LM,\n        )\n    # ... (adalora, ia3 methods also supported)\n    \n    # 3. Wrap model with PEFT\n    model = get_peft_model(model, peft_config)\n    \n    return model\n```\n\n### Integration Points\n1. **Training Pipeline:** Called in `train_actv1_impl()` after model creation\n2. **Memory Estimation:** Integrated into VRAM calculator in GUI\n3. **Checkpoint Saving:** PEFT adapters saved separately or merged\n4. **Inference:** Can load adapters dynamically\n\n---\n\n## Parameter Breakdown\n\n### 1. `use_peft` (Boolean)\n**Default:** `false`\n\n**Description:** Master switch to enable/disable PEFT.\n\n**When to Enable:**\n- \u2705 Limited VRAM (< 12 GB available)\n- \u2705 Want faster training iteration\n- \u2705 Fine-tuning for specific tasks\n- \u2705 Need to maintain multiple model variants\n\n**When to Disable:**\n- \u274c Full model capacity needed\n- \u274c Training from scratch (not fine-tuning)\n- \u274c Abundant VRAM available (24+ GB)\n\n---\n\n### 2. `peft_method` (String)\n**Default:** `\"lora\"`  \n**Options:** `lora`, `adalora`, `ia3`\n\n#### **LoRA (Low-Rank Adaptation)** \ud83c\udf1f *Recommended*\n- **Best for:** General purpose, most stable\n- **Params:** Configurable via `lora_r`\n- **Quality:** Excellent\n- **Speed:** Fast\n\n**How it works:** Adds low-rank matrices A and B to weight updates\n```\n\u0394W = B \u00d7 A (where B is d\u00d7r and A is r\u00d7k, r << d,k)\n```\n\n#### **AdaLoRA (Adaptive LoRA)**\n- **Best for:** Dynamic rank allocation\n- **Params:** Similar to LoRA\n- **Quality:** Potentially better than LoRA\n- **Speed:** Slightly slower (adaptive overhead)\n\n**How it works:** Dynamically adjusts rank across layers based on importance\n\n#### **IA3 (Infused Adapter)**\n- **Best for:** Minimal parameters (~100K)\n- **Params:** Fewest parameters\n- **Quality:** Good for specific tasks\n- **Speed:** Fastest\n\n**How it works:** Learns scaling vectors instead of full matrices\n\n---\n\n### 3. `lora_r` (Integer - Rank)\n**Default:** `16`  \n**Range:** `1-256` (practical: `4-64`)\n\n**Description:** The rank of the low-rank decomposition. Controls adapter capacity.\n\n**Impact on Model:**\n- **Higher rank** = More capacity, more parameters, more VRAM\n- **Lower rank** = Less capacity, fewer parameters, less VRAM\n\n**Parameter Count Formula:**\n```\nparams_per_layer = 2 \u00d7 rank \u00d7 layer_dimension\nFor GPT-2 (d=768), q_proj with r=16:\n  params = 2 \u00d7 16 \u00d7 768 = 24,576 params per layer\n```\n\n**Recommendations:**\n\n| Rank | Parameters | VRAM Impact | Use Case |\n|------|-----------|-------------|----------|\n| `r=4` | ~250K | +1 GB | Very simple fine-tuning |\n| `r=8` | ~500K | +1.5 GB | Minimal configuration |\n| `r=16` | ~2M | +2-3 GB | **Recommended default** |\n| `r=32` | ~8M | +4-5 GB | Complex tasks, high quality |\n| `r=64` | ~32M | +8-10 GB | Very complex, rarely needed |\n\n**Rule of Thumb:**\n- Start with `r=16`\n- Increase if model underfits\n- Decrease if VRAM limited or overfitting occurs\n\n---\n\n### 4. `lora_alpha` (Integer - Scaling)\n**Default:** `32`  \n**Range:** `1-1024` (practical: `8-128`)\n\n**Description:** Scaling parameter for LoRA adapter outputs.\n\n**Mathematical Impact:**\n```\neffective_adapter_contribution = (lora_alpha / lora_r) \u00d7 adapter_output\n```\n\n**Effective Learning Rate:**\n- Higher `alpha` relative to `r` = Stronger adapter influence\n- Lower `alpha` relative to `r` = More conservative adaptation\n\n**Recommendations:**\n\n| Configuration | Ratio | Use Case |\n|---------------|-------|----------|\n| `r=8, \u03b1=8` | 1:1 | Conservative, minimal changes |\n| `r=16, \u03b1=32` | 2:1 | **Standard (recommended)** |\n| `r=16, \u03b1=16` | 1:1 | More conservative |\n| `r=16, \u03b1=64` | 4:1 | Aggressive adaptation |\n| `r=32, \u03b1=64` | 2:1 | High capacity, standard scaling |\n\n**Best Practice:**\n- Use `lora_alpha = 2 \u00d7 lora_r` as starting point\n- Increase alpha if adapters aren't learning enough\n- Decrease alpha if training is unstable\n\n---\n\n### 5. `lora_dropout` (Float)\n**Default:** `0.05`  \n**Range:** `0.0-0.5` (practical: `0.0-0.2`)\n\n**Description:** Dropout probability applied to LoRA adapter layers for regularization.\n\n**Purpose:**\n- Prevent overfitting\n- Improve generalization\n- Add noise during training\n\n**Recommendations:**\n\n| Dropout | Regularization | Use Case |\n|---------|----------------|----------|\n| `0.0` | None | Large datasets (>100K samples) |\n| `0.05` | **Light (recommended)** | General purpose |\n| `0.1` | Medium | Medium datasets (10K-100K) |\n| `0.2-0.3` | High | Small datasets (<10K samples) |\n\n**When to Adjust:**\n- **Increase** if model overfits to training data\n- **Decrease** if model underfits or dataset is very large\n- **Set to 0** for maximum adapter capacity (stable datasets)\n\n---\n\n### 6. `lora_target_modules` (String - Comma-separated)\n**Default:** `\"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"`\n\n**Description:** Specifies which model layers should have LoRA adapters applied.\n\n**Available Modules in HRM ACTv1:**\n\n#### **Attention Modules** (Recommended)\n- `q_proj` - Query projection (attention keys)\n- `k_proj` - Key projection (attention values)\n- `v_proj` - Value projection (what gets attended to)\n- `o_proj` - Output projection (attention combination)\n\n#### **MLP/Feed-Forward Modules**\n- `gate_proj` - Gating mechanism\n- `up_proj` - Upward projection (expand)\n- `down_proj` - Downward projection (compress)\n\n#### **Always Trainable (Cannot be frozen)**\n- `lm_head` - Language model output head\n- `q_head` - HRM halting/pondering head\n\n---\n\n## Target Modules Explained\n\n### Preset Configurations\n\n#### **Minimal** (Recommended for VRAM < 8 GB)\n```\n\"q_proj,v_proj\"\n```\n- **Parameters:** ~500K-1M\n- **VRAM:** +1.5-2 GB\n- **Quality:** Good for most tasks\n- **Speed:** Fastest training\n- **Best for:** Limited hardware, quick iterations\n\n#### **Balanced** (Recommended Default)\n```\n\"q_proj,k_proj,v_proj,o_proj\"\n```\n- **Parameters:** ~2M-4M\n- **VRAM:** +2.5-4 GB\n- **Quality:** Very good\n- **Speed:** Fast\n- **Best for:** General fine-tuning, balanced quality/speed\n\n#### **Full** (Maximum Quality)\n```\n\"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n```\n- **Parameters:** ~6M-12M\n- **VRAM:** +4-6 GB\n- **Quality:** Best possible with PEFT\n- **Speed:** Moderate\n- **Best for:** Complex tasks, maximum quality needed\n\n### Module Impact Analysis\n\n| Module | Function | Impact on Performance | Training Cost |\n|--------|----------|----------------------|---------------|\n| `q_proj` | **Query generation** | \u2b50\u2b50\u2b50 High - Critical for attention | Low |\n| `k_proj` | **Key generation** | \u2b50\u2b50 Medium - Important for attention | Low |\n| `v_proj` | **Value generation** | \u2b50\u2b50\u2b50 High - What gets attended to | Low |\n| `o_proj` | **Attention output** | \u2b50\u2b50 Medium - Combines attention | Low |\n| `gate_proj` | **MLP gating** | \u2b50 Low-Medium - Controls information flow | Medium |\n| `up_proj` | **MLP expansion** | \u2b50 Low-Medium - Increases dimensionality | Medium |\n| `down_proj` | **MLP compression** | \u2b50 Low-Medium - Reduces dimensionality | Medium |\n\n**Key Insight:**\n- Attention modules (`q,k,v,o`) are most impactful per parameter\n- MLP modules add capacity but with diminishing returns\n- Always include `q_proj` and `v_proj` at minimum\n\n---\n\n## PEFT Methods Comparison\n\n### Detailed Comparison Table\n\n| Feature | LoRA | AdaLoRA | IA3 |\n|---------|------|---------|-----|\n| **Trainable Params** | 0.5M-8M | 0.5M-8M | 50K-500K |\n| **Memory Overhead** | +2-4 GB | +2.5-5 GB | +1-2 GB |\n| **Training Speed** | Fast | Medium | Fastest |\n| **Quality** | Excellent | Excellent+ | Good |\n| **Stability** | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50 |\n| **Complexity** | Low | Medium | Low |\n| **Recommended For** | **General use** | Research, optimal quality | Extreme efficiency |\n| **Hyperparameters** | `r`, `alpha`, `dropout` | `r`, `alpha`, `dropout` | None (module-specific) |\n\n### When to Use Each Method\n\n#### Use **LoRA** when:\n- \u2705 General fine-tuning (recommended default)\n- \u2705 Want predictable, stable results\n- \u2705 Well-documented hyperparameters\n- \u2705 Good community support\n\n#### Use **AdaLoRA** when:\n- \u2705 Want slightly better quality\n- \u2705 Have heterogeneous layers (some need more capacity)\n- \u2705 Willing to trade speed for quality\n- \u2705 Experimenting with optimal configurations\n\n#### Use **IA3** when:\n- \u2705 Extremely limited VRAM\n- \u2705 Need fastest possible training\n- \u2705 Task is relatively simple\n- \u2705 Every MB of memory counts\n\n---\n\n## Configuration Presets\n\n### Preset 1: **Budget** (< 8 GB VRAM)\n```yaml\nuse_peft: true\npeft_method: \"ia3\"\nlora_target_modules: \"q_proj,v_proj\"\n```\n- **Trainable params:** ~100K-200K\n- **VRAM overhead:** +1-1.5 GB\n- **Quality:** Good\n- **Use case:** Lightweight fine-tuning, minimal resources\n\n---\n\n### Preset 2: **Efficient** (8-12 GB VRAM) \ud83c\udf1f *Recommended*\n```yaml\nuse_peft: true\npeft_method: \"lora\"\nlora_r: 16\nlora_alpha: 32\nlora_dropout: 0.05\nlora_target_modules: \"q_proj,v_proj\"\n```\n- **Trainable params:** ~500K-1M\n- **VRAM overhead:** +2-2.5 GB\n- **Quality:** Very Good\n- **Use case:** Most common scenarios, balanced efficiency\n\n---\n\n### Preset 3: **Balanced** (12-16 GB VRAM)\n```yaml\nuse_peft: true\npeft_method: \"lora\"\nlora_r: 16\nlora_alpha: 32\nlora_dropout: 0.05\nlora_target_modules: \"q_proj,k_proj,v_proj,o_proj\"\n```\n- **Trainable params:** ~2M-3M\n- **VRAM overhead:** +3-4 GB\n- **Quality:** Excellent\n- **Use case:** Standard fine-tuning with ample resources\n\n---\n\n### Preset 4: **High Quality** (16-24 GB VRAM)\n```yaml\nuse_peft: true\npeft_method: \"lora\"\nlora_r: 32\nlora_alpha: 64\nlora_dropout: 0.05\nlora_target_modules: \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n```\n- **Trainable params:** ~8M-12M\n- **VRAM overhead:** +5-6 GB\n- **Quality:** Maximum (with PEFT)\n- **Use case:** Complex tasks, maximum quality needed\n\n---\n\n### Preset 5: **Adaptive** (Research/Optimization)\n```yaml\nuse_peft: true\npeft_method: \"adalora\"\nlora_r: 16\nlora_alpha: 32\nlora_dropout: 0.1\nlora_target_modules: \"q_proj,k_proj,v_proj,o_proj\"\n```\n- **Trainable params:** ~2M-3M (dynamic)\n- **VRAM overhead:** +3.5-4.5 GB\n- **Quality:** Excellent+\n- **Use case:** Research, finding optimal configurations\n\n---\n\n## Memory & Performance Impact\n\n### Memory Breakdown (GPT-2 124M Model Example)\n\n#### **Full Fine-Tuning** (no PEFT)\n```\nBase model:          ~500 MB\nGradients:          ~500 MB\nOptimizer states:   ~2000 MB (Adam)\nActivations:        ~8000 MB (batch=8, seq=1024)\n-----------------------------------\nTOTAL:              ~11 GB\n```\n\n#### **PEFT (LoRA r=16, Balanced)**\n```\nBase model:          ~500 MB (frozen, can use 8-bit)\nLoRA adapters:       ~20 MB\nLoRA gradients:      ~20 MB\nLoRA optimizer:      ~80 MB\nActivations:         ~8000 MB (same)\n-----------------------------------\nTOTAL:              ~8.6 GB  (23% reduction)\n```\n\n#### **PEFT + All Optimizations**\n```\nBase model (8-bit):  ~125 MB\nLoRA adapters:       ~20 MB\nLoRA optimizer:      ~80 MB\nActivations (gc):    ~2000 MB (gradient checkpointing)\n-----------------------------------\nTOTAL:              ~2.2 GB  (80% reduction!)\n```\n\n### Performance Benchmarks\n\n| Configuration | Trainable Params | VRAM | Training Speed | Quality |\n|--------------|-----------------|------|----------------|---------|\n| Full Fine-Tuning | 124M (100%) | 11 GB | 1.0\u00d7 (baseline) | 100% |\n| LoRA r=4 Minimal | 250K (0.2%) | 9 GB | 1.3\u00d7 | 85% |\n| LoRA r=8 Minimal | 500K (0.4%) | 9.5 GB | 1.25\u00d7 | 92% |\n| LoRA r=16 Minimal | 1M (0.8%) | 10 GB | 1.2\u00d7 | 97% |\n| LoRA r=16 Balanced | 2M (1.6%) | 10.5 GB | 1.15\u00d7 | 99% |\n| LoRA r=32 Full | 8M (6.5%) | 11 GB | 1.1\u00d7 | 99.5% |\n\n**Key Findings:**\n- LoRA r=16 with balanced modules achieves 99% quality at 1.6% parameters\n- Speed improvements come from fewer gradients to compute\n- VRAM savings enable larger batch sizes (\u2192 better quality)\n\n---\n\n## Best Practices\n\n### 1. **Start with Recommended Defaults**\n```yaml\nuse_peft: true\npeft_method: \"lora\"\nlora_r: 16\nlora_alpha: 32\nlora_dropout: 0.05\nlora_target_modules: \"q_proj,v_proj\"  # or \"q_proj,k_proj,v_proj,o_proj\"\n```\n\n### 2. **Tune Rank Based on Task Complexity**\n- **Simple tasks** (sentiment, classification): `r=4-8`\n- **Medium tasks** (summarization, QA): `r=8-16`\n- **Complex tasks** (creative writing, reasoning): `r=16-32`\n- **Very complex** (code generation, math): `r=32-64`\n\n### 3. **Adjust Alpha with Rank**\n- Maintain `alpha = 2 \u00d7 r` ratio\n- Increase alpha if adapters learn too slowly\n- Decrease alpha if training becomes unstable\n\n### 4. **Use Dropout for Small Datasets**\n- `dataset < 1K samples`: `dropout = 0.2-0.3`\n- `dataset 1K-10K`: `dropout = 0.1`\n- `dataset 10K-100K`: `dropout = 0.05`\n- `dataset > 100K`: `dropout = 0.0-0.05`\n\n### 5. **Target Modules Strategy**\n- **Always start with:** `q_proj,v_proj`\n- **If underfitting, add:** `k_proj,o_proj`\n- **If still underfitting, add:** `gate_proj,up_proj,down_proj`\n- **Never remove:** `q_proj,v_proj` (most impactful)\n\n### 6. **Combine with Other Optimizations**\nPEFT works great with:\n- \u2705 Gradient checkpointing (memory)\n- \u2705 AMP/mixed precision (speed + memory)\n- \u2705 8-bit optimizers (memory)\n- \u2705 CPU offloading (extreme memory savings)\n- \u2705 Flash Attention (speed)\n\n### 7. **Monitor Training Metrics**\n- **Trainable params** should be < 5% of total\n- **Loss convergence** should be similar to full fine-tuning\n- **VRAM usage** should be 20-50% lower\n- **Training speed** should be 1.1-1.5\u00d7 faster\n\n### 8. **Save and Merge Adapters**\n```python\n# Save adapter only (small file ~10-30 MB)\nmodel.save_pretrained(\"path/to/lora_adapter\")\n\n# Merge adapter into base model (optional)\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(\"path/to/merged_model\")\n```\n\n---\n\n## Testing & Validation\n\n### Validation Checklist\n\n#### \u2705 **Configuration Validation**\n- [ ] `use_peft` correctly enables/disables PEFT\n- [ ] All three methods (lora, adalora, ia3) work\n- [ ] Target modules parse correctly\n- [ ] Invalid configurations raise helpful errors\n\n#### \u2705 **Training Validation**\n- [ ] Model trains successfully with PEFT\n- [ ] Loss decreases over training\n- [ ] Gradients flow only to adapter parameters\n- [ ] Checkpoints save correctly\n\n#### \u2705 **Memory Validation**\n- [ ] VRAM usage is lower than full fine-tuning\n- [ ] Larger batch sizes fit in memory\n- [ ] Gradient checkpointing + PEFT works\n\n#### \u2705 **Quality Validation**\n- [ ] Eval metrics comparable to full fine-tuning\n- [ ] Model output quality is good\n- [ ] No catastrophic forgetting\n- [ ] Adapters load correctly for inference\n\n### Common Issues & Solutions\n\n#### **Issue: \"No trainable parameters\"**\n**Cause:** Target modules don't match model architecture  \n**Solution:** Use `q_proj,v_proj` for HRM models\n\n#### **Issue: \"PEFT library not available\"**\n**Cause:** `peft` package not installed  \n**Solution:** `pip install peft>=0.11.1`\n\n#### **Issue: \"Training loss doesn't decrease\"**\n**Cause:** `lora_alpha` too low or rank too small  \n**Solution:** Increase `lora_alpha` or `lora_r`\n\n#### **Issue: \"Out of memory with PEFT enabled\"**\n**Cause:** Other factors (batch size, sequence length)  \n**Solution:** Reduce batch size or enable gradient checkpointing\n\n#### **Issue: \"Training is unstable\"**\n**Cause:** `lora_alpha` too high  \n**Solution:** Reduce `lora_alpha` or add more dropout\n\n---\n\n## Commands (CLI)\n\nPowerShell examples for enabling PEFT with `aios hrm-hf train-actv1`:\n\nMinimal (q,v only \u2014 best VRAM efficiency):\n\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 `\n    --model gpt2 `\n    --dataset-file training_data/curated_datasets/test_sample.txt `\n    --steps 200 `\n    --batch-size 4 `\n    --halt-max-steps 1 `\n    --use-peft `\n    --peft-method lora `\n    --lora-r 16 `\n    --lora-alpha 32 `\n    --lora-dropout 0.05 `\n    --lora-target-modules \"q_proj,v_proj\" `\n    --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nBalanced (q,k,v,o):\n\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 `\n    --model gpt2 `\n    --dataset-file training_data/curated_datasets/test_sample.txt `\n    --steps 200 `\n    --batch-size 4 `\n    --halt-max-steps 1 `\n    --use-peft `\n    --peft-method lora `\n    --lora-r 16 `\n    --lora-alpha 32 `\n    --lora-dropout 0.05 `\n    --lora-target-modules \"q_proj,k_proj,v_proj,o_proj\" `\n    --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nAdaLoRA variant:\n\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 `\n    --model gpt2 `\n    --dataset-file training_data/curated_datasets/test_sample.txt `\n    --steps 200 `\n    --batch-size 4 `\n    --halt-max-steps 1 `\n    --use-peft `\n    --peft-method adalora `\n    --lora-r 16 `\n    --lora-alpha 32 `\n    --lora-dropout 0.1 `\n    --lora-target-modules \"q_proj,k_proj,v_proj,o_proj\" `\n    --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nNotes:\n- Flags are wired in `src/aios/cli/hrm_hf_cli.py` and applied in `src/aios/cli/hrm_hf/model_precision.py`.\n- Use `--amp` and `--gradient-checkpointing` with PEFT for best VRAM efficiency.\n\n## Inputs & Outputs\n\nInputs:\n- Base model: `--model <hf-id-or-local-path>`\n- Dataset: `--dataset-file <path or hf://\u2026>`\n- PEFT toggles: `--use-peft`, `--peft-method`, `--lora-r`, `--lora-alpha`, `--lora-dropout`, `--lora-target-modules`\n\nOutputs:\n- Brain bundle under `artifacts/brains/actv1/<brain-name>/`\n- Metrics JSONL at `artifacts/brains/actv1/metrics.jsonl`\n- Optional PEFT adapter save/merge (see code snippet below)\n\n## Try it (PowerShell)\n\nQuick dry-run to verify PEFT wiring:\n\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 `\n    --model gpt2 `\n    --dataset-file training_data/curated_datasets/test_sample.txt `\n    --steps 1 `\n    --batch-size 2 `\n    --halt-max-steps 1 `\n    --use-peft `\n    --peft-method lora `\n    --lora-r 8 `\n    --lora-alpha 16 `\n    --lora-target-modules \"q_proj,v_proj\" `\n    --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nExpected log lines include a `{\"peft\": \"enabled\", ...}` entry with trainable parameter percentages < 5%.\n\n---\n\n## Conclusion\n\nLoRA/PEFT in AI-OS provides a powerful, efficient way to fine-tune models with:\n- **95-99% fewer trainable parameters**\n- **40-60% VRAM savings**\n- **Faster training speeds**\n- **Comparable or better quality**\n\n### Recommended Starting Point\n```yaml\nuse_peft: true\npeft_method: \"lora\"\nlora_r: 16\nlora_alpha: 32\nlora_dropout: 0.05\nlora_target_modules: \"q_proj,v_proj\"\n```\n\n**Then adjust based on:**\n- VRAM availability \u2192 increase `r` or target modules\n- Task complexity \u2192 increase `r` and `alpha`\n- Dataset size \u2192 adjust `dropout`\n- Quality needs \u2192 add more target modules\n\n### Further Reading\n- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n- [PEFT Documentation](https://huggingface.co/docs/peft)\n- [AdaLoRA Paper](https://arxiv.org/abs/2303.10512)\n- [IA3 Paper](https://arxiv.org/abs/2205.05638)\n\n---\n\n**Last Updated:** October 19, 2025  \n**Version:** 1.0  \n**AI-OS Version:** Compatible with all ACTv1 models\n\nSee also: Memory Optimization \u2022 Core Training \u2022 GUI Features\n", "tags": ["cli", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "LoRA/PEFT Comprehensive Analysis for AI-OS"}, {"line": 15, "text": "Table of Contents"}, {"line": 32, "text": "Overview"}, {"line": 45, "text": "What is PEFT?"}, {"line": 53, "text": "Why Use PEFT?"}, {"line": 66, "text": "Implementation Details"}, {"line": 68, "text": "Code Location"}, {"line": 73, "text": "How It Works"}, {"line": 76, "text": "From model_precision.py"}, {"line": 81, "text": "1. Parse target modules"}, {"line": 84, "text": "2. Create PEFT config"}, {"line": 93, "text": "... (adalora, ia3 methods also supported)"}, {"line": 95, "text": "3. Wrap model with PEFT"}, {"line": 101, "text": "Integration Points"}, {"line": 109, "text": "Parameter Breakdown"}, {"line": 111, "text": "1. `use_peft` (Boolean)"}, {"line": 129, "text": "2. `peft_method` (String)"}, {"line": 133, "text": "**LoRA (Low-Rank Adaptation)** \ud83c\udf1f *Recommended*"}, {"line": 144, "text": "**AdaLoRA (Adaptive LoRA)**"}, {"line": 152, "text": "**IA3 (Infused Adapter)**"}, {"line": 162, "text": "3. `lora_r` (Integer - Rank)"}, {"line": 196, "text": "4. `lora_alpha` (Integer - Scaling)"}, {"line": 228, "text": "5. `lora_dropout` (Float)"}, {"line": 255, "text": "6. `lora_target_modules` (String - Comma-separated)"}, {"line": 262, "text": "**Attention Modules** (Recommended)"}, {"line": 268, "text": "**MLP/Feed-Forward Modules**"}, {"line": 273, "text": "**Always Trainable (Cannot be frozen)**"}, {"line": 279, "text": "Target Modules Explained"}, {"line": 281, "text": "Preset Configurations"}, {"line": 283, "text": "**Minimal** (Recommended for VRAM < 8 GB)"}, {"line": 293, "text": "**Balanced** (Recommended Default)"}, {"line": 303, "text": "**Full** (Maximum Quality)"}, {"line": 313, "text": "Module Impact Analysis"}, {"line": 332, "text": "PEFT Methods Comparison"}, {"line": 334, "text": "Detailed Comparison Table"}, {"line": 347, "text": "When to Use Each Method"}, {"line": 349, "text": "Use **LoRA** when:"}, {"line": 355, "text": "Use **AdaLoRA** when:"}, {"line": 361, "text": "Use **IA3** when:"}, {"line": 369, "text": "Configuration Presets"}, {"line": 371, "text": "Preset 1: **Budget** (< 8 GB VRAM)"}, {"line": 384, "text": "Preset 2: **Efficient** (8-12 GB VRAM) \ud83c\udf1f *Recommended*"}, {"line": 400, "text": "Preset 3: **Balanced** (12-16 GB VRAM)"}, {"line": 416, "text": "Preset 4: **High Quality** (16-24 GB VRAM)"}, {"line": 432, "text": "Preset 5: **Adaptive** (Research/Optimization)"}, {"line": 448, "text": "Memory & Performance Impact"}, {"line": 450, "text": "Memory Breakdown (GPT-2 124M Model Example)"}, {"line": 452, "text": "**Full Fine-Tuning** (no PEFT)"}, {"line": 462, "text": "**PEFT (LoRA r=16, Balanced)**"}, {"line": 473, "text": "**PEFT + All Optimizations**"}, {"line": 483, "text": "Performance Benchmarks"}, {"line": 501, "text": "Best Practices"}, {"line": 503, "text": "1. **Start with Recommended Defaults**"}, {"line": 513, "text": "2. **Tune Rank Based on Task Complexity**"}, {"line": 519, "text": "3. **Adjust Alpha with Rank**"}, {"line": 524, "text": "4. **Use Dropout for Small Datasets**"}, {"line": 530, "text": "5. **Target Modules Strategy**"}, {"line": 536, "text": "6. **Combine with Other Optimizations**"}, {"line": 544, "text": "7. **Monitor Training Metrics**"}, {"line": 550, "text": "8. **Save and Merge Adapters**"}, {"line": 552, "text": "Save adapter only (small file ~10-30 MB)"}, {"line": 555, "text": "Merge adapter into base model (optional)"}, {"line": 562, "text": "Testing & Validation"}, {"line": 564, "text": "Validation Checklist"}, {"line": 566, "text": "\u2705 **Configuration Validation**"}, {"line": 572, "text": "\u2705 **Training Validation**"}, {"line": 578, "text": "\u2705 **Memory Validation**"}, {"line": 583, "text": "\u2705 **Quality Validation**"}, {"line": 589, "text": "Common Issues & Solutions"}, {"line": 591, "text": "**Issue: \"No trainable parameters\"**"}, {"line": 595, "text": "**Issue: \"PEFT library not available\"**"}, {"line": 599, "text": "**Issue: \"Training loss doesn't decrease\"**"}, {"line": 603, "text": "**Issue: \"Out of memory with PEFT enabled\"**"}, {"line": 607, "text": "**Issue: \"Training is unstable\"**"}, {"line": 613, "text": "Commands (CLI)"}, {"line": 675, "text": "Inputs & Outputs"}, {"line": 687, "text": "Try it (PowerShell)"}, {"line": 710, "text": "Conclusion"}, {"line": 718, "text": "Recommended Starting Point"}, {"line": 734, "text": "Further Reading"}]}, {"path": "guide/features/MEMORY_OPTIMIZATION.md", "content": "# Memory Optimization - AI-OS\nGenerated: December 12, 2025\nPurpose: Techniques to reduce VRAM and enable larger models/contexts\nStatus: Implemented (some items require verification)\n\n## Gradient Checkpointing\n- Default: Enabled; CLI: `--gradient-checkpointing` / `--no-gradient-checkpointing`\n- ~30\u201350% memory reduction; ~20% slowdown\n- Applied during model setup in training\n\n## Mixed Precision (AMP)\n- Default: Enabled; CLI: `--amp` / `--no-amp`\n- Uses autocast + GradScaler (FP16/BF16)\n- ~40\u201350% memory reduction; 2\u20133x speedup\n\n## 8-bit Optimizer\n- CLI: `--use-8bit-optimizer`; uses `bitsandbytes` if available\n- Quantizes optimizer states; large savings for 100M+ params\n- Fallback to AdamW if bnb unavailable\n\n## Chunked Training (Long Context)\n- Config options exist: `use_chunked_training`, `chunk_size`\n- Goal: split long sequences into chunks with accumulation\n- Docs: PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md, CONFIGURABLE_DATASET_CHUNK_SIZE.md\n- Status: Verification needed for training loop integration\n\n## Dynamic Batch Size Reduction\n- Auto-reduce batch size on CUDA OOM until 1; then fail if still OOM\n\n## Gradient Accumulation\n- CLI: `--gradient-accumulation-steps <n>` to simulate larger batches\n\n## Attention Kernel Optimizations\n- Uses FlashAttention when available; SDPA fallback\n- See: FLASH_ATTENTION.md and FLASH_ATTENTION_VS_CHUNKING.md\n\n## Example configurations (Windows PowerShell)\n\n- Conservative VRAM profile:\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 50 --batch-size 1 --gradient-accumulation-steps 8 --gradient-checkpointing --amp --use-8bit-optimizer --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n- Long-context friendly (requires FA2-capable GPU or will fallback):\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 50 --batch-size 1 --gradient-checkpointing --amp --use-8bit-optimizer --window-size 2048 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n## Verifying savings\n- Watch metrics/logs for memory reports if emitted; compare throughput with/without options.\n- For FlashAttention, see the Flash Attention doc for how to confirm activation vs fallback.\n\n## Troubleshooting\n- CUDA OOM: Reduce `--batch-size`, increase `--gradient-accumulation-steps`, or enable `--use-8bit-optimizer` and checkpointing.\n- bitsandbytes missing: Install the extra or run without `--use-8bit-optimizer` (it will fallback).\n- Unstable FP16: Try BF16 if supported (`--amp` selection is automatic) or temporarily `--no-amp`.\n\nRelated: Core Training, Model Architecture\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Memory Optimization - AI-OS"}, {"line": 5, "text": "Gradient Checkpointing"}, {"line": 10, "text": "Mixed Precision (AMP)"}, {"line": 15, "text": "8-bit Optimizer"}, {"line": 20, "text": "Chunked Training (Long Context)"}, {"line": 26, "text": "Dynamic Batch Size Reduction"}, {"line": 29, "text": "Gradient Accumulation"}, {"line": 32, "text": "Attention Kernel Optimizations"}, {"line": 36, "text": "Example configurations (Windows PowerShell)"}, {"line": 48, "text": "Verifying savings"}, {"line": 52, "text": "Troubleshooting"}]}, {"path": "guide/features/MODEL_ARCHITECTURE.md", "content": "# Model Architecture - AI-OS\nGenerated: December 12, 2025\nPurpose: HRM model components and configurable parameters\nStatus: Implemented\n\n## Files\n- `src/aios/core/hrm_models/` \u2013 Core modules\n- `src/aios/core/hrm.py` \u2013 Top-level assembly\n\n## Hierarchical Structure\n- High-level (H) and low-level (L) blocks\n- Files: hierarchical_recurrence.py, higher_level_block.py, lower_level_block.py\n- Params: `--h-layers`, `--l-layers`, `--h-cycles`, `--l-cycles`\n\n## Adaptive Computation Time (ACT)\n- File: act.py; param: `--halt-max-steps`\n\n## Attention Mechanisms\n- Files: attention.py, efficient_attention.py\n- Types: MHA, sliding window, efficient variants\n- Params: `--num-heads`, `--window-size`\n\n## Position Encodings\n- File: position_encoding.py\n- Types: RoPE (default), sinusoidal, learned\n- Param: `--pos-encodings`\n\n## Feed-Forward Networks\n- File: ffn.py; GLU; `--expansion`\n\n## Residual Connections\n- Throughout; includes LayerNorm and skip paths\n\n## Selecting and validating architectures (CLI)\n\nArchitecture is configured through `aios hrm-hf train-actv1` flags. Typical flags include:\n- Depth: `--h-layers`, `--l-layers`\n- Cycles: `--h-cycles`, `--l-cycles`\n- Width/heads: `--hidden-size`, `--num-heads`, `--expansion`\n- Positional encodings: `--pos-encodings`\n- Attention window: `--window-size` (sliding window) with FlashAttention or SDPA\n\n### Example: small hierarchical model (Windows PowerShell)\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1 --batch-size 2 --h-layers 2 --l-layers 4 --hidden-size 512 --num-heads 8 --expansion 4 --pos-encodings rope --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n### Validate model/tokenizer resolution\nUse the same command with `--steps 1` to ensure it initializes and writes metrics/checkpoints under `artifacts/brains/actv1/`.\n\nNotes:\n- If you point `--model` to a local path, ensure tokenizer files are present or resolvable from HF.\n- Windowed attention (`--window-size`) limits attention range; see Flash Attention docs for trade-offs.\n\nRelated: Memory Optimization, Dynamic Subbrains/MoE\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Model Architecture - AI-OS"}, {"line": 5, "text": "Files"}, {"line": 9, "text": "Hierarchical Structure"}, {"line": 14, "text": "Adaptive Computation Time (ACT)"}, {"line": 17, "text": "Attention Mechanisms"}, {"line": 22, "text": "Position Encodings"}, {"line": 27, "text": "Feed-Forward Networks"}, {"line": 30, "text": "Residual Connections"}, {"line": 33, "text": "Selecting and validating architectures (CLI)"}, {"line": 42, "text": "Example: small hierarchical model (Windows PowerShell)"}, {"line": 47, "text": "Validate model/tokenizer resolution"}]}, {"path": "guide/features/MULTIGPU_GUI_GUIDE.md", "content": "# Multi-GPU GUI Guide\n**Last Updated**: December 12, 2025  \n**Purpose**: Explain how the AI-OS desktop GUI exposes multi-GPU selection and fallbacks across operating systems.  \n**Status**: Linux multi-GPU routing supported; Windows currently falls back to a single CUDA device.\n\n---\n\n## Overview\nThe Resources panel lets you pick one or more CUDA devices for inference. Those selections feed shared helpers in `src/aios/gui/services/resource_selection.py`, so both chat and evaluation flows inherit the same rules:\n- Device selections persist between sessions via GUI state.\n- Analytics events capture which GPU set the user attempted to run.\n- When platform capabilities change (Linux \u2194 Windows), the GUI reconciles the stored selection and surfaces warnings.\n\nSee also: [Multi-GPU & Distributed Training](MULTI_GPU_DISTRIBUTED.md) for CLI-focused orchestration details.\n\n---\n\n## Current Behaviour by Platform\n### Windows (single-GPU fallback)\n- Windows builds always run inference on the first CUDA device. Additional GPUs in the selector are kept for reference but ignored by the runtime.\n- The GUI surfaces a warning banner and status message if a multi-GPU selection is detected on Windows.\n- Chat and evaluation both log `windows_single_gpu_fallback` analytics events so telemetry captures the limitation.\n\n### Linux (multi-GPU routing)\n- Evaluation fan-out runs each selected device sequentially via `MultiGpuEvaluationRunner`, emitting per-device analytics entries.\n- Chat currently launches tensor-parallel stubs that lock to GPU 0 while foundation work for true tensor-parallel routing continues. A warning tooltip clarifies this.\n- Device lists longer than eight entries are truncated with a warning to keep tooltips concise.\n\n---\n\n## Verifying CUDA Visibility\nMake sure the environment exposes only the GPUs you intend to target.\n\n**PowerShell**\n```powershell\nGet-ChildItem Env:CUDA_VISIBLE_DEVICES\n$Env:CUDA_VISIBLE_DEVICES = \"0,1\"\n```\n\n**Bash**\n```bash\nprintenv CUDA_VISIBLE_DEVICES\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\n```\n\nAfter updating the variable, restart the GUI so the selector refreshes the list of available devices.\n\n---\n\n## Status Messages and Logs\n- The Resources panel displays inline warnings and sets a status message any time the OS forces a fallback.\n- The shared selector returns `warning_message(...)` strings that are re-used in chat, evaluation, and GUI tooltips for consistency.\n- Warnings are also forwarded to the log router so they appear in the Debug panel.\n\n---\n\n## FAQ\n**Why does chat still label GPU 0 as primary on Linux?**  \nTensor-parallel scaffolding is in place, but the router still executes on the first device until tensor slicing ships. Track progress in [planned DDP/tensor parallel work](../../planned_features/DEEPSPEED_ZERO_INFINITY.md).\n\n**Why do I see a Windows warning even after switching to Linux?**  \nIf you carried over a persisted state file from Windows, the GUI logs that the OS changed and re-normalises your selections. Open the Resources panel once to re-save the Linux layout.\n\n**Where is the full roadmap for chat-time multi-GPU?**  \nSee [DeepSpeed ZeRO-Infinity](../../planned_features/DEEPSPEED_ZERO_INFINITY.md) for planned tensor parallelism and distributed inference features.\n\n---\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)\n", "tags": ["cli", "evaluation", "gui", "training"], "headings": [{"line": 0, "text": "Multi-GPU GUI Guide"}, {"line": 7, "text": "Overview"}, {"line": 17, "text": "Current Behaviour by Platform"}, {"line": 18, "text": "Windows (single-GPU fallback)"}, {"line": 23, "text": "Linux (multi-GPU routing)"}, {"line": 30, "text": "Verifying CUDA Visibility"}, {"line": 49, "text": "Status Messages and Logs"}, {"line": 56, "text": "FAQ"}]}, {"path": "guide/features/MULTI_GPU_DISTRIBUTED.md", "content": "# Multi-GPU & Distributed\nGenerated: December 12, 2025\nPurpose: How to use multiple GPUs with AI-OS training (DDP and Windows-compatible parallel mode)\nStatus: DDP supported via launcher or internal spawn; Windows defaults to parallel-independent mode. DeepSpeed flags exist but engine wiring is limited.\n\n## Overview\n\nAI-OS supports two multi-GPU modes:\n\n1) DDP (torch.distributed) \u2014 classic data parallel synchronized training.\n2) Parallel independent training \u2014 Windows-friendly mode that assigns distinct data chunks to each GPU without DDP synchronization, then aggregates progress/checkpoints.\n\nOn Windows, DDP often fails due to backend limitations. The code provides an internal spawn pathway and falls back to parallel-independent mode when needed.\n\nKey flags in `aios hrm-hf train-actv1`:\n- --ddp: enable DDP when a launcher or internal spawn is available\n- --cuda-ids \"0,1,...\": which GPUs to use\n- --world-size N: number of DDP processes/GPUs (optional; inferred from --cuda-ids)\n- --parallel-independent: force the Windows-compatible multi-GPU path (no gradient sync)\n\nRelated files:\n- CLI: `src/aios/cli/hrm_hf_cli.py` (options: --ddp, --world-size, --cuda-ids, --parallel-independent)\n- DDP internals: `src/aios/cli/hrm_hf/ddp/utils.py`, `src/aios/cli/hrm_hf/ddp/worker_main.py`\n\n## Using DDP\n\nDDP requires either:\n- An external launcher (recommended on Linux): torchrun/torch.distributed.run\n- Internal spawn mode (set AIOS_DDP_SPAWN=1), useful on Windows/GUI\n\nWindows PowerShell examples (GPU IDs 0 and 1):\n\n1) Internal spawn (recommended on Windows)\n- Set an environment variable for this PowerShell session and run training:\n\n\t$env:AIOS_DDP_SPAWN = \"1\"\n\t.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --ddp --cuda-ids \"0,1\" --world-size 2 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 10 --batch-size 4 --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n\nNotes:\n- The process will spawn per-rank workers and use the gloo backend on Windows.\n- If early failures are detected during DDP init, training will fall back to single-GPU unless you passed --strict.\n\n2) External launcher (Linux-oriented, shown for reference)\n- torchrun --standalone --nproc_per_node 2 .venv/\u2026/python -m aios.cli.aios hrm-hf train-actv1 --ddp --model gpt2 --dataset-file \u2026\n- On Windows, torchrun may still be unstable; prefer internal spawn or parallel-independent.\n\nVerifying DDP:\n- Logs will include keys like {\"ddp\": \"external_launcher_detected\"} or {\"ddp\": \"spawning_workers\"}.\n- Each rank writes progress, and total throughput should scale with GPU count.\n\nTroubleshooting DDP on Windows:\n- If you see errors mentioning libuv/NCCL/backends or hostname resolution, the code automatically forces gloo, sets MASTER_ADDR=127.0.0.1, and disables libuv.\n- Still failing? Try:\n\t- Close Docker Desktop (prevents hostname pollution)\n\t- Ensure firewall allows local port 29500\n\t- Set $env:AIOS_DDP_SPAWN = \"1\" and retry\n\t- Or use --parallel-independent instead of --ddp\n\n## Parallel Independent Training (Windows-friendly)\n\nThis mode runs coordinated multi-GPU training without DDP. Each GPU trains on different dataset chunks (no duplication) and progress/checkpoints are tracked centrally. No gradient synchronization occurs, so results won\u2019t be identical to DDP, but it achieves strong throughput on Windows.\n\nExample (2 GPUs):\n\n\t.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --parallel-independent --cuda-ids \"0,1\" --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 200 --batch-size 4 --dataset-chunk-size 4000 --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n\nNotes:\n- Uses the block/chunk system to guarantee distinct chunks per GPU.\n- Creates/updates `chunk_tracker_state.json` under the brain bundle for resume and epoch tracking.\n- Ideal fallback when DDP isn\u2019t viable on Windows.\n\n## DeepSpeed ZeRO (status)\n\nFlag surface exists: --zero-stage {none|zero1|zero2|zero3}\nConfigs present: config/deepspeed_zero1.json, config/deepspeed_zero2.json, config/deepspeed_zero3.json\n\nCurrent state:\n- The CLI and GUI expose ZeRO stage selection and use it for estimators/UI.\n- `train_actv1` calls `initialize_deepspeed(...)`; when the `deepspeed` package is available and the requested stage is supported, the model is wrapped in a DeepSpeed engine automatically.\n- The code falls back to standard optimizers if import or initialization fails, logging the reason.\n- Treat ZeRO as experimental: there is minimal automated coverage, and successful initialization still depends on your environment (CUDA-only, compatible DeepSpeed build, etc.).\n\n## Inputs and Outputs\n\nInputs (typical):\n- --model <name or path>\n- --dataset-file <path or hf://\u2026>\n- --steps, --batch-size, --halt-max-steps, etc.\n- Multi-GPU picks: --ddp/--parallel-independent, --cuda-ids, --world-size\n\nOutputs:\n- Metrics JSONL: artifacts/brains/actv1/metrics.jsonl (configurable)\n- Brain bundle(s): artifacts/brains/actv1/<brain-name>/\n- chunk_tracker_state.json (parallel-independent runs)\n\n## Quick starts (PowerShell)\n\nDDP (internal spawn):\n\n\t$env:AIOS_DDP_SPAWN = \"1\"\n\t.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --ddp --cuda-ids \"0,1\" --world-size 2 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 5 --batch-size 2 --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n\nParallel independent (Windows-friendly):\n\n\t.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --parallel-independent --cuda-ids \"0,1\" --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 5 --batch-size 2 --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n\nSee also:\n- Parallel Training Block/Chunk System\n- Memory Optimization\n- Core Training\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Multi-GPU & Distributed"}, {"line": 5, "text": "Overview"}, {"line": 24, "text": "Using DDP"}, {"line": 58, "text": "Parallel Independent Training (Windows-friendly)"}, {"line": 71, "text": "DeepSpeed ZeRO (status)"}, {"line": 82, "text": "Inputs and Outputs"}, {"line": 95, "text": "Quick starts (PowerShell)"}]}, {"path": "guide/features/PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md", "content": "# Parallel Training Block/Chunk Distribution System\n\nCanonical source for how we stream and distribute data across GPUs without duplication. For the per-run chunk knob, see CONFIGURABLE_DATASET_CHUNK_SIZE.md.\n\n## Overview\n\nThe system streams datasets in large blocks and serves only the requested chunks to each GPU. A shared tracker guarantees no two GPUs train the same chunk. It supports both HuggingFace streaming datasets and local files, with epoch detection and resume.\n\n## Architecture\n\n### Hierarchy\n\n```\nDataset (e.g., 10M samples)\n  \u251c\u2500 Block 0 (\u2248100k samples) \u2500 streaming from HuggingFace or sliced from local files\n  \u2502   \u251c\u2500 Chunk 0 (e.g., 4k samples) \u2500 GPU 0 trains\n  \u2502   \u251c\u2500 Chunk 1 (e.g., 4k samples) \u2500 GPU 1 trains\n  \u2502   \u251c\u2500 Chunk 2 (e.g., 4k samples) \u2500 GPU 0 trains\n  \u2502   \u2514\u2500 ...\n  \u251c\u2500 Block 1 (\u2248100k samples)\n  \u2502   \u2514\u2500 ...\n  \u2514\u2500 ...\n\nEpoch = one full pass through ALL blocks\n```\n\n### Key Components\n\n1) BlockManager (`src/aios/cli/hrm_hf/block_manager.py`)\n- Streams HuggingFace datasets in blocks (default \u2248100k samples) and caches them on disk\n- Loads only the requested chunk into memory on demand\n- Prefetches metadata to detect last block\n- Works with local files by slicing them into block-sized windows\n\n2) ChunkTracker (state persisted in the brain bundle)\n- Tracks which (block_id, chunk_id) were trained\n- Prevents duplication across GPUs\n- Tracks blocks visited per epoch and total steps\n- Persists to `chunk_tracker_state.json` for resume\n\n3) Parallel Control\n- In parallel-independent mode, each GPU requests the next untrained chunk\n- In DDP, data loading can still use block/chunk mechanics while gradients are synchronized\n\n## Configuration surface\n\nTrainingConfig defaults (see `src/aios/core/hrm_training/training_config/base_fields.py`):\n\n- samples_per_block: 100000 (for HF streaming/local slicing; auto-detected/recorded)\n- dataset_chunk_size: 4000 (per-iteration chunk size; user knob `--dataset-chunk-size`)\n- stop_after_epoch: false (toggle via `--stop-after-epoch`)\n- iterate: false (toggle via `--iterate`)\n\nNotes:\n- There is no `--samples-per-block` CLI flag. samples_per_block is chosen/detected inside dataset setup and recorded into metrics for UI.\n- Adjust memory/throughput primarily via `--dataset-chunk-size`.\n\n## Stopping conditions\n\n1) Steps limit: `--steps N` stops after N steps.\n2) Stop after epoch: `--stop-after-epoch` stops after a full dataset pass.\n3) Iterate mode: `--iterate` loops indefinitely, rolling epochs.\n\n## Windows PowerShell examples\n\nTwo-GPU parallel independent training on a HuggingFace dataset:\n\n  .venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --parallel-independent --cuda-ids \"0,1\" --model gpt2 --dataset-file \"hf://wikitext:wikitext-2-raw-v1:train\" --dataset-chunk-size 4000 --steps 500 --batch-size 4 --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n\nStop after an epoch using a local dataset:\n\n  .venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --parallel-independent --cuda-ids \"0,1,2\" --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --dataset-chunk-size 2000 --stop-after-epoch --batch-size 4 --steps 10000 --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n\nContinuous iterate mode on two GPUs:\n\n  .venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --parallel-independent --cuda-ids \"0,1\" --model gpt2 --dataset-file \"hf://c4:en:train\" --dataset-chunk-size 4000 --iterate --batch-size 4 --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n\n## Chunk claiming (no-dup guarantee)\n\n### No Duplication Guarantee\n\nEach GPU claims chunks atomically from ChunkTracker:\n\n```python\ndef get_next_untrained_chunk(block_id, total_chunks, gpu_id):\n    with lock:\n        for chunk_id in range(total_chunks):\n            if (block_id, chunk_id) not in completed_chunks:\n                # Mark as in-progress\n                return chunk_id\n        return None  # All chunks trained\n```\n\nKey points:\n- Thread-safe: lock prevents races\n- Atomic claiming: chunk is marked before training\n- No duplicates: trained chunks are skipped\n- Fair distribution: first-come-first-served\n\n### Resume Capability\n\nTraining state saved to `chunk_tracker_state.json` (under the brain bundle):\n\n```json\n{\n  \"completed_chunks\": [\n    {\"block_id\": 0, \"chunk_id\": 0, \"gpu_id\": 0, \"step\": 125, \"samples_trained\": 4000},\n    {\"block_id\": 0, \"chunk_id\": 1, \"gpu_id\": 1, \"step\": 127, \"samples_trained\": 4000}\n  ],\n  \"current_epoch\": 0,\n  \"blocks_this_epoch\": [0, 1, 2],\n  \"total_steps\": 250\n}\n```\n\nOn resume:\n- Skips already-trained chunks\n- Continues from last step count\n- Maintains epoch tracking\n\n## Epoch Detection\n\nAn **epoch** = Training on ALL blocks in the dataset once.\n\n### Detection algorithm\n\n```python\ndef check_epoch_complete(total_blocks):\n  return len(blocks_this_epoch) >= total_blocks\n```\n\nWhen epoch completes:\n1. ChunkTracker marks epoch complete\n2. If `stop_after_epoch=True` \u2192 Stop training\n3. If `iterate=True` \u2192 Start new epoch (reset block tracking)\n4. Otherwise \u2192 Stop training\n\n### Last block detection\n\nBlockManager detects the last block by attempting to download block N+1:\n\n```python\n# Loading block 5\nblock_5 = load_block(5)  # Returns 100k samples\nblock_6 = load_block(6)  # Returns 0 samples \u2192 Last block detected\n\nblock_5.is_last_block = True\n```\n\nThis works for:\n- HuggingFace datasets: streaming ends naturally\n- Local files: EOF reached\n- Large datasets: consistent detection\n\n## Performance Considerations\n\n### Memory usage\n\nBlock metadata and chunks are cached. Only requested chunks are loaded into RAM at any time, keeping memory bounded mostly by `--dataset-chunk-size` and model batch/sequence length.\n\n### Network I/O\n\nHuggingFace streaming benefits from cached blocks on disk and metadata prefetching to hide latency.\n\n### GPU Utilization\n\nOptimal chunk size depends on VRAM and sequence length:\n- Smaller chunks reduce VRAM but can increase coordination overhead\n- Larger chunks improve throughput but increase memory\n- Defaults: 4000 works well for 12\u201316 GB VRAM\n\n## Troubleshooting\n\n### Issue: GPUs training duplicate data\n\nCause: chunk tracker state not found/shared.\n\nSolution: ensure a single brain bundle is used and that `chunk_tracker_state.json` is writable by all worker processes.\n\n### Issue: Training stops prematurely\n\nCheck stopping conditions:\n```python\nconfig.steps  # max steps reached?\nconfig.stop_after_epoch  # epoch completed?\n```\n\nDebug:\n```python\nstats = chunk_tracker.get_progress_stats()\nprint(stats)  # total_steps, blocks_this_epoch, current_epoch\n```\n\n### Issue: Epoch not completing\n\nPossible causes:\n1) total_blocks not detected yet (keep training until prefetch finishes)\n2) Some blocks never requested (short runs)\n3) ChunkTracker state corrupted\n\nSolution:\n```python\ntotal_blocks = block_manager.get_total_blocks()\nprint(f\"Total blocks: {total_blocks}\")\n\nstats = chunk_tracker.get_progress_stats()\nprint(f\"Blocks this epoch: {stats['blocks_this_epoch']}\")\n```\n\n### Issue: Out of memory\n\nReduce memory usage:\n```python\nconfig.dataset_chunk_size = 2000  # smaller chunk\nconfig.batch_size = 4             # smaller batch\nconfig.max_seq_len = 128          # shorter sequences\n```\n\n## System Architecture\n\n### Block Management\n\n```python\n# Streams data in blocks from HuggingFace or local files\nblock_manager = BlockManager(dataset_file, samples_per_block=100k)\n\n# Distributes chunks across GPUs without duplication\nchunk_tracker = ChunkTracker(state_file)\n\n# Features:\n# - Full training progress tracking\n# - Automatic epoch detection  \n# - Resume capability from checkpoints\n# - Support for all stopping conditions\n```\n\n### Key Components\n\n1. **BlockManager**: Downloads and caches 100k-sample blocks from datasets\n2. **ChunkTracker**: Tracks which chunks each GPU has processed\n3. **State Persistence**: Saves progress to enable resuming training\n4. **Epoch Detection**: Automatically detects when full dataset is processed\n\n## Testing\n\n### Test 1: No duplicate training\n\n```python\n# Track all chunks trained by each GPU\ngpu0_chunks = set()\ngpu1_chunks = set()\n\n# After training\nassert gpu0_chunks.isdisjoint(gpu1_chunks)  # No overlap\n```\n\n### Test 2: Epoch detection\n\n```python\n# Train with stop_after_epoch=True\nconfig.stop_after_epoch = True\n\n# Should stop when all blocks visited\nfinal_stats = chunk_tracker.get_progress_stats()\nassert final_stats['blocks_this_epoch'] == total_blocks\n```\n\n### Test 3: Resume capability\n\n```python\n# Train for 100 steps\nconfig.steps = 100\nrun_training()\n\n# Resume and train 100 more\nconfig.steps = 200\nrun_training()\n\n# Should have 200 total steps, no duplicate chunks\nassert chunk_tracker.total_steps == 200\n```\n\n## Related: Configurable dataset chunk size\n\nSee CONFIGURABLE_DATASET_CHUNK_SIZE.md for usage guidance, examples, and how it interacts with batch size and sequence length.\n\n## Future enhancements\n\n1) Dynamic load balancing (fast GPUs get more chunks)\n2) Chunk prioritization (curriculum)\n3) Distributed tracker (multi-node)\n4) Adaptive block sizing\n5) Chunk prefetching\n6) Partial-epoch checkpoints\n\n## Summary\n\nThis streaming block/chunk system provides:\n\n- No duplicate training across GPUs\n- Proper block management for HF/local datasets\n- Chunk-level tracking with resume\n- Epoch detection and iterate mode\n- Thread-safe coordination and persistence\n- Memory-efficient operation by loading only the needed chunks\n\nIt replaces older approaches that loaded entire datasets into memory without proper progress tracking.\n", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Parallel Training Block/Chunk Distribution System"}, {"line": 4, "text": "Overview"}, {"line": 8, "text": "Architecture"}, {"line": 10, "text": "Hierarchy"}, {"line": 26, "text": "Key Components"}, {"line": 44, "text": "Configuration surface"}, {"line": 57, "text": "Stopping conditions"}, {"line": 63, "text": "Windows PowerShell examples"}, {"line": 77, "text": "Chunk claiming (no-dup guarantee)"}, {"line": 79, "text": "No Duplication Guarantee"}, {"line": 88, "text": "Mark as in-progress"}, {"line": 99, "text": "Resume Capability"}, {"line": 120, "text": "Epoch Detection"}, {"line": 124, "text": "Detection algorithm"}, {"line": 137, "text": "Last block detection"}, {"line": 142, "text": "Loading block 5"}, {"line": 154, "text": "Performance Considerations"}, {"line": 156, "text": "Memory usage"}, {"line": 160, "text": "Network I/O"}, {"line": 164, "text": "GPU Utilization"}, {"line": 171, "text": "Troubleshooting"}, {"line": 173, "text": "Issue: GPUs training duplicate data"}, {"line": 179, "text": "Issue: Training stops prematurely"}, {"line": 193, "text": "Issue: Epoch not completing"}, {"line": 209, "text": "Issue: Out of memory"}, {"line": 218, "text": "System Architecture"}, {"line": 220, "text": "Block Management"}, {"line": 223, "text": "Streams data in blocks from HuggingFace or local files"}, {"line": 226, "text": "Distributes chunks across GPUs without duplication"}, {"line": 229, "text": "Features:"}, {"line": 230, "text": "- Full training progress tracking"}, {"line": 231, "text": "- Automatic epoch detection  "}, {"line": 232, "text": "- Resume capability from checkpoints"}, {"line": 233, "text": "- Support for all stopping conditions"}, {"line": 236, "text": "Key Components"}, {"line": 243, "text": "Testing"}, {"line": 245, "text": "Test 1: No duplicate training"}, {"line": 248, "text": "Track all chunks trained by each GPU"}, {"line": 252, "text": "After training"}, {"line": 256, "text": "Test 2: Epoch detection"}, {"line": 259, "text": "Train with stop_after_epoch=True"}, {"line": 262, "text": "Should stop when all blocks visited"}, {"line": 267, "text": "Test 3: Resume capability"}, {"line": 270, "text": "Train for 100 steps"}, {"line": 274, "text": "Resume and train 100 more"}, {"line": 278, "text": "Should have 200 total steps, no duplicate chunks"}, {"line": 282, "text": "Related: Configurable dataset chunk size"}, {"line": 286, "text": "Future enhancements"}, {"line": 295, "text": "Summary"}]}, {"path": "guide/features/TOKENIZERS.md", "content": "# Tokenizers - AI-OS\nGenerated: December 12, 2025\nPurpose: Tokenizer support and configuration\nStatus: Implemented (verification varies per model)\n\n## Files\n- `src/aios/core/tokenizers/`\n\n## Supported Tokenizers\n- Verified: GPT-2 family (default)\n- Likely supported via HuggingFace: Qwen, Mistral, Code Llama, DeepSeek-Coder, StarCoder2, Phi-3, Llama 3 (HF auth may be required)\n- Not supported: Vision/multimodal and specialized domain tokenizers\n\n## Configuration\n- Tokenizer is resolved from the selected `--model` (HF hub id or local path)\n- Examples: `gpt2`, `artifacts/hf_implant/base_model`, `mistralai/Mistral-7B-v0.1`\n- Local override: place tokenizer files under `artifacts/hf_implant/tokenizers/` and point `--model` to the matching local model path\n\n## Inputs\n- Text data from datasets (txt/jsonl), read by dataset readers; tokenization occurs during training/eval\n- Tokenizer model files resolved via HuggingFace AutoTokenizer or local tokenizer.json\n\n## Try it: quick check\nTokenization is engaged implicitly by training:\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1 --batch-size 2 --halt-max-steps 1 --eval-batches 1 --log-file artifacts/brains/actv1/metrics.jsonl\n```\nExpected: pipeline loads GPT-2 tokenizer, logs train/eval steps.\n\n## Notes and edge cases\n- HF auth for some models: set `HF_TOKEN` env var if private models are required\n- Sequence length: Max sequence governed by model config; adjust via training flags (see Core Training)\n- Unicode handling: Non-ASCII text is supported; `--ascii-only` exists on some dataset paths to filter\n- Mismatched model/tokenizer: Ensure the model path and tokenizer are compatible to avoid errors\n\nRelated: Datasets, Core Training\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["datasets", "evaluation", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Tokenizers - AI-OS"}, {"line": 5, "text": "Files"}, {"line": 8, "text": "Supported Tokenizers"}, {"line": 13, "text": "Configuration"}, {"line": 18, "text": "Inputs"}, {"line": 22, "text": "Try it: quick check"}, {"line": 29, "text": "Notes and edge cases"}]}, {"path": "guide/features/TOOLS_INTEGRATIONS.md", "content": "# Tools & Integrations\nGenerated: December 12, 2025\nPurpose: Built-in tools and OS/HF integrations with runnable commands, inputs/outputs, and platform notes\nStatus: Implemented (some features are gated/experimental and noted below)\n\n## Web tools (search, crawl, datasets)\n- Source: `src/aios/tools/web.py`, `src/aios/tools/crawler.py`\n- What you get:\n\t- DuckDuckGo HTML-only search with ad/redirect filtering (`ddg_search`)\n\t- Robust HTML parsing and link extraction\n\t- Polite crawler with robots.txt, BFS, per-origin throttling, optional Playwright rendering and Trafilatura extraction\n\t- Turn searches into datasets via CLI builders (images, text, videos, websites)\n\n### Configuration knobs\n- Env vars (affect search/crawl):\n\t- `AIOS_DDG_KL`, `AIOS_DDG_KAD` \u2192 locale/region for DDG (defaults from `config/default.yaml:web.ddg_params`)\n\t- `AIOS_WEB_UA` / `AIOS_WEB_UA_SUFFIX` \u2192 user agent override/suffix\n- YAML: `config/default.yaml:web` mirrors the above defaults and can be edited.\n\n### Crawl a URL \u2192 JSON summary or dataset\n- Command (Windows PowerShell):\n\t- One page fetch/parse\n\t\t- aios crawl https://example.com --ttl-sec 3600 --progress\n\t- Recursive BFS within the same domain, rate-limited\n\t\t- aios crawl https://example.com --recursive --max-pages 25 --max-depth 2 --rps 2 --progress\n\t- Store pages as a text dataset JSONL under datasets pool\n\t\t- aios crawl https://example.com --recursive --store-dataset web_crawl/example --overwrite --progress\n- Inputs/flags (subset):\n\t- `--ttl-sec` cache TTL seconds; `--render` Playwright render; `--trafilatura` article extraction; `--rps` or `--delay-ms` throttling\n\t- `--same-domain/--any-domain`, `--max-pages`, `--max-depth`, `--progress`\n\t- `--store-dataset NAME` outputs to datasets pool at NAME/data.jsonl; `--overwrite` to reset\n- Outputs:\n\t- Progress: JSONL lines on stdout when `--progress` is set, e.g. {\"event\":\"page\",\"n\":1,...}\n\t- Final JSON: pages summary, count, total_chars, and dataset_path/wrote_bytes when `--store-dataset` is used\n- Notes:\n\t- Respect robots.txt by default; pass `--no-robots` for tests only\n\t- Playwright requires browser install; on Windows run once: playwright install\n\n### Build datasets from the web\n- Images\n\t- Command:\n\t\t- aios datasets-build-images \"boats\" --store-dataset boats_v1 --max-images 200 --per-site 40 --pages-per-site 8 --search-results 10 --rps 2 --progress\n\t- Inputs: `--allow-ext jpg,png,webp`, `--near-duplicate-threshold 8`, `--file-prefix boats`\n\t- Outputs: `artifacts path`: datasets/images/boats_v1 with image files + manifest.jsonl (path,label,source_url,page_url,title,alt)\n- Text\n\t- Command:\n\t\t- aios datasets-build-text \"boats\" --store-dataset boats_text_v1 --max-docs 100 --search-results 10 --min-chars 400 --progress\n\t- Inputs: `--allow-ext txt,pdf,docx` (if set, fetches documents by extension/content-type); `--file-prefix`\n\t- Outputs: datasets/text/boats_text_v1/*.txt + manifest.jsonl (path,label,url,title,chars,excerpt)\n- Videos\n\t- Command:\n\t\t- aios datasets-build-videos \"boats\" --store-dataset boats_vid_v1 --max-videos 25 --per-site 5 --min-bytes 50000 --progress\n\t- Inputs: `--allow-ext mp4,webm,mov,m4v`, `--file-prefix`\n\t- Outputs: datasets/videos/boats_vid_v1/*.mp4|*.webm|\u2026 + manifest.jsonl (path,label,source_url,page_url,bytes)\n- Websites (HTML snapshots)\n\t- Command:\n\t\t- aios datasets-build-websites \"boats\" --store-dataset boats_sites_v1 --max-pages 30 --per-site 10 --search-results 10 --progress\n\t- Outputs: datasets/websites/boats_sites_v1/pages/*.html + manifest.jsonl (path,url,title,bytes,links)\n\nRelated docs: see `docs/guide/CORE_TRAINING.md` for using JSONL datasets; `docs/guide/DATASETS.md` for dataset pool and storage caps.\n\n## Filesystem and OS tools (guarded)\n- Source: `src/aios/tools/fs.py`, `src/aios/tools/os.py`\n- What you get:\n\t- `write_text(path, data, cfg, conn)` writes a file with WriteGuard and SafetyBudget enforcement\n\t- `get_system_info()` returns basic platform info\n- Guard/budget behavior:\n\t- Guards read allow/deny from config; budgets use DB-backed usage with tier defaults from `aios.core.budgets`\n\t- Domains charged: `file_writes`\n- Example usage via CLI budgets helpers:\n\t- Show guard paths: aios guards-show\n\t- Simulate a service change budget decision: aios service-restart ssh --dry-run\n\n## Root-helper and service adapters\n- Source: `src/aios/tools/root_helper_client.py`, `src/aios/tools/service_adapter.py`\n- What you get:\n\t- Optional privileged D-Bus client (Linux only). On Windows/macOS, gracefully returns via: \"unavailable\".\n\t- Read-only service diagnostics via local systemctl/journalctl fallback when root-helper is not available.\n- Read status and logs for a unit\n\t- aios status --recent 1 --unit ssh\n\t- For targeted triage, prefer Agent CLI operators below\n- CLI operators for triage (store artifacts):\n\t- aios op-run journal_summary_from_text --unit ssh --lines 200 --label ssh\n\t- aios op-run journal_trend_from_text --unit ssh --lines 200 --label ssh --buckets 12\n\t- Artifacts saved in DB (see Core CLI status for recent artifacts).\n- Notes:\n\t- On Linux, providing a running root-helper yields via: \"root-helper\" in outputs; otherwise via: \"local\".\n\t- On Windows, these commands return via: \"unavailable\" (no systemd).\n\n## Journal parser utilities\n- Source: `src/aios/tools/journal_parser.py`\n- Functions:\n\t- `severity_counts(text) -> Dict[str,int]` heuristic severity tallies across emerg\u2026debug\n- How it\u2019s used:\n\t- The Agent CLI operators compute summaries/trends and persist to DB artifacts. See: `aios op-run ...` above.\n\n## Package/service simulators (budgeted)\n- Source: `src/aios/tools/service.py`, `src/aios/tools/pkg.py`, `src/aios/tools/privileged.py`\n- What you get:\n\t- `restart_service(name, simulate=True)` and `pkg.install/remove(name, simulate=True)` record budget usage for service_changes/pkg_ops\n\t- `run_privileged(fn, ...)` wraps a function and charges privileged_calls budget\n- Try it:\n\t- aios service-restart docker --dry-run\n\t- aios pkg-install git --dry-run\n\n## MCP and external tools (GUI)\n- Source: GUI MCP Manager panel under `src/aios/gui/components/mcp_manager_panel/*`\n- What you get:\n\t- Visual editor for MCP servers and tool permissions using `config/mcp_servers.json` and `config/tool_permissions.json`\n\t- Enable/disable servers and toggle tool permissions; refresh state from disk\n- Status:\n\t- GUI available. Programmatic MCP wiring is scoped to UI; CLI equivalents are not exposed yet.\n\t- If config files are missing, the panel initializes with defaults and saves back on change.\n\t- Panel screenshots in GUI doc: see `docs/guide/features/GUI_FEATURES.md` (MCP & Tools tab).\n\n## Unlimiformer (planned)\n- Source: `src/aios/integrations/unlimiformer/__init__.py`\n- Status: Phase 1 scaffolding; disabled by default via config\n- Config key (example):\n\t- config.default.yaml \u2192 brains.trainer_overrides.unlimiformer.enabled: false\n- Notes:\n\t- When enabled in future phases, the model will be augmented for long-context eval using FAISS; Windows defaults to CPU FAISS.\n\n## Quick reference (commands)\n- Crawling\n\t- aios crawl <url> --recursive --max-pages 25 --max-depth 2 --rps 2 --progress\n- Datasets builders\n\t- aios datasets-build-images \"topic\" --store-dataset name --max-images 200 --progress\n\t- aios datasets-build-text \"topic\" --store-dataset name --max-docs 100 --progress\n\t- aios datasets-build-videos \"topic\" --store-dataset name --max-videos 50 --progress\n\t- aios datasets-build-websites \"topic\" --store-dataset name --max-pages 30 --progress\n- Budgets and guards\n\t- aios guards-show\n\t- aios service-restart ssh --dry-run\n\t- aios pkg-install git --dry-run\n\nRelated: Datasets, Advanced Features\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "gui"], "headings": [{"line": 0, "text": "Tools & Integrations"}, {"line": 5, "text": "Web tools (search, crawl, datasets)"}, {"line": 13, "text": "Configuration knobs"}, {"line": 19, "text": "Crawl a URL \u2192 JSON summary or dataset"}, {"line": 38, "text": "Build datasets from the web"}, {"line": 61, "text": "Filesystem and OS tools (guarded)"}, {"line": 73, "text": "Root-helper and service adapters"}, {"line": 89, "text": "Journal parser utilities"}, {"line": 96, "text": "Package/service simulators (budgeted)"}, {"line": 105, "text": "MCP and external tools (GUI)"}, {"line": 115, "text": "Unlimiformer (planned)"}, {"line": 123, "text": "Quick reference (commands)"}]}, {"path": "planned_features/ADAPTIVE_LEARNING_RATE.md", "content": "# Adaptive Learning Rate System\n\n## Overview\n\nA loss-reactive learning rate scheduler that dynamically adjusts the learning rate during training based on actual training dynamics. Unlike fixed schedules (cosine, step decay), this system monitors training progress and makes intelligent decisions to increase or decrease LR in real-time.\n\n## Motivation\n\n### Current State\nThe existing \"Auto-adjust LR\" feature is a **one-time pre-flight check** that clamps the learning rate for MoE models before training starts. It does not adapt during training.\n\n### Problem\nUsers unfamiliar with deep learning must guess appropriate learning rates:\n- **Too high** \u2192 Training diverges, loss explodes\n- **Too low** \u2192 Training is painfully slow, may get stuck\n- **Just right** \u2192 Requires expertise and experimentation\n\n### Solution\nA truly adaptive system that:\n1. Starts with a reasonable default or user-specified LR\n2. Monitors training loss over rolling windows\n3. **Increases LR** when training is progressing well (learn faster)\n4. **Decreases LR** when training stalls or becomes unstable (fine-tune)\n5. Respects safety bounds, especially for MoE models\n\n## Design\n\n### Core Principle: Follow the Loss Gradient\n\n```\nLoss dropping steadily  \u2192  Increase LR (we can learn faster!)\nLoss plateauing         \u2192  Decrease LR (need finer adjustments)\nLoss spiking            \u2192  Decrease LR significantly (stabilize)\nLoss oscillating        \u2192  Decrease LR (too aggressive)\n```\n\n### Architecture\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    AdaptiveLRScheduler                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502 Loss Monitor \u2502\u2500\u2500\u2500\u25b6\u2502   Decision   \u2502\u2500\u2500\u2500\u25b6\u2502  LR Adjuster \u2502     \u2502\n\u2502  \u2502   (Window)   \u2502    \u2502    Engine    \u2502    \u2502   (Bounded)  \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502         \u2502                   \u2502                   \u2502               \u2502\n\u2502         \u25bc                   \u25bc                   \u25bc               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502 Rolling Avg  \u2502    \u2502  Thresholds  \u2502    \u2502 Safety Bounds\u2502     \u2502\n\u2502  \u2502  Variance    \u2502    \u2502   Patience   \u2502    \u2502  Rate Limit  \u2502     \u2502\n\u2502  \u2502   Trend      \u2502    \u2502   Cooldown   \u2502    \u2502  MoE Clamps  \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Signal Processing\n\n#### 1. Rolling Loss Window\n```python\n# Collect losses over a window\nwindow_size = 50  # steps\nloss_window = deque(maxlen=window_size)\n\n# Compute statistics\ncurrent_avg = mean(loss_window)\ncurrent_std = std(loss_window)\nprevious_avg = mean(previous_window)  # From last evaluation\n```\n\n#### 2. Derived Metrics\n```python\n# Improvement: How much did loss drop?\nimprovement = previous_avg - current_avg  # Positive = good\n\n# Improvement rate: Normalized by previous loss\nimprovement_rate = improvement / previous_avg  # Percentage\n\n# Stability: How consistent is training?\ncoefficient_of_variation = current_std / current_avg  # Lower = more stable\n\n# Trend: Direction over the window (linear regression slope)\ntrend = compute_slope(loss_window)  # Negative = improving\n```\n\n### Decision Logic\n\n```python\nclass LRDecision(Enum):\n    INCREASE = \"increase\"   # Training going well, speed up\n    DECREASE = \"decrease\"   # Training struggling, slow down\n    HOLD = \"hold\"           # Maintain current LR\n    \ndef make_decision(metrics: LossMetrics, config: AdaptiveConfig) -> LRDecision:\n    # Safety: Don't adjust during cooldown\n    if steps_since_last_adjustment < config.cooldown_steps:\n        return LRDecision.HOLD\n    \n    # INCREASE: Consistent improvement + stable training\n    if (metrics.improvement_rate > config.increase_threshold and\n        metrics.cv < config.stability_threshold and\n        metrics.trend < 0):  # Still improving\n        return LRDecision.INCREASE\n    \n    # DECREASE (Spike): Sudden loss increase\n    if metrics.current_avg > metrics.previous_avg * config.spike_threshold:\n        return LRDecision.DECREASE\n    \n    # DECREASE (Plateau): No improvement for too long\n    if (metrics.improvement_rate < config.plateau_threshold and\n        patience_counter >= config.patience):\n        return LRDecision.DECREASE\n    \n    # DECREASE (Unstable): High variance in loss\n    if metrics.cv > config.instability_threshold:\n        return LRDecision.DECREASE\n    \n    return LRDecision.HOLD\n```\n\n### LR Adjustment Strategy\n\n#### Multiplicative Updates (Preferred)\n```python\n# Increase: Multiply by factor > 1\nnew_lr = current_lr * increase_factor  # e.g., 1.1 (+10%)\n\n# Decrease: Multiply by factor < 1  \nnew_lr = current_lr * decrease_factor  # e.g., 0.8 (-20%)\n```\n\n#### Asymmetric Factors\nDecreases should be **more aggressive** than increases to prevent runaway divergence:\n```python\nincrease_factor = 1.1   # +10% when things are good\ndecrease_factor = 0.7   # -30% when things are bad (faster recovery)\nspike_factor = 0.5      # -50% on loss spike (emergency brake)\n```\n\n### Safety Mechanisms\n\n#### 1. Absolute Bounds\n```python\nlr_min = 1e-7   # Floor: never go below this\nlr_max = 1e-2   # Ceiling: never go above this\n\nnew_lr = max(lr_min, min(lr_max, new_lr))\n```\n\n#### 2. MoE-Specific Bounds\nMoE router networks are sensitive to LR changes:\n```python\nif config.use_moe:\n    lr_min = max(lr_min, 1e-5)    # Higher floor for MoE\n    lr_max = min(lr_max, 2e-3)    # Lower ceiling for MoE\n    increase_factor = 1.05        # More conservative increases\n    decrease_factor = 0.85        # Less aggressive decreases\n```\n\n#### 3. Rate Limiting\nPrevent dramatic changes:\n```python\nmax_change_per_step = 0.3  # Max 30% change in either direction\nratio = new_lr / current_lr\nif ratio > 1 + max_change_per_step:\n    new_lr = current_lr * (1 + max_change_per_step)\nelif ratio < 1 - max_change_per_step:\n    new_lr = current_lr * (1 - max_change_per_step)\n```\n\n#### 4. Cooldown Period\nDon't adjust too frequently:\n```python\ncooldown_steps = 20  # Wait 20 steps after any adjustment\n```\n\n#### 5. Warmup Integration\nAdaptive scheduling starts AFTER warmup completes:\n```python\nif step < warmup_steps:\n    # Standard linear warmup\n    lr = base_lr * (step / warmup_steps)\nelse:\n    # Adaptive scheduling takes over\n    lr = adaptive_scheduler.step(loss)\n```\n\n## Implementation\n\n### Tuning via Config File\n\nAll adaptive LR scheduler knobs can be overridden via a config file.\n\n- CLI: pass `--adaptive-lr-config <path>` (JSON/TOML/YAML)\n- Precedence: safe defaults (derived from `--lr`) \u2192 config overrides\n\nExample:\n\n```bash\naios hrm-hf train-actv1 \\\n    --dataset-file training_datasets/actv1/your_dataset.txt \\\n    --auto-adjust-lr \\\n    --adaptive-lr-config config/adaptive_lr.toml.example\n```\n\nNotes:\n- TOML is dependency-free (Python 3.11+ includes `tomllib`).\n- YAML requires `pip install pyyaml`.\n- You can put keys at the file top-level or nest them under `adaptive_lr:` / `[adaptive_lr]`.\n\n### Class Design\n\n```python\n# File: src/aios/cli/hrm_hf/adaptive_lr.py\n\nfrom __future__ import annotations\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Optional, Callable, Any\nimport math\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass AdaptiveLRConfig:\n    \"\"\"Configuration for adaptive learning rate scheduling.\"\"\"\n    \n    # Core parameters (auto-calculated if not specified)\n    initial_lr: float = 5e-5\n    lr_min: float = 1e-7\n    lr_max: float = 1e-2\n    \n    # Window and patience\n    window_size: int = 50\n    patience: int = 20  # Steps without improvement before decreasing\n    cooldown_steps: int = 20  # Steps to wait after adjustment\n    \n    # Thresholds\n    increase_threshold: float = 0.02   # 2% improvement to consider increasing\n    plateau_threshold: float = 0.001   # <0.1% improvement = plateau\n    spike_threshold: float = 1.2       # 20% loss increase = spike\n    stability_threshold: float = 0.3   # CV > 0.3 = unstable\n    instability_threshold: float = 0.5 # CV > 0.5 = very unstable\n    \n    # Adjustment factors\n    increase_factor: float = 1.1   # +10%\n    decrease_factor: float = 0.7   # -30%\n    spike_factor: float = 0.5      # -50%\n    \n    # Safety\n    max_change_per_step: float = 0.3  # Max 30% change\n    \n    # MoE-specific (auto-applied when use_moe=True)\n    moe_lr_min: float = 1e-5\n    moe_lr_max: float = 2e-3\n    moe_increase_factor: float = 1.05\n    moe_decrease_factor: float = 0.85\n\n\nclass LRDecision(Enum):\n    INCREASE = \"increase\"\n    DECREASE = \"decrease\"\n    HOLD = \"hold\"\n    SPIKE_DECREASE = \"spike_decrease\"\n\n\n@dataclass\nclass LossMetrics:\n    \"\"\"Computed metrics from loss window.\"\"\"\n    current_avg: float\n    previous_avg: float\n    current_std: float\n    improvement: float\n    improvement_rate: float\n    cv: float  # Coefficient of variation\n    trend: float  # Slope of loss over window\n\n\nclass AdaptiveLRScheduler:\n    \"\"\"Loss-reactive learning rate scheduler.\n    \n    Monitors training loss and adjusts LR:\n    - Increases when training is progressing well\n    - Decreases when training stalls or becomes unstable\n    \n    Integrates with existing warmup phase.\n    \"\"\"\n    \n    def __init__(\n        self,\n        optimizer: Any,\n        config: AdaptiveLRConfig,\n        use_moe: bool = False,\n        log_fn: Optional[Callable] = None,\n    ):\n        self.optimizer = optimizer\n        self.config = self._apply_moe_overrides(config, use_moe)\n        self.use_moe = use_moe\n        self.log_fn = log_fn or (lambda x: None)\n        \n        # State\n        self.current_lr = config.initial_lr\n        self.loss_window: deque = deque(maxlen=config.window_size)\n        self.previous_window_avg: Optional[float] = None\n        self.steps_since_adjustment = 0\n        self.patience_counter = 0\n        self.total_steps = 0\n        self.adjustments_made = 0\n        \n        # History for debugging\n        self.lr_history: list[tuple[int, float, str]] = []\n        \n        # Set initial LR\n        self._set_lr(self.current_lr)\n        \n        self.log_fn({\n            \"event\": \"adaptive_lr_init\",\n            \"initial_lr\": self.current_lr,\n            \"lr_min\": self.config.lr_min,\n            \"lr_max\": self.config.lr_max,\n            \"use_moe\": use_moe,\n            \"window_size\": self.config.window_size,\n        })\n    \n    def _apply_moe_overrides(\n        self, config: AdaptiveLRConfig, use_moe: bool\n    ) -> AdaptiveLRConfig:\n        \"\"\"Apply MoE-specific conservative bounds.\"\"\"\n        if not use_moe:\n            return config\n        \n        return AdaptiveLRConfig(\n            initial_lr=config.initial_lr,\n            lr_min=max(config.lr_min, config.moe_lr_min),\n            lr_max=min(config.lr_max, config.moe_lr_max),\n            window_size=config.window_size,\n            patience=config.patience,\n            cooldown_steps=config.cooldown_steps,\n            increase_threshold=config.increase_threshold,\n            plateau_threshold=config.plateau_threshold,\n            spike_threshold=config.spike_threshold,\n            stability_threshold=config.stability_threshold,\n            instability_threshold=config.instability_threshold,\n            increase_factor=config.moe_increase_factor,\n            decrease_factor=config.moe_decrease_factor,\n            spike_factor=config.spike_factor,\n            max_change_per_step=config.max_change_per_step,\n            moe_lr_min=config.moe_lr_min,\n            moe_lr_max=config.moe_lr_max,\n            moe_increase_factor=config.moe_increase_factor,\n            moe_decrease_factor=config.moe_decrease_factor,\n        )\n    \n    def step(self, loss: float) -> float:\n        \"\"\"Process a training step and potentially adjust LR.\n        \n        Args:\n            loss: The loss value from this training step\n            \n        Returns:\n            The current learning rate (may have been adjusted)\n        \"\"\"\n        self.total_steps += 1\n        self.steps_since_adjustment += 1\n        self.loss_window.append(loss)\n        \n        # Need full window before making decisions\n        if len(self.loss_window) < self.config.window_size:\n            return self.current_lr\n        \n        # Compute metrics\n        metrics = self._compute_metrics()\n        \n        # Make decision\n        decision = self._make_decision(metrics)\n        \n        # Apply decision\n        if decision != LRDecision.HOLD:\n            self._apply_decision(decision, metrics)\n        \n        # Update state for next window\n        self.previous_window_avg = metrics.current_avg\n        self.loss_window.clear()\n        \n        return self.current_lr\n    \n    def _compute_metrics(self) -> LossMetrics:\n        \"\"\"Compute loss statistics from current window.\"\"\"\n        losses = list(self.loss_window)\n        n = len(losses)\n        \n        current_avg = sum(losses) / n\n        variance = sum((x - current_avg) ** 2 for x in losses) / n\n        current_std = math.sqrt(variance)\n        \n        # Coefficient of variation (normalized std)\n        cv = current_std / current_avg if current_avg > 0 else 0\n        \n        # Improvement from previous window\n        if self.previous_window_avg is not None:\n            improvement = self.previous_window_avg - current_avg\n            improvement_rate = improvement / self.previous_window_avg if self.previous_window_avg > 0 else 0\n        else:\n            improvement = 0\n            improvement_rate = 0\n        \n        # Trend: simple linear regression slope\n        trend = self._compute_trend(losses)\n        \n        return LossMetrics(\n            current_avg=current_avg,\n            previous_avg=self.previous_window_avg or current_avg,\n            current_std=current_std,\n            improvement=improvement,\n            improvement_rate=improvement_rate,\n            cv=cv,\n            trend=trend,\n        )\n    \n    def _compute_trend(self, losses: list[float]) -> float:\n        \"\"\"Compute slope of loss over window (negative = improving).\"\"\"\n        n = len(losses)\n        if n < 2:\n            return 0\n        \n        # Simple linear regression\n        x_mean = (n - 1) / 2\n        y_mean = sum(losses) / n\n        \n        numerator = sum((i - x_mean) * (y - y_mean) for i, y in enumerate(losses))\n        denominator = sum((i - x_mean) ** 2 for i in range(n))\n        \n        if denominator == 0:\n            return 0\n        \n        return numerator / denominator\n    \n    def _make_decision(self, metrics: LossMetrics) -> LRDecision:\n        \"\"\"Decide whether to adjust LR based on metrics.\"\"\"\n        cfg = self.config\n        \n        # Cooldown: don't adjust too frequently\n        if self.steps_since_adjustment < cfg.cooldown_steps:\n            return LRDecision.HOLD\n        \n        # SPIKE: Emergency decrease\n        if self.previous_window_avg and metrics.current_avg > self.previous_window_avg * cfg.spike_threshold:\n            self.patience_counter = 0\n            return LRDecision.SPIKE_DECREASE\n        \n        # INSTABILITY: High variance, decrease\n        if metrics.cv > cfg.instability_threshold:\n            self.patience_counter = 0\n            return LRDecision.DECREASE\n        \n        # INCREASE: Consistent improvement + stable\n        if (metrics.improvement_rate > cfg.increase_threshold and\n            metrics.cv < cfg.stability_threshold and\n            metrics.trend < 0 and  # Still improving\n            self.current_lr < cfg.lr_max):\n            self.patience_counter = 0\n            return LRDecision.INCREASE\n        \n        # PLATEAU: No improvement for too long\n        if metrics.improvement_rate < cfg.plateau_threshold:\n            self.patience_counter += 1\n            if self.patience_counter >= cfg.patience and self.current_lr > cfg.lr_min:\n                self.patience_counter = 0\n                return LRDecision.DECREASE\n        else:\n            self.patience_counter = 0\n        \n        return LRDecision.HOLD\n    \n    def _apply_decision(self, decision: LRDecision, metrics: LossMetrics) -> None:\n        \"\"\"Apply LR adjustment based on decision.\"\"\"\n        cfg = self.config\n        old_lr = self.current_lr\n        \n        if decision == LRDecision.INCREASE:\n            new_lr = self.current_lr * cfg.increase_factor\n            reason = f\"improvement_rate={metrics.improvement_rate:.4f} > threshold\"\n        elif decision == LRDecision.DECREASE:\n            new_lr = self.current_lr * cfg.decrease_factor\n            reason = f\"plateau/instability (cv={metrics.cv:.3f})\"\n        elif decision == LRDecision.SPIKE_DECREASE:\n            new_lr = self.current_lr * cfg.spike_factor\n            reason = f\"loss_spike ({metrics.current_avg:.4f} vs {metrics.previous_avg:.4f})\"\n        else:\n            return\n        \n        # Rate limiting\n        ratio = new_lr / old_lr\n        if ratio > 1 + cfg.max_change_per_step:\n            new_lr = old_lr * (1 + cfg.max_change_per_step)\n        elif ratio < 1 - cfg.max_change_per_step:\n            new_lr = old_lr * (1 - cfg.max_change_per_step)\n        \n        # Bound enforcement\n        new_lr = max(cfg.lr_min, min(cfg.lr_max, new_lr))\n        \n        # Apply\n        if new_lr != old_lr:\n            self._set_lr(new_lr)\n            self.steps_since_adjustment = 0\n            self.adjustments_made += 1\n            \n            self.lr_history.append((self.total_steps, new_lr, decision.value))\n            \n            self.log_fn({\n                \"event\": \"adaptive_lr_adjustment\",\n                \"step\": self.total_steps,\n                \"decision\": decision.value,\n                \"old_lr\": old_lr,\n                \"new_lr\": new_lr,\n                \"change_pct\": (new_lr - old_lr) / old_lr * 100,\n                \"reason\": reason,\n                \"metrics\": {\n                    \"loss_avg\": metrics.current_avg,\n                    \"improvement_rate\": metrics.improvement_rate,\n                    \"cv\": metrics.cv,\n                    \"trend\": metrics.trend,\n                },\n            })\n    \n    def _set_lr(self, lr: float) -> None:\n        \"\"\"Set learning rate on optimizer.\"\"\"\n        self.current_lr = lr\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n    \n    def get_lr(self) -> float:\n        \"\"\"Get current learning rate.\"\"\"\n        return self.current_lr\n    \n    def get_summary(self) -> dict:\n        \"\"\"Get summary statistics for logging.\"\"\"\n        return {\n            \"total_steps\": self.total_steps,\n            \"adjustments_made\": self.adjustments_made,\n            \"final_lr\": self.current_lr,\n            \"lr_history_length\": len(self.lr_history),\n        }\n```\n\n### Integration with Training Loop\n\n```python\n# In train_epoch.py\n\n# After warmup completes, create adaptive scheduler\nif config.auto_adjust_lr and steps_done >= warmup_steps:\n    if adaptive_scheduler is None:\n        adaptive_scheduler = AdaptiveLRScheduler(\n            optimizer=opt,\n            config=AdaptiveLRConfig(initial_lr=base_lr),\n            use_moe=config.use_moe,\n            log_fn=write_jsonl,\n        )\n    \n    # Call step() with current loss\n    new_lr = adaptive_scheduler.step(loss.item())\n```\n\n### Warmup + Adaptive Phases\n\n```\nPhase 1: Linear Warmup (unchanged)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nStep 0 \u2192 warmup_steps\nLR: 0 \u2192 base_lr (linear ramp)\n\nPhase 2: Adaptive Scheduling (NEW)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nStep warmup_steps \u2192 end\nLR: Dynamically adjusted based on loss\n\nExample timeline (1000 steps, base_lr=0.0001):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 0.0002 \u2500                        \u256d\u2500\u2500\u2500\u2500\u2500\u256e                        \u2502\n\u2502          \u2500                     \u2571       \u2572                       \u2502\n\u2502 0.0001 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2571\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2571         \u2572\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500             \u2502\n\u2502        \u2571                                   \u2572                   \u2502\n\u2502 0.00005             \u2571                        \u2572\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500    \u2502\n\u2502      \u2571                                                         \u2502\n\u2502    \u2571                                                           \u2502\n\u2502  \u2571    Warmup   \u2502      Adaptive Phase                           \u2502\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 \u2502\n\u2502 0     100    200    400    600    800    1000  steps           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502        \u2502         \u2502         \u2502\n         \u2502        \u2502         \u2502         \u2514\u2500 Plateau detected \u2192 decrease\n         \u2502        \u2502         \u2514\u2500 Loss improving \u2192 increase\n         \u2502        \u2514\u2500 Spike detected \u2192 emergency decrease\n         \u2514\u2500 Warmup complete, adaptive starts\n```\n\n## Configuration\n\n### Auto-Calculated Defaults\n\nWhen user checks \"Auto-adjust learning rate\", all parameters are computed automatically:\n\n| Parameter | Formula | Example |\n|-----------|---------|---------|\n| `initial_lr` | User's LR or 5e-5 | 0.00005 |\n| `lr_min` | `initial_lr / 100` | 5e-7 |\n| `lr_max` | `initial_lr * 20` | 0.001 |\n| `window_size` | `min(50, steps // 20)` | 50 |\n| `warmup_steps` | `steps * 0.1` | 100 (for 1000 steps) |\n\n### MoE Automatic Adjustments\n\nWhen `use_moe=True`, bounds are automatically tightened:\n\n| Parameter | Standard | MoE |\n|-----------|----------|-----|\n| `lr_min` | 1e-7 | 1e-5 |\n| `lr_max` | 1e-2 | 2e-3 |\n| `increase_factor` | 1.1 | 1.05 |\n| `decrease_factor` | 0.7 | 0.85 |\n\n## User Experience\n\n### Before (Current)\n```\n\u2611\ufe0f Auto-adjust learning rate\n   \u2192 One-time clamp for MoE, then fixed LR\n```\n\n### After (This Feature)\n```\n\u2611\ufe0f Auto-adjust learning rate\n   \u2192 Smart pre-flight clamp for MoE\n   \u2192 Linear warmup phase\n   \u2192 Loss-reactive adaptive phase:\n     \u2022 Speeds up when learning is going well\n     \u2022 Slows down when stuck or unstable\n     \u2022 Emergency brake on loss spikes\n   \u2192 All automatic, no tuning required\n```\n\n### Logging Output\n\nUsers will see LR adjustments in the training log:\n```json\n{\"event\": \"adaptive_lr_adjustment\", \"step\": 250, \"decision\": \"increase\", \"old_lr\": 0.0001, \"new_lr\": 0.00011, \"change_pct\": 10.0, \"reason\": \"improvement_rate=0.0312 > threshold\"}\n{\"event\": \"adaptive_lr_adjustment\", \"step\": 500, \"decision\": \"decrease\", \"old_lr\": 0.00015, \"new_lr\": 0.000105, \"change_pct\": -30.0, \"reason\": \"plateau/instability (cv=0.421)\"}\n{\"event\": \"adaptive_lr_adjustment\", \"step\": 750, \"decision\": \"spike_decrease\", \"old_lr\": 0.000105, \"new_lr\": 0.0000525, \"change_pct\": -50.0, \"reason\": \"loss_spike (2.451 vs 1.892)\"}\n```\n\n## Testing Strategy\n\n### Unit Tests\n1. `test_metrics_computation` - Verify loss statistics are correct\n2. `test_decision_logic` - Test each decision path\n3. `test_moe_bounds` - Verify MoE constraints are applied\n4. `test_rate_limiting` - Ensure changes are bounded\n5. `test_cooldown` - Verify cooldown prevents rapid changes\n\n### Integration Tests\n1. `test_warmup_to_adaptive_transition` - Seamless handoff\n2. `test_with_real_training_loop` - End-to-end with actual model\n3. `test_checkpoint_resume` - Scheduler state persists across restarts\n\n### Scenarios to Validate\n1. **Healthy training** - LR should gradually increase, then stabilize\n2. **Stuck training** - LR should decrease after patience exhausted\n3. **Diverging training** - LR should emergency decrease on spike\n4. **MoE training** - Bounds should be tighter, changes more conservative\n\n## Implementation Plan\n\n### Phase 1: Core Module\n- [ ] Create `src/aios/cli/hrm_hf/adaptive_lr.py`\n- [ ] Implement `AdaptiveLRScheduler` class\n- [ ] Implement `AdaptiveLRConfig` dataclass\n- [ ] Add comprehensive logging\n\n### Phase 2: Training Integration\n- [ ] Modify `train_epoch.py` to accept scheduler\n- [ ] Handle warmup \u2192 adaptive transition\n- [ ] Pass scheduler through training loop chain\n\n### Phase 3: Wiring\n- [ ] Create scheduler in `train_actv1.py` when `auto_adjust_lr=True`\n- [ ] Create scheduler in `parallel_training_v3.py` for multi-GPU\n- [ ] Ensure scheduler state is per-GPU in parallel mode\n\n### Phase 4: Checkpoint Support\n- [ ] Save scheduler state in checkpoints\n- [ ] Restore scheduler state on resume\n- [ ] Handle missing scheduler state (backward compatibility)\n\n### Phase 5: Testing & Validation\n- [ ] Unit tests for scheduler logic\n- [ ] Integration tests with training loop\n- [ ] Manual validation with real training runs\n- [ ] Performance profiling (ensure minimal overhead)\n\n## Risks and Mitigations\n\n| Risk | Impact | Mitigation |\n|------|--------|------------|\n| Scheduler makes bad decisions | Training quality degrades | Conservative defaults, rate limiting, MoE bounds |\n| Overhead slows training | Reduced throughput | Efficient windowed stats, only compute every window_size steps |\n| State lost on crash | Resume behaves differently | Save scheduler state in checkpoint |\n| Conflicts with DeepSpeed scheduler | Undefined behavior | Detect DeepSpeed scheduler, disable adaptive if present |\n\n## Future Enhancements\n\n### Gradient-Aware Adjustments\nMonitor gradient norms in addition to loss:\n- Exploding gradients \u2192 Immediate LR decrease\n- Vanishing gradients \u2192 LR increase\n\n### Learning Rate Range Test\nAuto-discover good LR bounds at start of training:\n- Run quick sweep from 1e-7 to 1e-1\n- Find range where loss decreases\n- Use as bounds for adaptive scheduling\n\n### Per-Parameter-Group LR\nDifferent LR schedules for different parts of model:\n- Base model: Conservative\n- Router (MoE): Very conservative\n- LoRA adapters: More aggressive\n\n## References\n\n- [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186) - Smith 2015\n- [Super-Convergence: Very Fast Training of Neural Networks](https://arxiv.org/abs/1708.07120) - Smith & Topin 2017\n- [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101) - Loshchilov & Hutter 2017 (AdamW)\n- Existing AI-OS `width_management.py` - Similar loss-reactive pattern for model width\n", "tags": ["evaluation", "experts", "training"], "headings": [{"line": 0, "text": "Adaptive Learning Rate System"}, {"line": 2, "text": "Overview"}, {"line": 6, "text": "Motivation"}, {"line": 8, "text": "Current State"}, {"line": 11, "text": "Problem"}, {"line": 17, "text": "Solution"}, {"line": 25, "text": "Design"}, {"line": 27, "text": "Core Principle: Follow the Loss Gradient"}, {"line": 36, "text": "Architecture"}, {"line": 58, "text": "Signal Processing"}, {"line": 60, "text": "1. Rolling Loss Window"}, {"line": 62, "text": "Collect losses over a window"}, {"line": 66, "text": "Compute statistics"}, {"line": 72, "text": "2. Derived Metrics"}, {"line": 74, "text": "Improvement: How much did loss drop?"}, {"line": 77, "text": "Improvement rate: Normalized by previous loss"}, {"line": 80, "text": "Stability: How consistent is training?"}, {"line": 83, "text": "Trend: Direction over the window (linear regression slope)"}, {"line": 87, "text": "Decision Logic"}, {"line": 96, "text": "Safety: Don't adjust during cooldown"}, {"line": 100, "text": "INCREASE: Consistent improvement + stable training"}, {"line": 106, "text": "DECREASE (Spike): Sudden loss increase"}, {"line": 110, "text": "DECREASE (Plateau): No improvement for too long"}, {"line": 115, "text": "DECREASE (Unstable): High variance in loss"}, {"line": 122, "text": "LR Adjustment Strategy"}, {"line": 124, "text": "Multiplicative Updates (Preferred)"}, {"line": 126, "text": "Increase: Multiply by factor > 1"}, {"line": 129, "text": "Decrease: Multiply by factor < 1  "}, {"line": 133, "text": "Asymmetric Factors"}, {"line": 141, "text": "Safety Mechanisms"}, {"line": 143, "text": "1. Absolute Bounds"}, {"line": 151, "text": "2. MoE-Specific Bounds"}, {"line": 161, "text": "3. Rate Limiting"}, {"line": 172, "text": "4. Cooldown Period"}, {"line": 178, "text": "5. Warmup Integration"}, {"line": 182, "text": "Standard linear warmup"}, {"line": 185, "text": "Adaptive scheduling takes over"}, {"line": 189, "text": "Implementation"}, {"line": 191, "text": "Tuning via Config File"}, {"line": 212, "text": "Class Design"}, {"line": 215, "text": "File: src/aios/cli/hrm_hf/adaptive_lr.py"}, {"line": 232, "text": "Core parameters (auto-calculated if not specified)"}, {"line": 237, "text": "Window and patience"}, {"line": 242, "text": "Thresholds"}, {"line": 249, "text": "Adjustment factors"}, {"line": 254, "text": "Safety"}, {"line": 257, "text": "MoE-specific (auto-applied when use_moe=True)"}, {"line": 305, "text": "State"}, {"line": 314, "text": "History for debugging"}, {"line": 317, "text": "Set initial LR"}, {"line": 371, "text": "Need full window before making decisions"}, {"line": 375, "text": "Compute metrics"}, {"line": 378, "text": "Make decision"}, {"line": 381, "text": "Apply decision"}, {"line": 385, "text": "Update state for next window"}, {"line": 400, "text": "Coefficient of variation (normalized std)"}, {"line": 403, "text": "Improvement from previous window"}, {"line": 411, "text": "Trend: simple linear regression slope"}, {"line": 430, "text": "Simple linear regression"}, {"line": 446, "text": "Cooldown: don't adjust too frequently"}, {"line": 450, "text": "SPIKE: Emergency decrease"}, {"line": 455, "text": "INSTABILITY: High variance, decrease"}, {"line": 460, "text": "INCREASE: Consistent improvement + stable"}, {"line": 468, "text": "PLATEAU: No improvement for too long"}, {"line": 496, "text": "Rate limiting"}, {"line": 503, "text": "Bound enforcement"}, {"line": 506, "text": "Apply"}, {"line": 550, "text": "Integration with Training Loop"}, {"line": 553, "text": "In train_epoch.py"}, {"line": 555, "text": "After warmup completes, create adaptive scheduler"}, {"line": 565, "text": "Call step() with current loss"}, {"line": 569, "text": "Warmup + Adaptive Phases"}, {"line": 602, "text": "Configuration"}, {"line": 604, "text": "Auto-Calculated Defaults"}, {"line": 616, "text": "MoE Automatic Adjustments"}, {"line": 627, "text": "User Experience"}, {"line": 629, "text": "Before (Current)"}, {"line": 635, "text": "After (This Feature)"}, {"line": 647, "text": "Logging Output"}, {"line": 656, "text": "Testing Strategy"}, {"line": 658, "text": "Unit Tests"}, {"line": 665, "text": "Integration Tests"}, {"line": 670, "text": "Scenarios to Validate"}, {"line": 676, "text": "Implementation Plan"}, {"line": 678, "text": "Phase 1: Core Module"}, {"line": 684, "text": "Phase 2: Training Integration"}, {"line": 689, "text": "Phase 3: Wiring"}, {"line": 694, "text": "Phase 4: Checkpoint Support"}, {"line": 699, "text": "Phase 5: Testing & Validation"}, {"line": 705, "text": "Risks and Mitigations"}, {"line": 714, "text": "Future Enhancements"}, {"line": 716, "text": "Gradient-Aware Adjustments"}, {"line": 721, "text": "Learning Rate Range Test"}, {"line": 727, "text": "Per-Parameter-Group LR"}, {"line": 733, "text": "References"}]}, {"path": "planned_features/AMD_INTEL_GPU_SUPPORT_FIX.md", "content": "# AMD and Intel GPU Support Enhancement\n\n**Status:** \ud83d\udccb Planned  \n**Priority:** High  \n**Complexity:** Medium (4-8 hours implementation)  \n**Target Version:** v1.1.0  \n**Created:** 2025-10-23  \n**Owner:** @AI-OS-Team\n\n## Overview\n\nEnhance detection, installation, and optimization support for AMD and Intel GPUs. Currently, AMD GPUs work but are mislabeled as \"CUDA\", and Intel XPU support is incomplete. This plan addresses all gaps to provide first-class support for non-NVIDIA GPUs.\n\n## Problem Statement\n\n### Current Issues\n\n1. **AMD ROCm GPUs** - Detected but mislabeled as generic \"CUDA\" devices\n2. **Intel Arc/Xe GPUs** - Basic support exists but memory optimization broken\n3. **Installation Scripts** - Only detect NVIDIA, auto-install wrong PyTorch variant\n4. **GUI** - Doesn't distinguish GPU vendors or show XPU devices\n5. **Memory Optimizer** - Only checks CUDA, reports 0 VRAM for XPU-only systems\n\n### Impact\n\n- AMD GPU users confused about which hardware is being used\n- Intel GPU users can't use memory optimization features\n- Users must manually install correct PyTorch variant\n- Sub-optimal experience for non-NVIDIA hardware\n\n## Goals\n\n### Must Have \u2705\n\n- [x] Correctly identify and label AMD GPUs\n- [x] Add Intel XPU memory detection\n- [x] Update installation scripts to detect all GPU vendors\n- [x] Add `intel-extension-for-pytorch` to dependencies\n- [x] Update GUI to show vendor information\n- [x] Update `aios torch-info` command to report all vendors\n\n### Nice to Have \ud83c\udfaf\n\n- [ ] Vendor-specific optimization hints\n- [ ] Auto-install correct PyTorch variant based on detected hardware\n- [ ] XPU distributed training support (oneCCL backend)\n- [ ] Performance benchmarking across vendors\n\n### Out of Scope \u274c\n\n- Mixed-vendor parallel training (see `MIXED_GPU_VENDOR_SUPPORT.md`)\n- Vendor-specific kernel optimizations\n- GPU virtualization support\n- Cloud GPU support (AWS/Azure specific configurations)\n\n## Implementation Plan\n\n### Phase 1: Core Detection (High Priority)\n\n**Estimated Time:** 2 hours\n\n#### Task 1.1: Add GPU Vendor Identification Function\n**Files:** `src/aios/cli/training/torch_info_cmd.py`\n\n- [ ] Create `identify_gpu_vendor()` function\n- [ ] Check GPU name for vendor keywords (AMD, Radeon, MI, Intel, Arc, Xe, NVIDIA)\n- [ ] Check `torch.version.hip` for ROCm builds\n- [ ] Return vendor enum: `'NVIDIA' | 'AMD' | 'Intel' | 'Unknown'`\n\n**Implementation:**\n```python\ndef identify_gpu_vendor(gpu_name: str, check_rocm: bool = False) -> str:\n    \"\"\"Identify GPU vendor from device name.\n    \n    Args:\n        gpu_name: GPU device name from torch\n        check_rocm: Also check torch.version.hip attribute\n        \n    Returns:\n        'NVIDIA' | 'AMD' | 'Intel' | 'Unknown'\n    \"\"\"\n    name_lower = gpu_name.lower()\n    \n    # NVIDIA detection\n    if any(kw in name_lower for kw in ['nvidia', 'geforce', 'rtx', 'gtx', 'quadro', 'tesla', 'a100', 'h100']):\n        return 'NVIDIA'\n    \n    # AMD detection\n    if any(kw in name_lower for kw in ['amd', 'radeon', 'rx ', 'vega', 'navi', 'mi50', 'mi100', 'mi200', 'mi300']):\n        return 'AMD'\n    \n    # Intel detection\n    if any(kw in name_lower for kw in ['intel', 'arc', 'xe', 'iris', 'uhd']):\n        return 'Intel'\n    \n    # Check ROCm build as fallback\n    if check_rocm:\n        try:\n            import torch\n            if getattr(torch.version, 'hip', None):\n                return 'AMD'\n        except Exception:\n            pass\n    \n    return 'Unknown'\n```\n\n**Tests:**\n- [ ] Test with NVIDIA GPU names (GeForce RTX 4090, Tesla V100)\n- [ ] Test with AMD GPU names (Radeon RX 7900 XTX, MI210)\n- [ ] Test with Intel GPU names (Arc A770, Xe HPG)\n- [ ] Test ROCm detection fallback\n\n#### Task 1.2: Update torch-info Command\n**Files:** `src/aios/cli/training/torch_info_cmd.py`\n\n- [ ] Import vendor identification function\n- [ ] Add `vendor` field to each GPU in `cuda_devices` list\n- [ ] Update `rocm` field to use vendor detection\n- [ ] Add `gpu_vendor_summary` field with counts per vendor\n\n**Changes:**\n```python\n# In torch_info() function, for each CUDA device:\ndev: dict = {\"id\": i, \"name\": name}\nif total_mb:\n    dev[\"total_mem_mb\"] = int(total_mb)\n\n# ADD:\ndev[\"vendor\"] = identify_gpu_vendor(name, check_rocm=rocm)\n\ncuda_devices.append(dev)\n\n# ADD after device enumeration:\nvendor_counts = {}\nfor dev in cuda_devices:\n    vendor = dev.get(\"vendor\", \"Unknown\")\n    vendor_counts[vendor] = vendor_counts.get(vendor, 0) + 1\n\ninfo[\"gpu_vendor_summary\"] = vendor_counts\n```\n\n**Tests:**\n- [ ] Run `aios torch-info` with NVIDIA GPU\n- [ ] Run with AMD GPU (if available)\n- [ ] Run with Intel GPU (if available)\n- [ ] Verify JSON output includes vendor field\n\n#### Task 1.3: Update GUI Device Detection\n**Files:** `src/aios/gui/app/panel_setup.py`\n\n- [ ] Import vendor identification function\n- [ ] Add `vendor` field to detected devices\n- [ ] Update status label to show vendor counts\n- [ ] Differentiate AMD from NVIDIA in device list\n\n**Changes:**\n```python\n# In _detect_devices_info() function:\ncuda_devices.append({\n    \"id\": i,\n    \"name\": name,\n    \"total_mem_mb\": total_mem_mb,\n    \"vendor\": identify_gpu_vendor(name, check_rocm=bool(getattr(torch.version, 'hip', None)))\n})\n\n# Update return dict:\nreturn {\n    \"cuda_available\": cuda_available,\n    \"cuda_devices\": cuda_devices,\n    \"nvidia_smi_devices\": cuda_devices,  # Keep for compatibility\n    \"vendor_summary\": calculate_vendor_summary(cuda_devices)\n}\n```\n\n**Tests:**\n- [ ] Launch GUI and verify vendor shown in Resources panel\n- [ ] Verify AMD GPUs labeled correctly\n- [ ] Verify status message distinguishes vendors\n\n### Phase 2: Intel XPU Support (High Priority)\n\n**Estimated Time:** 3 hours\n\n#### Task 2.1: Add IPEX Dependency\n**Files:** `pyproject.toml`\n\n- [ ] Add `intel-extension-for-pytorch` to `[project.optional-dependencies.hf]`\n- [ ] Version constraint: `>=2.0.0`\n- [ ] Platform constraint: exclude ARM64\n\n**Changes:**\n```toml\n[project.optional-dependencies]\nhf = [\n  # ... existing dependencies ...\n  \"intel-extension-for-pytorch>=2.0.0; platform_machine != 'ARM64'\",\n]\n```\n\n**Tests:**\n- [ ] Fresh install in new venv\n- [ ] Verify IPEX installs on x64 systems\n- [ ] Verify skipped on ARM64 systems\n\n#### Task 2.2: Add XPU Memory Detection\n**Files:** `src/aios/core/hrm_models/training_optimizer.py`\n\n- [ ] Update `detect_available_vram()` to check XPU\n- [ ] Add XPU device enumeration\n- [ ] Handle IPEX not installed gracefully\n\n**Implementation:**\n```python\ndef detect_available_vram(self) -> Tuple[float, List[Dict[str, Any]]]:\n    \"\"\"\n    Detect available VRAM across all GPUs (CUDA, XPU).\n    \n    Returns:\n        (total_vram_gb, list of GPU info dicts)\n    \"\"\"\n    gpus = []\n    total_vram = 0.0\n    \n    # Check CUDA devices (NVIDIA + AMD ROCm)\n    if torch.cuda.is_available():\n        for gpu_id in range(torch.cuda.device_count()):\n            props = torch.cuda.get_device_properties(gpu_id)\n            total_gb = props.total_memory / (1024 ** 3)\n            \n            torch.cuda.set_device(gpu_id)\n            torch.cuda.empty_cache()\n            gc.collect()\n            \n            allocated_gb = torch.cuda.memory_allocated(gpu_id) / (1024 ** 3)\n            reserved_gb = torch.cuda.memory_reserved(gpu_id) / (1024 ** 3)\n            available_gb = total_gb - reserved_gb\n            \n            gpus.append({\n                \"id\": gpu_id,\n                \"backend\": \"cuda\",\n                \"name\": props.name,\n                \"total_gb\": total_gb,\n                \"allocated_gb\": allocated_gb,\n                \"reserved_gb\": reserved_gb,\n                \"available_gb\": available_gb,\n            })\n            \n            total_vram += total_gb\n    \n    # Check Intel XPU devices\n    try:\n        if hasattr(torch, 'xpu') and torch.xpu.is_available():\n            for gpu_id in range(torch.xpu.device_count()):\n                props = torch.xpu.get_device_properties(gpu_id)\n                total_gb = props.total_memory / (1024 ** 3)\n                \n                # XPU memory info\n                try:\n                    free_mem, total_mem = torch.xpu.mem_get_info(gpu_id)\n                    available_gb = free_mem / (1024 ** 3)\n                    reserved_gb = (total_mem - free_mem) / (1024 ** 3)\n                except Exception:\n                    available_gb = total_gb * 0.9  # Estimate\n                    reserved_gb = total_gb * 0.1\n                \n                gpus.append({\n                    \"id\": len(gpus),  # Global ID\n                    \"backend\": \"xpu\",\n                    \"name\": props.name,\n                    \"total_gb\": total_gb,\n                    \"allocated_gb\": 0.0,  # Not available via public API\n                    \"reserved_gb\": reserved_gb,\n                    \"available_gb\": available_gb,\n                })\n                \n                total_vram += total_gb\n    except ImportError:\n        # intel-extension-for-pytorch not installed\n        pass\n    except Exception as e:\n        # Log but don't crash\n        print(f\"XPU detection warning: {e}\")\n    \n    return total_vram, gpus\n```\n\n**Tests:**\n- [ ] Test with CUDA-only system (existing behavior)\n- [ ] Test with XPU-only system (new behavior)\n- [ ] Test with both CUDA and XPU\n- [ ] Test without IPEX installed\n\n#### Task 2.3: Update GUI XPU Device Enumeration\n**Files:** `src/aios/gui/app/panel_setup.py`\n\n- [ ] Add XPU detection to `_detect_devices_info()`\n- [ ] Return separate `xpu_devices` list\n- [ ] Update `set_detected()` to handle XPU\n\n**Changes:**\n```python\ndef _detect_devices_info() -> dict:\n    try:\n        import torch\n        \n        # ... existing CUDA detection ...\n        \n        # Detect Intel XPU devices\n        xpu_available = False\n        xpu_devices = []\n        try:\n            if hasattr(torch, 'xpu') and torch.xpu.is_available():\n                xpu_available = True\n                device_count = torch.xpu.device_count()\n                for i in range(device_count):\n                    try:\n                        props = torch.xpu.get_device_properties(i)\n                        total_mem_mb = props.total_memory // (1024 * 1024)\n                        xpu_devices.append({\n                            \"id\": i,\n                            \"name\": props.name,\n                            \"total_mem_mb\": total_mem_mb,\n                            \"vendor\": \"Intel\"\n                        })\n                    except Exception:\n                        xpu_devices.append({\n                            \"id\": i,\n                            \"name\": f\"Intel XPU {i}\",\n                            \"total_mem_mb\": 0,\n                            \"vendor\": \"Intel\"\n                        })\n        except Exception:\n            xpu_available = False\n        \n        return {\n            \"cuda_available\": cuda_available,\n            \"cuda_devices\": cuda_devices,\n            \"nvidia_smi_devices\": cuda_devices,\n            \"xpu_available\": xpu_available,\n            \"xpu_devices\": xpu_devices,\n            # ... existing fields ...\n        }\n    except Exception:\n        return {\n            \"cuda_available\": False,\n            \"cuda_devices\": [],\n            \"nvidia_smi_devices\": [],\n            \"xpu_available\": False,\n            \"xpu_devices\": []\n        }\n```\n\n**Tests:**\n- [ ] GUI shows XPU devices when available\n- [ ] XPU devices selectable in Resources panel\n- [ ] Training can target XPU devices\n\n#### Task 2.4: Update device.py for XPU\n**Files:** `src/aios/cli/hrm_hf/device.py`\n\n- [ ] Add XPU case to `resolve_device()`\n- [ ] Handle XPU device objects\n- [ ] Update strict mode error messages\n\n**Changes:**\n```python\ndef resolve_device(device: str, strict: bool, torch) -> Tuple[str, Any, Any]:\n    \"\"\"Resolve device string and return (dev_str, device_obj, dml_device).\n\n    Handles auto, cuda, xpu, dml with strict mode constraints.\n    \"\"\"\n    dev = device\n    dml_device = None\n    \n    if dev == \"auto\":\n        if torch.cuda.is_available():\n            dev = \"cuda\"\n        elif hasattr(torch, 'xpu') and torch.xpu.is_available():\n            dev = \"xpu\"\n        else:\n            try:\n                import torch_directml as _dml\n                _ = _dml.device()\n                dev = \"dml\"\n            except Exception:\n                dev = \"cpu\"\n    else:\n        # Validate requested device\n        if str(dev).lower() == \"cuda\" and not torch.cuda.is_available():\n            # ... existing CUDA error handling ...\n        elif str(dev).lower() == \"xpu\":\n            if not (hasattr(torch, 'xpu') and torch.xpu.is_available()):\n                if strict:\n                    from rich import print\n                    import typer\n                    print({\n                        \"error\": \"XPU requested but not available\",\n                        \"hint\": \"Install intel-extension-for-pytorch or choose --device cpu/cuda/dml\",\n                        \"device_request\": \"xpu\"\n                    })\n                    raise typer.Exit(code=2)\n                else:\n                    dev = \"cpu\"\n    \n    device_obj = None\n    if dev == \"dml\":\n        # ... existing DML handling ...\n    else:\n        device_obj = torch.device(dev)\n    \n    return dev, device_obj, dml_device\n```\n\n**Tests:**\n- [ ] `--device xpu` works with IPEX installed\n- [ ] `--device xpu` fails gracefully without IPEX\n- [ ] `--device auto` prefers CUDA > XPU > DML > CPU\n\n### Phase 3: Installation Scripts (Medium Priority)\n\n**Estimated Time:** 2 hours\n\n#### Task 3.1: Windows GPU Detection Enhancement\n**Files:** `scripts/install_aios_on_windows.ps1`\n\n- [ ] Add AMD GPU detection via WMI\n- [ ] Add Intel GPU detection via WMI\n- [ ] Offer PyTorch variant selection\n- [ ] Add ROCm installation option\n\n**Implementation:**\n```powershell\nfunction Get-GpuInfo() {\n  $info = @{ \n    Vendor = 'Unknown'\n    Model = ''\n    HasNvidia = $false\n    HasAmd = $false\n    HasIntel = $false\n    NvidiaSmi = $false \n  }\n  \n  try {\n    $gpus = Get-CimInstance Win32_VideoController | Select-Object -Property Name, AdapterCompatibility\n    foreach ($g in $gpus) {\n      $vendor = [string]$g.AdapterCompatibility\n      $model = [string]$g.Name\n      \n      if ($vendor -match 'NVIDIA' -or $model -match 'NVIDIA|GeForce|RTX|GTX') {\n        $info.HasNvidia = $true\n        if ($info.Vendor -eq 'Unknown') { $info.Vendor = 'NVIDIA'; $info.Model = $model }\n      }\n      elseif ($vendor -match 'AMD' -or $model -match 'AMD|Radeon|RX') {\n        $info.HasAmd = $true\n        if ($info.Vendor -eq 'Unknown') { $info.Vendor = 'AMD'; $info.Model = $model }\n      }\n      elseif ($vendor -match 'Intel' -or $model -match 'Intel.*Arc|Intel.*Xe') {\n        $info.HasIntel = $true\n        if ($info.Vendor -eq 'Unknown') { $info.Vendor = 'Intel'; $info.Model = $model }\n      }\n    }\n  } catch {}\n  \n  try { \n    if (Get-Command nvidia-smi -ErrorAction SilentlyContinue) { \n      $info.NvidiaSmi = $true \n    } \n  } catch {}\n  \n  return $info\n}\n\nfunction Install-PyTorch([string]$pipPath, [string]$pythonPath, [string]$GpuPref) {\n  $gpu = $GpuPref\n  $gpuInfo = Get-GpuInfo\n  \n  Write-Host (\"[i] GPU detection: Vendor={0} Model={1}\" -f $gpuInfo.Vendor, $gpuInfo.Model)\n  Write-Host (\"[i] GPUs detected: NVIDIA={0} AMD={1} Intel={2}\" -f $gpuInfo.HasNvidia, $gpuInfo.HasAmd, $gpuInfo.HasIntel)\n\n  if ($gpu -eq 'auto') {\n    if ($gpuInfo.HasNvidia) { \n      $gpu = 'cuda' \n    }\n    elseif ($gpuInfo.HasAmd) {\n      # ROCm on Windows is experimental, offer DirectML as safer option\n      $choice = Read-Host \"AMD GPU detected. Use (1) DirectML [Stable] or (2) ROCm [Experimental]? [1/2]\"\n      if ($choice -eq '2') {\n        $gpu = 'rocm'\n      } else {\n        $gpu = 'dml'\n      }\n    }\n    elseif ($gpuInfo.HasIntel) {\n      $gpu = 'xpu'\n    }\n    else { \n      $gpu = 'cpu' \n    }\n  }\n\n  if ($gpu -eq 'cuda') {\n    # ... existing CUDA installation ...\n  }\n  elseif ($gpu -eq 'rocm') {\n    Write-Host \"[i] Installing PyTorch with ROCm support...\"\n    Write-Host \"[!] Note: ROCm on Windows is experimental\" -ForegroundColor Yellow\n    try {\n      & $pipPath install --upgrade pip wheel setuptools | Out-Null\n      # ROCm 6.0 for Windows (if available)\n      & $pipPath install torch --index-url https://download.pytorch.org/whl/rocm6.0\n      Write-Host \"[+] PyTorch installed with ROCm support.\" -ForegroundColor Green\n      return\n    } catch {\n      Write-Host \"[!] ROCm install failed. Falling back to DirectML...\" -ForegroundColor Yellow\n      $gpu = 'dml'\n    }\n  }\n  elseif ($gpu -eq 'xpu') {\n    Write-Host \"[i] Installing PyTorch + Intel Extension for PyTorch (XPU)...\"\n    try {\n      & $pipPath install --upgrade pip wheel setuptools | Out-Null\n      # CPU PyTorch first\n      & $pipPath install torch --index-url https://download.pytorch.org/whl/cpu\n      # Then Intel Extension\n      & $pipPath install intel-extension-for-pytorch\n      \n      $code = \"import torch, intel_extension_for_pytorch as ipex; print({'torch': torch.__version__, 'ipex': ipex.__version__, 'xpu_available': torch.xpu.is_available()})\"\n      $res = & $pythonPath -c $code\n      Write-Host \"[+] PyTorch installed with Intel XPU support. Probe: $res\" -ForegroundColor Green\n      return\n    } catch {\n      Write-Host \"[!] Intel XPU install failed. Falling back to DirectML...\" -ForegroundColor Yellow\n      $gpu = 'dml'\n    }\n  }\n  elseif ($gpu -eq 'dml') {\n    # ... existing DirectML installation ...\n  }\n\n  # ... existing CPU fallback ...\n}\n```\n\n**Tests:**\n- [ ] Test on NVIDIA-only system\n- [ ] Test on AMD-only system (if available)\n- [ ] Test on Intel-only system (if available)\n- [ ] Test GPU preference override (`-Gpu cuda/rocm/xpu/dml/cpu`)\n\n#### Task 3.2: Ubuntu GPU Detection Enhancement\n**Files:** `scripts/install_aios_on_ubuntu.sh`\n\n- [ ] Add AMD GPU detection via lspci\n- [ ] Add Intel GPU detection via lspci\n- [ ] Add ROCm installation option\n- [ ] Add IPEX installation option\n\n**Implementation:**\n```bash\n# GPU Detection\nGPU_VENDOR=\"unknown\"\nif command -v nvidia-smi >/dev/null 2>&1 || lspci | grep -i nvidia >/dev/null 2>&1; then\n  GPU_VENDOR=\"nvidia\"\nelif lspci | grep -iE 'amd|radeon' >/dev/null 2>&1; then\n  GPU_VENDOR=\"amd\"\nelif lspci | grep -iE 'intel.*(arc|xe|iris)' >/dev/null 2>&1; then\n  GPU_VENDOR=\"intel\"\nfi\n\necho \"[AI-OS] Detected GPU vendor: $GPU_VENDOR\"\n\n# PyTorch Installation\ncase $GPU_VENDOR in\n  nvidia)\n    echo \"[AI-OS] Installing CUDA build of PyTorch (cu121)\"\n    \"$venv/bin/pip\" install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 || {\n      echo \"[AI-OS] CUDA wheel install failed, falling back to CPU build\"\n      \"$venv/bin/pip\" install torch torchvision torchaudio\n    }\n    ;;\n  amd)\n    echo \"[AI-OS] AMD GPU detected. Installing ROCm build of PyTorch\"\n    echo \"[AI-OS] Note: ROCm requires additional system packages\"\n    # Check if ROCm is installed\n    if command -v rocminfo >/dev/null 2>&1; then\n      \"$venv/bin/pip\" install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0 || {\n        echo \"[AI-OS] ROCm wheel install failed, falling back to CPU build\"\n        \"$venv/bin/pip\" install torch torchvision torchaudio\n      }\n    else\n      echo \"[AI-OS] ROCm not installed on system. Installing CPU build.\"\n      echo \"[AI-OS] To use AMD GPU, install ROCm: https://rocm.docs.amd.com/en/latest/\"\n      \"$venv/bin/pip\" install torch torchvision torchaudio\n    fi\n    ;;\n  intel)\n    echo \"[AI-OS] Intel GPU detected. Installing Intel Extension for PyTorch\"\n    # CPU PyTorch first\n    \"$venv/bin/pip\" install torch torchvision torchaudio\n    # Then Intel Extension\n    \"$venv/bin/pip\" install intel-extension-for-pytorch || {\n      echo \"[AI-OS] Intel Extension install failed, continuing with CPU-only PyTorch\"\n    }\n    ;;\n  *)\n    echo \"[AI-OS] No discrete GPU detected; installing CPU build of PyTorch\"\n    \"$venv/bin/pip\" install torch torchvision torchaudio\n    ;;\nesac\n```\n\n**Tests:**\n- [ ] Test on Ubuntu with NVIDIA GPU\n- [ ] Test on Ubuntu with AMD GPU (if available)\n- [ ] Test on Ubuntu with Intel GPU (if available)\n- [ ] Test on Ubuntu with no GPU\n\n### Phase 4: GUI Enhancements (Low Priority)\n\n**Estimated Time:** 1 hour\n\n#### Task 4.1: Update Resources Panel Device Labels\n**Files:** `src/aios/gui/components/resources_panel/device_management.py`\n\n- [ ] Show vendor in GPU device name\n- [ ] Add vendor badges/icons (optional)\n- [ ] Color-code by vendor (optional)\n\n**Changes:**\n```python\ndef build_cuda_rows(panel: \"ResourcesPanel\", devices: list[dict], which: str) -> None:\n    \"\"\"Build GPU selection rows for training or inference.\"\"\"\n    # ... existing code ...\n    \n    for dev in devices:\n        # ... existing parsing ...\n        \n        name = str(dev.get(\"name\") or f\"CUDA {did}\")\n        vendor = dev.get(\"vendor\", \"\")\n        \n        # Add vendor prefix if known and not already in name\n        if vendor and vendor not in name:\n            display_name = f\"[{vendor}] {name}\"\n        else:\n            display_name = name\n        \n        # ... rest of row building with display_name ...\n```\n\n**Tests:**\n- [ ] Launch GUI with mixed GPUs\n- [ ] Verify vendor labels shown\n- [ ] Verify selection still works\n\n#### Task 4.2: Update Status Messages\n**Files:** `src/aios/gui/components/resources_panel/device_management.py`\n\n- [ ] Show vendor breakdown in status\n- [ ] Differentiate \"X NVIDIA GPUs, Y AMD GPUs\" vs just \"X GPUs\"\n\n**Changes:**\n```python\ndef set_detected(panel: \"ResourcesPanel\", info: dict) -> None:\n    # ... existing code ...\n    \n    # Build vendor summary for status\n    vendor_summary = info.get(\"vendor_summary\", {})\n    if vendor_summary:\n        vendor_parts = [f\"{count} {vendor}\" for vendor, count in vendor_summary.items()]\n        gpu_desc = \", \".join(vendor_parts)\n    else:\n        gpu_desc = f\"{device_count} GPU(s)\"\n    \n    if device_count > 0:\n        panel._status_label.config(\n            text=f\"\u2713 {gpu_desc} detected\",\n            foreground=\"green\"\n        )\n```\n\n**Tests:**\n- [ ] Single vendor system shows correct label\n- [ ] Mixed vendor system shows breakdown\n- [ ] No GPU system shows fallback message\n\n### Phase 5: Documentation (Low Priority)\n\n**Estimated Time:** 1 hour\n\n#### Task 5.1: Update User Documentation\n\n- [ ] Add AMD GPU setup guide to `docs/guide/`\n- [ ] Add Intel GPU setup guide to `docs/guide/`\n- [ ] Update `README.md` with multi-vendor support claims\n- [ ] Add troubleshooting section for GPU detection\n\n**Files to create/update:**\n- `docs/guide/AMD_GPU_SETUP.md`\n- `docs/guide/INTEL_GPU_SETUP.md`\n- `README.md`\n\n#### Task 5.2: Update Developer Documentation\n\n- [ ] Document vendor identification function\n- [ ] Add GPU backend architecture notes\n- [ ] Update API reference\n\n**Files to create/update:**\n- `docs/development/GPU_BACKEND_ARCHITECTURE.md`\n\n### Phase 6: Testing & Validation (Critical)\n\n**Estimated Time:** 2 hours\n\n#### Unit Tests\n- [ ] Test `identify_gpu_vendor()` with sample GPU names\n- [ ] Test XPU memory detection with mocked torch.xpu\n- [ ] Test vendor summary calculation\n- [ ] Test device resolution with XPU\n\n**Files:** `tests/test_gpu_detection.py` (new)\n\n#### Integration Tests\n- [ ] Test installation script on clean Windows VM\n- [ ] Test installation script on clean Ubuntu VM\n- [ ] Test GUI with NVIDIA-only system\n- [ ] Test training with XPU device (if hardware available)\n- [ ] Test torch-info command output\n\n#### Manual Testing Checklist\n- [ ] NVIDIA GPU: Detection, labeling, training\n- [ ] AMD GPU: Detection, labeling, training (if available)\n- [ ] Intel GPU: Detection, labeling, training (if available)\n- [ ] Mixed system: All vendors shown correctly (if available)\n- [ ] No GPU: Graceful CPU fallback\n\n## Success Criteria\n\n### Phase 1-2 (Core Functionality)\n- [x] AMD GPUs correctly labeled with vendor in torch-info\n- [x] Intel XPU GPUs detected and shown in GUI\n- [x] XPU memory optimization works\n- [x] Training works on XPU devices\n- [x] No regression for NVIDIA GPU users\n\n### Phase 3-4 (Installation & UX)\n- [ ] Installation scripts detect AMD/Intel GPUs\n- [ ] Correct PyTorch variant installed automatically\n- [ ] GUI shows vendor information\n- [ ] Users can distinguish GPU hardware\n\n### Phase 5-6 (Polish & Validation)\n- [ ] Documentation complete\n- [ ] Tests passing\n- [ ] No critical bugs reported\n\n## Known Limitations\n\n### After Implementation\n1. **ROCm on Windows** - Experimental, may not work on all AMD GPUs\n2. **Intel XPU DDP** - Distributed training requires oneCCL backend (not included)\n3. **External GPUs** - Thunderbolt bandwidth limitations (hardware, not software)\n4. **Hotplug** - PyTorch may need restart to detect newly connected eGPUs\n\n### Permanent Limitations\n1. **Vendor-specific optimizations** - Out of scope, would require per-vendor kernels\n2. **Mixed-vendor DDP** - Not possible (requires same backend)\n3. **GPU virtualization** - Not tested, may work but unsupported\n\n## Rollback Plan\n\n### If Issues Arise\n1. **Vendor detection bugs** - Revert to generic \"CUDA\" labeling\n2. **XPU crashes** - Disable XPU code paths, CPU fallback\n3. **Installation issues** - Keep NVIDIA-only detection as fallback\n\n### Breaking Changes\nNone expected. All changes are additive.\n\n## Performance Impact\n\nExpected performance impact: **None**\n\n- Vendor detection: One-time on startup\n- XPU memory checks: Only when XPU devices present\n- No changes to training loops\n\n## Security Considerations\n\nNone. No security-sensitive changes.\n\n## Dependencies\n\n### New Dependencies\n- `intel-extension-for-pytorch>=2.0.0` (optional, hf extra)\n\n### System Requirements\n- **AMD GPU users**: ROCm 5.7+ (Linux) or 6.0+ (Windows experimental)\n- **Intel GPU users**: Intel GPU drivers, oneAPI Base Toolkit (optional)\n- **No change for NVIDIA users**\n\n## Migration Guide\n\n### For Users\n\n**Upgrading from v1.0.x:**\n\n1. Update AI-OS: `git pull && pip install -e .[ui,hf]`\n2. AMD GPU users: Reinstall PyTorch with ROCm\n   ```bash\n   pip install torch --index-url https://download.pytorch.org/whl/rocm6.0\n   ```\n3. Intel GPU users: Install IPEX\n   ```bash\n   pip install intel-extension-for-pytorch\n   ```\n4. Run `aios torch-info` to verify detection\n\n**No action needed for NVIDIA users** - everything works as before.\n\n### For Developers\n\n**API Changes:**\n- `torch_info()` output now includes `vendor` field in device dicts\n- `detect_available_vram()` now returns XPU devices\n- `_detect_devices_info()` now includes `xpu_devices` key\n\n**New Functions:**\n- `identify_gpu_vendor(gpu_name: str, check_rocm: bool) -> str`\n\n## Timeline\n\n### Aggressive (1 week)\n- Days 1-2: Phase 1-2 (Detection + XPU)\n- Day 3: Phase 3 (Installation scripts)\n- Day 4: Phase 4 (GUI)\n- Day 5: Phase 5 (Documentation)\n- Days 6-7: Phase 6 (Testing)\n\n### Realistic (2 weeks)\n- Week 1: Phases 1-3 (Core functionality + installation)\n- Week 2: Phases 4-6 (Polish, docs, testing)\n\n### Conservative (3 weeks)\n- Week 1: Phases 1-2 (Core functionality)\n- Week 2: Phases 3-4 (Installation + GUI)\n- Week 3: Phases 5-6 (Documentation + thorough testing)\n\n## Open Questions\n\n1. ~~Should we auto-install ROCm/IPEX or require manual install?~~\n   - **Decision:** Auto-detect and offer installation, but don't force\n2. ~~Should DirectML remain default for AMD on Windows?~~\n   - **Decision:** Yes, ROCm on Windows is experimental\n3. ~~How to handle Intel integrated GPUs vs discrete Arc?~~\n   - **Decision:** Detect both, let user choose in GUI\n4. ~~Version constraints for IPEX?~~\n   - **Decision:** `>=2.0.0` for PyTorch 2.x compatibility\n\n## References\n\n### Documentation\n- AMD ROCm: https://rocm.docs.amd.com/\n- Intel XPU: https://intel.github.io/intel-extension-for-pytorch/\n- PyTorch Multi-Backend: https://pytorch.org/docs/stable/notes/hip.html\n\n### Related Issues\n- None (proactive enhancement)\n\n### Related PRs\n- To be created after implementation\n\n## Approval & Tracking\n\n- [ ] Technical Review - @AI-OS-Team\n- [ ] Architecture Review - @AI-OS-Team\n- [ ] Implementation Started\n- [ ] Phase 1 Complete\n- [ ] Phase 2 Complete\n- [ ] Phase 3 Complete\n- [ ] Phase 4 Complete\n- [ ] Phase 5 Complete\n- [ ] Phase 6 Complete\n- [ ] Documentation Complete\n- [ ] Released in v1.1.0\n\n---\n\n**Last Updated:** 2025-10-23  \n**Next Review:** After Phase 2 completion  \n**Owner:** @AI-OS-Team\n\n## Progress Tracking\n\n### Overall Progress\n- [ ] 0% - Not started\n- [ ] 25% - Phase 1-2 complete (Core detection + XPU)\n- [ ] 50% - Phase 3-4 complete (Installation + GUI)\n- [ ] 75% - Phase 5 complete (Documentation)\n- [ ] 100% - Phase 6 complete (Testing + Release)\n\n### Current Status\n**Status:** \ud83d\udccb Planned  \n**Start Date:** TBD  \n**Target Completion:** TBD  \n**Actual Completion:** N/A\n\n### Blockers\nNone currently identified.\n\n### Notes\n- Consider coordinating with MIXED_GPU_VENDOR_SUPPORT.md for future mixed-vendor training\n- May want to implement basic GPU abstraction layer first for easier future expansion\n", "tags": ["cli", "gui", "training"], "headings": [{"line": 0, "text": "AMD and Intel GPU Support Enhancement"}, {"line": 9, "text": "Overview"}, {"line": 13, "text": "Problem Statement"}, {"line": 15, "text": "Current Issues"}, {"line": 23, "text": "Impact"}, {"line": 30, "text": "Goals"}, {"line": 32, "text": "Must Have \u2705"}, {"line": 41, "text": "Nice to Have \ud83c\udfaf"}, {"line": 48, "text": "Out of Scope \u274c"}, {"line": 55, "text": "Implementation Plan"}, {"line": 57, "text": "Phase 1: Core Detection (High Priority)"}, {"line": 61, "text": "Task 1.1: Add GPU Vendor Identification Function"}, {"line": 83, "text": "NVIDIA detection"}, {"line": 87, "text": "AMD detection"}, {"line": 91, "text": "Intel detection"}, {"line": 95, "text": "Check ROCm build as fallback"}, {"line": 113, "text": "Task 1.2: Update torch-info Command"}, {"line": 123, "text": "In torch_info() function, for each CUDA device:"}, {"line": 128, "text": "ADD:"}, {"line": 133, "text": "ADD after device enumeration:"}, {"line": 148, "text": "Task 1.3: Update GUI Device Detection"}, {"line": 158, "text": "In _detect_devices_info() function:"}, {"line": 166, "text": "Update return dict:"}, {"line": 180, "text": "Phase 2: Intel XPU Support (High Priority)"}, {"line": 184, "text": "Task 2.1: Add IPEX Dependency"}, {"line": 195, "text": "... existing dependencies ..."}, {"line": 205, "text": "Task 2.2: Add XPU Memory Detection"}, {"line": 224, "text": "Check CUDA devices (NVIDIA + AMD ROCm)"}, {"line": 250, "text": "Check Intel XPU devices"}, {"line": 257, "text": "XPU memory info"}, {"line": 278, "text": "intel-extension-for-pytorch not installed"}, {"line": 281, "text": "Log but don't crash"}, {"line": 293, "text": "Task 2.3: Update GUI XPU Device Enumeration"}, {"line": 306, "text": "... existing CUDA detection ..."}, {"line": 308, "text": "Detect Intel XPU devices"}, {"line": 341, "text": "... existing fields ..."}, {"line": 358, "text": "Task 2.4: Update device.py for XPU"}, {"line": 388, "text": "Validate requested device"}, {"line": 390, "text": "... existing CUDA error handling ..."}, {"line": 407, "text": "... existing DML handling ..."}, {"line": 419, "text": "Phase 3: Installation Scripts (Medium Priority)"}, {"line": 423, "text": "Task 3.1: Windows GPU Detection Enhancement"}, {"line": 485, "text": "ROCm on Windows is experimental, offer DirectML as safer option"}, {"line": 502, "text": "... existing CUDA installation ..."}, {"line": 509, "text": "ROCm 6.0 for Windows (if available)"}, {"line": 522, "text": "CPU PyTorch first"}, {"line": 524, "text": "Then Intel Extension"}, {"line": 537, "text": "... existing DirectML installation ..."}, {"line": 540, "text": "... existing CPU fallback ..."}, {"line": 550, "text": "Task 3.2: Ubuntu GPU Detection Enhancement"}, {"line": 560, "text": "GPU Detection"}, {"line": 572, "text": "PyTorch Installation"}, {"line": 584, "text": "Check if ROCm is installed"}, {"line": 598, "text": "CPU PyTorch first"}, {"line": 600, "text": "Then Intel Extension"}, {"line": 618, "text": "Phase 4: GUI Enhancements (Low Priority)"}, {"line": 622, "text": "Task 4.1: Update Resources Panel Device Labels"}, {"line": 633, "text": "... existing code ..."}, {"line": 636, "text": "... existing parsing ..."}, {"line": 641, "text": "Add vendor prefix if known and not already in name"}, {"line": 647, "text": "... rest of row building with display_name ..."}, {"line": 655, "text": "Task 4.2: Update Status Messages"}, {"line": 664, "text": "... existing code ..."}, {"line": 666, "text": "Build vendor summary for status"}, {"line": 686, "text": "Phase 5: Documentation (Low Priority)"}, {"line": 690, "text": "Task 5.1: Update User Documentation"}, {"line": 702, "text": "Task 5.2: Update Developer Documentation"}, {"line": 711, "text": "Phase 6: Testing & Validation (Critical)"}, {"line": 715, "text": "Unit Tests"}, {"line": 723, "text": "Integration Tests"}, {"line": 730, "text": "Manual Testing Checklist"}, {"line": 737, "text": "Success Criteria"}, {"line": 739, "text": "Phase 1-2 (Core Functionality)"}, {"line": 746, "text": "Phase 3-4 (Installation & UX)"}, {"line": 752, "text": "Phase 5-6 (Polish & Validation)"}, {"line": 757, "text": "Known Limitations"}, {"line": 759, "text": "After Implementation"}, {"line": 765, "text": "Permanent Limitations"}, {"line": 770, "text": "Rollback Plan"}, {"line": 772, "text": "If Issues Arise"}, {"line": 777, "text": "Breaking Changes"}, {"line": 780, "text": "Performance Impact"}, {"line": 788, "text": "Security Considerations"}, {"line": 792, "text": "Dependencies"}, {"line": 794, "text": "New Dependencies"}, {"line": 797, "text": "System Requirements"}, {"line": 802, "text": "Migration Guide"}, {"line": 804, "text": "For Users"}, {"line": 821, "text": "For Developers"}, {"line": 831, "text": "Timeline"}, {"line": 833, "text": "Aggressive (1 week)"}, {"line": 840, "text": "Realistic (2 weeks)"}, {"line": 844, "text": "Conservative (3 weeks)"}, {"line": 849, "text": "Open Questions"}, {"line": 860, "text": "References"}, {"line": 862, "text": "Documentation"}, {"line": 867, "text": "Related Issues"}, {"line": 870, "text": "Related PRs"}, {"line": 873, "text": "Approval & Tracking"}, {"line": 893, "text": "Progress Tracking"}, {"line": 895, "text": "Overall Progress"}, {"line": 902, "text": "Current Status"}, {"line": 908, "text": "Blockers"}, {"line": 911, "text": "Notes"}]}, {"path": "planned_features/argilla-trl-preference-alignment.md", "content": "## PF-001: Preference data via Argilla + TRL (RM/DPO baselines)\n\nThis document is an end-to-end integration guide and checklist for adding an open-source human-feedback workflow powered by Argilla and Hugging Face TRL into AI-OS. It covers the architecture, data flow, CLI and GUI elements, runbooks for Windows/Linux, acceptance criteria, and rollout.\n\n## Outcomes and success criteria\n\n- End-to-end: Annotators create preferences in Argilla \u2192 data exported \u2192 ingested to TRL schema \u2192 RM and DPO baselines trained \u2192 artifacts logged and evaluated \u2192 optional signals consumed by HRM.\n- CLI: Three new commands available and documented: argilla-export, rm-train, dpo-train.\n- GUI: Minimal \u201cAlignment\u201d section adds dataset stats, quick-run buttons (ingest/RM/DPO), and links to artifacts/logs.\n- Validation: On a toy dataset, RM AUC > 0.6 and DPO improves win-rate vs reference \u2265 5% (see Acceptance Checklists).\n\n## Architecture overview\n\nData flow (text diagram):\n- Argilla (annotation UI) \u2192 Export (JSONL/Parquet)\n- aios data argilla-export \u2192 TRL-ready splits under training_data/curated_datasets/preferences/{train,eval,test}.jsonl\n- aios hrm-hf rm-train \u2192 artifacts/rm/<run_name>/ (model, tokenizer, metrics.jsonl)\n- aios hrm-hf dpo-train \u2192 artifacts/dpo/<run_name>/ (policy, tokenizer, metrics.jsonl)\n- Optional: Evaluation and HRM-side consumption or distillation in later PFs\n\n## Scope\n\nIn scope:\n- New CLI commands to train RM and DPO models using TRL on HF backbones.\n- Data ingestion/export path from Argilla to TRL-ready datasets.\n- Minimal GUI hooks: dataset stats, quick-run toggles, links to artifacts.\n- Documentation and examples to run end-to-end on a small subset.\n\nOut of scope (this PF):\n- Direct TRL training on custom HRM architecture.\n- Large UI rewrites; we add a minimal Alignment area and buttons.\n\n## Repository integration points\n\n- Data location: `training_data/curated_datasets/preferences/`\n- New CLI modules:\n  - `src/aios/cli/hrm_hf/rm_train.py` \u2014 wraps TRL RewardTrainer\n  - `src/aios/cli/hrm_hf/dpo_train.py` \u2014 wraps TRL DPOTrainer\n- Shared utils: `src/aios/data/argilla_ingest.py` \u2014 converts Argilla exports \u2192 TRL-ready datasets\n- Config: either extend `TrainingConfig` or define small dataclasses local to each CLI for clarity\n- Artifacts:\n  - RM: `artifacts/rm/<name>/` (model, tokenizer, metrics.jsonl, config.json)\n  - DPO: `artifacts/dpo/<name>/` (policy, tokenizer, metrics.jsonl, config.json)\n\n## Prerequisites\n\n- Python environment (see repo installers). Required packages (minimal):\n  - torch (CUDA-enabled when using GPU), transformers, datasets, trl, accelerate, peft, evaluate, scikit-learn, pandas, pyarrow\n  - Optional: bitsandbytes (8-bit), deepspeed (multi-GPU), wandb (telemetry)\n- Argilla server for annotation (Docker or pip). Quick start:\n  - Docker: `docker run -p 6900:6900 -e ARGILLA_ENABLE_TELEMETRY=0 argilla/argilla:latest`\n  - Or follow Argilla docs; create a binary preference project and annotate a few pairs.\n- Disk layout created:\n  - `training_data/curated_datasets/preferences/`\n  - `artifacts/{rm,dpo}/`\n\n## Data formats and schema\n\nTarget TRL schema (JSONL or Arrow) with splits: `train.jsonl`, `eval.jsonl`, `test.jsonl`.\n- DPO: Each row requires keys `prompt`, `chosen`, `rejected`.\n- RM: Either the same triplet schema (derived pairs) or an explicit pair with labels; internally RM trainer can consume `(prompt, chosen, rejected)` by forming preference pairs.\n\nValidation rules (ingestion command enforces):\n- Non-empty strings for all required fields\n- Reasonable length bounds (configurable), deduplication by hash\n- Drop rows where `chosen == rejected`\n- Compute and write a `schema_report.json` with counts and dropped reasons\n\n## CLI specification (final)\n\n1) Argilla export ingestion\n- Command:\n  - `aios data argilla-export --input <export.{jsonl,parquet}> --output training_data/curated_datasets/preferences --format trl` \n- Important flags:\n  - `--train-ratio 0.9 --eval-ratio 0.05 --test-ratio 0.05`\n  - `--min-prompt-toks 3 --max-prompt-toks 2048` (approx by chars if tokenizer not available)\n  - `--dedup-by prompt+chosen+rejected` (hash function)\n  - `--anonymize` (optional; integrates with Presidio later PF-006)\n  - `--shuffle-seed 42`\n- Output:\n  - Writes `train.jsonl`, `eval.jsonl`, `test.jsonl` & `schema_report.json` under the output dir\n  - Returns exit code 0 with summary printed; warns if <95% valid\n\n2) Reward model training\n- Command:\n  - `aios hrm-hf rm-train --base-model <hf_model> --dataset-dir training_data/curated_datasets/preferences --output-dir artifacts/rm/<name> --epochs 1 --lr 2e-5 --batch-size 8`\n- Additional flags:\n  - `--max-steps <int>` (overrides epochs), `--gradient-accumulation 1`, `--bf16/--fp16`, `--lr-scheduler cosine`, `--warmup-ratio 0.03`, `--weight-decay 0.01`\n  - `--max-length 1024`, `--truncation-right` (or left), `--eval-steps 100`, `--save-steps 200`\n  - `--deepspeed config/deepspeed_zero2.json` (optional)\n  - `--wandb <project>` (optional)\n- Model:\n  - HF `AutoModelForSequenceClassification` with 1-dim head or TRL RewardModel head\n- Metrics:\n  - AUC on eval pairs; accuracy of predicting `chosen > rejected`; loss curve\n- Artifacts:\n  - Model, tokenizer, `metrics.jsonl`, `config.json`, training logs\n\n3) DPO baseline training\n- Command:\n  - `aios hrm-hf dpo-train --policy-model <hf_model> --ref-model <hf_model_or_policy_ckpt> --dataset-dir training_data/curated_datasets/preferences --output-dir artifacts/dpo/<name> --epochs 1 --lr 5e-6 --batch-size 4`\n- Additional flags:\n  - `--beta 0.1` (DPO temperature), `--label-smoothing 0.0`, `--max-length 1024`, `--gradient-accumulation 1`, `--bf16/--fp16`\n  - `--lr-scheduler cosine`, `--warmup-ratio 0.03`, `--eval-steps 100`, `--save-steps 200`\n  - `--deepspeed config/deepspeed_zero2.json` (optional), `--wandb <project>` (optional)\n- Metrics:\n  - Relative log-prob of chosen vs rejected; eval loss; win-rate on held-out\n- Artifacts:\n  - Policy model, tokenizer, `metrics.jsonl`, `config.json`, logs\n\n## GUI integration (minimal, additive)\n\nAdd an \u201cAlignment\u201d section/panel in the HRM GUI (or CLI dashboard) with:\n- Dataset card\n  - Shows counts from `schema_report.json`: total, dropped by reason, splits\n  - Buttons: \u201cRe-ingest from Argilla export\u2026\u201d, \u201cOpen dataset folder\u201d\n- Quick actions\n  - Run RM: pick base model, batch size, max steps, precision toggle; Start \u2192 streams `metrics.jsonl`\n  - Run DPO: pick policy + reference; Start \u2192 streams `metrics.jsonl`\n  - Links: \u201cOpen artifacts/rm/<name>\u201d, \u201cOpen artifacts/dpo/<name>\u201d, \u201cView logs\u201d\n- Status + telemetry\n  - Real-time tail of `metrics.jsonl` and last checkpoint info\n  - Optional W&B run URL if enabled\n\nImplementation notes:\n- Wire buttons to the same CLI under the hood (subprocess) and follow our existing logging conventions.\n- Use existing logging.yaml; stream both stdout and metrics JSONL.\n- Non-blocking runs: spawn background process; allow stop/cancel.\n\n## End-to-end runbook (Windows PowerShell)\n\n1) Prepare environment\n```powershell\n# Activate repo venv if available\n. .\\.venv\\Scripts\\Activate.ps1\n\n# Install required packages (example; adjust torch for your CUDA)\npip install transformers datasets trl accelerate peft evaluate scikit-learn pandas pyarrow\n# Optional\npip install wandb bitsandbytes deepspeed\n```\n\n2) Start Argilla server and annotate a few pairs\n```powershell\ndocker run --pull always -p 6900:6900 -e ARGILLA_ENABLE_TELEMETRY=0 argilla/argilla:latest\n# Open http://localhost:6900, create a preference project, add prompt/chosen/rejected.\n```\n\n3) Export from Argilla to JSONL (via UI or API), then ingest\n```powershell\naios data argilla-export --input path\\to\\export.jsonl --output training_data/curated_datasets/preferences --format trl --shuffle-seed 42\n```\n\n4) Train a small Reward Model on CPU (toy)\n```powershell\naios hrm-hf rm-train --base-model gpt2 --dataset-dir training_data/curated_datasets/preferences --output-dir artifacts/rm/demo --max-steps 100 --batch-size 2 --eval-steps 20\n```\n\n5) Train a small DPO policy on CPU (toy)\n```powershell\naios hrm-hf dpo-train --policy-model gpt2 --ref-model gpt2 --dataset-dir training_data/curated_datasets/preferences --output-dir artifacts/dpo/demo --max-steps 100 --batch-size 1 --eval-steps 20\n```\n\n6) Inspect outputs\n- Open `artifacts/rm/demo/metrics.jsonl` and `artifacts/dpo/demo/metrics.jsonl` to verify loss decreasing.\n- Check `schema_report.json` for ingestion health.\n\n7) Optional GPU acceleration\n- Install the correct torch build for your CUDA, add `--bf16` or `--fp16`, and optionally `--deepspeed config/deepspeed_zero2.json`.\n\n## Directory structure conventions\n\n```\ntraining_data/\n  curated_datasets/\n    preferences/\n      train.jsonl\n      eval.jsonl\n      test.jsonl\n      schema_report.json\nartifacts/\n  rm/\n    <run_name>/\n      config.json\n      metrics.jsonl\n      tokenizer/\n      model files\u2026\n  dpo/\n    <run_name>/\n      config.json\n      metrics.jsonl\n      tokenizer/\n      model files\u2026\n```\n\n## Acceptance checklists\n\nIngestion (argilla-export)\n- [ ] >95% rows valid; <5% dropped for schema/length\n- [ ] train/eval/test splits present\n- [ ] `schema_report.json` written with counts by reason\n\nReward Model (rm-train)\n- [ ] Training finishes 100 toy steps without errors on CPU\n- [ ] AUC on eval set > 0.6 (toy acceptable)\n- [ ] `metrics.jsonl` shows monotonic loss decrease on average\n- [ ] Artifacts saved under `artifacts/rm/<name>/`\n\nDPO (dpo-train)\n- [ ] Training finishes 100 toy steps without errors on CPU\n- [ ] Validation chosen log-prob improves vs reference \u2265 5% win-rate on held-out pairs\n- [ ] `metrics.jsonl` present with eval loss improvements\n- [ ] Artifacts saved under `artifacts/dpo/<name>/`\n\nDocs & GUI\n- [ ] This runbook executes end-to-end on CPU\n- [ ] GUI Alignment panel shows dataset counts and can launch runs\n- [ ] Links open artifacts and live logs\n\n## Troubleshooting\n\n- OOM on GPU: reduce `--batch-size`, increase `--gradient-accumulation`, enable `--fp16/--bf16`, or use `bitsandbytes` 8-bit\n- Slow CPU runs: use tiny HF models (e.g., `sshleifer/tiny-gpt2`), reduce `--max-length`, `--max-steps`\n- Tokenizer mismatch: ensure policy/ref and tokenizer directories align; remove stale cached tokenizers under `artifacts/hf_implant/tokenizers/` if needed\n- Deepspeed import errors: reinstall with matching CUDA; fall back to single-GPU accelerate if blocked\n- Dataset schema errors: open `schema_report.json`; fix malformed rows or re-export\n\n## Security & privacy\n\n- Avoid PII in free-text; if required, enable `--anonymize` (PF-006 Presidio integration)\n- Do not upload private data to external services unless explicitly configured (disable telemetry by default)\n\n## Telemetry (optional)\n\n- Enable with `--wandb <project>`; include run name and tags like `pf-001`, `rm` or `dpo`\n- Always write local `metrics.jsonl` for reproducibility\n\n## Implementation plan (engineering)\n\nMinimal code stubs to add:\n- `src/aios/data/argilla_ingest.py`\n  - read Argilla JSONL/Parquet \u2192 normalize \u2192 splits \u2192 schema report\n- `src/aios/cli/hrm_hf/rm_train.py`\n  - TRL RewardTrainer wrapper; HF model+tokenizer load; metrics logging\n- `src/aios/cli/hrm_hf/dpo_train.py`\n  - TRL DPOTrainer wrapper; policy+ref load; metrics logging\n\nConfig options:\n- Either extend global `TrainingConfig` with rm/dpo sections or define lightweight local configs in each CLI module to reduce coupling for PF-001.\n\nLogging:\n- Reuse `logging.yaml`; write structured metrics to JSONL under the output dir.\n\n## Milestones\n\n- M1 (1\u20132 days): Ingestion and validation CLI; skeleton TRL trainers\n- M2 (1\u20132 days): End-to-end toy run; artifact packaging; documentation and GUI buttons\n\n## Go/No-Go criteria\n\n- Go when all Acceptance checklists are satisfied on CPU; GPU smoke test completes with bf16/fp16; no critical regressions to existing HRM CLI.\n- No-Go if ingestion validity < 90% or trainers fail to complete toy runs on CPU.\n\n## Appendix A \u2014 Dataset examples\n\nSample DPO row (JSONL):\n```\n{\"prompt\": \"Write a haiku about the moon.\", \"chosen\": \"Silver moon whispers\\nTides hum ancient lullabies\\nNight cradles the seas.\", \"rejected\": \"The moon is round. It is in the sky.\"}\n```\n\n## Appendix B \u2014 Suggested VS Code tasks (optional)\n\nAdd tasks to reproduce quick runs (mirroring existing HRM tasks):\n- Run ingestion (argilla-export) with a sample file\n- Run RM trainer with tiny model\n- Run DPO trainer with tiny model\n\nThese tasks should log to `artifacts/{rm,dpo}/<name>/metrics.jsonl` and be discoverable from the Command Palette.\n\n---\n\nNotes:\n- TRL is used for baselines and RM training; HRM remains the primary custom model. Distillation into HRM can be explored in a later PF.\n", "tags": ["cli", "datasets", "evaluation", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "PF-001: Preference data via Argilla + TRL (RM/DPO baselines)"}, {"line": 4, "text": "Outcomes and success criteria"}, {"line": 11, "text": "Architecture overview"}, {"line": 20, "text": "Scope"}, {"line": 32, "text": "Repository integration points"}, {"line": 44, "text": "Prerequisites"}, {"line": 56, "text": "Data formats and schema"}, {"line": 68, "text": "CLI specification (final)"}, {"line": 110, "text": "GUI integration (minimal, additive)"}, {"line": 129, "text": "End-to-end runbook (Windows PowerShell)"}, {"line": 133, "text": "Activate repo venv if available"}, {"line": 136, "text": "Install required packages (example; adjust torch for your CUDA)"}, {"line": 138, "text": "Optional"}, {"line": 145, "text": "Open http://localhost:6900, create a preference project, add prompt/chosen/rejected."}, {"line": 170, "text": "Directory structure conventions"}, {"line": 195, "text": "Acceptance checklists"}, {"line": 219, "text": "Troubleshooting"}, {"line": 227, "text": "Security & privacy"}, {"line": 232, "text": "Telemetry (optional)"}, {"line": 237, "text": "Implementation plan (engineering)"}, {"line": 253, "text": "Milestones"}, {"line": 258, "text": "Go/No-Go criteria"}, {"line": 263, "text": "Appendix A \u2014 Dataset examples"}, {"line": 270, "text": "Appendix B \u2014 Suggested VS Code tasks (optional)"}]}, {"path": "planned_features/AUTO_UPDATER_SETTINGS_INTEGRATION.md", "content": "# PF-020: Settings Auto Updater Integration\n\n**Status**: Planning Phase  \n**Date**: November 16, 2025  \n**Priority**: High  \n**Complexity**: Medium-High\n\n---\n\n## Summary\n\nIntroduce a first-party auto updater that is discoverable and controllable from the Settings tab. The updater must run as an external helper process so it can shut down the running AI-OS instance, apply the new build, and relaunch the application without user scripting. Version detection relies on GitHub Releases: when the latest tagged semantic version exceeds the local version, the updater downloads and installs the release; otherwise it reports that the installation is current. The main application remains responsible for configuring, scheduling, and updating the updater helper through the Settings menu.\n\n---\n\n## Success Criteria\n\n- Users can navigate to Settings \u25b8 Updates, trigger a check, and see clear status messaging (up to date, update available, update running, error).\n- When an update is available and approved, the helper process gracefully shuts down the main app, replaces binaries/scripts, and restarts AI-OS without manual steps.\n- Update logic works on Linux and Windows using consistently packaged artifacts and paths.\n- Version comparison uses semantic version precedence and tolerates pre-release identifiers.\n- Update helper is itself versioned and refreshed by the main app when a newer helper package ships.\n- Update operations log to a dedicated channel consumable by diagnostics tooling.\n\n---\n\n## User Flow (Settings Tab)\n\n1. Settings \u25b8 Updates view exposes controls for manual check, auto-check interval, update channel (stable vs pre-release), and viewing the update log.\n2. Selecting \"Check for updates\" triggers an async call to the GitHub Releases API, comparing `latest.tag_name` to `aios.__version__`.\n3. If the remote version is newer, the UI shows release notes and prompts the user to \"Update and Restart\"; otherwise it reports \"AI-OS is up to date\".\n4. On acceptance, the main process persists update intent (temporary manifest) and spawns the updater helper.\n5. The UI displays progress streamed from the helper; once complete the application either restarts automatically or prompts the user if relaunch fails.\n\n---\n\n## System Architecture\n\n```\n+---------------------+        +-------------------------+\n| Main AI-OS Process  |        | Updater Helper Process  |\n| (Settings Module)   |        | (Standalone Python exe) |\n|                     |        |                         |\n| - Render Settings UI|        | - Validate manifest     |\n| - Call GitHub API   |        | - Close main process    |\n| - Compare versions  |        | - Download release      |\n| - Spawn helper      | -----> | - Verify checksum       |\n| - Stream progress   | <----- | - Install & relaunch    |\n+---------------------+        +-------------------------+\n```\n\n**Key Interfaces**\n- A `VersionService` abstraction encapsulates GitHub Release discovery, environment overrides, and semantic comparison.\n- An `UpdaterTransport` (likely named pipe on Windows, UNIX domain socket on Linux, TCP fallback) carries JSON-RPC style progress events between the helper and the Settings UI.\n- The helper process is packaged as either a console-script entry point (`python -m aios.updater`) or self-contained executable produced via PyInstaller for Windows convenience.\n\n---\n\n## Functional Requirements\n\n- **Version Source**: Query `https://api.github.com/repos/Wulfic/AI-OS/releases` (with auth token fallback) respecting rate limits and caching results for configurable intervals.\n- **Artifact Selection**: Resolve the correct asset per platform (Linux tarball, Windows installer). Configurable via Settings for air-gapped environments.\n- **Graceful Shutdown**: Send a structured shutdown command so the main process flushes state (`gui_state.json`, running jobs) before the helper forcibly terminates as a fallback.\n- **Install Location Awareness**: Detect whether AI-OS runs from editable source, pip install, or packaged binary; block or warn if the install layout does not support automated replacement.\n- **Rollback Point**: Before installing, create a restore point (copy or rename current install) to enable quick rollback if restart fails.\n- **Restart Strategy**: After successful install, re-launch using the same command line the user invoked (respecting venv and CLI arguments). On failure, surface the necessary re-launch command.\n- **Updater Self-Update**: During normal runs the main process checks the helper version. If mismatched, it downloads the new helper payload and updates the helper before offering core updates.\n\n---\n\n## Constraints & Considerations\n\n- **Cross-Platform**: Paths, process signalling, and relaunch commands must support Linux (systemd and raw shell) and Windows (PowerShell, Start-Process). macOS support is optional but should not be broken.\n- **Security**: Release downloads must be checksum-verified (SHA256 from `SHA256SUMS.txt`), optionally signed. Support GitHub API tokens stored securely in credentials manager.\n- **Offline / Air-Gapped**: Provide documented override allowing manual placement of update packages and offline installation.\n- **Async Integration**: All Settings-side operations must be non-blocking, using existing asyncio loop and thread executors. UI progress should update without freezing other tabs.\n- **Error Handling**: Categorize errors (network, permission, disk space) with user-facing remediation tips.\n- **Telemetry**: Emit structured update events to analytics with opt-out respect.\n\n---\n\n## Implementation Plan\n\n### Phase 0 \u2013 Discovery & Design (1 week)\n- Audit current Settings tab implementation and identify UI extension points.\n- Confirm packaging formats for Linux (tar.gz or wheel) and Windows (`.zip` or `.exe`).\n- Define semantic version comparison rules and finalize API contract for helper transport.\n- Document shutdown sequence requirements for core subsystems.\n\n### Phase 1 \u2013 Version & Settings Wiring (1-2 weeks)\n- Implement `VersionService` with GitHub Releases polling, caching, and comparison utilities.\n- Extend Settings view with \"Updates\" panel using existing GUI architecture.\n- Provide manual check, channel selection, auto-check scheduling, and state persistence in `gui_state.json`.\n\n### Phase 2 \u2013 Helper Process Scaffold (2 weeks)\n- Create `src/aios/updater/__main__.py` containing CLI entry point for helper process.\n- Implement manifest ingestion (paths, desired version, restart command).\n- Add inter-process channel with heartbeat and progress events.\n- Implement graceful shutdown handshake with main process (signal or RPC) with fallback kill-after-timeout logic.\n\n### Phase 3 \u2013 Download, Verify, Install (2 weeks)\n- Integrate streaming download with resume support and checksum verification.\n- Extract or run platform-specific installer into target directory, backing up current installation.\n- Implement rollback procedure triggered when restart health check fails.\n\n### Phase 4 \u2013 Relaunch & UX Polish (1 week)\n- Capture original invocation command (argv, environment) and reuse for restart.\n- Update Settings UI to display progress, logs, and final status from helper events.\n- Add notifications/log entries for success, failure, rollback actions.\n\n### Phase 5 \u2013 Updater Maintenance Channel (1 week)\n- Package helper assets alongside main releases; add manifest metadata so the main app can refresh the helper.\n- Document fallback manual update process if helper fails, and provide telemetry dashboards.\n\n---\n\n## Testing Strategy\n\n- **Unit Tests**: Version comparator edge cases, GitHub API response parsing, manifest serialization, restart command builder.\n- **Integration Tests**: Use temporary directories to simulate install roots, running helper end-to-end against mock release server.\n- **Process Tests**: Automated tests that spawn a dummy main process, verify the helper closes it, writes backups, and restarts a stub executable.\n- **Windows & Linux Matrix**: CI jobs run update integration suite on both OSes; include PowerShell script validation for Windows.\n- **Failure Injection**: Simulate network errors, checksum mismatch, insufficient disk, and permission errors to ensure retries and rollback behave correctly.\n- **Smoke Tests**: Manual or scripted validation against staging GitHub releases prior to public release.\n\n---\n\n## Documentation & Support\n\n- Update `docs/guide/` with an \"Updates\" section covering manual checks, auto-check scheduling, logs, and troubleshooting.\n- Add FAQ entries addressing rate limits, offline installs, and helper location.\n- Provide release note template entries calling out auto updater behaviour changes.\n- Ensure logging.yaml includes dedicated `auto_updater` logger with rotation guidance.\n\n---\n\n## Open Questions & Risks\n\n- How are release artifacts structured today, and do they contain platform installers that support unattended runs?\n- What permissions does the running user require to overwrite installation directories, especially on Windows Program Files?\n- Do we need signature verification beyond checksums for enterprise environments?\n- How do we migrate existing users installed from source clones where automated replacement is unsafe?\n- Should auto-check be opt-in or default-on (with configurable intervals)?\n\n---\n\n## Deliverables Checklist\n\n- Settings \u25b8 Updates UI with manual and scheduled check controls.\n- `VersionService` and helper manifest schema committed with unit coverage.\n- Updater helper package capable of graceful shutdown, download, install, relaunch, and rollback.\n- Platform-specific restart logic validated on Linux and Windows.\n- CI jobs executing update integration tests across platforms.\n- Documentation updates and release note templates.\n- Telemetry and logging integration for update lifecycle events.\n\n---\n\n## Next Steps\n\n1. Review and approve architecture and phase estimates with stakeholders.\n2. Identify engineers for Settings UI, helper process, and packaging streams.\n3. Schedule Phase 0 tasks and prepare GitHub Release environment (test releases, tokens).\n", "tags": [], "headings": [{"line": 0, "text": "PF-020: Settings Auto Updater Integration"}, {"line": 9, "text": "Summary"}, {"line": 15, "text": "Success Criteria"}, {"line": 26, "text": "User Flow (Settings Tab)"}, {"line": 36, "text": "System Architecture"}, {"line": 58, "text": "Functional Requirements"}, {"line": 70, "text": "Constraints & Considerations"}, {"line": 81, "text": "Implementation Plan"}, {"line": 83, "text": "Phase 0 \u2013 Discovery & Design (1 week)"}, {"line": 89, "text": "Phase 1 \u2013 Version & Settings Wiring (1-2 weeks)"}, {"line": 94, "text": "Phase 2 \u2013 Helper Process Scaffold (2 weeks)"}, {"line": 100, "text": "Phase 3 \u2013 Download, Verify, Install (2 weeks)"}, {"line": 105, "text": "Phase 4 \u2013 Relaunch & UX Polish (1 week)"}, {"line": 110, "text": "Phase 5 \u2013 Updater Maintenance Channel (1 week)"}, {"line": 116, "text": "Testing Strategy"}, {"line": 127, "text": "Documentation & Support"}, {"line": 136, "text": "Open Questions & Risks"}, {"line": 146, "text": "Deliverables Checklist"}, {"line": 158, "text": "Next Steps"}]}, {"path": "planned_features/data-backends-vector-stores.md", "content": "## PF-005: Data backends and vector stores\n\nThis document is a comprehensive design and delivery guide for adding pluggable dataset backends (Hugging Face Datasets streaming, WebDataset shards) and a minimal vector store layer (Qdrant or LanceDB) to enable scalable data ingestion and future retrieval/memory features across CLI and GUI.\n\n### Goals (What we ship)\n\n- Add a first-class dataset backend abstraction used by ACTV1 training flows.\n- Provide two new backends in addition to the current custom loader:\n\t- Hugging Face Datasets (streaming, with light filtering and caching knobs)\n\t- WebDataset shards (local/URL tar streams)\n- Add a minimal vector store interface with swappable drivers (Qdrant, LanceDB) for embedding upsert/query.\n- Expose all of the above via CLI and the HRM Training GUI panel with a clear, safe default path and backward compatibility.\n\n### Motivation\n\n- Improve scalability and reproducibility of dataset handling.\n- Prepare for retrieval-augmented features and expert selection memories.\n- Enable persistent cognitive memory for attention traces and crystallized motifs (see `PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md`).\n\n### Non-goals (Out of scope for PF-005)\n\n- Full RAG pipelines, memory policies, or retrieval-integrated training loops.\n- On-the-fly embedding model training; we only provide an API and a thin client.\n- Cognitive memory integration (attention traces, crystallized motifs) is implemented separately in Phase 2.5 of `PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md` after both foundations are complete.\n\n---\n\n## Architecture overview\n\n### 1) Dataset backends\n\nIntroduce a small interface and pluggable implementations:\n\n- Module: `src/aios/cli/hrm_hf/data_backends/`\n\t- `base.py`: defines the interface and utilities\n\t- `custom.py`: current file/dir/CSV/jsonl sampler (existing behavior)\n\t- `hf.py`: HF Datasets streaming loader\n\t- `webdataset.py`: WebDataset shard reader\n\nContract (Pythonic sketch):\n\n```python\n# src/aios/cli/hrm_hf/data_backends/base.py\nfrom typing import Iterable, Protocol, Optional, Dict, Any\n\nclass SampleBatch(Protocol):\n\t\tinput_ids: Any  # torch.LongTensor [B, T]\n\t\tattn_mask: Any  # Optional, same shape\n\nclass Backend(Protocol):\n\t\tdef __init__(self, cfg: Dict[str, Any]): ...\n\t\tdef iter_text(self) -> Iterable[str]: ...  # raw text stream (pre-tokenization)\n\t\t# Optional: direct batching/tokenization if desired\n\t\tdef iter_batches(self) -> Iterable[SampleBatch]: ...\n```\n\nIntegration point in training:\n\n- In `src/aios/cli/hrm_hf/train_actv1.py` the data loader path delegates to a builder:\n\t- `from .data_backends import build_backend`\n\t- `backend = build_backend(config)`\n\t- Use `backend.iter_text()` and preserve existing tokenization/batching logic, unless `iter_batches()` is present and compatible.\n\n### 2) Vector store layer\n\n- Module: `src/aios/memory/vector_store.py`\n\t- Defines `VectorStoreClient` protocol and factory\n\t- Drivers: `qdrant.py`, `lancedb.py`\n\nContract (sketch):\n\n```python\nfrom typing import Sequence, Dict, Any, List, Tuple, Optional\n\nclass VectorStoreClient:\n\t\tdef upsert(self, ids: Sequence[str], vectors: Sequence[Sequence[float]], metadata: Optional[Sequence[Dict[str, Any]]] = None) -> None: ...\n\t\tdef query(self, vector: Sequence[float], top_k: int = 5, filter: Optional[Dict[str, Any]] = None) -> List[Tuple[str, float, Dict[str, Any]]]: ...\n\t\tdef delete(self, ids: Sequence[str]) -> None: ...\n\t\tdef close(self) -> None: ...\n```\n\nUse cases now: basic smoke tests and future expert-selection memories. No training-time coupling required in PF-005.\n\n### 3) Cognitive memory integration (Future)\n\n**Note**: This section describes planned integration with `PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md` (implemented in their Phase 2.5, dependent on PF-005 completion).\n\nThe `VectorStoreClient` provides the storage backend for two cognitive memory components:\n\n#### A) Attention Trace Storage\n\n**Purpose**: Persist high-salience attention patterns across training sessions.\n\n**Wrapper class**: `TraceVectorStore`\n```python\n# src/aios/core/hrm_models/cognitive/vector_wrappers.py\nfrom aios.memory.vector_store import VectorStoreClient\n\nclass TraceVectorStore:\n    \"\"\"Specialized wrapper for attention trace persistence.\"\"\"\n    \n    def __init__(self, vector_client: VectorStoreClient, collection: str = \"attention_traces\"):\n        self.client = vector_client\n        self.collection = collection\n        self.embedder = TraceEmbedder(embed_dim=128)  # Converts sparse trace to dense vector\n    \n    def upsert_traces(self, traces: List[AttentionTrace], training_step: int):\n        \"\"\"Store batch of attention traces.\"\"\"\n        ids = [f\"trace_{t.layer_id}_{t.head_id}_{t.query_idx}_{t.key_idx}_{training_step}\" \n               for t in traces]\n        vectors = [self.embedder.embed(t) for t in traces]  # List[np.ndarray[128]]\n        metadata = [{\n            \"layer\": t.layer_id,\n            \"head\": t.head_id,\n            \"query_idx\": t.query_idx,\n            \"key_idx\": t.key_idx,\n            \"salience\": t.salience,\n            \"age\": t.age,\n            \"training_step\": training_step,\n        } for t in traces]\n        self.client.upsert(ids, vectors, metadata)\n    \n    def query_similar_traces(self, query_trace: AttentionTrace, top_k: int = 100):\n        \"\"\"Retrieve similar traces for warm-starting.\"\"\"\n        query_vec = self.embedder.embed(query_trace)\n        return self.client.query(query_vec, top_k=top_k)\n```\n\n**Data flow**:\n1. Training runs with `TraceManager` accumulating traces in RAM (~24 MB)\n2. Every `trace_sync_interval` steps (e.g., 1000), `TraceManager.sync_to_vector_store()` calls `TraceVectorStore.upsert_traces()`\n3. On training restart, `TraceManager.load_from_vector_store()` retrieves historical traces for warm start\n4. Benefit: Cross-session learning - model remembers useful attention patterns from previous training runs\n\n**Collections**: `attention_traces` (one per model or shared)\n\n#### B) Crystallized Motif Storage\n\n**Purpose**: Store and share high-utility expert routing patterns (\"thought primitives\").\n\n**Wrapper class**: `MotifVectorStore`\n```python\nclass MotifVectorStore:\n    \"\"\"Specialized wrapper for crystallized motif persistence.\"\"\"\n    \n    def __init__(self, vector_client: VectorStoreClient, collection: str = \"crystallized_motifs\"):\n        self.client = vector_client\n        self.collection = collection\n        self.embedder = MotifEmbedder(embed_dim=256)  # Encodes expert sequence to vector\n    \n    def upsert_motif(self, motif: CrystallizedMotif):\n        \"\"\"Store a single crystallized motif.\"\"\"\n        motif_id = f\"motif_{motif.id}\"\n        vector = self.embedder.embed(motif)  # np.ndarray[256]\n        metadata = {\n            \"motif_id\": motif.id,\n            \"expert_sequence\": motif.expert_sequence,  # [2, 7, 3, 1]\n            \"frequency\": motif.count,\n            \"utility\": motif.utility,\n            \"entropy\": motif.entropy,\n            \"age\": motif.age,\n            \"task_tags\": motif.task_tags,  # [\"retrieval\", \"QA\"]\n        }\n        self.client.upsert([motif_id], [vector], [metadata])\n    \n    def query_motifs_for_task(self, task_tag: str, top_k: int = 10):\n        \"\"\"Retrieve best motifs for specific task type.\"\"\"\n        # Filter by task tag, return highest utility\n        return self.client.query(\n            vector=None,  # Use filter-only query if backend supports\n            top_k=top_k,\n            filter={\"task_tags\": task_tag}\n        )\n    \n    def transfer_motifs_to_model(self, target_model_id: str, motif_ids: List[str]):\n        \"\"\"Enable cross-model motif sharing.\"\"\"\n        # Retrieve motifs and return for injection into target model's RoutingPathTree\n        results = [self.client.query_by_id(mid) for mid in motif_ids]\n        return [self._reconstruct_motif_from_metadata(r[2]) for r in results]\n```\n\n**Data flow**:\n1. During training, `RoutingPathTree` tracks expert routing patterns\n2. When motif crystallizes (high frequency + utility + stability), `RoutingPathTree.persist_motif()` calls `MotifVectorStore.upsert_motif()`\n3. Other models can query motifs via `query_motifs_for_task()` for zero-shot transfer\n4. Benefit: Multi-model collaboration - models share learned reasoning strategies\n\n**Collections**: `crystallized_motifs` (shared across all models for collaborative learning)\n\n#### C) Scalability Comparison\n\n| Storage Mode | Capacity | Retrieval Speed | Persistence | Sharing |\n|--------------|----------|-----------------|-------------|----------|\n| **RAM-only** (baseline) | ~2M traces, 512 motifs | O(n) scan | None (lost on restart) | No |\n| **Vector Store** (enhanced) | Billions of traces/motifs | O(log n) ANN | Persistent | Multi-model |\n\n**Memory overhead**:\n- RAM-only: 30 MB total\n- With vector store: 30 MB (RAM) + 5 MB (embedding models) = 35 MB total\n- External storage: Qdrant/LanceDB (on disk or service)\n\n**Configuration** (see unified schema below):\n```yaml\nmemory:\n  vector_store:\n    backend: \"qdrant\"  # Shared by datasets, traces, and motifs\n  persistent_traces:\n    persist_to_vector_store: true  # Enable cross-session persistence\n    trace_sync_interval: 1000\n  semantic_crystallization:\n    persist_motifs: true  # Auto-save crystallized motifs\n```\n\n**Integration timeline**: Week 6.5-7.5 of Persistent Traces roadmap (after PF-005 vector store foundation is complete).\n\n---\n\n## User-facing changes\n\n### CLI additions (Typer)\n\nTarget: `src/aios/cli/hrm_hf_cli.py` \u2192 `train-actv1` command\n\nNew options (safe defaults maintain existing behavior):\n\n- `--dataset-backend [custom|hf|webdataset]` (default: `custom`)\n- Backend-specific flags:\n\t- HF Datasets:\n\t\t- `--hf-name TEXT` (e.g., \"wikitext\")\n\t\t- `--hf-config TEXT` (optional; e.g., \"wikitext-103-raw-v1\")\n\t\t- `--hf-split TEXT` (default: `train`)\n\t\t- `--hf-streaming/--no-hf-streaming` (default: enabled)\n\t\t- `--hf-cache-dir PATH` (optional)\n\t\t- `--hf-num-workers INT` (tokenization workers; default 2)\n\t\t- `--hf-token-env TEXT` (env var name for auth token; default `HUGGING_FACE_HUB_TOKEN`)\n\t- WebDataset:\n\t\t- `--wds-pattern TEXT` (e.g., `data/shards/shard-{000000..000099}.tar` or `https://.../shard-{000..099}.tar`)\n\t\t- `--wds-resampled/--no-wds-resampled` (default: false)\n\t\t- `--wds-shuffle INT` (buffer size; default 1000)\n\t\t- `--wds-decode [text|bytes]` (default: `text`)\n\t\t- `--wds-key TEXT` (key to read in tar, default: `txt`)\n\nBackward compatibility:\n\n- `--dataset-file` continues to work with `--dataset-backend=custom` (default).\n- If users pass an `hf://dataset:config:split` value to `--dataset-file`, we auto-map to `--dataset-backend=hf` and parse parts (non-breaking convenience already used in GUI).\n\nExamples:\n\n```powershell\n# Custom (unchanged)\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 10\n\n# HF streaming\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-backend hf `\n\t--hf-name wikitext --hf-config wikitext-103-raw-v1 --hf-split train `\n\t--steps 10 --batch-size 2\n\n# WebDataset shards (local)\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-backend webdataset `\n\t--wds-pattern \"C:/data/shards/shard-{000000..000009}.tar\" `\n\t--steps 10 --batch-size 2\n```\n\nWindows notes:\n\n- Use PowerShell quoting as shown above. For large brace-globs, prefer quotes (\") to prevent premature expansion.\n\n### GUI additions (HRM Training Panel)\n\nTarget: `src/aios/gui/components/hrm_training_panel.py`\n\n- Add a \"Dataset backend\" dropdown next to \"Dataset file/dir\" with values: `custom`, `hf`, `webdataset`.\n- When `hf` is selected, show inline fields: Name, Config, Split, Streaming (checkbox), Cache dir.\n- When `webdataset` is selected, show: Pattern, Resampled (checkbox), Shuffle buf, Decode, Key.\n- Continue to support the existing \"hf://\u2026\" entry shortcut. If user pastes `hf://<name>:<config>:<split>`, auto-select `hf` and populate fields.\n- Persist new fields via `get_state()/set_state()` and include them when building `TrainingConfig`.\n\nBehavioral notes:\n\n- Keep dataset selection UX simple: either paste a path/URI or use the new dropdown to reveal backend-specific fields.\n- Add small helper tooltips explaining streaming and shard patterns.\n- Respect existing ASCII-only filter and memory estimator displays.\n\n---\n\n## Config and validation\n\nExtend `TrainingConfig` (module: `src/aios/core/hrm_training/training_config.py`):\n\n- New fields:\n\t- `dataset_backend: Literal[\"custom\", \"hf\", \"webdataset\"] = \"custom\"`\n\t- HF: `hf_name: Optional[str]`, `hf_config: Optional[str]`, `hf_split: str = \"train\"`, `hf_streaming: bool = True`, `hf_cache_dir: Optional[str]`, `hf_num_workers: int = 2`, `hf_token_env: str = \"HUGGING_FACE_HUB_TOKEN\"`\n\t- WDS: `wds_pattern: Optional[str]`, `wds_resampled: bool = False`, `wds_shuffle: int = 1000`, `wds_decode: str = \"text\"`, `wds_key: str = \"txt\"`\n\nValidation rules:\n\n- If `dataset_backend == \"custom\"`, require `dataset_file` (existing behavior).\n- If `dataset_backend == \"hf\"`, require `hf_name` (or parse from `dataset_file` if `hf://` URI). Validate split exists if metadata is available.\n- If `dataset_backend == \"webdataset\"`, require `wds_pattern` and ensure it resolves to at least one shard (or allow late-binding with clear log warnings).\n- ASCII filter, batching, and max sequence length behavior must remain unchanged from user\u2019s POV.\n\n---\n\n## Implementation details\n\n### A) Data backends package\n\nFiles to add:\n\n- `src/aios/cli/hrm_hf/data_backends/base.py`\n- `src/aios/cli/hrm_hf/data_backends/custom.py` (wrap current logic)\n- `src/aios/cli/hrm_hf/data_backends/hf.py`\n- `src/aios/cli/hrm_hf/data_backends/webdataset.py`\n- `src/aios/cli/hrm_hf/data_backends/__init__.py` with `build_backend(config)`\n\nKey behaviors:\n\n- HF Datasets\n\t- Use `datasets.load_dataset(hf_name, hf_config, split=hf_split, streaming=hf_streaming)`\n\t- When streaming, iterate and yield `example[\"text\"]` or join fields if text column not obvious (configurable via simple heuristic: prefer `text`, else `content`, else JSON stringify line)\n\t- Respect `ascii_only` and filtering already present in training pipeline\n\t- Support `hf_cache_dir` and `HUGGINGFACE_HUB_CACHE`/`HF_HOME`\n\t- Prefetch: use a small async queue with size 2\u20134 to smooth tokenizer throughput\n\n- WebDataset\n\t- If `webdataset` lib is available: use `wds.WebDataset(pattern).shuffle(wds_shuffle)`; otherwise provide a minimal tar iterator (local-only) that reads `*.txt` entries matching key\n\t- Map `wds_decode\": text|bytes` to return `str` or `bytes` and let tokenizer branch accordingly\n\t- Support `wds_resampled` with `wds.ResampledShards` when library is present\n\n### B) Tokenization and batching\n\n- Preserve current tokenization path to avoid regressions.\n- If a backend exposes `iter_batches()` matching our `SampleBatch`, we can optionally bypass text tokenization, but keep this disabled by default in PF-005.\n- Ensure deterministic seeding with existing RNG controls.\n\n### C) HRM CLI wiring\n\n- Update `src/aios/cli/hrm_hf_cli.py` to add new options and to pass them into `TrainingConfig`.\n- Add validation hints in Typer help messages for Windows path quoting and HF auth.\n\n### D) GUI wiring\n\n- In `HRMTrainingPanel`, add a new `dataset_backend_var` and conditional UI stacks.\n- Extend `build_training_config()` to populate the new `TrainingConfig` fields.\n- Update `get_state()/set_state()` to persist and restore the new fields.\n- Continue supporting the `hf://` inline format. If user switches backend manually, keep the inline field synchronized.\n\n### E) Vector store minimal layer\n\nFiles to add:\n\n- `src/aios/memory/vector_store.py` (interface + factory)\n- `src/aios/memory/vector_stores/qdrant.py`\n- `src/aios/memory/vector_stores/lancedb.py`\n\nFunctionality:\n\n- `upsert(ids, vectors, metadata)` and `query(vector, top_k)` with cosine similarity.\n- Qdrant: depend on `qdrant-client`; collection auto-create if missing; index on cosine.\n- LanceDB: depend on `lancedb`; create table if missing; approximate query OK for starter.\n\nOptional CLI (lightweight utility for smoke tests):\n\n- `aios memory vs-upsert` and `aios memory vs-query` that call the above client; documented only, can be added in a follow-up small PR.\n\n### F) Cognitive memory integration (Future)\n\n**Note**: Implemented in `PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md` Phase 2.5 (Week 6.5-7.5), dependent on PF-005 completion.\n\nFiles to add by cognitive architecture team:\n\n- `src/aios/core/hrm_models/cognitive/vector_wrappers.py` (`TraceVectorStore`, `MotifVectorStore`)\n- `src/aios/core/hrm_models/cognitive/embedders.py` (`TraceEmbedder`, `MotifEmbedder`)\n\nIntegration points in existing files:\n\n- `src/aios/core/hrm_models/cognitive/trace_manager.py`: Add `sync_to_vector_store()`, `load_from_vector_store()` methods\n- `src/aios/core/hrm_models/cognitive/routing_tree.py`: Add `persist_motif()` method\n\nSee \"Cognitive memory integration\" section above for detailed specifications.\n\n---\n\n## Docker and local services\n\nAdd a Qdrant service snippet to `docker-compose.yml` (either appended or documented):\n\n```yaml\nservices:\n\tqdrant:\n\t\timage: qdrant/qdrant:latest\n\t\tports:\n\t\t\t- \"6333:6333\"\n\t\tvolumes:\n\t\t\t- ./artifacts/qdrant:/qdrant/storage\n\t\thealthcheck:\n\t\t\ttest: [\"CMD\", \"wget\", \"-qO-\", \"http://localhost:6333/readyz\"]\n\t\t\tinterval: 10s\n\t\t\ttimeout: 5s\n\t\t\tretries: 5\n```\n\nWindows quickstart:\n\n```powershell\ndocker compose up -d qdrant\n```\n\nFor LanceDB (no service), nothing to run; it\u2019s an embedded store.\n\n---\n\n## Testing and acceptance criteria\n\nWhat we test:\n\n1) HF streaming backend\n\t - Run a 1\u20132 step training invoking an HF dataset by name/config/split\n\t - Verify tokenization continues to function; metrics JSONL emits at least one record\n2) WebDataset backend (local shards)\n\t - Create 1\u20132 tiny tar shards with small `*.txt` samples\n\t - Run 1\u20132 step training using `--wds-pattern` and verify iteration + metrics\n3) Vector store\n\t - With Qdrant running or LanceDB selected, upsert 100 random 128D vectors, query a held-out vector, verify top-5 return with decreasing scores\n\nSuggested smoke commands (PowerShell):\n\n```powershell\n# HF streaming smoke\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-backend hf --hf-name wikitext --hf-config wikitext-103-raw-v1 --hf-split train `\n\t--steps 1 --batch-size 2 --halt-max-steps 1 `\n\t--log-file artifacts/brains/actv1/metrics.jsonl\n\n# WebDataset smoke (assuming shards exist)\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-backend webdataset --wds-pattern \"training_data/shards/shard-{000000..000001}.tar\" `\n\t--steps 1 --batch-size 2 --halt-max-steps 1 `\n\t--log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nAcceptance criteria (pass/fail):\n\n- HF: Iterates and trains for 1 step on a public dataset split; no blocking warnings\n- WDS: Iterates and trains for 1 step from shards; no blocking warnings\n- VS: Upsert/query test returns top-5 with non-increasing similarity\n\n---\n\n## Implementation roadmap\n\n**Total timeline**: 6 weeks for core features + 1.5 weeks for cognitive memory integration (optional)\n\n### Week 1-2: Core Infrastructure\n**Owner**: Backend team  \n**Deliverables**:\n- [ ] Create `src/aios/memory/vector_store.py` with `VectorStoreClient` protocol\n- [ ] Implement Qdrant driver (`src/aios/memory/vector_stores/qdrant.py`)\n- [ ] Implement LanceDB driver (`src/aios/memory/vector_stores/lancedb.py`)\n- [ ] Factory function `create_vector_store(backend, config)`\n- [ ] Unit tests for upsert/query/delete operations\n- [ ] Docker Compose service definition for Qdrant\n\n**Acceptance**:\n- Qdrant driver: Upsert 1000 vectors, query returns correct top-5\n- LanceDB driver: Same test, embedded mode (no service)\n- Both drivers pass same test suite (interface compliance)\n\n### Week 3-4: Dataset Backend Integration\n**Owner**: Training pipeline team  \n**Deliverables**:\n- [ ] Create `src/aios/cli/hrm_hf/data_backends/` package\n- [ ] Implement `custom.py` (wrap existing file/dir/CSV logic)\n- [ ] Implement `hf.py` (HuggingFace Datasets streaming)\n- [ ] Implement `webdataset.py` (tar shard reader)\n- [ ] Update `TrainingConfig` with new dataset backend fields\n- [ ] CLI integration: `train-actv1 --dataset-backend hf --hf-name wikitext`\n- [ ] Integration tests with small HF dataset and local tar shards\n\n**Acceptance**:\n- HF backend: Train 10 steps on wikitext, metrics JSONL contains 10 records\n- WebDataset backend: Train 10 steps from 2 local tar shards, no errors\n- Custom backend: Existing tests still pass (backward compatibility)\n\n### Week 5-6: GUI and Production Hardening\n**Owner**: GUI + DevOps  \n**Deliverables**:\n- [ ] GUI integration: `HRMTrainingPanel` dropdown for dataset backend selection\n- [ ] Conditional UI fields for HF and WebDataset options\n- [ ] Configuration validation (prevent missing required fields)\n- [ ] Error handling for missing Qdrant service, HF auth failures, invalid shard patterns\n- [ ] Documentation: User guide for dataset backends and vector store setup\n- [ ] PowerShell examples for Windows users\n\n**Acceptance**:\n- GUI: Select HF backend, populate fields, launch training successfully\n- Error handling: Missing Qdrant service shows clear error message\n- Docs: New user can follow guide to set up Qdrant and run HF training\n\n### Week 6.5-7.5: Cognitive Memory Integration (Optional)\n**Prerequisites**: \n- PF-005 vector store complete (Weeks 1-6)\n- Persistent Traces Phase 0-2 complete (see `PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md`)\n\n**Owner**: Cognitive architecture team  \n**Deliverables**:\n- [ ] Implement `TraceVectorStore` wrapper class\n- [ ] Implement `MotifVectorStore` wrapper class\n- [ ] Implement `TraceEmbedder` (sparse trace \u2192 128D vector)\n- [ ] Implement `MotifEmbedder` (expert sequence \u2192 256D vector)\n- [ ] Integration tests: Trace persistence/retrieval cycle\n- [ ] Cross-model motif transfer test\n\n**Acceptance**:\n- TraceVectorStore: Persist 10K traces, reload, verify salience within 1% error\n- MotifVectorStore: Query similar motifs, cosine similarity > 0.8 for same-task motifs\n- Works with both Qdrant and LanceDB backends\n\n**Notes**:\n- This phase is implemented by the Persistent Traces team, not PF-005 core team\n- See `PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md` Phase 2.5 for detailed spec\n- Can be skipped if cognitive memory features not needed\n\n### Week 8: Deployment and Monitoring (Final)\n**Owner**: DevOps  \n**Deliverables**:\n- [ ] Production Qdrant deployment guide (cloud or self-hosted)\n- [ ] Monitoring dashboard for vector store metrics (latency, storage size)\n- [ ] Backup/restore scripts for Qdrant collections\n- [ ] Performance benchmarks: Dataset iteration speed, vector query latency\n\n**Acceptance**:\n- Qdrant service runs in production with health checks\n- Monitoring shows vector store query p95 latency < 50ms\n- Backup script successfully exports and restores 1M vectors\n\n---\n\n## Coordination with Persistent Traces plan\n\n**Parallel development strategy**:\n- **Weeks 1-6**: PF-005 (this document) and Persistent Traces Phases 0-2 proceed **independently**\n- **Week 6.5-7.5**: Integration phase requires **both systems complete**\n- **Week 7+**: Persistent Traces continues independently, optionally using vector store\n\n**Critical dependencies**:\n1. `VectorStoreClient` interface must be finalized by end of Week 2\n2. Qdrant or LanceDB must be deployable by end of Week 4\n3. TraceVectorStore/MotifVectorStore depend on VectorStoreClient being stable\n\n**Configuration namespace**:\nBoth plans share unified `memory:` section in `config/default.yaml` (see Unified Configuration Schema below).\n\n**Cross-references**:\n- For cognitive memory use cases, see: `PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md`\n- For trace/motif embedding specifications, see: `PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md` \u00a7 Vector Store Integration\n- For unified config schema, see: Unified Configuration Schema section below\n\n---\n\n## Unified configuration schema\n\n**Location**: `config/default.yaml`\n\n**Namespace**: All memory-related features unified under `memory:` top-level key to avoid conflicts.\n\n```yaml\n# Memory Architecture (Unified Schema)\n# Covers: Dataset backends (PF-005), Vector stores (PF-005), \n#         Persistent traces (Cognitive), Semantic crystallization (Cognitive)\nmemory:\n  # ============================================\n  # Dataset Backends (PF-005)\n  # ============================================\n  dataset:\n    backend: \"custom\"  # custom|hf|webdataset\n    \n    # HuggingFace Datasets streaming\n    hf:\n      name: null                      # e.g., \"wikitext\"\n      config: null                    # e.g., \"wikitext-103-raw-v1\"\n      split: \"train\"                  # train|validation|test\n      streaming: true                 # Enable streaming mode\n      cache_dir: null                 # Optional cache directory\n      num_workers: 2                  # Tokenization workers\n      token_env: \"HUGGING_FACE_HUB_TOKEN\"  # Auth token env var\n    \n    # WebDataset tar shards\n    webdataset:\n      pattern: null                   # e.g., \"data/shards/shard-{000000..000099}.tar\"\n      resampled: false                # Use ResampledShards for infinite iteration\n      shuffle: 1000                   # Shuffle buffer size\n      decode: \"text\"                  # text|bytes\n      key: \"txt\"                      # Key to extract from tar entries\n  \n  # ============================================\n  # Vector Storage Backend (PF-005)\n  # ============================================\n  vector_store:\n    backend: \"qdrant\"  # qdrant|lancedb|disabled\n    \n    # Qdrant configuration\n    qdrant:\n      host: \"localhost\"\n      port: 6333\n      collection_prefix: \"aios_memory\"  # Collections: aios_memory_traces, aios_memory_motifs\n      api_key: null                     # Optional authentication\n    \n    # LanceDB configuration\n    lancedb:\n      path: \"artifacts/memory/lancedb\" # Embedded database path\n  \n  # ============================================\n  # Persistent Attention Traces (Cognitive)\n  # See: PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md\n  # ============================================\n  persistent_traces:\n    enabled: false                    # Enable persistent trace memory\n    \n    # Core trace parameters\n    sparsity: 0.001                   # Capture top 0.1% of attention edges\n    quota_per_head: 2048              # Max traces per attention head\n    salience_threshold: 0.05          # Minimum salience to persist\n    retention_rate: 0.95              # \u03bb (EMA momentum)\n    decay_rate: 0.98                  # \u03b3 (forgetting rate for unused traces)\n    bias_strength: 0.1                # \u03b1 (injection strength into attention)\n    update_interval: 100              # Steps between trace consolidation\n    warmup_steps: 1000                # Standard attention before trace capture\n    \n    # Vector store integration (optional - requires vector_store.backend != disabled)\n    persist_to_vector_store: false    # Enable cross-session persistence\n    trace_sync_interval: 1000         # Steps between DB syncs\n    embedding_dim: 128                # Trace embedding dimensionality\n    warm_start: false                 # Load traces from DB on training start\n    task_tag: null                    # Filter traces by task type (\"QA\", \"generation\", etc.)\n  \n  # ============================================\n  # Semantic Crystallization (Cognitive)\n  # See: PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md\n  # ============================================\n  semantic_crystallization:\n    enabled: false                    # Enable motif crystallization\n    \n    # Crystallization criteria\n    min_frequency: 100                # f_min (minimum traversals)\n    min_utility: 0.05                 # U_min (5% improvement over baseline)\n    max_entropy: 1.0                  # H_max (routing stability threshold)\n    min_age: 500                      # Temporal stability requirement (steps)\n    max_motifs: 512                   # Hard limit on crystallized primitives\n    \n    # Vector store integration (optional)\n    persist_motifs: false             # Auto-save crystallized motifs to DB\n    motif_embedding_dim: 256          # Motif embedding dimensionality\n    share_across_models: false        # Allow other models to query/reuse motifs\n```\n\n**Validation rules**:\n- If `memory.dataset.backend == \"hf\"`, require `memory.dataset.hf.name`\n- If `memory.dataset.backend == \"webdataset\"`, require `memory.dataset.webdataset.pattern`\n- If `memory.persistent_traces.persist_to_vector_store == true`, require `memory.vector_store.backend != \"disabled\"`\n- If `memory.semantic_crystallization.persist_motifs == true`, require `memory.vector_store.backend != \"disabled\"`\n\n**Backward compatibility**:\n- Existing `dataset_file` config maps to `memory.dataset.backend == \"custom\"`\n- All `memory.*` fields default to disabled/conservative values\n- No breaking changes to existing training configs\n\n**Windows compatibility**:\n- All paths use forward slashes internally, converted by PathLib\n- PowerShell examples in documentation\n- Qdrant via Docker Desktop (Windows native)\n- LanceDB pure Python (Windows native)\n\n---\n\n## Rollout plan\n\n1) M1 (2 days): Data backends + docs\n\t - Implement backend package and CLI/GUI wiring\n\t - Ship a doc with examples, and enable a dry-run path for each backend\n\t - Add unit tests for HF URI parsing and WDS pattern parsing\n2) M2 (1 day): Vector store + example\n\t - Implement minimal drivers\n\t - Add docker-compose snippet and a tiny usage doc\n\nFeature flagging/back-compat:\n\n- Default remains `custom` loader; no behavior change for current users.\n- `hf://` path auto-detection provides an additive convenience, not a breaking change.\n\n---\n\n## Risks and mitigations\n\n- Streaming backpressure: Use small prefetch queues and timeouts; allow `--hf-num-workers` to adjust throughput.\n- HF auth/rate limits: Use `--hf-token-env` and document how to set `HUGGING_FACE_HUB_TOKEN`.\n- WebDataset lib availability: Provide a minimal built-in tar reader for local shards when `webdataset` is not installed.\n- Windows Docker friction: Provide LanceDB as an embedded alternative; document `docker compose up -d qdrant`.\n\n---\n\n## Checklists\n\n### Engineering checklist\n\n- [ ] Add `data_backends/` package and register `build_backend(config)`\n- [ ] Extend `TrainingConfig` with new fields + validation\n- [ ] Update `hrm_hf_cli.py` to surface new flags\n- [ ] Wire `train_actv1_impl` to use backend builder\n- [ ] Add GUI controls (dropdown + dynamic fields) and persist state\n- [ ] Ensure `hf://` URI auto-maps to HF backend in both CLI and GUI\n- [ ] Add `vector_store.py` interface and Qdrant/LanceDB drivers\n- [ ] Document docker snippet and LanceDB alternative\n\n### QA checklist\n\n- [ ] CLI help shows new flags with clear descriptions\n- [ ] Run HF smoke with 1 step; verify metrics JSONL appended\n- [ ] Run WDS smoke with 1 step; verify metrics JSONL appended\n- [ ] VS: upsert/query 100 embeddings; query returns 5 nearest with sensible scores\n- [ ] GUI: switching backends updates visible fields and persists across restarts\n- [ ] GUI: dataset estimator and memory estimator still render without errors\n\n### Docs checklist\n\n- [ ] Update quick starts (`docs/QUICK_START.md`, `docs/ACTV1_MOE_QUICK_START.md` [placeholder]) to mention backends\n- [ ] Add a short \u201cData Backends\u201d section in `docs/INDEX.md`\n- [ ] Add a \u201cVector Store (starter)\u201d section in `docs/INDEX.md`\n\n---\n\n## Appendix: mapping from dataset URIs to config\n\n- `file:///path/to/data.txt` \u2192 backend=custom, dataset_file=that path\n- `hf://<name>:<config>:<split>` \u2192 backend=hf, populate `hf_name`, `hf_config`, `hf_split`\n- `wds://<pattern>` \u2192 backend=webdataset, `wds_pattern=<pattern>`\n\nIf users paste raw values (no scheme):\n\n- If it looks like a local file/dir \u2192 `custom`\n- If it matches `hf://` \u2192 `hf`\n- If it contains `{000..999}.tar` or endswith `.tar` \u2192 suggest `webdataset`\n\nThis keeps the happy path minimal while enabling more scalable backends when needed.\n", "tags": ["cli", "datasets", "evaluation", "experts", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "PF-005: Data backends and vector stores"}, {"line": 4, "text": "Goals (What we ship)"}, {"line": 13, "text": "Motivation"}, {"line": 19, "text": "Non-goals (Out of scope for PF-005)"}, {"line": 27, "text": "Architecture overview"}, {"line": 29, "text": "1) Dataset backends"}, {"line": 42, "text": "src/aios/cli/hrm_hf/data_backends/base.py"}, {"line": 52, "text": "\t\t# Optional: direct batching/tokenization if desired"}, {"line": 63, "text": "2) Vector store layer"}, {"line": 83, "text": "3) Cognitive memory integration (Future)"}, {"line": 89, "text": "A) Attention Trace Storage"}, {"line": 95, "text": "src/aios/core/hrm_models/cognitive/vector_wrappers.py"}, {"line": 136, "text": "B) Crystallized Motif Storage"}, {"line": 167, "text": "Filter by task tag, return highest utility"}, {"line": 176, "text": "Retrieve motifs and return for injection into target model's RoutingPathTree"}, {"line": 189, "text": "C) Scalability Comparison"}, {"line": 217, "text": "User-facing changes"}, {"line": 219, "text": "CLI additions (Typer)"}, {"line": 250, "text": "Custom (unchanged)"}, {"line": 253, "text": "HF streaming"}, {"line": 259, "text": "WebDataset shards (local)"}, {"line": 270, "text": "GUI additions (HRM Training Panel)"}, {"line": 288, "text": "Config and validation"}, {"line": 306, "text": "Implementation details"}, {"line": 308, "text": "A) Data backends package"}, {"line": 332, "text": "B) Tokenization and batching"}, {"line": 338, "text": "C) HRM CLI wiring"}, {"line": 343, "text": "D) GUI wiring"}, {"line": 350, "text": "E) Vector store minimal layer"}, {"line": 368, "text": "F) Cognitive memory integration (Future)"}, {"line": 386, "text": "Docker and local services"}, {"line": 415, "text": "Testing and acceptance criteria"}, {"line": 431, "text": "HF streaming smoke"}, {"line": 437, "text": "WebDataset smoke (assuming shards exist)"}, {"line": 452, "text": "Implementation roadmap"}, {"line": 456, "text": "Week 1-2: Core Infrastructure"}, {"line": 471, "text": "Week 3-4: Dataset Backend Integration"}, {"line": 487, "text": "Week 5-6: GUI and Production Hardening"}, {"line": 502, "text": "Week 6.5-7.5: Cognitive Memory Integration (Optional)"}, {"line": 526, "text": "Week 8: Deployment and Monitoring (Final)"}, {"line": 541, "text": "Coordination with Persistent Traces plan"}, {"line": 563, "text": "Unified configuration schema"}, {"line": 570, "text": "Memory Architecture (Unified Schema)"}, {"line": 571, "text": "Covers: Dataset backends (PF-005), Vector stores (PF-005), "}, {"line": 572, "text": "Persistent traces (Cognitive), Semantic crystallization (Cognitive)"}, {"line": 574, "text": "============================================"}, {"line": 575, "text": "Dataset Backends (PF-005)"}, {"line": 576, "text": "============================================"}, {"line": 580, "text": "HuggingFace Datasets streaming"}, {"line": 590, "text": "WebDataset tar shards"}, {"line": 598, "text": "============================================"}, {"line": 599, "text": "Vector Storage Backend (PF-005)"}, {"line": 600, "text": "============================================"}, {"line": 604, "text": "Qdrant configuration"}, {"line": 611, "text": "LanceDB configuration"}, {"line": 615, "text": "============================================"}, {"line": 616, "text": "Persistent Attention Traces (Cognitive)"}, {"line": 617, "text": "See: PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md"}, {"line": 618, "text": "============================================"}, {"line": 622, "text": "Core trace parameters"}, {"line": 632, "text": "Vector store integration (optional - requires vector_store.backend != disabled)"}, {"line": 639, "text": "============================================"}, {"line": 640, "text": "Semantic Crystallization (Cognitive)"}, {"line": 641, "text": "See: PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md"}, {"line": 642, "text": "============================================"}, {"line": 646, "text": "Crystallization criteria"}, {"line": 653, "text": "Vector store integration (optional)"}, {"line": 678, "text": "Rollout plan"}, {"line": 695, "text": "Risks and mitigations"}, {"line": 704, "text": "Checklists"}, {"line": 706, "text": "Engineering checklist"}, {"line": 717, "text": "QA checklist"}, {"line": 726, "text": "Docs checklist"}, {"line": 734, "text": "Appendix: mapping from dataset URIs to config"}]}, {"path": "planned_features/DEEPSPEED_ZERO_INFINITY.md", "content": "# DeepSpeed ZeRO-Infinity Integration Plan\n\n## Executive Summary\n\n**Goal**: Integrate DeepSpeed ZeRO-Infinity to enable training of multi-billion parameter models that exceed GPU and CPU memory capacity by leveraging NVMe storage offloading.\n\n**Current State**: AI-OS supports ZeRO Stages 1-3 with basic CPU offloading for optimizer states.\n\n**Target State**: Full ZeRO-Infinity support with intelligent NVMe offloading for parameters, gradients, and optimizer states, enabling training of 10B+ parameter models on consumer hardware.\n\n**Impact**: \n- Train models 10-100x larger than current limits\n- Utilize high-speed NVMe storage as memory extension\n- Enable training of GPT-3 scale models (175B params) on multi-GPU consumer setups\n- Minimal speed penalty (5-15% slower) with fast NVMe drives\n\n---\n\n## Background\n\n### What is ZeRO-Infinity?\n\nZeRO-Infinity extends DeepSpeed's ZeRO-3 optimization by adding **NVMe offloading** as a third tier of memory hierarchy:\n\n1. **GPU VRAM** (fastest, smallest) - Active computation\n2. **CPU RAM** (fast, medium) - Optimizer states, gradients  \n3. **NVMe Storage** (slower, massive) - Parameters, checkpoints\n\n**Key Innovation**: Overlapped data movement between tiers masks latency, maintaining 90%+ training efficiency even with NVMe offloading.\n\n### Current AI-OS Capabilities\n\n\u2705 **Already Implemented**:\n- ZeRO Stage 1: Optimizer state partitioning (~25% VRAM savings)\n- ZeRO Stage 2: Optimizer + gradient partitioning (~50% VRAM savings)\n- ZeRO Stage 3: Full parameter partitioning (~75% VRAM savings)\n- Basic CPU offloading for carry states in extreme contexts\n- 8-bit optimizers via bitsandbytes\n- Gradient checkpointing\n- Chunked training for extreme contexts (100K+ tokens)\n\n\u274c **Missing for ZeRO-Infinity**:\n- NVMe offload configuration and initialization\n- Parameter offloading to NVMe storage\n- Optimizer state offloading to NVMe\n- Gradient offloading to NVMe (optional)\n- Async prefetching from NVMe to CPU/GPU\n- NVMe bandwidth monitoring and optimization\n- Memory hierarchy management\n- Pin memory optimizations for fast transfers\n\n---\n\n## Technical Architecture\n\n### Memory Hierarchy with ZeRO-Infinity\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Training Iteration                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                               \u2502\n\u2502  GPU VRAM (11GB)                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 Active Parameters (32 layers out of 96)             \u2502   \u2502\n\u2502  \u2502 Activations & Gradients (current batch)             \u2502   \u2502\n\u2502  \u2502 Optimizer States (for active params)                \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502         \u2195 PCIe 3.0/4.0 (15-30 GB/s)                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  CPU RAM (32-64GB)                                          \u2502\n\u2502  \u2502 Prefetched Parameters (next 64 layers)              \u2502   \u2502\n\u2502  \u2502 Optimizer States (frozen params)                    \u2502   \u2502\n\u2502  \u2502 Gradients (waiting to be applied)                   \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502         \u2195 NVMe PCIe 4.0 (5-7 GB/s)                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  NVMe Storage (1-2TB)                                       \u2502\n\u2502  \u2502 Full Model Parameters (10B-175B params)             \u2502   \u2502\n\u2502  \u2502 Optimizer State History (Adam momentum/variance)    \u2502   \u2502\n\u2502  \u2502 Checkpoints & Intermediate States                   \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Data Flow During Training\n\n```\nForward Pass:\n1. NVMe \u2192 CPU: Async prefetch next layer params\n2. CPU \u2192 GPU: Transfer layer params for computation  \n3. GPU: Compute activations\n4. Repeat for all layers\n\nBackward Pass:\n1. GPU: Compute gradients\n2. GPU \u2192 CPU: Offload gradients (optional)\n3. CPU \u2192 NVMe: Archive gradients (optional)\n4. GPU: Update params with optimizer\n5. GPU \u2192 CPU \u2192 NVMe: Offload updated params\n\nKey Optimization: Overlap transfers with computation!\n```\n\n---\n\n## Implementation Plan\n\n### Phase 1: Configuration and Setup (Week 1)\n\n#### 1.1 DeepSpeed Configuration Files\n\nCreate new configuration profiles for ZeRO-Infinity:\n\n**File**: `config/deepspeed_zero_infinity.json`\n\n```json\n{\n  \"train_batch_size\": \"auto\",\n  \"train_micro_batch_size_per_gpu\": \"auto\",\n  \"gradient_accumulation_steps\": \"auto\",\n  \n  \"optimizer\": {\n    \"type\": \"AdamW\",\n    \"params\": {\n      \"lr\": \"auto\",\n      \"betas\": [0.9, 0.999],\n      \"eps\": 1e-8,\n      \"weight_decay\": 0.01\n    }\n  },\n  \n  \"scheduler\": {\n    \"type\": \"WarmupDecayLR\",\n    \"params\": {\n      \"warmup_min_lr\": 0,\n      \"warmup_max_lr\": \"auto\",\n      \"warmup_num_steps\": \"auto\",\n      \"total_num_steps\": \"auto\"\n    }\n  },\n  \n  \"zero_optimization\": {\n    \"stage\": 3,\n    \n    \"offload_optimizer\": {\n      \"device\": \"nvme\",\n      \"nvme_path\": \"/tmp/deepspeed_offload\",\n      \"pin_memory\": true,\n      \"buffer_count\": 5,\n      \"fast_init\": false\n    },\n    \n    \"offload_param\": {\n      \"device\": \"nvme\",\n      \"nvme_path\": \"/tmp/deepspeed_offload\",\n      \"pin_memory\": true,\n      \"buffer_count\": 5,\n      \"buffer_size\": 1e8,\n      \"max_in_cpu\": 1e9\n    },\n    \n    \"overlap_comm\": true,\n    \"contiguous_gradients\": true,\n    \"sub_group_size\": 1e9,\n    \"reduce_bucket_size\": 1e6,\n    \"stage3_prefetch_bucket_size\": 1e6,\n    \"stage3_param_persistence_threshold\": 1e5,\n    \"stage3_max_live_parameters\": 1e9,\n    \"stage3_max_reuse_distance\": 1e9,\n    \"stage3_gather_16bit_weights_on_model_save\": true\n  },\n  \n  \"aio\": {\n    \"block_size\": 1048576,\n    \"queue_depth\": 8,\n    \"thread_count\": 1,\n    \"single_submit\": false,\n    \"overlap_events\": true\n  },\n  \n  \"bf16\": {\n    \"enabled\": true\n  },\n  \n  \"gradient_clipping\": 1.0,\n  \"steps_per_print\": 100,\n  \"wall_clock_breakdown\": false\n}\n```\n\n**File**: `config/deepspeed_zero_infinity_cpu.json` (CPU fallback)\n\n```json\n{\n  \"train_batch_size\": \"auto\",\n  \"train_micro_batch_size_per_gpu\": \"auto\",\n  \"gradient_accumulation_steps\": \"auto\",\n  \n  \"optimizer\": {\n    \"type\": \"AdamW\",\n    \"params\": {\n      \"lr\": \"auto\",\n      \"betas\": [0.9, 0.999],\n      \"eps\": 1e-8,\n      \"weight_decay\": 0.01\n    }\n  },\n  \n  \"zero_optimization\": {\n    \"stage\": 3,\n    \n    \"offload_optimizer\": {\n      \"device\": \"cpu\",\n      \"pin_memory\": true\n    },\n    \n    \"offload_param\": {\n      \"device\": \"cpu\",\n      \"pin_memory\": true\n    },\n    \n    \"overlap_comm\": true,\n    \"contiguous_gradients\": true\n  },\n  \n  \"bf16\": {\n    \"enabled\": true\n  }\n}\n```\n\n#### 1.2 Training Configuration Fields\n\n**File**: `src/aios/core/hrm_training/training_config/optimization_fields.py`\n\nAdd new fields to `OptimizationFields` dataclass:\n\n```python\n# ZeRO-Infinity NVMe Offloading\nnvme_offload_path: str = \"/tmp/deepspeed_offload\"\n\"\"\"Path to NVMe directory for ZeRO-Infinity offloading.\n\nWhen zero_stage=\"infinity\", this directory is used to offload:\n- Model parameters\n- Optimizer states  \n- Optionally gradients\n\nRequirements:\n- Must be on NVMe SSD for best performance (PCIe 3.0/4.0)\n- Needs ~2-10x model size in free space\n- Should be on fast storage (>2 GB/s sequential write)\n\nRecommended paths:\n- Linux: /mnt/nvme/deepspeed_offload (mount NVMe here)\n- Windows: D:/deepspeed_offload (if D: is NVMe)\n- Temp fallback: /tmp/deepspeed_offload (auto-cleanup)\n\nSize requirements (approximate):\n- 1B params: ~20GB NVMe space\n- 10B params: ~200GB NVMe space  \n- 100B params: ~2TB NVMe space\n\nDefault: /tmp/deepspeed_offload\n\"\"\"\n\nnvme_offload_optimizer: bool = True\n\"\"\"Offload optimizer states to NVMe storage.\n\nWhen using ZeRO-Infinity, optimizer states (Adam momentum & variance) \nconsume 2x parameter memory. Offloading to NVMe frees CPU RAM.\n\nImpact:\n- Memory: Saves 2x param size in CPU RAM\n- Speed: ~5-10% slower (with fast NVMe)\n- Requires: Fast NVMe drive (>2 GB/s)\n\nDefault: True (enabled for infinity mode)\n\"\"\"\n\nnvme_offload_params: bool = True\n\"\"\"Offload model parameters to NVMe storage.\n\nStores frozen/inactive parameters on NVMe, loading only active layers\nto GPU during computation. Essential for models >10B parameters.\n\nImpact:\n- Memory: Stores full model on NVMe vs RAM\n- Speed: ~10-15% slower (with fast NVMe)\n- Requires: NVMe with model_size * 4 bytes free space\n\nDefault: True (enabled for infinity mode)\n\"\"\"\n\nnvme_offload_gradients: bool = False\n\"\"\"Offload gradients to NVMe storage (optional).\n\nExperimental: Offload computed gradients to NVMe between backward passes.\nOnly beneficial for extremely large models (>100B params) or small RAM.\n\nImpact:\n- Memory: Saves param_size in CPU RAM\n- Speed: ~20-30% slower\n- Rarely needed: Usually CPU RAM sufficient\n\nDefault: False (not recommended unless desperate)\n\"\"\"\n\naio_block_size: int = 1048576\n\"\"\"Async I/O block size for NVMe operations (bytes).\n\nControls the block size for asynchronous I/O transfers between NVMe and CPU.\nLarger blocks = better throughput, higher latency.\n\nRecommended values:\n- 1048576 (1MB): Balanced (default)\n- 2097152 (2MB): High throughput NVMe (PCIe 4.0)\n- 524288 (512KB): Lower latency, slower drives\n\nDefault: 1048576 (1MB)\n\"\"\"\n\naio_queue_depth: int = 8\n\"\"\"Async I/O queue depth for NVMe operations.\n\nNumber of concurrent I/O requests to NVMe. Higher = better parallelism.\n\nRecommended values:\n- 4: Conservative, older NVMe drives\n- 8: Balanced (default)\n- 16: High-performance NVMe (Samsung 980 Pro, etc.)\n- 32: Extreme performance (server-grade NVMe)\n\nDefault: 8\n\"\"\"\n\npin_memory: bool = True\n\"\"\"Pin CPU memory for faster GPU transfers.\n\nPinned memory enables DMA transfers without CPU involvement,\nreducing latency for CPU\u2194GPU transfers during offloading.\n\nImpact:\n- Speed: ~20-30% faster transfers\n- Memory: Uses non-swappable RAM\n- Stability: May cause issues with low RAM systems\n\nDefault: True (recommended unless <16GB RAM)\n\"\"\"\n```\n\n#### 1.3 CLI Arguments\n\n**File**: `src/aios/cli/hrm_hf_cli.py`\n\nAdd new arguments to `train_actv1` function:\n\n```python\n# ZeRO-Infinity (NVMe Offloading) options\nnvme_offload_path: str = typer.Option(\n    \"/tmp/deepspeed_offload\", \n    \"--nvme-offload-path\",\n    help=\"Path to NVMe directory for ZeRO-Infinity offloading. Must be on fast NVMe SSD (>2 GB/s). Requires ~2-10x model size in free space.\"\n),\nnvme_offload_optimizer: bool = typer.Option(\n    True, \n    \"--nvme-offload-optimizer/--no-nvme-offload-optimizer\",\n    help=\"Offload optimizer states to NVMe (saves 2x param size in RAM, ~5-10%% slower). Requires zero-stage=infinity.\"\n),\nnvme_offload_params: bool = typer.Option(\n    True,\n    \"--nvme-offload-params/--no-nvme-offload-params\", \n    help=\"Offload model parameters to NVMe (essential for 10B+ models, ~10-15%% slower). Requires zero-stage=infinity.\"\n),\nnvme_offload_gradients: bool = typer.Option(\n    False,\n    \"--nvme-offload-gradients/--no-nvme-offload-gradients\",\n    help=\"Offload gradients to NVMe (rarely needed, ~20-30%% slower). Only for 100B+ models.\"\n),\naio_block_size: int = typer.Option(\n    1048576,\n    \"--aio-block-size\",\n    help=\"Async I/O block size for NVMe (bytes). 1MB=balanced, 2MB=high throughput, 512KB=low latency.\"\n),\naio_queue_depth: int = typer.Option(\n    8,\n    \"--aio-queue-depth\", \n    help=\"Async I/O queue depth for NVMe. 8=balanced, 16=high-performance NVMe, 32=server-grade.\"\n),\npin_memory: bool = typer.Option(\n    True,\n    \"--pin-memory/--no-pin-memory\",\n    help=\"Pin CPU memory for faster GPU transfers. Disable if <16GB RAM.\"\n),\n```\n\nUpdate `zero_stage` argument options:\n\n```python\nzero_stage: str = typer.Option(\n    \"none\", \n    \"--zero-stage\", \n    help=\"DeepSpeed ZeRO optimization: none, zero1 (\u219325%% VRAM), zero2 (\u219350%% VRAM, recommended), zero3 (\u219375%% VRAM), infinity (NVMe offload, train 10B+ models). Auto-selected if --optimize is used.\"\n),\n```\n\n---\n\n### Phase 2: Core Implementation (Week 2-3)\n\n#### 2.1 NVMe Configuration Builder\n\n**File**: `src/aios/cli/hrm_hf/nvme_config.py` (new)\n\n```python\n\"\"\"NVMe configuration and validation for ZeRO-Infinity.\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport shutil\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional, Tuple\nimport json\n\n\ndef validate_nvme_path(nvme_path: str, required_space_gb: float) -> Tuple[bool, str]:\n    \"\"\"\n    Validate NVMe offload path has sufficient space and write permissions.\n    \n    Args:\n        nvme_path: Path to NVMe directory\n        required_space_gb: Required free space in GB\n        \n    Returns:\n        (is_valid, error_message)\n    \"\"\"\n    path = Path(nvme_path)\n    \n    # Check if path exists or can be created\n    try:\n        path.mkdir(parents=True, exist_ok=True)\n    except PermissionError:\n        return False, f\"Permission denied: Cannot create {nvme_path}\"\n    except Exception as e:\n        return False, f\"Failed to create {nvme_path}: {e}\"\n    \n    # Check write permissions\n    test_file = path / \".deepspeed_test\"\n    try:\n        test_file.touch()\n        test_file.unlink()\n    except Exception as e:\n        return False, f\"Cannot write to {nvme_path}: {e}\"\n    \n    # Check available space\n    stat = shutil.disk_usage(path)\n    available_gb = stat.free / (1024**3)\n    \n    if available_gb < required_space_gb:\n        return False, f\"Insufficient space: {available_gb:.1f}GB available, {required_space_gb:.1f}GB required\"\n    \n    return True, \"\"\n\n\ndef estimate_nvme_space_required(\n    total_params: int,\n    offload_params: bool,\n    offload_optimizer: bool,\n    offload_gradients: bool,\n    safety_factor: float = 1.5\n) -> float:\n    \"\"\"\n    Estimate required NVMe space in GB.\n    \n    Args:\n        total_params: Total model parameters\n        offload_params: Whether offloading parameters\n        offload_optimizer: Whether offloading optimizer states\n        offload_gradients: Whether offloading gradients\n        safety_factor: Multiply by this for safety margin\n        \n    Returns:\n        Required space in GB\n    \"\"\"\n    bytes_per_param = 4  # FP32\n    \n    space_gb = 0.0\n    \n    if offload_params:\n        # Parameters: 4 bytes each\n        space_gb += (total_params * bytes_per_param) / (1024**3)\n    \n    if offload_optimizer:\n        # Optimizer states: 8 bytes per param (Adam: momentum + variance)\n        space_gb += (total_params * 8) / (1024**3)\n    \n    if offload_gradients:\n        # Gradients: 4 bytes per param\n        space_gb += (total_params * bytes_per_param) / (1024**3)\n    \n    # Apply safety factor for temp files, fragmentation, etc.\n    space_gb *= safety_factor\n    \n    return space_gb\n\n\ndef check_nvme_performance(nvme_path: str) -> Dict[str, Any]:\n    \"\"\"\n    Check NVMe performance characteristics.\n    \n    Returns dict with:\n    - sequential_read_gb_s: Sequential read speed (GB/s)\n    - sequential_write_gb_s: Sequential write speed (GB/s)  \n    - is_ssd: Whether drive appears to be SSD\n    - warnings: List of performance warnings\n    \"\"\"\n    import subprocess\n    import platform\n    \n    result = {\n        \"sequential_read_gb_s\": None,\n        \"sequential_write_gb_s\": None,\n        \"is_ssd\": None,\n        \"warnings\": []\n    }\n    \n    path = Path(nvme_path).resolve()\n    \n    # Get device info based on OS\n    if platform.system() == \"Linux\":\n        try:\n            # Try to identify device from path\n            df_output = subprocess.check_output(\n                [\"df\", str(path)], \n                universal_newlines=True\n            )\n            device = df_output.split('\\n')[1].split()[0]\n            \n            # Check if rotational (HDD=1, SSD=0)\n            device_name = device.split('/')[-1].rstrip('0123456789')\n            rotational_file = f\"/sys/block/{device_name}/queue/rotational\"\n            \n            if os.path.exists(rotational_file):\n                with open(rotational_file) as f:\n                    is_rotational = f.read().strip() == '1'\n                    result[\"is_ssd\"] = not is_rotational\n                    \n                    if is_rotational:\n                        result[\"warnings\"].append(\n                            \"WARNING: Path appears to be on HDD, not SSD. \"\n                            \"ZeRO-Infinity will be very slow. Use NVMe SSD for best results.\"\n                        )\n        except Exception:\n            pass  # Can't determine, not critical\n    \n    elif platform.system() == \"Windows\":\n        # On Windows, harder to detect programmatically\n        # Just warn if path is C: (usually OS drive, may be SATA SSD)\n        if str(path).startswith(\"C:\"):\n            result[\"warnings\"].append(\n                \"Path is on C: drive. For best performance, use dedicated NVMe drive (D:, E:, etc.)\"\n            )\n    \n    # TODO: Could add actual speed test with dd/fio, but may be too slow for startup\n    \n    return result\n\n\ndef build_infinity_config(\n    base_config: Dict[str, Any],\n    nvme_path: str,\n    offload_params: bool,\n    offload_optimizer: bool,\n    offload_gradients: bool,\n    aio_block_size: int,\n    aio_queue_depth: int,\n    pin_memory: bool,\n    max_in_cpu_gb: float = 1.0,\n) -> Dict[str, Any]:\n    \"\"\"\n    Build ZeRO-Infinity configuration dict.\n    \n    Args:\n        base_config: Base DeepSpeed ZeRO-3 config\n        nvme_path: Path for NVMe offloading\n        offload_params: Offload parameters to NVMe\n        offload_optimizer: Offload optimizer states to NVMe\n        offload_gradients: Offload gradients to NVMe\n        aio_block_size: Async I/O block size\n        aio_queue_depth: Async I/O queue depth\n        pin_memory: Use pinned memory\n        max_in_cpu_gb: Max params to keep in CPU RAM (GB)\n        \n    Returns:\n        Updated config dict with ZeRO-Infinity settings\n    \"\"\"\n    config = base_config.copy()\n    \n    # Ensure we're using ZeRO stage 3\n    config[\"zero_optimization\"][\"stage\"] = 3\n    \n    # Configure parameter offloading\n    if offload_params:\n        config[\"zero_optimization\"][\"offload_param\"] = {\n            \"device\": \"nvme\",\n            \"nvme_path\": nvme_path,\n            \"pin_memory\": pin_memory,\n            \"buffer_count\": 5,\n            \"buffer_size\": int(1e8),  # 100MB buffer\n            \"max_in_cpu\": int(max_in_cpu_gb * 1e9),  # Keep some in CPU\n        }\n    else:\n        config[\"zero_optimization\"][\"offload_param\"] = {\n            \"device\": \"cpu\",\n            \"pin_memory\": pin_memory,\n        }\n    \n    # Configure optimizer offloading  \n    if offload_optimizer:\n        config[\"zero_optimization\"][\"offload_optimizer\"] = {\n            \"device\": \"nvme\",\n            \"nvme_path\": nvme_path,\n            \"pin_memory\": pin_memory,\n            \"buffer_count\": 5,\n            \"fast_init\": False,\n        }\n    else:\n        config[\"zero_optimization\"][\"offload_optimizer\"] = {\n            \"device\": \"cpu\",\n            \"pin_memory\": pin_memory,\n        }\n    \n    # Configure gradient offloading (rare)\n    if offload_gradients:\n        # Note: Gradient offload to NVMe not officially in DeepSpeed API\n        # This would require custom implementation\n        config[\"zero_optimization\"][\"offload_gradients\"] = {\n            \"device\": \"nvme\",\n            \"nvme_path\": nvme_path,\n        }\n    \n    # Configure AIO (Async I/O)\n    config[\"aio\"] = {\n        \"block_size\": aio_block_size,\n        \"queue_depth\": aio_queue_depth,\n        \"thread_count\": 1,\n        \"single_submit\": False,\n        \"overlap_events\": True,\n    }\n    \n    return config\n\n\ndef save_infinity_config(\n    config: Dict[str, Any],\n    output_path: str\n) -> None:\n    \"\"\"Save ZeRO-Infinity config to JSON file.\"\"\"\n    with open(output_path, 'w') as f:\n        json.dump(config, f, indent=2)\n```\n\n#### 2.2 Update Distributed Setup\n\n**File**: `src/aios/cli/hrm_hf/distributed_setup.py`\n\nUpdate `initialize_deepspeed` function:\n\n```python\ndef initialize_deepspeed(\n    model: Any,\n    config: \"TrainingConfig\",\n    device_obj: torch.device,\n    log_fn\n) -> Tuple[Optional[Any], bool]:\n    \"\"\"Initialize DeepSpeed ZeRO optimizer with Infinity support.\"\"\"\n    \n    zero_stage = config.zero_stage\n    \n    if not zero_stage or zero_stage == \"none\":\n        return None, False\n    \n    dev = str(device_obj).split(':')[0]\n    if dev != \"cuda\":\n        log_fn({\n            \"deepspeed\": \"skipped\",\n            \"reason\": \"Only CUDA devices supported\",\n            \"device\": dev\n        })\n        return None, False\n    \n    try:\n        import deepspeed\n        \n        # Determine config file\n        if zero_stage == \"infinity\":\n            from .nvme_config import (\n                validate_nvme_path,\n                estimate_nvme_space_required,\n                check_nvme_performance,\n                build_infinity_config,\n                save_infinity_config,\n            )\n            \n            # Calculate model size\n            total_params = sum(p.numel() for p in model.parameters())\n            \n            # Estimate NVMe space needed\n            required_space_gb = estimate_nvme_space_required(\n                total_params=total_params,\n                offload_params=config.nvme_offload_params,\n                offload_optimizer=config.nvme_offload_optimizer,\n                offload_gradients=config.nvme_offload_gradients,\n                safety_factor=1.5,\n            )\n            \n            # Validate NVMe path\n            is_valid, error_msg = validate_nvme_path(\n                config.nvme_offload_path,\n                required_space_gb\n            )\n            \n            if not is_valid:\n                log_fn({\n                    \"deepspeed\": \"nvme_validation_failed\",\n                    \"error\": error_msg,\n                    \"fallback\": \"Using CPU offload instead\",\n                })\n                # Fallback to CPU offload\n                ds_config_path = \"config/deepspeed_zero_infinity_cpu.json\"\n            else:\n                # Check NVMe performance\n                perf_info = check_nvme_performance(config.nvme_offload_path)\n                \n                for warning in perf_info.get(\"warnings\", []):\n                    log_fn({\"nvme_warning\": warning})\n                \n                # Load base ZeRO-3 config\n                import json\n                with open(\"config/deepspeed_zero3.json\") as f:\n                    base_config = json.load(f)\n                \n                # Build Infinity config\n                ds_config = build_infinity_config(\n                    base_config=base_config,\n                    nvme_path=config.nvme_offload_path,\n                    offload_params=config.nvme_offload_params,\n                    offload_optimizer=config.nvme_offload_optimizer,\n                    offload_gradients=config.nvme_offload_gradients,\n                    aio_block_size=config.aio_block_size,\n                    aio_queue_depth=config.aio_queue_depth,\n                    pin_memory=config.pin_memory,\n                    max_in_cpu_gb=1.0,  # Keep 1GB in CPU RAM\n                )\n                \n                # Save to temp file\n                import tempfile\n                temp_config = tempfile.NamedTemporaryFile(\n                    mode='w',\n                    suffix='.json',\n                    delete=False,\n                    dir='artifacts/brains/actv1'\n                )\n                save_infinity_config(ds_config, temp_config.name)\n                ds_config_path = temp_config.name\n                \n                log_fn({\n                    \"deepspeed\": \"infinity_config_created\",\n                    \"nvme_path\": config.nvme_offload_path,\n                    \"required_space_gb\": round(required_space_gb, 2),\n                    \"offload_params\": config.nvme_offload_params,\n                    \"offload_optimizer\": config.nvme_offload_optimizer,\n                    \"offload_gradients\": config.nvme_offload_gradients,\n                    \"aio_block_size\": config.aio_block_size,\n                    \"aio_queue_depth\": config.aio_queue_depth,\n                })\n        \n        elif zero_stage in [\"zero1\", \"zero2\", \"zero3\"]:\n            # Existing ZeRO stage logic\n            ds_config_path = f\"config/deepspeed_{zero_stage}.json\"\n        \n        else:\n            log_fn({\n                \"deepspeed\": \"invalid_stage\",\n                \"zero_stage\": zero_stage,\n            })\n            return None, False\n        \n        # Rest of initialization...\n        # (existing code continues)\n        \n    except ImportError as e:\n        # Handle missing DeepSpeed\n        # (existing error handling)\n        pass\n```\n\n#### 2.3 GUI Integration\n\n**File**: `src/aios/gui/components/hrm_training_panel/ui_optimizations.py`\n\nUpdate ZeRO dropdown options:\n\n```python\n# Row 5: DeepSpeed ZeRO (updated with Infinity)\nzero_row = ttk.Frame(self)\nzero_row.grid(row=5, column=0, columnspan=3, sticky=\"ew\", padx=5, pady=2)\n\nttk.Label(zero_row, text=\"DeepSpeed:\", width=15, anchor=\"e\", font=(\"TkDefaultFont\", 9, \"bold\")).pack(side=\"left\")\n\nzero_combo = ttk.Combobox(\n    zero_row,\n    textvariable=self.zero_stage_var,\n    values=[\"none\", \"zero1\", \"zero2\", \"zero3\", \"infinity\"],  # Added \"infinity\"\n    state=\"readonly\",\n    width=12\n)\nzero_combo.pack(side=\"left\", padx=5)\nself.zero_combo = zero_combo\n\n# Dynamic label for memory savings\nzero_savings_lbl = ttk.Label(zero_row, text=\"\", foreground=\"blue\")\nzero_savings_lbl.pack(side=\"left\", padx=5)\nself.zero_savings_lbl = zero_savings_lbl\n\ndef update_zero_label(*args):\n    stage = self.zero_stage_var.get()\n    savings_text = {\n        \"none\": \"\",\n        \"zero1\": \"\u219325% VRAM, ~2% slower\",\n        \"zero2\": \"\u219350% VRAM, ~5% slower (recommended)\",\n        \"zero3\": \"\u219375% VRAM, ~15% slower\",\n        \"infinity\": \"\u219390%+ VRAM, train 10B+ models (requires NVMe)\",  # NEW\n    }.get(stage, \"\")\n    zero_savings_lbl.config(text=savings_text)\n\nself.zero_stage_var.trace_add(\"write\", update_zero_label)\nupdate_zero_label()\n\n# Updated tooltip\nadd_tooltip(\n    zero_combo,\n    \"DeepSpeed ZeRO: Distributed memory optimization\\n\"\n    \"\u2022 none: Standard training\\n\"\n    \"\u2022 zero1: Partition optimizer states (\u219325% VRAM)\\n\"\n    \"\u2022 zero2: Partition optimizer + gradients (\u219350% VRAM) [RECOMMENDED]\\n\"\n    \"\u2022 zero3: Partition everything (\u219375% VRAM, slower)\\n\"\n    \"\u2022 infinity: NVMe offload for 10B+ models (\u219390%+ VRAM, requires fast NVMe SSD)\"\n)\n```\n\nAdd NVMe configuration section (collapsible):\n\n```python\n# Row 6: NVMe Configuration (shown when infinity selected)\nnvme_section = ttk.LabelFrame(self, text=\"ZeRO-Infinity NVMe Settings\", padding=5)\n# Hidden by default, shown when zero_stage == \"infinity\"\n\nnvme_path_row = ttk.Frame(nvme_section)\nnvme_path_row.pack(fill=\"x\", pady=2)\nttk.Label(nvme_path_row, text=\"NVMe Path:\", width=15).pack(side=\"left\")\nnvme_path_entry = ttk.Entry(nvme_path_row, textvariable=self.nvme_offload_path_var, width=30)\nnvme_path_entry.pack(side=\"left\", padx=5, fill=\"x\", expand=True)\nttk.Button(nvme_path_row, text=\"Browse\", command=self._browse_nvme_path, width=8).pack(side=\"left\")\n\n# Checkboxes for what to offload\noffload_frame = ttk.Frame(nvme_section)\noffload_frame.pack(fill=\"x\", pady=2)\nttk.Checkbutton(\n    offload_frame,\n    text=\"Offload Parameters\",\n    variable=self.nvme_offload_params_var\n).pack(side=\"left\", padx=5)\nttk.Checkbutton(\n    offload_frame,\n    text=\"Offload Optimizer\",\n    variable=self.nvme_offload_optimizer_var\n).pack(side=\"left\", padx=5)\nttk.Checkbutton(\n    offload_frame,\n    text=\"Offload Gradients\",\n    variable=self.nvme_offload_gradients_var\n).pack(side=\"left\", padx=5)\n\n# AIO settings (advanced, maybe collapsible)\naio_frame = ttk.LabelFrame(nvme_section, text=\"Advanced I/O Settings\", padding=3)\naio_frame.pack(fill=\"x\", pady=2)\n\naio_row1 = ttk.Frame(aio_frame)\naio_row1.pack(fill=\"x\")\nttk.Label(aio_row1, text=\"Block Size:\").pack(side=\"left\")\nttk.Spinbox(\n    aio_row1,\n    from_=524288,\n    to=4194304,\n    increment=524288,\n    textvariable=self.aio_block_size_var,\n    width=10\n).pack(side=\"left\", padx=5)\nttk.Label(aio_row1, text=\"bytes\").pack(side=\"left\")\n\naio_row2 = ttk.Frame(aio_frame)\naio_row2.pack(fill=\"x\")\nttk.Label(aio_row2, text=\"Queue Depth:\").pack(side=\"left\")\nttk.Spinbox(\n    aio_row2,\n    from_=4,\n    to=32,\n    increment=4,\n    textvariable=self.aio_queue_depth_var,\n    width=10\n).pack(side=\"left\", padx=5)\n\nself.nvme_section = nvme_section\n\ndef toggle_nvme_section(*args):\n    if self.zero_stage_var.get() == \"infinity\":\n        nvme_section.grid(row=6, column=0, columnspan=3, sticky=\"ew\", padx=5, pady=5)\n    else:\n        nvme_section.grid_forget()\n\nself.zero_stage_var.trace_add(\"write\", toggle_nvme_section)\n```\n\n**File**: `src/aios/gui/components/hrm_training_panel/variable_setup.py`\n\nAdd new variables:\n\n```python\n# ZeRO-Infinity NVMe Offloading\nself.nvme_offload_path_var = tk.StringVar(value=\"/tmp/deepspeed_offload\")\nself.nvme_offload_params_var = tk.BooleanVar(value=True)\nself.nvme_offload_optimizer_var = tk.BooleanVar(value=True)\nself.nvme_offload_gradients_var = tk.BooleanVar(value=False)\nself.aio_block_size_var = tk.IntVar(value=1048576)\nself.aio_queue_depth_var = tk.IntVar(value=8)\nself.pin_memory_var = tk.BooleanVar(value=True)\n```\n\n---\n\n### Phase 3: Memory Estimation Updates (Week 3)\n\n#### 3.1 Update VRAM Estimator\n\n**File**: `src/aios/gui/components/hrm_training/memory_estimator/vram_estimation.py`\n\nUpdate `estimate_vram` function to handle ZeRO-Infinity:\n\n```python\ndef estimate_vram(estimator: \"MemoryEstimator\") -> Dict[str, Any]:\n    \"\"\"Estimate VRAM usage accounting for ZeRO-Infinity.\"\"\"\n    \n    # ... existing code ...\n    \n    # Handle ZeRO-Infinity (most aggressive savings)\n    if estimator.zero_stage == \"infinity\":\n        # With Infinity, almost everything can be offloaded\n        # Only keep active computation in VRAM\n        \n        # Model parameters: Only active layers in VRAM (~5-10% of model)\n        model_gb_per_gpu = model_gb * 0.08  # ~8% in VRAM at once\n        \n        # Optimizer: Offloaded to NVMe or CPU\n        optimizer_gb_per_gpu = 0.0\n        \n        # Gradients: Mostly on NVMe/CPU, small buffer in VRAM\n        gradients_gb_per_gpu = gradients_gb * 0.1  # 10% buffer\n        \n        # Activations: Still need these in VRAM (not offloadable during computation)\n        # Keep activations calculation same\n        \n        savings_note = (\n            f\"ZeRO-Infinity: ~92% memory offloaded to NVMe. \"\n            f\"Requires {estimator.nvme_offload_path} with \"\n            f\"~{model_gb * 3:.1f}GB free space.\"\n        )\n    \n    elif estimator.zero_stage == \"zero3\":\n        # ... existing ZeRO-3 logic ...\n    \n    # ... rest of function ...\n```\n\n---\n\n### Phase 4: Testing and Validation (Week 4)\n\n#### 4.1 Unit Tests\n\n**File**: `tests/test_zero_infinity.py` (new)\n\n```python\n\"\"\"Unit tests for ZeRO-Infinity integration.\"\"\"\n\nimport pytest\nimport tempfile\nimport shutil\nfrom pathlib import Path\n\nfrom aios.cli.hrm_hf.nvme_config import (\n    validate_nvme_path,\n    estimate_nvme_space_required,\n    build_infinity_config,\n)\n\n\ndef test_validate_nvme_path():\n    \"\"\"Test NVMe path validation.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Valid path with enough space\n        is_valid, msg = validate_nvme_path(tmpdir, required_space_gb=0.001)\n        assert is_valid\n        assert msg == \"\"\n        \n        # Invalid path (doesn't exist, can't create)\n        is_valid, msg = validate_nvme_path(\"/root/forbidden\", required_space_gb=0.001)\n        assert not is_valid\n        assert \"Permission denied\" in msg or \"Failed to create\" in msg\n\n\ndef test_estimate_nvme_space():\n    \"\"\"Test NVMe space estimation.\"\"\"\n    # 1B params, offload everything\n    space_gb = estimate_nvme_space_required(\n        total_params=1_000_000_000,\n        offload_params=True,\n        offload_optimizer=True,\n        offload_gradients=True,\n        safety_factor=1.5,\n    )\n    \n    # Expected: (4 + 8 + 4) * 1B / 1e9 * 1.5 = 24 GB\n    assert 20 < space_gb < 30\n    \n    # Only optimizer offload\n    space_gb = estimate_nvme_space_required(\n        total_params=1_000_000_000,\n        offload_params=False,\n        offload_optimizer=True,\n        offload_gradients=False,\n        safety_factor=1.0,\n    )\n    \n    # Expected: 8 * 1B / 1e9 = 8 GB\n    assert 7 < space_gb < 9\n\n\ndef test_build_infinity_config():\n    \"\"\"Test Infinity config builder.\"\"\"\n    base_config = {\n        \"train_batch_size\": \"auto\",\n        \"zero_optimization\": {\n            \"stage\": 2,\n        }\n    }\n    \n    with tempfile.TemporaryDirectory() as tmpdir:\n        config = build_infinity_config(\n            base_config=base_config,\n            nvme_path=tmpdir,\n            offload_params=True,\n            offload_optimizer=True,\n            offload_gradients=False,\n            aio_block_size=1048576,\n            aio_queue_depth=8,\n            pin_memory=True,\n            max_in_cpu_gb=1.0,\n        )\n        \n        # Check stage upgraded to 3\n        assert config[\"zero_optimization\"][\"stage\"] == 3\n        \n        # Check param offload to NVMe\n        assert config[\"zero_optimization\"][\"offload_param\"][\"device\"] == \"nvme\"\n        assert config[\"zero_optimization\"][\"offload_param\"][\"nvme_path\"] == tmpdir\n        \n        # Check optimizer offload to NVMe\n        assert config[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] == \"nvme\"\n        \n        # Check AIO settings\n        assert config[\"aio\"][\"block_size\"] == 1048576\n        assert config[\"aio\"][\"queue_depth\"] == 8\n```\n\n#### 4.2 Integration Tests\n\n**File**: `tests/integration/test_infinity_training.py` (new)\n\n```python\n\"\"\"Integration tests for ZeRO-Infinity training.\"\"\"\n\nimport pytest\nimport tempfile\nfrom pathlib import Path\n\nfrom aios.core.hrm_training.training_config import TrainingConfig\n\n\n@pytest.mark.skipif(\n    not Path(\"/dev/nvme0n1\").exists(),\n    reason=\"NVMe device not available\"\n)\ndef test_infinity_training_small_model():\n    \"\"\"Test ZeRO-Infinity on small model to verify setup.\"\"\"\n    \n    with tempfile.TemporaryDirectory() as nvme_dir:\n        config = TrainingConfig(\n            model=\"gpt2\",\n            dataset_file=\"training_data/curated_datasets/test_sample.txt\",\n            steps=2,\n            batch_size=1,\n            zero_stage=\"infinity\",\n            nvme_offload_path=nvme_dir,\n            nvme_offload_params=True,\n            nvme_offload_optimizer=True,\n            nvme_offload_gradients=False,\n            device=\"cuda\",\n        )\n        \n        # Should complete without errors\n        from aios.cli.hrm_hf.train_actv1 import train_actv1_impl\n        train_actv1_impl(config)\n        \n        # Verify NVMe directory was created and used\n        assert Path(nvme_dir).exists()\n        assert len(list(Path(nvme_dir).iterdir())) > 0\n\n\ndef test_infinity_config_fallback_to_cpu():\n    \"\"\"Test fallback to CPU offload when NVMe unavailable.\"\"\"\n    \n    # Use invalid NVMe path\n    config = TrainingConfig(\n        model=\"gpt2\",\n        dataset_file=\"training_data/curated_datasets/test_sample.txt\",\n        steps=1,\n        batch_size=1,\n        zero_stage=\"infinity\",\n        nvme_offload_path=\"/invalid/path/no/permissions\",\n        device=\"cuda\",\n    )\n    \n    # Should fallback to CPU offload gracefully\n    from aios.cli.hrm_hf.train_actv1 import train_actv1_impl\n    # Should complete without crashing\n    train_actv1_impl(config)\n```\n\n#### 4.3 Manual Testing Plan\n\n**Test Case 1: Small Model (1B params) on 2x 11GB GPUs**\n\n```bash\n# Create NVMe offload directory\nmkdir -p /mnt/nvme/deepspeed_offload\n\n# Test with Infinity\naios hrm-hf train-actv1 \\\n  --model gpt2-medium \\\n  --dataset-file training_data/curated_datasets/test_sample.txt \\\n  --steps 10 \\\n  --batch-size 2 \\\n  --zero-stage infinity \\\n  --nvme-offload-path /mnt/nvme/deepspeed_offload \\\n  --nvme-offload-params \\\n  --nvme-offload-optimizer \\\n  --ddp \\\n  --cuda-ids 0,1\n\n# Expected: Training completes, NVMe directory contains offload files\n```\n\n**Test Case 2: Large Model (10B params) on 2x 11GB GPUs**\n\n```bash\n# This would OOM without Infinity\naios hrm-hf train-actv1 \\\n  --hidden-size 4096 \\\n  --h-layers 16 \\\n  --l-layers 16 \\\n  --num-heads 32 \\\n  --expansion 4.0 \\\n  --dataset-file training_data/curated_datasets/test_sample.txt \\\n  --steps 5 \\\n  --batch-size 1 \\\n  --zero-stage infinity \\\n  --nvme-offload-path /mnt/nvme/deepspeed_offload \\\n  --ddp \\\n  --cuda-ids 0,1\n\n# Expected: Training works, uses ~30-50GB NVMe space\n```\n\n**Test Case 3: Performance Comparison**\n\n```bash\n# Baseline: ZeRO-2\ntime aios hrm-hf train-actv1 --zero-stage zero2 [... other args ...]\n\n# With Infinity\ntime aios hrm-hf train-actv1 --zero-stage infinity [... other args ...]\n\n# Expected: Infinity ~10-20% slower with good NVMe\n```\n\n---\n\n### Phase 5: Documentation and Polish (Week 4-5)\n\n#### 5.1 User Documentation\n\n**File**: `docs/guide/zero_infinity_guide.md` (new)\n\nCreate comprehensive user guide covering:\n- What is ZeRO-Infinity and when to use it\n- Hardware requirements (NVMe SSD, PCIe 3.0+)\n- Setup instructions for Linux and Windows\n- Performance tuning (AIO settings, queue depth)\n- Troubleshooting common issues\n- Example configurations for different model sizes\n\n#### 5.2 API Documentation\n\nUpdate docstrings in:\n- `optimization_fields.py`: Document all Infinity-related fields\n- `nvme_config.py`: Comprehensive module documentation\n- `distributed_setup.py`: Infinity initialization flow\n\n#### 5.3 GUI Tooltips\n\nAdd informative tooltips to all Infinity UI elements:\n- NVMe path selector\n- Offload checkboxes  \n- AIO settings\n- Performance warnings\n\n---\n\n## Implementation Checklist\n\n### Phase 1: Configuration \u2705\n- [ ] Create `config/deepspeed_zero_infinity.json`\n- [ ] Create `config/deepspeed_zero_infinity_cpu.json`\n- [ ] Add fields to `optimization_fields.py`\n- [ ] Add CLI arguments to `hrm_hf_cli.py`\n- [ ] Update `zero_stage` help text\n\n### Phase 2: Core Implementation \u2705\n- [ ] Create `src/aios/cli/hrm_hf/nvme_config.py`\n  - [ ] `validate_nvme_path()`\n  - [ ] `estimate_nvme_space_required()`\n  - [ ] `check_nvme_performance()`\n  - [ ] `build_infinity_config()`\n  - [ ] `save_infinity_config()`\n- [ ] Update `distributed_setup.py`\n  - [ ] Infinity detection and config generation\n  - [ ] NVMe validation\n  - [ ] Fallback to CPU offload\n- [ ] Update GUI\n  - [ ] Add \"infinity\" to ZeRO dropdown\n  - [ ] Create NVMe settings section\n  - [ ] Add browse button for NVMe path\n  - [ ] Add offload checkboxes\n  - [ ] Add AIO settings (advanced)\n- [ ] Update `variable_setup.py`\n  - [ ] Add Infinity-related variables\n\n### Phase 3: Memory Estimation \u2705\n- [ ] Update `vram_estimation.py`\n  - [ ] Handle `zero_stage == \"infinity\"`\n  - [ ] Calculate ~92% VRAM savings\n  - [ ] Show NVMe space requirements\n- [ ] Update `memory_estimator/estimator.py`\n  - [ ] Add Infinity mode support\n- [ ] Update GUI VRAM display\n  - [ ] Show \"Offloaded to NVMe\" label\n  - [ ] Display NVMe space needed\n\n### Phase 4: Testing \u2705\n- [ ] Create `tests/test_zero_infinity.py`\n  - [ ] Path validation tests\n  - [ ] Space estimation tests\n  - [ ] Config builder tests\n- [ ] Create `tests/integration/test_infinity_training.py`\n  - [ ] Small model test\n  - [ ] Fallback test\n  - [ ] Large model test (if hardware available)\n- [ ] Manual testing\n  - [ ] 1B model on 2x GPUs\n  - [ ] 10B model on 2x GPUs\n  - [ ] Performance benchmarks\n  - [ ] Windows compatibility (if possible)\n\n### Phase 5: Documentation \u2705\n- [ ] Create `docs/guide/zero_infinity_guide.md`\n- [ ] Update API docstrings\n- [ ] Add GUI tooltips\n- [ ] Update README with Infinity examples\n- [ ] Create troubleshooting guide\n\n---\n\n## Success Metrics\n\n### Functional Requirements \u2705\n1. \u2705 Can train 10B+ parameter models on 2x 11GB GPUs\n2. \u2705 NVMe offloading works with validation and error handling\n3. \u2705 Graceful fallback to CPU offload when NVMe unavailable\n4. \u2705 GUI exposes all Infinity settings\n5. \u2705 CLI supports all Infinity arguments\n\n### Performance Requirements \u2705\n1. \u2705 Infinity mode <20% slower than ZeRO-2 (with fast NVMe)\n2. \u2705 Supports models up to 100B params (with sufficient NVMe space)\n3. \u2705 Memory footprint <2GB VRAM per GPU for 10B model\n4. \u2705 NVMe I/O throughput >2 GB/s (hardware dependent)\n\n### Quality Requirements \u2705\n1. \u2705 Comprehensive error messages for NVMe issues\n2. \u2705 Performance warnings for slow storage\n3. \u2705 Unit test coverage >80% for new code\n4. \u2705 Integration tests for key scenarios\n5. \u2705 Complete user documentation\n\n---\n\n## Risks and Mitigations\n\n### Risk 1: NVMe Performance Varies Widely\n\n**Impact**: Users with slow SSDs may see 50%+ slowdown instead of 10-20%.\n\n**Mitigation**:\n- Detect storage type (HDD vs SSD) at startup\n- Warn users if not on NVMe\n- Provide performance expectations in docs\n- Suggest testing with small models first\n\n### Risk 2: Windows NVMe Support\n\n**Impact**: Windows may have issues with AIO or NVMe detection.\n\n**Mitigation**:\n- Test on Windows with fast SSD\n- Provide CPU fallback automatically\n- Document Windows-specific limitations\n- Consider Windows-optimized AIO settings\n\n### Risk 3: Insufficient NVMe Space\n\n**Impact**: Training fails mid-way when NVMe fills up.\n\n**Mitigation**:\n- Validate space before starting\n- Reserve 20% safety margin\n- Monitor space during training\n- Provide clear error messages\n\n### Risk 4: DeepSpeed Version Compatibility\n\n**Impact**: Older DeepSpeed may not support all Infinity features.\n\n**Mitigation**:\n- Require `deepspeed>=0.8.0` in `pyproject.toml`\n- Check DeepSpeed version at runtime\n- Provide upgrade instructions\n- Test with multiple DeepSpeed versions\n\n### Risk 5: User Confusion\n\n**Impact**: Users may enable Infinity unnecessarily for small models.\n\n**Mitigation**:\n- Clear GUI guidance (\"For 10B+ models only\")\n- Recommend ZeRO-2 as default\n- Auto-suggest Infinity in optimizer tool\n- Show model size vs memory comparison\n\n---\n\n## Future Enhancements\n\n### Phase 6: Advanced Optimizations (Optional)\n\n1. **Adaptive Prefetching**: Predict which layers needed next, prefetch ahead\n2. **Compression**: Compress parameters on NVMe (trade space for speed)\n3. **Multi-tier Caching**: Smart caching of frequently used params\n4. **Bandwidth Monitoring**: Real-time I/O performance tracking\n5. **Auto-tuning**: Automatically adjust AIO settings based on hardware\n\n### Phase 7: Multi-Node Support (Optional)\n\n1. **Distributed NVMe**: Coordinate offloading across multiple nodes\n2. **Network-attached Storage**: Support NFS/SMB for shared offload\n3. **Heterogeneous Hardware**: Mix GPU + CPU + NVMe across nodes\n\n---\n\n## Dependencies\n\n### Required Python Packages\n\n```toml\n# pyproject.toml\n[project.optional-dependencies]\nhf = [\n    \"deepspeed>=0.8.0\",  # Required for ZeRO-Infinity\n    \"psutil>=5.8.0\",     # For disk space checking\n]\n```\n\n### System Requirements\n\n**Linux**:\n- NVMe SSD with PCIe 3.0 or later (2+ GB/s)\n- `libaio` installed (`sudo apt-get install libaio-dev`)\n- Kernel 4.0+ (for modern AIO support)\n\n**Windows** (limited support):\n- NVMe SSD with >1 GB/s write speed\n- May require Windows 10/11 with latest updates\n- Performance may be lower than Linux\n\n### Hardware Requirements\n\n**Minimum**:\n- 1x NVIDIA GPU (11GB+ VRAM)\n- 32GB RAM\n- 256GB+ NVMe SSD (PCIe 3.0)\n\n**Recommended**:\n- 2x NVIDIA GPUs (11GB+ VRAM each)\n- 64GB RAM\n- 1TB+ NVMe SSD (PCIe 4.0, >5 GB/s)\n\n**Optimal**:\n- 4x NVIDIA GPUs (24GB+ VRAM each)\n- 128GB+ RAM\n- 2TB+ NVMe SSD (PCIe 4.0/5.0, >7 GB/s)\n\n---\n\n## Performance Expectations\n\n### Small Model (1B params)\n- **Baseline (no ZeRO)**: 100% speed, 8GB VRAM\n- **ZeRO-2**: 95% speed, 4GB VRAM\n- **ZeRO-Infinity**: 85% speed, 1GB VRAM, 20GB NVMe\n\n### Medium Model (10B params)\n- **Baseline**: OOM on 11GB GPU\n- **ZeRO-2**: OOM on 11GB GPU  \n- **ZeRO-3**: 85% speed, 8GB VRAM (multi-GPU)\n- **ZeRO-Infinity**: 75% speed, 2GB VRAM, 200GB NVMe\n\n### Large Model (100B params)\n- **Baseline**: OOM\n- **ZeRO-2**: OOM\n- **ZeRO-3**: OOM on <8 GPUs\n- **ZeRO-Infinity**: 65% speed, 4GB VRAM per GPU, 2TB NVMe (8x GPUs)\n\n*Percentages relative to baseline training speed.*\n\n---\n\n## Timeline\n\n- **Week 1**: Phase 1 (Configuration) - Complete\n- **Week 2-3**: Phase 2 (Core Implementation) - Complete\n- **Week 3**: Phase 3 (Memory Estimation) - Complete  \n- **Week 4**: Phase 4 (Testing) - Complete\n- **Week 4-5**: Phase 5 (Documentation) - Complete\n- **Total**: ~5 weeks for full integration\n\n---\n\n## Open Questions\n\n1. **Should we support gradient offloading?**\n   - Rarely needed even for 100B models\n   - Adds complexity for minimal benefit\n   - **Recommendation**: Add flag but default to False, document as experimental\n\n2. **How to handle multi-node Infinity?**\n   - Requires shared storage or per-node offload coordination\n   - Complex to implement and test\n   - **Recommendation**: Defer to future enhancement (Phase 7)\n\n3. **Should GUI expose AIO settings?**\n   - Most users won't understand or need to change\n   - Could clutter interface\n   - **Recommendation**: Advanced collapsible section, use sane defaults\n\n4. **Windows support priority?**\n   - Limited testing hardware\n   - May have AIO compatibility issues\n   - **Recommendation**: Best-effort support, Linux-first, document limitations\n\n---\n\n## References\n\n- [DeepSpeed ZeRO-Infinity Paper](https://arxiv.org/abs/2104.07857)\n- [DeepSpeed Documentation](https://www.deepspeed.ai/tutorials/zero-infinity/)\n- [Microsoft Blog: ZeRO-Infinity](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/)\n- [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed)\n\n---\n\n## Conclusion\n\nZeRO-Infinity integration will enable AI-OS to train models 10-100x larger than currently possible, unlocking multi-billion parameter models on consumer hardware. The implementation is well-defined with clear phases, comprehensive testing, and proper fallbacks for edge cases.\n\n**Key Benefits**:\n- Train 10B+ models on 2x 11GB GPUs\n- Minimal slowdown (10-20%) with fast NVMe\n- Graceful degradation to CPU offload\n- Full GUI and CLI support\n- Comprehensive validation and error handling\n\n**Recommendation**: Proceed with implementation. Start with Phase 1 configuration, validate with small models, then scale up to large models. Prioritize Linux support, provide Windows compatibility as best-effort.\n", "tags": ["training"], "headings": [{"line": 0, "text": "DeepSpeed ZeRO-Infinity Integration Plan"}, {"line": 2, "text": "Executive Summary"}, {"line": 18, "text": "Background"}, {"line": 20, "text": "What is ZeRO-Infinity?"}, {"line": 30, "text": "Current AI-OS Capabilities"}, {"line": 53, "text": "Technical Architecture"}, {"line": 55, "text": "Memory Hierarchy with ZeRO-Infinity"}, {"line": 86, "text": "Data Flow During Training"}, {"line": 107, "text": "Implementation Plan"}, {"line": 109, "text": "Phase 1: Configuration and Setup (Week 1)"}, {"line": 111, "text": "1.1 DeepSpeed Configuration Files"}, {"line": 233, "text": "1.2 Training Configuration Fields"}, {"line": 240, "text": "ZeRO-Infinity NVMe Offloading"}, {"line": 352, "text": "1.3 CLI Arguments"}, {"line": 359, "text": "ZeRO-Infinity (NVMe Offloading) options"}, {"line": 409, "text": "Phase 2: Core Implementation (Week 2-3)"}, {"line": 411, "text": "2.1 NVMe Configuration Builder"}, {"line": 439, "text": "Check if path exists or can be created"}, {"line": 447, "text": "Check write permissions"}, {"line": 455, "text": "Check available space"}, {"line": 490, "text": "Parameters: 4 bytes each"}, {"line": 494, "text": "Optimizer states: 8 bytes per param (Adam: momentum + variance)"}, {"line": 498, "text": "Gradients: 4 bytes per param"}, {"line": 501, "text": "Apply safety factor for temp files, fragmentation, etc."}, {"line": 529, "text": "Get device info based on OS"}, {"line": 532, "text": "Try to identify device from path"}, {"line": 539, "text": "Check if rotational (HDD=1, SSD=0)"}, {"line": 557, "text": "On Windows, harder to detect programmatically"}, {"line": 558, "text": "Just warn if path is C: (usually OS drive, may be SATA SSD)"}, {"line": 564, "text": "TODO: Could add actual speed test with dd/fio, but may be too slow for startup"}, {"line": 599, "text": "Ensure we're using ZeRO stage 3"}, {"line": 602, "text": "Configure parameter offloading"}, {"line": 618, "text": "Configure optimizer offloading  "}, {"line": 633, "text": "Configure gradient offloading (rare)"}, {"line": 635, "text": "Note: Gradient offload to NVMe not officially in DeepSpeed API"}, {"line": 636, "text": "This would require custom implementation"}, {"line": 642, "text": "Configure AIO (Async I/O)"}, {"line": 663, "text": "2.2 Update Distributed Setup"}, {"line": 695, "text": "Determine config file"}, {"line": 705, "text": "Calculate model size"}, {"line": 708, "text": "Estimate NVMe space needed"}, {"line": 717, "text": "Validate NVMe path"}, {"line": 729, "text": "Fallback to CPU offload"}, {"line": 732, "text": "Check NVMe performance"}, {"line": 738, "text": "Load base ZeRO-3 config"}, {"line": 743, "text": "Build Infinity config"}, {"line": 756, "text": "Save to temp file"}, {"line": 779, "text": "Existing ZeRO stage logic"}, {"line": 789, "text": "Rest of initialization..."}, {"line": 790, "text": "(existing code continues)"}, {"line": 793, "text": "Handle missing DeepSpeed"}, {"line": 794, "text": "(existing error handling)"}, {"line": 798, "text": "2.3 GUI Integration"}, {"line": 805, "text": "Row 5: DeepSpeed ZeRO (updated with Infinity)"}, {"line": 821, "text": "Dynamic label for memory savings"}, {"line": 840, "text": "Updated tooltip"}, {"line": 855, "text": "Row 6: NVMe Configuration (shown when infinity selected)"}, {"line": 857, "text": "Hidden by default, shown when zero_stage == \"infinity\""}, {"line": 866, "text": "Checkboxes for what to offload"}, {"line": 885, "text": "AIO settings (advanced, maybe collapsible)"}, {"line": 930, "text": "ZeRO-Infinity NVMe Offloading"}, {"line": 942, "text": "Phase 3: Memory Estimation Updates (Week 3)"}, {"line": 944, "text": "3.1 Update VRAM Estimator"}, {"line": 954, "text": "... existing code ..."}, {"line": 956, "text": "Handle ZeRO-Infinity (most aggressive savings)"}, {"line": 958, "text": "With Infinity, almost everything can be offloaded"}, {"line": 959, "text": "Only keep active computation in VRAM"}, {"line": 961, "text": "Model parameters: Only active layers in VRAM (~5-10% of model)"}, {"line": 964, "text": "Optimizer: Offloaded to NVMe or CPU"}, {"line": 967, "text": "Gradients: Mostly on NVMe/CPU, small buffer in VRAM"}, {"line": 970, "text": "Activations: Still need these in VRAM (not offloadable during computation)"}, {"line": 971, "text": "Keep activations calculation same"}, {"line": 980, "text": "... existing ZeRO-3 logic ..."}, {"line": 982, "text": "... rest of function ..."}, {"line": 987, "text": "Phase 4: Testing and Validation (Week 4)"}, {"line": 989, "text": "4.1 Unit Tests"}, {"line": 1011, "text": "Valid path with enough space"}, {"line": 1016, "text": "Invalid path (doesn't exist, can't create)"}, {"line": 1024, "text": "1B params, offload everything"}, {"line": 1033, "text": "Expected: (4 + 8 + 4) * 1B / 1e9 * 1.5 = 24 GB"}, {"line": 1036, "text": "Only optimizer offload"}, {"line": 1045, "text": "Expected: 8 * 1B / 1e9 = 8 GB"}, {"line": 1071, "text": "Check stage upgraded to 3"}, {"line": 1074, "text": "Check param offload to NVMe"}, {"line": 1078, "text": "Check optimizer offload to NVMe"}, {"line": 1081, "text": "Check AIO settings"}, {"line": 1086, "text": "4.2 Integration Tests"}, {"line": 1121, "text": "Should complete without errors"}, {"line": 1125, "text": "Verify NVMe directory was created and used"}, {"line": 1133, "text": "Use invalid NVMe path"}, {"line": 1144, "text": "Should fallback to CPU offload gracefully"}, {"line": 1146, "text": "Should complete without crashing"}, {"line": 1150, "text": "4.3 Manual Testing Plan"}, {"line": 1155, "text": "Create NVMe offload directory"}, {"line": 1158, "text": "Test with Infinity"}, {"line": 1171, "text": "Expected: Training completes, NVMe directory contains offload files"}, {"line": 1177, "text": "This would OOM without Infinity"}, {"line": 1192, "text": "Expected: Training works, uses ~30-50GB NVMe space"}, {"line": 1198, "text": "Baseline: ZeRO-2"}, {"line": 1201, "text": "With Infinity"}, {"line": 1204, "text": "Expected: Infinity ~10-20% slower with good NVMe"}, {"line": 1209, "text": "Phase 5: Documentation and Polish (Week 4-5)"}, {"line": 1211, "text": "5.1 User Documentation"}, {"line": 1223, "text": "5.2 API Documentation"}, {"line": 1230, "text": "5.3 GUI Tooltips"}, {"line": 1240, "text": "Implementation Checklist"}, {"line": 1242, "text": "Phase 1: Configuration \u2705"}, {"line": 1249, "text": "Phase 2: Core Implementation \u2705"}, {"line": 1269, "text": "Phase 3: Memory Estimation \u2705"}, {"line": 1280, "text": "Phase 4: Testing \u2705"}, {"line": 1295, "text": "Phase 5: Documentation \u2705"}, {"line": 1304, "text": "Success Metrics"}, {"line": 1306, "text": "Functional Requirements \u2705"}, {"line": 1313, "text": "Performance Requirements \u2705"}, {"line": 1319, "text": "Quality Requirements \u2705"}, {"line": 1328, "text": "Risks and Mitigations"}, {"line": 1330, "text": "Risk 1: NVMe Performance Varies Widely"}, {"line": 1340, "text": "Risk 2: Windows NVMe Support"}, {"line": 1350, "text": "Risk 3: Insufficient NVMe Space"}, {"line": 1360, "text": "Risk 4: DeepSpeed Version Compatibility"}, {"line": 1370, "text": "Risk 5: User Confusion"}, {"line": 1382, "text": "Future Enhancements"}, {"line": 1384, "text": "Phase 6: Advanced Optimizations (Optional)"}, {"line": 1392, "text": "Phase 7: Multi-Node Support (Optional)"}, {"line": 1400, "text": "Dependencies"}, {"line": 1402, "text": "Required Python Packages"}, {"line": 1405, "text": "pyproject.toml"}, {"line": 1413, "text": "System Requirements"}, {"line": 1425, "text": "Hardware Requirements"}, {"line": 1444, "text": "Performance Expectations"}, {"line": 1446, "text": "Small Model (1B params)"}, {"line": 1451, "text": "Medium Model (10B params)"}, {"line": 1457, "text": "Large Model (100B params)"}, {"line": 1467, "text": "Timeline"}, {"line": 1478, "text": "Open Questions"}, {"line": 1502, "text": "References"}, {"line": 1511, "text": "Conclusion"}]}, {"path": "planned_features/DRAGON.md", "content": "# DRAGON: Distributed Routing and GPU Open Network\n> Note: Any references to `docs/features/*` are placeholders. Use `docs/INDEX.md` and `docs/guide/` for current documentation.\n\n**Complete Implementation Guide**\n\n**Status**: \ud83d\udfe1 Planning Phase  \n**Date**: October 18, 2025  \n**Priority**: High  \n**Complexity**: Very High  \n\n---\n\n## Table of Contents\n\n1. [Executive Summary](#executive-summary)\n2. [Core Concept](#core-concept)\n3. [System Architecture](#system-architecture)\n4. [Technical Components](#technical-components)\n5. [User Experience](#user-experience)\n6. [Backend Infrastructure](#backend-infrastructure)\n7. [Security & Privacy](#security-privacy)\n8. [Implementation Phases](#implementation-phases)\n9. [Implementation Checklist](#implementation-checklist)\n10. [Technical Quick Reference](#technical-quick-reference)\n11. [Testing & Deployment](#testing-deployment)\n12. [Success Metrics](#success-metrics)\n\n---\n\n## Executive Summary\n\n### What is DRAGON?\n\nDRAGON is a crowd-sourced distributed training system that enables AI-OS users to donate their GPU/CPU time toward training a massive collaborative HRM-sMoE model. Users can participate with a simple one-click interface that automatically downloads training data batches, trains on locally allocated resources, and uploads results to a central aggregation server.\n\n**Vision**: Transform AI-OS into a decentralized AI training network where thousands of volunteers collectively train state-of-the-art models.\n\n### Key Value Propositions\n\n1. **Democratic AI Training**: Anyone with a GPU can contribute to cutting-edge AI research\n2. **Cost Efficiency**: Distribute training costs across community (200x cheaper than cloud)\n3. **Accessibility**: One-click interface, no technical expertise required\n4. **Community Building**: Gamification, leaderboards, and shared success\n5. **Innovation**: Enable training of models too large for individual users\n\n---\n\n## Core Concept\n\n### Federated Learning Flow\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    DRAGON Aggregation Server                 \u2502\n\u2502                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Model Registry \u2502  \u2502 Gradient Queue  \u2502  \u2502 Aggregator   \u2502 \u2502\n\u2502  \u2502 (current model)\u2502  \u2502 (worker updates)\u2502  \u2502 (FedAvg/Adam)\u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502            \u2502                  \u25b2                    \u2502         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502                  \u2502                    \u2502\n             \u25bc                  \u2502                    \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502   Worker A       \u2502  \u2502   Worker B       \u2502  \u2502   Worker C       \u2502\n   \u2502  (RTX 4090)      \u2502  \u2502  (RTX 3080)      \u2502  \u2502  (GTX 1080 Ti)   \u2502\n   \u2502                  \u2502  \u2502                  \u2502  \u2502                  \u2502\n   \u2502  1. Download     \u2502  \u2502  1. Download     \u2502  \u2502  1. Download     \u2502\n   \u2502     model        \u2502  \u2502     model        \u2502  \u2502     model        \u2502\n   \u2502  2. Fetch batch  \u2502  \u2502  2. Fetch batch  \u2502  \u2502  2. Fetch batch  \u2502\n   \u2502  3. Train local  \u2502  \u2502  3. Train local  \u2502  \u2502  3. Train local  \u2502\n   \u2502  4. Upload grads \u2502  \u2502  4. Upload grads \u2502  \u2502  4. Upload grads \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### How It Works\n\n1. **Distributing Work**: Breaking training into small, manageable batches\n2. **Volunteer Computing**: Users donate idle GPU/CPU time\n3. **Gradient Aggregation**: Central server combines updates from all workers\n4. **Simple UX**: One-button start/stop interface\n5. **Resource Aware**: Respects user-defined GPU/CPU limits from Resources tab\n\n---\n\n## System Architecture\n\n### Architecture: Centralized Aggregation Server (Recommended)\n\n**Pros**:\n- Simple to implement and debug\n- Full control over aggregation algorithm\n- Easy to monitor training progress\n- Standard federated learning approach\n\n**Cons**:\n- Single point of failure\n- Server bandwidth requirements\n- Hosting costs\n\n**Components**:\n\n1. **DRAGON Server** (Python/FastAPI)\n   - Model registry (current global model)\n   - Batch distribution queue\n   - Gradient aggregation service\n   - Worker authentication and stats\n   - Progress tracking\n\n2. **DRAGON Client** (AI-OS GUI Tab)\n   - Download model checkpoint\n   - Request training batch\n   - Local training loop\n   - Gradient computation and upload\n   - Automatic retry logic\n\n3. **Communication Protocol** (REST API + WebSockets)\n   - REST: Model downloads, batch requests, gradient uploads\n   - WebSocket: Real-time status updates, heartbeat\n\n---\n\n## Technical Components\n\n### 1. DRAGON GUI Tab\n\n**Location**: `src/aios/gui/components/dragon_panel.py`\n\n**UI Layout**:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        DRAGON Network                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                \u2502\n\u2502  Status: \u25cfIDLE / \u25cfTRAINING / \u25cfUPLOADING                       \u2502\n\u2502                                                                \u2502\n\u2502  Global Model: HRM-sMoE-125M (v1.2.3)                         \u2502\n\u2502  Your Contribution: 1,234 batches (12.3M tokens)              \u2502\n\u2502  Network Stats: 523 active workers, 45.2B tokens processed    \u2502\n\u2502                                                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                Resource Allocation                        \u2502 \u2502\n\u2502  \u2502                                                           \u2502 \u2502\n\u2502  \u2502  Use settings from Resources tab: \u2611                      \u2502 \u2502\n\u2502  \u2502  Override GPU memory limit: [ 80% ]                      \u2502 \u2502\n\u2502  \u2502  Max batch size: [ 4 ]                                   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                Training Progress                          \u2502 \u2502\n\u2502  \u2502                                                           \u2502 \u2502\n\u2502  \u2502  Current batch: 15/100 (15%)                             \u2502 \u2502\n\u2502  \u2502  [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]                         \u2502 \u2502\n\u2502  \u2502                                                           \u2502 \u2502\n\u2502  \u2502  Tokens processed: 15,234                                \u2502 \u2502\n\u2502  \u2502  Loss: 2.456                                             \u2502 \u2502\n\u2502  \u2502  Speed: 1,245 tok/s                                      \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                \u2502\n\u2502       [  START TRAINING  ]        [  STOP  ]                  \u2502\n\u2502                                                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                Activity Log                               \u2502 \u2502\n\u2502  \u2502                                                           \u2502 \u2502\n\u2502  \u2502  [12:34:56] Connected to DRAGON server                   \u2502 \u2502\n\u2502  \u2502  [12:35:01] Downloaded model v1.2.3 (125M params)        \u2502 \u2502\n\u2502  \u2502  [12:35:05] Fetched batch #1234 (1000 samples)           \u2502 \u2502\n\u2502  \u2502  [12:35:45] Training complete (loss: 2.456)              \u2502 \u2502\n\u2502  \u2502  [12:35:48] Uploaded gradients (2.3 MB)                  \u2502 \u2502\n\u2502  \u2502  [12:35:50] Contribution recorded \u2713                      \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### 2. DRAGON Client Module\n\n**Location**: `src/aios/dragon/client.py`\n\n**Key Methods**:\n\n```python\nclass DRAGONClient:\n    def __init__(self, server_url: str, api_key: str):\n        \"\"\"Initialize connection to DRAGON server.\"\"\"\n        \n    async def connect(self) -> bool:\n        \"\"\"Authenticate with server, verify connection.\"\"\"\n        \n    async def download_model(self, version: str) -> Path:\n        \"\"\"Download global model checkpoint to cache.\"\"\"\n        \n    async def fetch_batch(self) -> Dict:\n        \"\"\"Request next training batch from server.\"\"\"\n        \n    async def train_batch(self, batch: Dict) -> Dict:\n        \"\"\"Train on batch, compute gradients.\"\"\"\n        \n    async def upload_gradients(self, gradients: Dict, metadata: Dict):\n        \"\"\"Upload computed gradients to server.\"\"\"\n        \n    async def start_training_loop(self):\n        \"\"\"Main training loop: fetch \u2192 train \u2192 upload.\"\"\"\n```\n\n### 3. DRAGON Server (Backend)\n\n**Technology Stack**:\n- **FastAPI** - REST API framework\n- **PostgreSQL** - Database for workers, batches, model versions\n- **Redis** - Queue for batch distribution, gradient aggregation\n- **S3/MinIO** - Object storage for model checkpoints\n- **Docker** - Containerized deployment\n\n**API Endpoints**:\n\n```\nPOST   /api/v1/auth/register          # Register new worker\nPOST   /api/v1/auth/login             # Authenticate worker\nGET    /api/v1/model/current          # Get current global model version\nGET    /api/v1/model/download/{ver}   # Download model checkpoint\nPOST   /api/v1/batch/request          # Request next training batch\nPOST   /api/v1/gradients/upload       # Upload computed gradients\nGET    /api/v1/stats/global           # Get network statistics\nGET    /api/v1/worker/stats           # Get worker's contribution stats\nWS     /api/v1/ws/status              # WebSocket for real-time updates\n```\n\n**Database Schema**:\n\n```sql\n-- Workers table\nCREATE TABLE workers (\n    id UUID PRIMARY KEY,\n    api_key VARCHAR(64) UNIQUE NOT NULL,\n    username VARCHAR(64),\n    hardware_info JSONB,\n    total_batches INT DEFAULT 0,\n    total_tokens BIGINT DEFAULT 0,\n    reputation_score FLOAT DEFAULT 1.0,\n    created_at TIMESTAMP DEFAULT NOW(),\n    last_seen TIMESTAMP\n);\n\n-- Model versions table\nCREATE TABLE model_versions (\n    version VARCHAR(16) PRIMARY KEY,\n    checkpoint_url TEXT NOT NULL,\n    num_parameters BIGINT,\n    created_at TIMESTAMP DEFAULT NOW(),\n    metrics JSONB\n);\n\n-- Training batches table\nCREATE TABLE training_batches (\n    batch_id UUID PRIMARY KEY,\n    dataset_name VARCHAR(128),\n    data_url TEXT NOT NULL,\n    num_samples INT,\n    status VARCHAR(32),  -- 'pending', 'assigned', 'completed', 'failed'\n    assigned_to UUID REFERENCES workers(id),\n    assigned_at TIMESTAMP,\n    completed_at TIMESTAMP,\n    result_hash VARCHAR(64)\n);\n\n-- Gradient updates table\nCREATE TABLE gradient_updates (\n    update_id UUID PRIMARY KEY,\n    worker_id UUID REFERENCES workers(id),\n    batch_id UUID REFERENCES training_batches(batch_id),\n    model_version VARCHAR(16),\n    gradient_url TEXT NOT NULL,\n    loss FLOAT,\n    num_samples INT,\n    uploaded_at TIMESTAMP DEFAULT NOW(),\n    aggregated BOOLEAN DEFAULT FALSE\n);\n```\n\n### 4. Aggregation Algorithm\n\n**FedAvg (Federated Averaging)**:\n\n```python\ndef aggregate_gradients(gradient_updates: List[GradientUpdate]) -> ModelUpdate:\n    \"\"\"\n    Aggregate gradients from multiple workers using weighted averaging.\n    \n    Args:\n        gradient_updates: List of gradient updates from workers\n        \n    Returns:\n        Aggregated model update\n    \"\"\"\n    # Calculate total samples across all workers\n    total_samples = sum(update.num_samples for update in gradient_updates)\n    \n    # Initialize aggregated gradients dictionary\n    aggregated_gradients = {}\n    \n    # For each parameter in the model\n    for param_name in gradient_updates[0].gradients.keys():\n        weighted_sum = 0\n        \n        # Weighted average based on number of samples\n        for update in gradient_updates:\n            weight = update.num_samples / total_samples\n            weighted_sum += weight * update.gradients[param_name]\n        \n        aggregated_gradients[param_name] = weighted_sum\n    \n    return ModelUpdate(gradients=aggregated_gradients)\n```\n\n---\n\n## User Experience\n\n### User Journey: First-Time Contributor\n\n1. **Discovery**:\n   - User opens AI-OS GUI\n   - Sees new \"DRAGON\" tab with \"Contribute to AI Training\"\n   - Clicks to explore\n\n2. **Registration** (One-time):\n   - Click \"Join DRAGON Network\"\n   - Enter optional username\n   - System generates API key automatically\n   - Connection test \u2192 Success \u2713\n\n3. **Training Session**:\n   - Click \"START TRAINING\"\n   - Progress bar shows batch download\n   - Training begins (see real-time loss, speed)\n   - Activity log shows each step\n   - User can minimize or use other tabs\n\n4. **Stopping**:\n   - User clicks \"STOP\"\n   - Current batch finishes\n   - Gradients uploaded\n   - Model unloaded from memory\n\n5. **Viewing Contribution**:\n   - \"Your Contribution\" section shows stats\n   - Leaderboard shows rank\n   - Badges for milestones (1K, 10K, 100K batches)\n\n### Resource Management Integration\n\n**Automatic Mode** (default):\n- Reads GPU/CPU settings from Resources tab\n- Respects memory limits\n- Uses configured GPUs only\n\n**Override Mode**:\n- User can set DRAGON-specific limits\n- Example: \"Use 80% GPU for regular tasks, 60% for DRAGON\"\n\n---\n\n## Backend Infrastructure\n\n### Deployment Architecture\n\n**Recommended Setup**:\n\n1. **Application Server** (FastAPI + Gunicorn)\n   - 4 CPU cores, 8GB RAM\n   - Handles API requests\n   - Load balancer for high availability\n\n2. **Database** (PostgreSQL)\n   - 2 CPU cores, 4GB RAM\n   - Persistent storage for metadata\n   - Read replicas for analytics\n\n3. **Message Queue** (Redis)\n   - 2 CPU cores, 4GB RAM\n   - Persistent storage for work queue\n\n4. **Object Storage** (S3/MinIO)\n   - Store model checkpoints\n   - Store gradient files\n   - CDN for fast downloads\n\n5. **Monitoring**:\n   - Prometheus + Grafana for metrics\n   - Alerting for failures\n   - CloudWatch/similar for logs\n\n**Estimated Costs** (AWS):\n- Development: ~$50/month\n- Production (100 workers): ~$200/month\n- Production (1000 workers): ~$500/month\n\n### Data Distribution Strategy\n\n**Hybrid Approach** (Recommended):\n- Pre-chunk common sizes (256, 512, 1024 samples)\n- Generate custom sizes on-demand\n- Cache in Redis for fast distribution\n- Store in S3 for durability\n\n---\n\n## Security & Privacy\n\n### Critical Security Measures\n\n1. **No Code Execution on Workers**\n   - Workers only receive model weights (safetensors format)\n   - No pickle, eval(), or dynamic code\n   - No eval() or pickle usage\n\n2. **Gradient Validation**\n   - Server checks gradient shapes match model architecture\n   - Outlier detection (reject if > 3\u03c3 from mean)\n   - Rate limiting to prevent poisoning attacks\n\n3. **Authentication**\n   - API keys generated client-side\n   - Rate limiting per worker\n   - Optional OAuth for verified contributors\n\n4. **Byzantine Fault Tolerance**\n   - Reputation scoring (workers build trust over time)\n   - Krum aggregation (reject outliers)\n   - Random batch verification (server trains same batch to validate)\n\n5. **Data Privacy**\n   - Training data is public domain (no PII)\n   - No user data uploaded\n   - Differential privacy (future enhancement)\n\n### Attack Vectors & Mitigations\n\n| Attack | Mitigation |\n|--------|------------|\n| **Gradient Poisoning** | Outlier detection, Krum aggregation, reputation scores |\n| **Model Extraction** | Rate limiting, partial model access (MoE experts only) |\n| **Sybil Attack** | CAPTCHA on registration, proof-of-work, rate limits |\n| **DDoS** | Cloudflare, rate limiting, worker throttling |\n| **Data Poisoning** | Curated public datasets only, no user-uploaded data |\n\n---\n\n## Implementation Phases\n\n### Phase 1: MVP (1-2 months)\n\n**Goal**: Proof of concept with basic functionality\n\n**Features**:\n- Simple DRAGON tab in GUI\n- FastAPI server with basic endpoints\n- Single global model (no versioning)\n- FedAvg aggregation\n- Manual start/stop only\n- Fixed batch size (512 samples)\n- 10-50 test workers\n\n**Deliverables**:\n- `src/aios/dragon/client.py`\n- `src/aios/gui/components/dragon_panel.py`\n- `backend/dragon_server/` (FastAPI app)\n- Docker compose for local testing\n- Documentation: setup guide, API reference\n\n### Phase 2: Production Ready (2-3 months)\n\n**Features**:\n- Model versioning and rollback\n- Dynamic batch sizing\n- Automatic retry logic\n- Worker reputation system\n- Real-time monitoring dashboard\n- Progress persistence (resume after restart)\n- 100-500 workers\n\n**Enhancements**:\n- WebSocket for live updates\n- Gradient compression (reduce upload size)\n- Checkpoint caching (avoid re-downloads)\n- Advanced aggregation (FedAdam)\n\n### Phase 3: Scale & Optimize (3-6 months)\n\n**Features**:\n- Auto-scaling infrastructure\n- 1000+ concurrent workers\n- Expert-level distribution (different workers train different MoE experts)\n- Multi-model support (users choose which model to contribute to)\n- Contribution leaderboard and gamification\n- Mobile app for cross-platform training\n\n---\n\n## Implementation Checklist\n\n### Pre-Implementation Setup\n\n**Documentation**:\n- [x] Complete architectural plan\n- [x] Technical quick reference\n- [ ] API specification (OpenAPI/Swagger)\n- [ ] Database schema SQL scripts\n- [ ] Frontend mockups (wireframes)\n\n**Research**:\n- [ ] Review FedAvg paper (McMahan et al., 2017)\n- [ ] Review FedAdam paper (Reddi et al., 2020)\n- [ ] Benchmark gradient compression methods\n- [ ] Survey existing platforms (Flower, PySyft)\n\n**Environment Setup**:\n- [ ] Set up development server (local Docker)\n- [ ] Configure PostgreSQL database\n- [ ] Configure Redis message queue\n- [ ] Set up S3/MinIO for object storage\n- [ ] Configure monitoring (Prometheus + Grafana)\n\n### Backend Development\n\n**Core Infrastructure**:\n- [ ] Initialize FastAPI project structure\n- [ ] Create database migration system (Alembic)\n- [ ] Implement database tables\n- [ ] Add indexes for performance\n- [ ] Seed initial data\n\n**API Endpoints**:\n- [ ] `POST /api/v1/auth/register` - Worker registration\n- [ ] `POST /api/v1/auth/login` - Authentication\n- [ ] `GET /api/v1/model/current` - Get latest model\n- [ ] `GET /api/v1/model/download/{ver}` - Download checkpoint\n- [ ] `POST /api/v1/batch/request` - Request training batch\n- [ ] `POST /api/v1/gradients/upload` - Upload gradients\n- [ ] `GET /api/v1/stats/global` - Network statistics\n- [ ] `GET /api/v1/worker/stats` - Worker stats\n\n**Aggregation System**:\n- [ ] Implement FedAvg algorithm\n- [ ] Aggregation scheduler (every 60s or 10 updates)\n- [ ] Model version management\n- [ ] Gradient queue management\n\n**Testing**:\n- [ ] Unit tests for API endpoints\n- [ ] Integration tests for aggregation\n- [ ] Load tests (100 concurrent requests)\n\n### Client Development\n\n**Core Module**:\n- [ ] Create `src/aios/dragon/` package\n- [ ] Implement `DRAGONClient` class\n- [ ] `connect()` - authenticate with server\n- [ ] `download_model()` - fetch checkpoint\n- [ ] `fetch_batch()` - request training data\n- [ ] `train_batch()` - local training loop\n- [ ] `upload_gradients()` - send results\n- [ ] `start_training_loop()` - main loop\n\n**Training Integration**:\n- [ ] Adapt `train_epoch.py` for single-batch training\n- [ ] Gradient extraction and serialization\n- [ ] Checkpoint management\n\n**Resource Integration**:\n- [ ] Read GPU limits from Resources panel\n- [ ] Auto-detect batch size\n- [ ] Error handling & retry logic\n\n**Testing**:\n- [ ] Unit tests for `DRAGONClient`\n- [ ] Mock server for integration tests\n- [ ] End-to-end test\n\n### GUI Development\n\n**DRAGON Panel**:\n- [ ] Create `src/aios/gui/components/dragon_panel.py`\n- [ ] Implement `DRAGONPanel(ttk.Frame)`\n- [ ] Status indicator widget\n- [ ] Progress section\n- [ ] Stats section\n- [ ] Controls section\n\n**Integration**:\n- [ ] Add DRAGON tab to main app\n- [ ] Initialize DRAGON panel\n- [ ] Event handlers (start, stop, update)\n\n**Testing**:\n- [ ] Manual testing (visual inspection)\n- [ ] Start/stop functionality\n- [ ] Progress updates accuracy\n\n### Integration Testing\n\n- [ ] Test with 1 worker (local)\n- [ ] Test with 2 workers (different batches)\n- [ ] Test with 5 workers (concurrent)\n- [ ] Worker disconnect during training\n- [ ] Server restart during aggregation\n- [ ] Network interruption handling\n\n### Deployment Preparation\n\n- [ ] Create Docker configuration\n- [ ] Set up reverse proxy (nginx)\n- [ ] Configure SSL certificates\n- [ ] Set up database backups\n- [ ] Configure logging\n- [ ] Set up error tracking\n\n---\n\n## Technical Quick Reference\n\n### Core Files\n\n| Component | File | Purpose |\n|-----------|------|---------|\n| GUI Tab | `src/aios/gui/components/dragon_panel.py` | User interface |\n| Client | `src/aios/dragon/client.py` | Server communication |\n| Server | `backend/dragon_server/main.py` | API endpoints |\n| Database | `backend/dragon_server/models.py` | Schema definitions |\n| Aggregation | `backend/dragon_server/aggregator.py` | FedAvg implementation |\n\n### Data Flow\n\n```\n1. Worker clicks \"START TRAINING\"\n2. Client connects to server (auth)\n3. Download current global model checkpoint\n4. Request training batch from work queue\n5. Train locally (compute gradients)\n6. Upload gradients to server\n7. Server aggregates using FedAvg\n8. Repeat steps 4-7 until user clicks \"STOP\"\n```\n\n### API Request/Response Examples\n\n**Request Batch**:\n```json\nPOST /api/v1/batch/request\n{\n  \"worker_id\": \"uuid\",\n  \"hardware\": {\n    \"gpu\": \"RTX 4090\",\n    \"vram_gb\": 24,\n    \"batch_size\": 8\n  }\n}\n\nResponse:\n{\n  \"batch_id\": \"uuid\",\n  \"model_version\": \"v1.2.3\",\n  \"data_url\": \"https://s3.../batch.jsonl.gz\",\n  \"num_samples\": 1000\n}\n```\n\n**Upload Gradients**:\n```json\nPOST /api/v1/gradients/upload\n{\n  \"worker_id\": \"uuid\",\n  \"batch_id\": \"uuid\",\n  \"model_version\": \"v1.2.3\",\n  \"gradients_url\": \"https://s3.../grads.safetensors\",\n  \"loss\": 2.456,\n  \"num_samples\": 1000\n}\n```\n\n### Configuration\n\n**Client Config** (`config/dragon.yaml`):\n```yaml\nserver:\n  url: \"https://dragon.aios.ai\"\n  api_key: \"generated_on_first_run\"\n\ntraining:\n  batch_size: auto\n  max_batches_per_session: 100\n  retry_attempts: 3\n  timeout_seconds: 300\n\nuploads:\n  compress_gradients: true\n  compression_ratio: 0.1\n```\n\n**Server Config** (`.env`):\n```env\nDATABASE_URL=postgresql://user:pass@localhost/dragon\nREDIS_URL=redis://localhost:6379\nS3_BUCKET=dragon-checkpoints\nAGGREGATION_INTERVAL=60\nMIN_GRADIENTS_FOR_UPDATE=10\n```\n\n---\n\n## Testing & Deployment\n\n### Testing Strategy\n\n**Unit Tests**:\n- Client methods (`test_dragon_client.py`)\n- Aggregation correctness (`test_aggregation.py`)\n- Security validation (`test_security.py`)\n\n**Integration Tests**:\n- 2-worker local test\n- Server aggregation\n- Fault tolerance\n\n**Load Tests**:\n- 100 concurrent workers\n- 1000 requests/second\n- 10 GB/hour gradient uploads\n\n### Monitoring\n\n**Metrics to Track**:\n- Active workers (gauge)\n- Batches trained per hour (counter)\n- Average loss per model version (gauge)\n- Gradient upload success rate (%)\n- Server response time (p50, p95, p99)\n\n**Alerts**:\n- Worker count drops by >50%\n- Average loss increases by >10%\n- Server error rate >1%\n- Gradient upload failures >5%\n\n### Deployment\n\n**Development** (Docker Compose):\n```bash\ncd backend/dragon_server\ndocker-compose up -d\n```\n\n**Production** (Kubernetes):\n```bash\nkubectl apply -f k8s/dragon-server.yaml\n```\n\n---\n\n## Success Metrics\n\n### Phase 1 MVP Success\n\n- [ ] 10+ workers successfully training\n- [ ] Model converging (loss decreasing)\n- [ ] Zero critical bugs\n- [ ] Average uptime >95%\n\n### Phase 2 Production Success\n\n- [ ] 100+ active workers\n- [ ] 1M+ tokens processed per day\n- [ ] Worker retention >30%\n- [ ] Infrastructure cost <$200/month\n\n### Long-term Impact Metrics\n\n- **Community Growth**: 1000+ active contributors\n- **Training Efficiency**: 10B+ tokens/month\n- **Cost Savings**: $10K+ equivalent compute donated\n- **Model Quality**: Competitive with commercial models\n- **Innovation**: 5+ research papers using DRAGON\n\n---\n\n## FAQ\n\n**Q: How long does training one batch take?**  \nA: ~2 minutes on RTX 3090 (1000 samples, batch_size=4)\n\n**Q: How much bandwidth is needed?**  \nA: ~660 MB/hour (model cached, 12 batches/hour \u00d7 55 MB)\n\n**Q: What if my internet disconnects?**  \nA: Retry logic auto-reconnects. Current batch is lost, but resume from next batch.\n\n**Q: Can I pause and resume later?**  \nA: Yes! Click STOP, your progress is saved. Click START to continue.\n\n**Q: Do I get credit for my contribution?**  \nA: Yes! Leaderboard tracks your batches and reputation score.\n\n**Q: Is my data private?**  \nA: Yes. Only gradients are uploaded, no personal data leaves your machine.\n\n**Q: What hardware do I need?**  \nA: Any NVIDIA GPU with 4GB+ VRAM. CPU-only mode supported but slower.\n\n---\n\n## Next Steps\n\n### Immediate Actions (Week 1)\n\n1. **Stakeholder Review**: Present this plan to AI-OS maintainers\n2. **Environment Setup**: Configure development server (Docker)\n3. **Team Assembly**: Assign roles (backend, client, GUI, testing)\n\n### Short-term Goals (Month 1)\n\n1. **Backend MVP**: FastAPI server with core endpoints\n2. **Client MVP**: Basic training loop working\n3. **GUI MVP**: Simple tab with start/stop buttons\n4. **Testing**: End-to-end test with 2 workers\n\n### Medium-term Goals (Months 2-3)\n\n1. **Alpha Testing**: 20-50 internal users\n2. **Production Hardening**: Error handling, monitoring, security\n3. **Documentation**: User guide, API docs, troubleshooting\n4. **Beta Launch**: Public announcement, community onboarding\n\n---\n\n## References\n\n### Papers\n- McMahan et al., \"Communication-Efficient Learning of Deep Networks from Decentralized Data\" (FedAvg)\n- Reddi et al., \"Adaptive Federated Optimization\" (FedAdam)\n- Blanchard et al., \"Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent\" (Krum)\n\n### Existing Platforms\n- **Flower**: Federated learning framework\n- **PySyft**: Privacy-preserving ML\n- **BOINC**: Volunteer computing platform\n- **Folding@home**: Distributed protein folding\n\n### AI-OS Integration Points\n- Streaming Datasets: (placeholder) `docs/DATASET_STREAM_QUEUE.md`\n- Checkpoint System: (placeholder) `docs/features/AUTOMATIC_CHECKPOINT_SAVING.md`\n- Resource Management: (placeholder) `docs/fixes/RESOURCES_TAB_SETTINGS_PERSISTENCE_FIX.md`\n- HRM-sMoE: (placeholder) `docs/HRM_MOE_SUMMARY.md`\n\nNote: The above items are planning placeholders for docs that are not yet created or published. For current docs, start at `docs/INDEX.md` and explore `docs/guide/`.\n\n---\n\n**Document Version**: 1.0  \n**Last Updated**: October 18, 2025  \n**Status**: \ud83d\udfe2 Ready for Implementation  \n\n---\n\n## Appendix: Code Examples\n\n### Example: Client Training Loop\n\n```python\nasync def training_loop(self):\n    \"\"\"Main DRAGON training loop.\"\"\"\n    while not self.stop_flag:\n        try:\n            # 1. Fetch batch\n            batch = await self.fetch_batch()\n            self.update_status(\"TRAINING\")\n            \n            # 2. Train locally\n            gradients, loss = await self.train_batch(batch)\n            \n            # 3. Upload results\n            self.update_status(\"UPLOADING\")\n            await self.upload_gradients(gradients, {\n                \"batch_id\": batch[\"batch_id\"],\n                \"loss\": loss,\n                \"num_samples\": len(batch[\"data\"])\n            })\n            \n            # 4. Update stats\n            self.total_batches += 1\n            self.update_progress()\n            \n        except Exception as e:\n            self.log_error(f\"Training failed: {e}\")\n            await asyncio.sleep(30)  # Retry after 30s\n    \n    self.update_status(\"IDLE\")\n```\n\n### Example: Server Aggregation\n\n```python\n@app.post(\"/api/v1/aggregate\")\nasync def trigger_aggregation():\n    \"\"\"Aggregate pending gradients and update global model.\"\"\"\n    \n    # Fetch pending gradients\n    updates = await db.fetch_pending_gradients(limit=100)\n    \n    if len(updates) < MIN_GRADIENTS_FOR_UPDATE:\n        return {\"status\": \"waiting\", \"pending\": len(updates)}\n    \n    # Aggregate using FedAvg\n    aggregated = aggregate_gradients(updates)\n    \n    # Update global model\n    current_model = await load_model()\n    current_model.apply_gradients(aggregated)\n    \n    # Save new version\n    new_version = increment_version()\n    checkpoint_url = await save_checkpoint(current_model, new_version)\n    \n    # Mark gradients as aggregated\n    await db.mark_aggregated(updates)\n    \n    return {\n        \"status\": \"success\",\n        \"version\": new_version,\n        \"num_updates\": len(updates)\n    }\n```\n\n---\n\n**End of Document**\n", "tags": ["cli", "experts", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "DRAGON: Distributed Routing and GPU Open Network"}, {"line": 12, "text": "Table of Contents"}, {"line": 29, "text": "Executive Summary"}, {"line": 31, "text": "What is DRAGON?"}, {"line": 37, "text": "Key Value Propositions"}, {"line": 47, "text": "Core Concept"}, {"line": 49, "text": "Federated Learning Flow"}, {"line": 75, "text": "How It Works"}, {"line": 85, "text": "System Architecture"}, {"line": 87, "text": "Architecture: Centralized Aggregation Server (Recommended)"}, {"line": 122, "text": "Technical Components"}, {"line": 124, "text": "1. DRAGON GUI Tab"}, {"line": 175, "text": "2. DRAGON Client Module"}, {"line": 205, "text": "3. DRAGON Server (Backend)"}, {"line": 280, "text": "4. Aggregation Algorithm"}, {"line": 295, "text": "Calculate total samples across all workers"}, {"line": 298, "text": "Initialize aggregated gradients dictionary"}, {"line": 301, "text": "For each parameter in the model"}, {"line": 305, "text": "Weighted average based on number of samples"}, {"line": 317, "text": "User Experience"}, {"line": 319, "text": "User Journey: First-Time Contributor"}, {"line": 350, "text": "Resource Management Integration"}, {"line": 363, "text": "Backend Infrastructure"}, {"line": 365, "text": "Deployment Architecture"}, {"line": 398, "text": "Data Distribution Strategy"}, {"line": 408, "text": "Security & Privacy"}, {"line": 410, "text": "Critical Security Measures"}, {"line": 437, "text": "Attack Vectors & Mitigations"}, {"line": 449, "text": "Implementation Phases"}, {"line": 451, "text": "Phase 1: MVP (1-2 months)"}, {"line": 471, "text": "Phase 2: Production Ready (2-3 months)"}, {"line": 488, "text": "Phase 3: Scale & Optimize (3-6 months)"}, {"line": 500, "text": "Implementation Checklist"}, {"line": 502, "text": "Pre-Implementation Setup"}, {"line": 524, "text": "Backend Development"}, {"line": 554, "text": "Client Development"}, {"line": 581, "text": "GUI Development"}, {"line": 601, "text": "Integration Testing"}, {"line": 610, "text": "Deployment Preparation"}, {"line": 621, "text": "Technical Quick Reference"}, {"line": 623, "text": "Core Files"}, {"line": 633, "text": "Data Flow"}, {"line": 646, "text": "API Request/Response Examples"}, {"line": 682, "text": "Configuration"}, {"line": 712, "text": "Testing & Deployment"}, {"line": 714, "text": "Testing Strategy"}, {"line": 731, "text": "Monitoring"}, {"line": 746, "text": "Deployment"}, {"line": 761, "text": "Success Metrics"}, {"line": 763, "text": "Phase 1 MVP Success"}, {"line": 770, "text": "Phase 2 Production Success"}, {"line": 777, "text": "Long-term Impact Metrics"}, {"line": 787, "text": "FAQ"}, {"line": 812, "text": "Next Steps"}, {"line": 814, "text": "Immediate Actions (Week 1)"}, {"line": 820, "text": "Short-term Goals (Month 1)"}, {"line": 827, "text": "Medium-term Goals (Months 2-3)"}, {"line": 836, "text": "References"}, {"line": 838, "text": "Papers"}, {"line": 843, "text": "Existing Platforms"}, {"line": 849, "text": "AI-OS Integration Points"}, {"line": 865, "text": "Appendix: Code Examples"}, {"line": 867, "text": "Example: Client Training Loop"}, {"line": 874, "text": "1. Fetch batch"}, {"line": 878, "text": "2. Train locally"}, {"line": 881, "text": "3. Upload results"}, {"line": 889, "text": "4. Update stats"}, {"line": 900, "text": "Example: Server Aggregation"}, {"line": 907, "text": "Fetch pending gradients"}, {"line": 913, "text": "Aggregate using FedAvg"}, {"line": 916, "text": "Update global model"}, {"line": 920, "text": "Save new version"}, {"line": 924, "text": "Mark gradients as aggregated"}]}, {"path": "planned_features/EVALUATION_SYSTEM_ENHANCEMENTS.md", "content": "# Evaluation System Enhancements\n\n**Status:** \ud83d\udccb Planned  \n**Priority:** Medium  \n**Category:** Model Evaluation & Benchmarking  \n**Created:** October 19, 2025  \n**Based on:** Evaluation system testing results (Oct 19-20, 2025)\n\n---\n\n## Overview\n\nEnhance the AI-OS evaluation system with extended benchmarking capabilities, advanced metrics, and comparison tools based on systematic testing of the current evaluation functionality.\n\n**Current State:** Basic corpus analysis and checkpoint evaluation working via `aios english-eval`  \n**Goal:** Comprehensive evaluation suite with industry-standard benchmarks and automated comparison\n\n---\n\n## Motivation\n\nRecent systematic testing (Oct 2025) confirmed that:\n- \u2705 Current evaluation system works correctly for corpus analysis\n- \u2705 Multiple checkpoint formats supported (.pt, .safetensors)\n- \u2705 Artifact storage and retrieval functional\n- \u26a0\ufe0f Limited to readability metrics (Flesch scores, word counts)\n- \u274c No perplexity or loss-based quality metrics\n- \u274c No industry-standard benchmark support (hellaswag, arc, etc.)\n- \u274c No automated comparison between checkpoints\n\n---\n\n## Planned Enhancements\n\n### 1. LM-Evaluation-Harness Integration\n\n**Objective:** Add industry-standard benchmark evaluation capabilities\n\n#### Tasks:\n- [ ] Install `lm-eval` dependency\n  ```bash\n  pip install lm-eval\n  ```\n- [ ] Integrate with existing `aios eval` commands\n- [ ] Enable standard benchmarks:\n  - [ ] HellaSwag (commonsense reasoning)\n  - [ ] ARC (science questions)\n  - [ ] MMLU (multitask understanding)\n  - [ ] TruthfulQA (truthfulness)\n  - [ ] GSM8K (math reasoning)\n  - [ ] HumanEval (code generation)\n- [ ] Add custom task configuration support\n- [ ] Store benchmark results in artifact system\n\n#### Implementation Notes:\n```python\n# Example integration\nfrom lm_eval import evaluator, tasks\n\ndef run_lm_eval_benchmark(model_path: str, tasks: list[str]):\n    results = evaluator.simple_evaluate(\n        model=\"hf\",\n        model_args=f\"pretrained={model_path}\",\n        tasks=tasks,\n        num_fewshot=0,\n        batch_size=8\n    )\n    return results\n```\n\n#### Benefits:\n- Compare against published baselines\n- Validate model capabilities across diverse tasks\n- Standard metrics for model comparison\n- Community-recognized benchmarks\n\n---\n\n### 2. Perplexity & Quality Metrics\n\n**Objective:** Add model-specific quality metrics to checkpoint evaluations\n\n#### Tasks:\n- [ ] Implement perplexity calculation on test datasets\n- [ ] Add cross-entropy loss metrics\n- [ ] Calculate bits-per-character/byte\n- [ ] Track token-level accuracy\n- [ ] Add BLEU/ROUGE scores for generation tasks\n- [ ] Implement diversity metrics (distinct-n)\n- [ ] Add coherence scoring\n\n#### Metrics to Add:\n```yaml\nquality_metrics:\n  - perplexity: \"Lower is better - measures prediction confidence\"\n  - cross_entropy: \"Average loss on test set\"\n  - bits_per_byte: \"Compression efficiency metric\"\n  - token_accuracy: \"Exact match rate for next token\"\n  - distinct_1/distinct_2: \"Vocabulary diversity in generations\"\n  - coherence_score: \"Semantic consistency measure\"\n```\n\n#### Implementation Approach:\n```python\ndef calculate_checkpoint_metrics(model, dataset):\n    metrics = {\n        'perplexity': calculate_perplexity(model, dataset),\n        'cross_entropy': calculate_loss(model, dataset),\n        'bits_per_byte': calculate_bpb(model, dataset),\n        'token_accuracy': calculate_accuracy(model, dataset),\n        'generation_quality': evaluate_generations(model, dataset)\n    }\n    return metrics\n```\n\n#### Integration Points:\n- Extend `aios english-eval` to include these metrics when checkpoint provided\n- Store in artifact data structure\n- Display in `aios artifacts-show` output\n\n---\n\n### 3. Automated Comparison Tools\n\n**Objective:** Enable side-by-side comparison of evaluation results\n\n#### Tasks:\n- [ ] Implement `aios eval compare` command\n- [ ] Support multi-checkpoint comparison (2+ models)\n- [ ] Generate comparison tables (markdown/HTML)\n- [ ] Add visualization support:\n  - [ ] Performance radar charts\n  - [ ] Metric progression over training\n  - [ ] Task-specific comparison graphs\n- [ ] Statistical significance testing\n- [ ] Automated regression detection\n\n#### CLI Interface:\n```bash\n# Compare two checkpoints\naios eval compare --checkpoints checkpoint1.pt checkpoint2.pt --dataset eval.txt\n\n# Compare multiple evaluations by artifact ID\naios eval compare --artifact-ids 2 3 4 5\n\n# Compare with baseline\naios eval compare --checkpoint my_model.pt --baseline gpt2\n\n# Generate report\naios eval compare --checkpoints model1.pt model2.pt --output comparison_report.html\n```\n\n#### Comparison Report Features:\n- **Metric Deltas:** Show improvement/regression percentages\n- **Statistical Tests:** P-values for significance\n- **Ranking:** Best-to-worst across metrics\n- **Recommendations:** Identify which checkpoint to use for what purpose\n- **Regression Alerts:** Flag significant performance drops\n\n#### Data Structure:\n```python\n@dataclass\nclass ComparisonResult:\n    checkpoints: list[str]\n    metrics: dict[str, list[float]]\n    deltas: dict[str, list[float]]  # Percentage changes\n    statistical_significance: dict[str, float]  # p-values\n    rankings: dict[str, list[int]]\n    recommendations: str\n    regression_alerts: list[str]\n```\n\n---\n\n## Implementation Plan\n\n### Phase 1: LM-Eval Integration (Week 1-2)\n1. Install and test lm-eval library\n2. Create wrapper functions for common benchmarks\n3. Integrate with existing CLI commands\n4. Test on ActV1 models\n5. Document usage and available tasks\n\n### Phase 2: Perplexity Metrics (Week 2-3)\n1. Implement perplexity calculation\n2. Add to english-eval output\n3. Store in artifact system\n4. Add generation quality metrics\n5. Test across different checkpoints\n\n### Phase 3: Comparison Tools (Week 3-4)\n1. Design comparison data structures\n2. Implement `aios eval compare` command\n3. Add table/visualization generation\n4. Implement statistical testing\n5. Create automated reports\n6. Add regression detection\n\n### Phase 4: Documentation & Testing (Week 4)\n1. Comprehensive user documentation\n2. Example workflows and tutorials\n3. Unit tests for all new functions\n4. Integration tests with real checkpoints\n5. Performance benchmarking\n\n---\n\n## Technical Requirements\n\n### Dependencies:\n```toml\n[dependencies]\nlm-eval = \"^0.4.0\"  # LM Evaluation Harness\nscipy = \"^1.11.0\"   # Statistical tests\nmatplotlib = \"^3.8.0\"  # Visualizations\nseaborn = \"^0.13.0\"  # Enhanced plots\njinja2 = \"^3.1.0\"   # HTML report templates\n```\n\n### Compatibility:\n- Python 3.10+\n- PyTorch 2.0+\n- Transformers 4.35+\n- Works with existing .pt and .safetensors checkpoints\n\n---\n\n## File Structure\n\n```\nsrc/aios/\n\u251c\u2500\u2500 evaluation/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 lm_eval_wrapper.py      # LM-eval integration\n\u2502   \u251c\u2500\u2500 metrics.py               # Perplexity, quality metrics\n\u2502   \u251c\u2500\u2500 comparison.py            # Comparison tools\n\u2502   \u251c\u2500\u2500 reports.py               # Report generation\n\u2502   \u2514\u2500\u2500 visualization.py         # Plotting functions\n\u251c\u2500\u2500 cli/\n\u2502   \u2514\u2500\u2500 eval_commands.py         # Extended CLI commands\n\u2514\u2500\u2500 templates/\n    \u251c\u2500\u2500 comparison_report.html   # HTML template\n    \u2514\u2500\u2500 comparison_table.md      # Markdown template\n```\n\n---\n\n## Usage Examples\n\n### Example 1: Standard Benchmark\n```bash\n# Run hellaswag benchmark\naios eval run --checkpoint artifacts/brains/actv1/final_model.pt \\\n              --tasks hellaswag \\\n              --label \"actv1-hellaswag\"\n\n# View results\naios artifacts-show-latest\n```\n\n### Example 2: Comprehensive Evaluation\n```bash\n# Run multiple benchmarks with quality metrics\naios eval run --checkpoint my_model.pt \\\n              --tasks hellaswag,arc_easy,arc_challenge \\\n              --dataset eval_dataset.txt \\\n              --include-perplexity \\\n              --include-generation-metrics \\\n              --label \"comprehensive-eval\"\n```\n\n### Example 3: Compare Checkpoints\n```bash\n# Compare training progression\naios eval compare \\\n    --checkpoints artifacts/brains/actv1/English-v1/actv1_student.safetensors \\\n                  artifacts/brains/actv1/English-v2/actv1_student.safetensors \\\n                  artifacts/brains/actv1/English-v3/actv1_student.safetensors \\\n                  artifacts/brains/actv1/English-v4/actv1_student.safetensors \\\n    --dataset training_data/eval_test_dataset.txt \\\n    --output training_progression.html \\\n    --show-deltas\n```\n\n### Example 4: Automated Testing\n```bash\n# Compare new checkpoint against baseline\naios eval compare \\\n    --checkpoint new_checkpoint.pt \\\n    --baseline artifacts/brains/actv1/final_model.pt \\\n    --dataset validation_set.txt \\\n    --fail-on-regression \\\n    --threshold 5.0  # Fail if >5% regression on any metric\n```\n\n---\n\n## Success Metrics\n\n### Quantitative:\n- [ ] 10+ standard benchmarks supported\n- [ ] 5+ quality metrics per evaluation\n- [ ] Comparison reports generated in <30 seconds\n- [ ] 100% compatibility with existing checkpoints\n- [ ] <1 minute evaluation time for standard datasets\n\n### Qualitative:\n- [ ] Users can easily compare model versions\n- [ ] Clear identification of best checkpoint for tasks\n- [ ] Automated CI/CD integration possible\n- [ ] Reports are readable and actionable\n\n---\n\n## Testing Strategy\n\n### Unit Tests:\n- Metric calculation accuracy\n- Statistical test correctness\n- Report generation validity\n\n### Integration Tests:\n- End-to-end benchmark runs\n- Multi-checkpoint comparisons\n- Artifact storage/retrieval\n\n### Validation Tests:\n- Compare against known baselines\n- Verify statistical significance calculations\n- Cross-check with manual evaluations\n\n---\n\n## Risks & Mitigations\n\n| Risk | Impact | Mitigation |\n|------|--------|------------|\n| lm-eval dependency conflicts | High | Pin compatible versions, test thoroughly |\n| Slow benchmark evaluation | Medium | Add batching, caching, parallel execution |\n| Large artifact storage | Medium | Implement result compression, selective storage |\n| API changes in lm-eval | Medium | Pin version, abstract wrapper layer |\n| Comparison complexity | Low | Start simple, iterate based on feedback |\n\n---\n\n## Future Enhancements\n\n### Post-V1:\n- [ ] Multi-GPU distributed evaluation\n- [ ] Cloud-based benchmark execution\n- [ ] Continuous evaluation dashboard\n- [ ] A/B testing framework\n- [ ] Automatic hyperparameter tuning based on eval results\n- [ ] Custom benchmark creation wizard\n- [ ] Integration with experiment tracking (MLflow, W&B)\n\n### Advanced Features:\n- [ ] Model capability mapping (what tasks is model good at?)\n- [ ] Automatic prompt optimization based on eval results\n- [ ] Cross-model ensemble recommendations\n- [ ] Failure analysis and debugging tools\n\n---\n\n## References\n\n- [EleutherAI LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)\n- [Hugging Face Evaluate Library](https://huggingface.co/docs/evaluate)\n- [OpenAI Evals Framework](https://github.com/openai/evals)\n- Current evaluation test results: `artifacts/evaluation/evaluation_test_results.md`\n\n---\n\n## Related Issues\n\n- Extends existing `aios english-eval` functionality\n- Complements training metrics in `artifacts/brains/actv1/metrics.jsonl`\n- Supports model selection for production deployment\n\n---\n\n## Changelog\n\n- **2025-10-19:** Initial plan created based on systematic evaluation testing\n- **Next:** Prioritize and schedule implementation\n\n---\n\n**Note:** This plan is based on successful validation of the current evaluation system. All proposed enhancements build on working infrastructure and verified checkpoint compatibility.\n", "tags": ["datasets", "evaluation", "training"], "headings": [{"line": 0, "text": "Evaluation System Enhancements"}, {"line": 10, "text": "Overview"}, {"line": 19, "text": "Motivation"}, {"line": 32, "text": "Planned Enhancements"}, {"line": 34, "text": "1. LM-Evaluation-Harness Integration"}, {"line": 38, "text": "Tasks:"}, {"line": 54, "text": "Implementation Notes:"}, {"line": 56, "text": "Example integration"}, {"line": 70, "text": "Benefits:"}, {"line": 78, "text": "2. Perplexity & Quality Metrics"}, {"line": 82, "text": "Tasks:"}, {"line": 91, "text": "Metrics to Add:"}, {"line": 102, "text": "Implementation Approach:"}, {"line": 115, "text": "Integration Points:"}, {"line": 122, "text": "3. Automated Comparison Tools"}, {"line": 126, "text": "Tasks:"}, {"line": 137, "text": "CLI Interface:"}, {"line": 139, "text": "Compare two checkpoints"}, {"line": 142, "text": "Compare multiple evaluations by artifact ID"}, {"line": 145, "text": "Compare with baseline"}, {"line": 148, "text": "Generate report"}, {"line": 152, "text": "Comparison Report Features:"}, {"line": 159, "text": "Data Structure:"}, {"line": 174, "text": "Implementation Plan"}, {"line": 176, "text": "Phase 1: LM-Eval Integration (Week 1-2)"}, {"line": 183, "text": "Phase 2: Perplexity Metrics (Week 2-3)"}, {"line": 190, "text": "Phase 3: Comparison Tools (Week 3-4)"}, {"line": 198, "text": "Phase 4: Documentation & Testing (Week 4)"}, {"line": 207, "text": "Technical Requirements"}, {"line": 209, "text": "Dependencies:"}, {"line": 219, "text": "Compatibility:"}, {"line": 227, "text": "File Structure"}, {"line": 247, "text": "Usage Examples"}, {"line": 249, "text": "Example 1: Standard Benchmark"}, {"line": 251, "text": "Run hellaswag benchmark"}, {"line": 256, "text": "View results"}, {"line": 260, "text": "Example 2: Comprehensive Evaluation"}, {"line": 262, "text": "Run multiple benchmarks with quality metrics"}, {"line": 271, "text": "Example 3: Compare Checkpoints"}, {"line": 273, "text": "Compare training progression"}, {"line": 284, "text": "Example 4: Automated Testing"}, {"line": 286, "text": "Compare new checkpoint against baseline"}, {"line": 297, "text": "Success Metrics"}, {"line": 299, "text": "Quantitative:"}, {"line": 306, "text": "Qualitative:"}, {"line": 314, "text": "Testing Strategy"}, {"line": 316, "text": "Unit Tests:"}, {"line": 321, "text": "Integration Tests:"}, {"line": 326, "text": "Validation Tests:"}, {"line": 333, "text": "Risks & Mitigations"}, {"line": 345, "text": "Future Enhancements"}, {"line": 347, "text": "Post-V1:"}, {"line": 356, "text": "Advanced Features:"}, {"line": 364, "text": "References"}, {"line": 373, "text": "Related Issues"}, {"line": 381, "text": "Changelog"}]}, {"path": "planned_features/experiment-tracking-orchestration-hpo.md", "content": "## PF-004: Orchestration, experiment tracking, and hyperparameter tuning\n\n### Summary\n\nThis PF introduces optional Weights & Biases (W&B) experiment tracking, Prefect-powered flows to orchestrate data \u2192 train \u2192 eval \u2192 package, and Optuna-based hyperparameter tuning. It includes both CLI and GUI surfaces and a full developer checklist to implement and verify end-to-end.\n\n### Why this matters\n\n- Make runs observable beyond local JSONL.\n- Reproduce pipelines with Python-native orchestration.\n- Systematically tune key knobs (LR, warmup, chunking, MoE stability) without guesswork.\n\n---\n\n## What ships in PF-004\n\nIn scope (new capabilities):\n- W&B tracking: mirror metrics and upload artifacts from HRM training.\n- Prefect flows: a simple, local-first flow for dataset prep \u2192 train \u2192 eval \u2192 package.\n- Optuna autotune: `aios hrm-hf autotune` to search safe bounds with early-stopping.\n- GUI hooks: toggles and forms to launch training with W&B, run flows, and kick off HPO.\n\nOut of scope (future PF candidates):\n- Enterprise schedulers (Airflow, K8s operators), distributed orchestration, and model registry integrations.\n\n---\n\n## Dependencies and installation\n\nAll features are optional and guarded by availability checks. Recommended extras per feature:\n\n- Tracking: `wandb>=0.17`\n- Orchestration: `prefect>=2.16`\n- HPO: `optuna>=3.6`\n\nInstallation (PowerShell, Windows):\n\n```powershell\n# Activate venv if needed\n. .venv\\Scripts\\Activate.ps1\n\n# Install optional packages (any subset is fine)\npip install wandb prefect optuna\n```\n\nNote: W&B is optional and respects offline mode. Credentials are read from environment and W&B\u2019s standard login flow.\n\n---\n\n## CLI design and UX\n\n### 1) W&B flags in training\n\nCommand: `aios hrm-hf train-actv1`\n\nNew flags (all optional):\n- `--wandb/--no-wandb` (default: no-wandb)\n- `--wandb-project TEXT` (default: `aios-hrm`)\n- `--wandb-entity TEXT` (optional)\n- `--wandb-group TEXT` (optional)\n- `--wandb-tags TEXT` (comma-separated)\n- `--wandb-offline/--wandb-online` (default: online if logged in; else offline)\n- `--wandb-run-name TEXT` (optional)\n\nBehavior:\n- If enabled, initialize run with `TrainingConfig.to_dict()` as config.\n- Stream metrics each step and on eval; attach artifacts (metrics.jsonl, latest checkpoints, brain bundle, GPU metrics from `artifacts/optimization/*gpu_metrics*.jsonl` when present).\n- Respect `WANDB_MODE=offline` and lack of network gracefully (no crash, local-only logging continues).\n\nExamples:\n\n```powershell\n# Minimal live tracking\naios hrm-hf train-actv1 --dataset-file training_data/curated_datasets/test_sample.txt --steps 50 --batch-size 4 --wandb --wandb-project aios-hrm\n\n# Offline (no network), with named run and tags\naios hrm-hf train-actv1 --dataset-file training_data/curated_datasets/test_sample.txt --steps 10 --batch-size 2 --wandb --wandb-offline --wandb-run-name dev-dryrun --wandb-tags smoke,debug\n```\n\nFallback if `aios` entrypoint unavailable:\n\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --dataset-file training_data/curated_datasets/test_sample.txt --steps 10 --batch-size 2 --wandb\n```\n\nMetrics mirrored to W&B (initial set):\n- step, train_loss, eval_loss, lr, tokens_per_sec, sec_per_step\n- batch_size, max_seq_len, halt_max_steps\n- memory: vram_alloc_gb (when available), cpu_ram_gb, gpu_overflow_gb (if detected)\n- moe: load_balance_loss (when enabled), num_experts, num_experts_per_tok, capacity_factor\n\nArtifacts:\n- `metrics.jsonl` (if `--log-file` is set)\n- last N checkpoints from `save_dir`\n- packaged brain bundle under `bundle_dir/brain_name` (if used)\n\n---\n\n### 2) Prefect flow entry\n\nCommand: `aios flow hrm-train`\n\nPurpose: End-to-end local flow for:\n1) dataset prep (no-op if a plain text file is provided)\n2) training via `hrm-hf train-actv1`\n3) evaluation via `aios eval run` (when `--eval-file` is provided)\n4) packaging into brain bundle (if `--brain-name` is provided)\n\nFlags (representative):\n- `--dataset-file PATH` (required)\n- `--eval-file PATH` (optional)\n- `--brain-name TEXT` (optional)\n- `--wandb` (optional; passes through to train)\n- Plus a passthrough `--train-args \"...\"` for advanced control\n\nExamples:\n\n```powershell\n# Simple flow with W&B\naios flow hrm-train --dataset-file training_data/curated_datasets/test_sample.txt --wandb\n\n# Full flow with eval + packaging\naios flow hrm-train --dataset-file training_data/curated_datasets/test_sample.txt --eval-file training_data/curated_datasets/test_sample.txt --brain-name demo-brain --wandb --train-args \"--steps 100 --batch-size 4\"\n```\n\nImplementation surfaces:\n- `src/aios/flows/hrm_train_flow.py`: Prefect `@flow` and `@task`s (`prepare_dataset`, `train_model`, `run_eval`, `package_brain`).\n- `src/aios/cli/flows_cli.py`: Typer command group that invokes Prefect flow (Python-native run; users don\u2019t need a Prefect daemon).\n\n---\n\n### 3) Optuna HPO\n\nCommand: `aios hrm-hf autotune`\n\nCore flags:\n- `--trials INT` (default: 10)\n- `--timeout-minutes INT` (optional)\n- `--sampler {tpe,random}` (default: tpe)\n- `--pruner {median,successive_halving,None}` (default: median)\n- `--direction {minimize,maximize}` (default: minimize eval_loss)\n- `--eval-batches INT` (default: 3 for quick signal)\n- `--study-name TEXT` (optional, for resuming)\n- `--storage TEXT` (optional, Optuna RDB string for persistence)\n- `--seed INT` (optional; repeatable trials)\n- Search-space overrides (optional):\n\t- `--lr-min 1e-6 --lr-max 1e-4`\n\t- `--warmup-min 20 --warmup-max 400`\n\t- `--chunk-choices 1024,2048,4096`\n\t- `--moe-balance-min 5e-3 --moe-balance-max 2e-2`\n\nBehavior:\n- Each trial runs a short `train-actv1` with trial params applied to `TrainingConfig`.\n- OOM or fatal errors are caught; trial marked failed with readable metadata.\n- Best trial is reported and optionally exported to a JSON config snippet.\n\nExamples:\n\n```powershell\n# Fast 5-trial smoke test\naios hrm-hf autotune --dataset-file training_data/curated_datasets/test_sample.txt --trials 5 --eval-batches 2\n\n# Longer tune with persistent study on SQLite\naios hrm-hf autotune --dataset-file training_data/curated_datasets/test_sample.txt --trials 30 --storage sqlite:///artifacts/optimization/autotune.db --study-name actv1-tune-v1 --seed 42 --wandb\n```\n\nImplementation surfaces:\n- `src/aios/cli/hrm_hf/autotune.py`: Typer command; Optuna study setup; objective wrapper.\n- `src/aios/hpo/spaces.py`: central search-space builders and safe bounds.\n- `src/aios/hpo/objectives.py`: training/eval objective, robust exception handling.\n\n---\n\n## GUI design and UX\n\nLocations: `src/aios/gui/components/`\n\nAdditions:\n- Training panel: W&B toggle and advanced fields (project, entity, group, tags, run-name, offline). These map directly to CLI flags and `TrainingConfig` passthrough.\n- Autotune panel: a small form for trial count, timeout, pruner, sampler, search-space limits, and a \u201cStart Autotune\u201d button that spawns `hrm-hf autotune` as a subprocess with a progress view and best-trial summary.\n- Orchestration tab: run the `hrm-train` flow with inputs (dataset, eval, brain-name, W&B toggle). Show live logs and a link to Prefect UI (optional).\n\nSuggested files:\n- `hrm_training/wandb_fields.py` (reusable widget group)\n- `hrm_training/autotune_panel.py`\n- `flows/flow_runner.py` (thin wrapper to call Python flows or CLI)\n\nProcess notes:\n- Use `TrainingConfig.to_cli_args()` for argument generation and append W&B/HPO/flow specific flags.\n- Ensure long-running subprocesses drain stdout continuously to avoid deadlocks.\n- Persist last-used settings in `~/.config/aios/gui_prefs.yaml` for convenience.\n\n---\n\n## Implementation plan (dev checklist)\n\n1) W&B shim and wiring\n- [ ] Create `src/aios/core/logging/wandb_logger.py` with a tiny adapter: `init(config: dict, flags)`, `log(dict, step)`, `log_artifact(path, name, type)`, `finish()`; internally no-op if W&B missing or disabled.\n- [ ] Add CLI flags to `hrm_hf_cli.train_actv1` (see above) and plumb into `TrainingConfig` or function kwargs.\n- [ ] In `train_actv1_impl`, when enabled:\n\t- [ ] init run with config\n\t- [ ] per-step: log metrics\n\t- [ ] on checkpoint/eval/end: upload artifacts\n\t- [ ] handle offline/no-auth gracefully\n\n2) Prefect flow\n- [ ] Add `src/aios/flows/hrm_train_flow.py` with `@task` steps and `@flow` wrapper.\n- [ ] Add `src/aios/cli/flows_cli.py` exposing `aios flow hrm-train`.\n- [ ] Optional: emit a short README in `docs/flows/HRM_TRAIN_FLOW.md` showing usage and Prefect UI link.\n\n3) Optuna autotune\n- [ ] Add `src/aios/cli/hrm_hf/autotune.py` Typer command and register in `hrm_hf_cli.register`.\n- [ ] Implement `src/aios/hpo/spaces.py` and `src/aios/hpo/objectives.py`.\n- [ ] Robust OOM capture: detect CUDA OOM and DML errors; mark trial failed with note.\n- [ ] Emit `artifacts/optimization/autotune_<timestamp>.jsonl` with trial summaries.\n\n4) Schema and config\n- [ ] Update `TrainingConfig` with W&B-related passthrough fields only if needed, or treat as non-config CLI flags.\n- [ ] Add env-driven defaults: `AIOS_WANDB_PROJECT`, `AIOS_WANDB_ENTITY`, `WANDB_MODE`.\n\n5) Packaging and optional deps\n- [ ] Add extras in `pyproject.toml`:\n\t- `[project.optional-dependencies] tracking = [\"wandb>=0.17\"]`\n\t- `orchestration = [\"prefect>=2.16\"]`\n\t- `hpo = [\"optuna>=3.6\"]`\n- [ ] Document install snippets in `docs` and help texts.\n\n6) Tests and dry-runs\n- [ ] Unit test the W&B shim in no-op mode (no wandb installed) and with env offline.\n- [ ] CLI smoke test: run 1-step training with `--log-file artifacts/brains/actv1/metrics.jsonl` (see VS Code tasks already present) and verify no exceptions when `--wandb` is toggled.\n- [ ] HPO smoke: 2\u20133 trials, `--eval-batches 1`, ensure failure handling works.\n- [ ] Flow smoke: run with toy dataset; verify all tasks execute and files exist.\n\n---\n\n## HPO search space (initial)\n\nPrimary knobs and safe ranges:\n- `lr`: loguniform [1e-6, 1e-4] (reduced for MoE stability)\n- `warmup_steps`: int [20, 400]\n- `chunk_size`: categorical [1024, 2048, 4096]\n- `moe_load_balance_loss_coef`: loguniform [5e-3, 2e-2]\n\nObjective: minimize final eval loss on a small held-out slice (`--eval-batches` 2\u20135 for speed). Consider averaging 2 seeds in later iterations for stability.\n\nPruners/samplers:\n- Default pruner: MedianPruner (fast convergence on short runs)\n- Default sampler: TPE\n\nOutput:\n- Best-trial JSON emitted to `artifacts/optimization/best_trial.json`\n- Full trial history: `artifacts/optimization/autotune_*.jsonl`\n\n---\n\n## Observability and artifacts\n\nMetrics source of truth remains JSONL when `--log-file` is supplied. W&B mirrors and aggregates these:\n- Per-step metrics: train_loss, lr, throughput\n- Per-eval metrics: eval_loss, ppl (if computed)\n- System: memory stats, GPU overflow indicators when available\n\nArtifacts to attach (when present):\n- Checkpoints from `save_dir`\n- Final brain bundle from `bundle_dir`\n- `metrics.jsonl`, `gpu_metrics_*.jsonl`, model card HTML if generated\n\n---\n\n## Security and privacy\n\n- W&B: Respect offline mode; never crash if not logged in or network blocked.\n- Redact secrets from configs before logging.\n- Allow disabling artifact uploads via `--no-wandb` or env `WANDB_MODE=disabled`.\n\n---\n\n## Troubleshooting\n\nCommon issues and fixes:\n\n- No module named wandb/prefect/optuna\n\t- Install optional deps: `pip install wandb prefect optuna`\n\n- W&B login required\n\t- Run `wandb login` or use `--wandb-offline` for offline runs.\n\n- CUDA OOM during HPO\n\t- Optuna trial should be marked failed automatically; reduce `batch_size` or enable `--use-chunked-training` with smaller `--chunk-size`.\n\n- Prefect UI not accessible\n\t- This PF uses in-process Python flows; the UI is optional. You can still use `prefect orion start` separately if you want dashboards.\n\nWindows/PowerShell notes:\n- Prefer the `aios` entrypoint; if missing, call the module with `.venv\\Scripts\\python.exe -m aios.cli.aios ...`.\n- Paths in examples use forward slashes or PowerShell-friendly backslashes.\n\n---\n\n## Testing and acceptance criteria\n\nW&B\n- [ ] Enabling `--wandb` produces a new run with step/eval metrics.\n- [ ] Artifacts (at least metrics.jsonl) upload at run end or are skipped gracefully if offline.\n\nPrefect flow\n- [ ] `aios flow hrm-train` completes end-to-end on a toy dataset.\n- [ ] If `--eval-file` is given, eval metrics are produced.\n- [ ] If `--brain-name` is given, a packaged bundle exists.\n\nOptuna\n- [ ] At least 5 trials complete; failures are recorded (not fatal for the command).\n- [ ] Best-trial config improves eval loss vs default on the toy dataset.\n\n---\n\n## Milestones\n\n- M1 (1\u20132 days): W&B logging shipped + docs; CLI flags integrated; GUI toggle wired.\n- M2 (1\u20132 days): Prefect flow + CLI wrapper; Optuna autotune command + GUI panel; smoke tests and docs.\n\n---\n\n## Appendix A \u2013 Example quickstarts (copy/paste)\n\nDry-run training (JSONL only):\n\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1 --batch-size 2 --halt-max-steps 1 --eval-batches 1 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nTraining with W&B (online):\n\n```powershell\naios hrm-hf train-actv1 --dataset-file training_data/curated_datasets/test_sample.txt --steps 50 --batch-size 4 --wandb --wandb-project aios-hrm --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nRun the flow (with eval + packaging):\n\n```powershell\naios flow hrm-train --dataset-file training_data/curated_datasets/test_sample.txt --eval-file training_data/curated_datasets/test_sample.txt --brain-name demo-brain --wandb --train-args \"--steps 100 --batch-size 4\"\n```\n\nAutotune (5 quick trials):\n\n```powershell\naios hrm-hf autotune --dataset-file training_data/curated_datasets/test_sample.txt --trials 5 --eval-batches 2 --wandb\n```\n\nIf the `aios` script is not available:\n\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1 --batch-size 2 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n---\n\n## Appendix B \u2013 Proposed file map\n\n- `src/aios/core/logging/wandb_logger.py` \u2013 W&B adapter shim (no-op when unavailable).\n- `src/aios/flows/hrm_train_flow.py` \u2013 Prefect tasks and flow.\n- `src/aios/cli/flows_cli.py` \u2013 Typer command `aios flow hrm-train`.\n- `src/aios/cli/hrm_hf/autotune.py` \u2013 Typer command and Optuna objective.\n- `src/aios/hpo/spaces.py` \u2013 reusable search spaces.\n- `src/aios/hpo/objectives.py` \u2013 training objective with robust error handling.\n- `src/aios/gui/components/hrm_training/wandb_fields.py` \u2013 W&B UI widgets.\n- `src/aios/gui/components/hrm_training/autotune_panel.py` \u2013 GUI for HPO.\n- `docs/flows/HRM_TRAIN_FLOW.md` \u2013 optional flow readme.\n", "tags": ["cli", "datasets", "evaluation", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "PF-004: Orchestration, experiment tracking, and hyperparameter tuning"}, {"line": 2, "text": "Summary"}, {"line": 6, "text": "Why this matters"}, {"line": 14, "text": "What ships in PF-004"}, {"line": 27, "text": "Dependencies and installation"}, {"line": 38, "text": "Activate venv if needed"}, {"line": 41, "text": "Install optional packages (any subset is fine)"}, {"line": 49, "text": "CLI design and UX"}, {"line": 51, "text": "1) W&B flags in training"}, {"line": 72, "text": "Minimal live tracking"}, {"line": 75, "text": "Offline (no network), with named run and tags"}, {"line": 98, "text": "2) Prefect flow entry"}, {"line": 118, "text": "Simple flow with W&B"}, {"line": 121, "text": "Full flow with eval + packaging"}, {"line": 131, "text": "3) Optuna HPO"}, {"line": 159, "text": "Fast 5-trial smoke test"}, {"line": 162, "text": "Longer tune with persistent study on SQLite"}, {"line": 173, "text": "GUI design and UX"}, {"line": 194, "text": "Implementation plan (dev checklist)"}, {"line": 235, "text": "HPO search space (initial)"}, {"line": 255, "text": "Observability and artifacts"}, {"line": 269, "text": "Security and privacy"}, {"line": 277, "text": "Troubleshooting"}, {"line": 299, "text": "Testing and acceptance criteria"}, {"line": 316, "text": "Milestones"}, {"line": 323, "text": "Appendix A \u2013 Example quickstarts (copy/paste)"}, {"line": 357, "text": "Appendix B \u2013 Proposed file map"}]}, {"path": "planned_features/GRADIENT_ACCUMULATION_IMPLEMENTATION.md", "content": "# Gradient Accumulation Implementation Plan\n\n**Status**: Planned  \n**Priority**: High  \n**Objective**: Fix loss instability from small batch sizes by implementing gradient accumulation  \n**Created**: 2025-10-23\n\n---\n\n## \ud83d\udccb Executive Summary\n\n### The Problem\nCurrent training exhibits severe loss instability due to small batch sizes:\n```\nStep 448: Loss = 7.0073\nStep 449: Loss = 8.1779  \u2190 Wild 17% jump\nStep 450: Loss = 7.3505\nStep 451: Loss = 8.1017\nStep 452: Loss = 8.7440  \u2190 Peak instability\nStep 453: Loss = 7.5572\n```\n\n**Root Cause**: Batch size too small (2-8 samples) \u2192 noisy gradient estimates \u2192 unstable training\n\n### The Solution\n**Gradient Accumulation**: Accumulate gradients over N batches before updating weights\n- **Effective batch size** = physical_batch_size \u00d7 gradient_accumulation_steps\n- **VRAM usage** = same as physical_batch_size (no increase!)\n- **Training stability** = equivalent to large batch training\n\n### Expected Results\n```\n# With batch=8, gradient_accumulation_steps=4\nStep 112: Loss = 7.0073  (effective_batch=32)\nStep 113: Loss = 6.8234  \u2190 Smooth 2.6% decline\nStep 114: Loss = 6.6512\nStep 115: Loss = 6.4891\nStep 116: Loss = 6.3201  \u2190 Stable convergence\n```\n\n---\n\n## \ud83c\udf93 Technical Background\n\n### How Gradient Accumulation Works\n\n**Standard Training** (current):\n```python\nfor batch in dataloader:\n    loss = model(batch)\n    loss.backward()      # Compute gradients\n    optimizer.step()     # Update weights immediately\n    optimizer.zero_grad()\n```\n\n**With Gradient Accumulation**:\n```python\nfor i, batch in enumerate(dataloader):\n    loss = model(batch) / accumulation_steps  # \u26a0\ufe0f CRITICAL: Scale loss!\n    loss.backward()      # Gradients accumulate in model.parameters()\n    \n    if (i + 1) % accumulation_steps == 0:\n        optimizer.step()     # Update weights every N batches\n        optimizer.zero_grad()\n```\n\n### Why Loss Scaling is Critical\n```python\n# \u274c WRONG - Gradients will sum instead of average\nloss.backward()\n\n# \u2705 CORRECT - Scale loss so gradients average\nloss = loss / gradient_accumulation_steps\nloss.backward()\n```\n\n### Memory Usage Comparison\n```\nModel: 1.3B parameters\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Batch Size  \u2502 Accum    \u2502 Effective  \u2502 VRAM Usage  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2           \u2502 1        \u2502 2          \u2502 ~6 GB       \u2502\n\u2502 2           \u2502 16       \u2502 32         \u2502 ~6 GB       \u2502 \u2190 Same!\n\u2502 8           \u2502 1        \u2502 8          \u2502 ~10 GB      \u2502\n\u2502 8           \u2502 4        \u2502 32         \u2502 ~10 GB      \u2502 \u2190 Same!\n\u2502 32          \u2502 1        \u2502 32         \u2502 ~24 GB      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nConclusion: Accumulation doesn't increase VRAM!\n```\n\n---\n\n## \ud83c\udfd7\ufe0f Implementation Architecture\n\n### Files to Modify\n\n```\nsrc/aios/core/hrm_training/training_config/\n\u251c\u2500\u2500 optimization_fields.py          [ADD] gradient_accumulation_steps field\n\u251c\u2500\u2500 config_main.py                  [MODIFY] to_cli_args() method\n\nsrc/aios/cli/\n\u251c\u2500\u2500 hrm_hf_cli.py                   [ADD] --gradient-accumulation-steps option\n\nsrc/aios/cli/hrm_hf/training_logic/\n\u251c\u2500\u2500 train_epoch.py                  [MODIFY] backward/step logic (MAIN CHANGE)\n\nsrc/aios/gui/components/hrm_training_panel/\n\u251c\u2500\u2500 ui_optimizations.py             [ADD] UI controls for gradient accumulation\n\u251c\u2500\u2500 variable_setup.py               [ADD] gradient_accumulation_var\n\u251c\u2500\u2500 config_builder.py               [MODIFY] include in config build\n\u251c\u2500\u2500 state_management.py             [MODIFY] save/load state\n```\n\n---\n\n## \ud83d\udcdd Implementation Steps\n\n### \u2705 Phase 1: Configuration Layer\n\n#### Step 1.1: Add Configuration Field\n**File**: `src/aios/core/hrm_training/training_config/optimization_fields.py`\n\n**Location**: After line 118 (after `load_in_4bit` field)\n\n**Code to Add**:\n```python\n    # ============================================================================\n    # Gradient Accumulation\n    # ============================================================================\n    gradient_accumulation_steps: int = 1\n    \"\"\"Number of batches to accumulate gradients before updating weights.\n    \n    Enables training with larger effective batch sizes without increasing VRAM.\n    The effective batch size is: physical_batch_size \u00d7 gradient_accumulation_steps\n    \n    Benefits:\n    - Fixes loss instability from small batch sizes\n    - No VRAM increase (memory usage stays at physical batch size)\n    - Smoother training dynamics\n    - Better gradient estimates\n    \n    Example:\n    - batch_size=8, gradient_accumulation_steps=4 \u2192 effective_batch_size=32\n    - VRAM usage: ~10GB (for batch=8)\n    - Training stability: equivalent to batch=32\n    \n    Recommended values:\n    - 1: No accumulation (default, update every batch)\n    - 2-4: Mild accumulation for slightly smoother training\n    - 4-8: Moderate accumulation (recommended for most cases)\n    - 8-16: High accumulation for very small batch sizes\n    - 16+: Extreme accumulation (use when batch=1-2 required)\n    \n    How to choose:\n    1. Start with current batch_size and desired effective_batch_size\n    2. Calculate: gradient_accumulation_steps = effective_batch_size / batch_size\n    3. Test and adjust based on loss stability\n    \n    Memory impact:\n    - Gradients: +1\u00d7 model size (same as normal training)\n    - Activations: Only for physical batch size\n    - Total overhead: Negligible (<5% of total memory)\n    \n    Performance impact:\n    - Slightly slower due to more forward passes\n    - ~5-15% overhead depending on accumulation_steps\n    - Worth it for stability improvement\n    \n    Compatibility:\n    - Works with: AMP, gradient checkpointing, DeepSpeed ZeRO, PEFT/LoRA\n    - Works across: DDP, parallel independent, single-GPU modes\n    - Scheduler: Automatically adjusted to step with weight updates\n    \"\"\"\n```\n\n---\n\n#### Step 1.2: Update CLI Args Conversion\n**File**: `src/aios/core/hrm_training/training_config/config_main.py`\n\n**Location**: In `to_cli_args()` method, after batch_size (around line 109)\n\n**Find**:\n```python\n        args.extend([\"--batch-size\", str(self.batch_size)])\n        args.extend([\"--steps\", str(self.steps)])\n```\n\n**Replace with**:\n```python\n        args.extend([\"--batch-size\", str(self.batch_size)])\n        args.extend([\"--steps\", str(self.steps)])\n        \n        # Gradient accumulation\n        if self.gradient_accumulation_steps > 1:\n            args.extend([\"--gradient-accumulation-steps\", str(self.gradient_accumulation_steps)])\n```\n\n---\n\n### \u2705 Phase 2: CLI Integration\n\n#### Step 2.1: Add CLI Option\n**File**: `src/aios/cli/hrm_hf_cli.py`\n\n**Location**: In `train_actv1` function, after `batch_size` parameter (around line 137)\n\n**Find**:\n```python\n    batch_size: int = typer.Option(8, \"--batch-size\"),\n    steps: int = typer.Option(200, \"--steps\"),\n```\n\n**Replace with**:\n```python\n    batch_size: int = typer.Option(8, \"--batch-size\"),\n    gradient_accumulation_steps: int = typer.Option(\n        1, \n        \"--gradient-accumulation-steps\",\n        help=\"Accumulate gradients over N batches before updating weights. \"\n             \"Effective batch size = batch_size \u00d7 gradient_accumulation_steps. \"\n             \"Use to train with larger effective batches without increasing VRAM. \"\n             \"Example: batch=8, accum=4 \u2192 effective_batch=32\"\n    ),\n    steps: int = typer.Option(200, \"--steps\"),\n```\n\n---\n\n### \u2705 Phase 3: Training Loop Modification (CRITICAL)\n\n#### Step 3.1: Modify Training Logic\n**File**: `src/aios/cli/hrm_hf/training_logic/train_epoch.py`\n\n**Location**: Around line 350-380 (backward/step section)\n\n**Find** (the entire backward pass section):\n```python\n                # Backward pass\n                if deepspeed_engine is not None:\n                    deepspeed_engine.backward(loss)\n                    deepspeed_engine.step()\n                elif use_amp and scaler is not None and dev == \"cuda\":\n                    scaler.scale(loss).backward()\n                    scaler.unscale_(opt)\n                    average_gradients_if_distributed(model_student, is_distributed=ddp_actually_working, world_sz=world_sz)\n                    grad_norm = torch.nn.utils.clip_grad_norm_(model_student.parameters(), 0.5)\n                    if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n                        opt.zero_grad(set_to_none=True)\n                        scaler.update()\n                    else:\n                        scaler.step(opt)\n                        scaler.update()\n                else:\n                    loss.backward()\n                    average_gradients_if_distributed(model_student, is_distributed=ddp_actually_working, world_sz=world_sz)\n                    grad_norm = torch.nn.utils.clip_grad_norm_(model_student.parameters(), 0.5)\n                    if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n                        opt.zero_grad(set_to_none=True)\n                    else:\n                        opt.step()\n                    \n                steps_done += 1\n```\n\n**Replace with**:\n```python\n                # Get gradient accumulation config\n                gradient_accumulation_steps = getattr(config, 'gradient_accumulation_steps', 1)\n                accumulation_steps = max(1, int(gradient_accumulation_steps))\n                \n                # Scale loss for gradient accumulation\n                # CRITICAL: This ensures gradients average instead of sum\n                scaled_loss = loss / accumulation_steps\n                \n                # Backward pass (gradients accumulate automatically)\n                if deepspeed_engine is not None:\n                    # DeepSpeed handles accumulation internally\n                    deepspeed_engine.backward(scaled_loss)\n                    \n                    # Only step optimizer every N batches\n                    if (batch_idx + 1) % accumulation_steps == 0:\n                        deepspeed_engine.step()\n                        \n                elif use_amp and scaler is not None and dev == \"cuda\":\n                    # AMP with gradient accumulation\n                    scaler.scale(scaled_loss).backward()\n                    \n                    # Only update weights every N batches\n                    if (batch_idx + 1) % accumulation_steps == 0:\n                        scaler.unscale_(opt)\n                        average_gradients_if_distributed(model_student, is_distributed=ddp_actually_working, world_sz=world_sz)\n                        grad_norm = torch.nn.utils.clip_grad_norm_(model_student.parameters(), 0.5)\n                        \n                        if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n                            opt.zero_grad(set_to_none=True)\n                            scaler.update()\n                        else:\n                            scaler.step(opt)\n                            scaler.update()\n                            opt.zero_grad(set_to_none=True)\n                else:\n                    # Standard mode with gradient accumulation\n                    scaled_loss.backward()\n                    \n                    # Only update weights every N batches\n                    if (batch_idx + 1) % accumulation_steps == 0:\n                        average_gradients_if_distributed(model_student, is_distributed=ddp_actually_working, world_sz=world_sz)\n                        grad_norm = torch.nn.utils.clip_grad_norm_(model_student.parameters(), 0.5)\n                        \n                        if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n                            opt.zero_grad(set_to_none=True)\n                        else:\n                            opt.step()\n                            opt.zero_grad(set_to_none=True)\n                \n                # Increment step counter (count actual weight updates, not batches)\n                if (batch_idx + 1) % accumulation_steps == 0:\n                    steps_done += 1\n```\n\n---\n\n#### Step 3.2: Update Logging\n**File**: `src/aios/cli/hrm_hf/training_logic/train_epoch.py`\n\n**Location**: Around line 400 (logging section)\n\n**Find**:\n```python\n                if steps_done % 5 == 0 or steps_done == 1:\n                    try:\n                        import torch\n                        if dev == \"cuda\" and torch.cuda.is_available():\n                            torch.cuda.synchronize()\n                            allocated_gb = torch.cuda.memory_allocated(device_obj) / 1024**3\n                            reserved_gb = torch.cuda.memory_reserved(device_obj) / 1024**3\n                            max_allocated_gb = torch.cuda.max_memory_allocated(device_obj) / 1024**3\n                            total_gb = torch.cuda.get_device_properties(device_obj).total_memory / 1024**3\n                            \n                            write_jsonl({\n                                \"step\": steps_done,\n                                \"memory_gb\": round(allocated_gb, 3),\n```\n\n**Replace with**:\n```python\n                if steps_done % 5 == 0 or steps_done == 1:\n                    try:\n                        import torch\n                        if dev == \"cuda\" and torch.cuda.is_available():\n                            torch.cuda.synchronize()\n                            allocated_gb = torch.cuda.memory_allocated(device_obj) / 1024**3\n                            reserved_gb = torch.cuda.memory_reserved(device_obj) / 1024**3\n                            max_allocated_gb = torch.cuda.max_memory_allocated(device_obj) / 1024**3\n                            total_gb = torch.cuda.get_device_properties(device_obj).total_memory / 1024**3\n                            \n                            # Get accumulation info\n                            gradient_accumulation_steps = getattr(config, 'gradient_accumulation_steps', 1)\n                            accumulation_steps = max(1, int(gradient_accumulation_steps))\n                            \n                            write_jsonl({\n                                \"step\": steps_done,\n                                \"batch_idx\": batch_idx + 1,\n                                \"loss\": float(loss.item()),  # Unscaled loss for logging\n                                \"gradient_accumulation_steps\": accumulation_steps,\n                                \"effective_batch_size\": batch_size * accumulation_steps,\n                                \"physical_batch_size\": batch_size,\n                                \"memory_gb\": round(allocated_gb, 3),\n```\n\n---\n\n### \u2705 Phase 4: GUI Integration\n\n#### Step 4.1: Add Variable\n**File**: `src/aios/gui/components/hrm_training_panel/variable_setup.py`\n\n**Location**: In `setup_variables()` function, after existing optimization variables (around line 60)\n\n**Find**:\n```python\n    panel.use_8bit_optimizer_var = tk.BooleanVar(value=False)\n    panel.use_cpu_offload_var = tk.BooleanVar(value=False)\n```\n\n**Add after**:\n```python\n    panel.use_8bit_optimizer_var = tk.BooleanVar(value=False)\n    panel.use_cpu_offload_var = tk.BooleanVar(value=False)\n    \n    # Gradient accumulation\n    panel.gradient_accumulation_var = tk.StringVar(value=\"1\")\n```\n\n---\n\n#### Step 4.2: Add UI Controls\n**File**: `src/aios/gui/components/hrm_training_panel/ui_optimizations.py`\n\n**Location**: After Row 1 (Memory Optimizations), around line 38\n\n**Find**:\n```python\n    cpu_offload_btn.pack(side=\"left\", padx=(8, 0))\n    \n    # Row 2: PEFT (Parameter-Efficient Fine-Tuning)\n```\n\n**Add between**:\n```python\n    cpu_offload_btn.pack(side=\"left\", padx=(8, 0))\n    \n    # Row 1.5: Gradient Accumulation\n    grad_accum_row = ttk.Frame(opt_frame)\n    grad_accum_row.pack(fill=\"x\", pady=2)\n    ttk.Label(grad_accum_row, text=\"Batch Scaling:\", width=15, anchor=\"e\", font=(\"TkDefaultFont\", 9, \"bold\")).pack(side=\"left\")\n    ttk.Label(grad_accum_row, text=\"Accum Steps:\").pack(side=\"left\", padx=(0, 2))\n    accum_combo = ttk.Combobox(grad_accum_row, textvariable=panel.gradient_accumulation_var, width=8, state=\"readonly\")\n    accum_combo['values'] = ('1', '2', '4', '8', '16', '32')\n    accum_combo.pack(side=\"left\")\n    ttk.Label(grad_accum_row, text=\"\u2192\").pack(side=\"left\", padx=4)\n    panel.effective_batch_lbl = ttk.Label(grad_accum_row, text=\"Effective Batch: 8\", foreground=\"blue\")\n    panel.effective_batch_lbl.pack(side=\"left\")\n    \n    # Row 2: PEFT (Parameter-Efficient Fine-Tuning)\n```\n\n---\n\n#### Step 4.3: Add Tooltip and Update Callback\n**File**: `src/aios/gui/components/hrm_training_panel/ui_optimizations.py`\n\n**Location**: In tooltip section, around line 100\n\n**Find**:\n```python\n        add_tooltip(cpu_offload_btn, \"CPU Offload: Moves optimizer states to system RAM\\nSaves VRAM \u2022 ~30% slower training\")\n        add_tooltip(peft_enable_btn, \"Enable PEFT: Use Low-Rank Adaptation (LoRA) for efficient fine-tuning\\n\u219395-99% trainable parameters (87M \u2192 500K-2M)\")\n```\n\n**Add after**:\n```python\n        add_tooltip(cpu_offload_btn, \"CPU Offload: Moves optimizer states to system RAM\\nSaves VRAM \u2022 ~30% slower training\")\n        add_tooltip(accum_combo, \n            \"Gradient Accumulation: Accumulate gradients over N batches\\n\"\n            \"before updating weights.\\n\\n\"\n            \"Benefits:\\n\"\n            \"\u2022 Fixes loss instability from small batches\\n\"\n            \"\u2022 No VRAM increase\\n\"\n            \"\u2022 Smoother training dynamics\\n\\n\"\n            \"Effective Batch = Physical Batch \u00d7 Accum Steps\\n\"\n            \"Example: batch=8, accum=4 \u2192 effective=32\\n\\n\"\n            \"Recommended:\\n\"\n            \"\u2022 1: No accumulation (default)\\n\"\n            \"\u2022 4: Balanced (recommended)\\n\"\n            \"\u2022 8-16: High stability (small batches)\\n\"\n            \"\u2022 32: Maximum stability (batch=1-2)\")\n        add_tooltip(peft_enable_btn, \"Enable PEFT: Use Low-Rank Adaptation (LoRA) for efficient fine-tuning\\n\u219395-99% trainable parameters (87M \u2192 500K-2M)\")\n```\n\n---\n\n#### Step 4.4: Add Update Callback Function\n**File**: `src/aios/gui/components/hrm_training_panel/ui_optimizations.py`\n\n**Location**: After tooltips section, before the end of `build_optimizations_section()` function\n\n**Add**:\n```python\n    # Setup callback to update effective batch label\n    def update_effective_batch_label(*args):\n        try:\n            batch = int(panel.batch_var.get() or 8)\n            accum = int(panel.gradient_accumulation_var.get() or 1)\n            effective = batch * accum\n            panel.effective_batch_lbl.config(text=f\"Effective Batch: {effective}\")\n        except Exception:\n            pass\n    \n    panel.gradient_accumulation_var.trace_add(\"write\", update_effective_batch_label)\n    panel.batch_var.trace_add(\"write\", update_effective_batch_label)\n    \n    # Initial update\n    update_effective_batch_label()\n```\n\n---\n\n#### Step 4.5: Update Config Builder\n**File**: `src/aios/gui/components/hrm_training_panel/config_builder.py`\n\n**Location**: In `build_training_config()` function, after batch_size (around line 25)\n\n**Find**:\n```python\n        batch_size=int(panel.batch_var.get() or 8),\n        steps=int(panel.steps_var.get() or 200),\n```\n\n**Replace with**:\n```python\n        batch_size=int(panel.batch_var.get() or 8),\n        gradient_accumulation_steps=int(panel.gradient_accumulation_var.get() or 1),\n        steps=int(panel.steps_var.get() or 200),\n```\n\n---\n\n#### Step 4.6: Update State Management\n**File**: `src/aios/gui/components/hrm_training_panel/state_management.py`\n\n**Location**: In `get_state()` function, around line 30\n\n**Find**:\n```python\n            \"batch_size\": panel.batch_var.get(),\n            \"steps\": panel.steps_var.get(),\n```\n\n**Add after**:\n```python\n            \"batch_size\": panel.batch_var.get(),\n            \"gradient_accumulation_steps\": panel.gradient_accumulation_var.get(),\n            \"steps\": panel.steps_var.get(),\n```\n\n**Location**: In `set_state()` function, around line 100\n\n**Find**:\n```python\n    if \"batch_size\" in state:\n        panel.batch_var.set(str(state[\"batch_size\"]))\n    if \"steps\" in state:\n        panel.steps_var.set(str(state[\"steps\"]))\n```\n\n**Add after**:\n```python\n    if \"batch_size\" in state:\n        panel.batch_var.set(str(state[\"batch_size\"]))\n    if \"gradient_accumulation_steps\" in state:\n        panel.gradient_accumulation_var.set(str(state[\"gradient_accumulation_steps\"]))\n    if \"steps\" in state:\n        panel.steps_var.set(str(state[\"steps\"]))\n```\n\n---\n\n## \ud83e\uddea Testing Plan\n\n### Test 1: Basic Functionality\n**Objective**: Verify gradient accumulation works correctly\n\n```bash\n# Terminal test\ncd /path/to/AI-OS  # Replace with your AI-OS directory\n.\\.venv\\Scripts\\Activate.ps1\n\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 4 `\n  --gradient-accumulation-steps 8 `\n  --steps 20 `\n  --log-file artifacts/test_grad_accum.jsonl\n\n# Check log file\ncat artifacts/test_grad_accum.jsonl | Select-String \"gradient_accumulation\"\n```\n\n**Expected Output**:\n```json\n{\"gradient_accumulation_steps\": 8, \"effective_batch_size\": 32, \"physical_batch_size\": 4}\n```\n\n---\n\n### Test 2: Loss Stability Comparison\n**Objective**: Demonstrate loss stability improvement\n\n**Test 2a: Without accumulation (baseline)**\n```bash\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 2 `\n  --gradient-accumulation-steps 1 `\n  --steps 100 `\n  --log-file artifacts/test_without_accum.jsonl\n```\n\n**Test 2b: With accumulation**\n```bash\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 2 `\n  --gradient-accumulation-steps 16 `\n  --steps 100 `\n  --log-file artifacts/test_with_accum.jsonl\n```\n\n**Analysis Script**:\n```python\nimport json\nimport matplotlib.pyplot as plt\n\n# Load logs\nwith open('artifacts/test_without_accum.jsonl') as f:\n    without = [json.loads(line) for line in f if 'loss' in line]\n\nwith open('artifacts/test_with_accum.jsonl') as f:\n    with_accum = [json.loads(line) for line in f if 'loss' in line]\n\n# Plot comparison\nplt.figure(figsize=(12, 6))\nplt.plot([x['step'] for x in without], [x['loss'] for x in without], \n         label='Without accumulation (batch=2)', alpha=0.7)\nplt.plot([x['step'] for x in with_accum], [x['loss'] for x in with_accum], \n         label='With accumulation (batch=2, accum=16, effective=32)', alpha=0.7)\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.title('Loss Stability: With vs Without Gradient Accumulation')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.savefig('artifacts/gradient_accumulation_comparison.png')\nprint(\"Chart saved to artifacts/gradient_accumulation_comparison.png\")\n```\n\n---\n\n### Test 3: VRAM Usage Verification\n**Objective**: Confirm VRAM usage doesn't increase with accumulation\n\n```bash\n# Test A: batch=2, accum=1\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 2 `\n  --gradient-accumulation-steps 1 `\n  --steps 10 `\n  --log-file artifacts/vram_test_no_accum.jsonl\n\n# Test B: batch=2, accum=16 (should use same VRAM!)\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 2 `\n  --gradient-accumulation-steps 16 `\n  --steps 10 `\n  --log-file artifacts/vram_test_with_accum.jsonl\n```\n\n**Verification**:\n```powershell\n# Compare peak VRAM usage\n$noAccum = Get-Content artifacts/vram_test_no_accum.jsonl | ConvertFrom-Json | Where-Object {$_.peak_gb} | Select-Object -Last 1\n$withAccum = Get-Content artifacts/vram_test_with_accum.jsonl | ConvertFrom-Json | Where-Object {$_.peak_gb} | Select-Object -Last 1\n\nWrite-Host \"No accumulation VRAM: $($noAccum.peak_gb) GB\"\nWrite-Host \"With accumulation VRAM: $($withAccum.peak_gb) GB\"\nWrite-Host \"Difference: $(($withAccum.peak_gb - $noAccum.peak_gb)) GB (should be ~0)\"\n```\n\n---\n\n### Test 4: GUI Integration\n**Objective**: Verify GUI controls work correctly\n\n**Steps**:\n1. Launch GUI: `aios gui`\n2. Navigate to HRM Training panel\n3. Verify new controls appear:\n   - \"Batch Scaling:\" label\n   - \"Accum Steps:\" dropdown\n   - \"\u2192 Effective Batch: X\" label\n4. Test interactions:\n   - Set Batch Size: 8\n   - Set Accum Steps: 4\n   - Verify label shows \"Effective Batch: 32\"\n   - Change Batch Size to 16\n   - Verify label updates to \"Effective Batch: 64\"\n5. Start training and verify:\n   - Training runs without errors\n   - Log shows correct accumulation settings\n   - Loss curve is smoother than without accumulation\n\n---\n\n### Test 5: Compatibility Tests\n\n**Test 5a: With AMP**\n```bash\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 4 `\n  --gradient-accumulation-steps 8 `\n  --amp `\n  --steps 20\n```\n\n**Test 5b: With Gradient Checkpointing**\n```bash\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 4 `\n  --gradient-accumulation-steps 8 `\n  --gradient-checkpointing `\n  --steps 20\n```\n\n**Test 5c: With DeepSpeed ZeRO-2**\n```bash\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 4 `\n  --gradient-accumulation-steps 8 `\n  --zero-stage zero2 `\n  --steps 20\n```\n\n**Test 5d: With PEFT/LoRA**\n```bash\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 4 `\n  --gradient-accumulation-steps 8 `\n  --use-peft `\n  --lora-r 16 `\n  --steps 20\n```\n\n**Test 5e: Parallel Independent Mode**\n```bash\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 4 `\n  --gradient-accumulation-steps 8 `\n  --parallel-independent `\n  --cuda-ids 0,1 `\n  --steps 20\n```\n\n---\n\n## \u2705 Implementation Checklist\n\n### Phase 1: Configuration Layer\n- [ ] Add `gradient_accumulation_steps` field to `optimization_fields.py`\n- [ ] Update `to_cli_args()` in `config_main.py`\n- [ ] Test: Import TrainingConfig and verify new field exists\n\n### Phase 2: CLI Integration\n- [ ] Add `--gradient-accumulation-steps` option to `hrm_hf_cli.py`\n- [ ] Test: `aios hrm-hf train-actv1 --help` shows new option\n\n### Phase 3: Training Loop\n- [ ] Modify backward/step logic in `train_epoch.py`\n- [ ] Add loss scaling\n- [ ] Add conditional optimizer step\n- [ ] Update logging to show accumulation metrics\n- [ ] Test: Run basic training with accumulation\n\n### Phase 4: GUI Integration\n- [ ] Add `gradient_accumulation_var` in `variable_setup.py`\n- [ ] Add UI controls in `ui_optimizations.py`\n- [ ] Add tooltip\n- [ ] Add update callback for effective batch label\n- [ ] Update `config_builder.py`\n- [ ] Update `state_management.py` (get_state)\n- [ ] Update `state_management.py` (set_state)\n- [ ] Test: Launch GUI and verify controls appear\n\n### Phase 5: Testing\n- [ ] Test 1: Basic functionality\n- [ ] Test 2: Loss stability comparison\n- [ ] Test 3: VRAM usage verification\n- [ ] Test 4: GUI integration\n- [ ] Test 5a: AMP compatibility\n- [ ] Test 5b: Gradient checkpointing compatibility\n- [ ] Test 5c: DeepSpeed ZeRO compatibility\n- [ ] Test 5d: PEFT/LoRA compatibility\n- [ ] Test 5e: Parallel independent mode compatibility\n\n### Phase 6: Documentation\n- [ ] Update CLI help text (done via option definition)\n- [ ] Add tooltip to GUI (done in Phase 4)\n- [ ] Update `docs/guide/features/TRAINING_OPTIMIZATIONS.md` (if exists)\n- [ ] Add note to CHANGELOG.md\n\n---\n\n## \ud83d\udcca Success Criteria\n\n### Functional Requirements\n\u2705 Gradient accumulation configurable via CLI and GUI  \n\u2705 Loss stability improves with accumulation enabled  \n\u2705 VRAM usage remains constant regardless of accumulation steps  \n\u2705 Compatible with all existing optimizations (AMP, checkpointing, ZeRO, PEFT)  \n\u2705 Works in all training modes (single-GPU, DDP, parallel independent)  \n\n### Performance Requirements\n\u2705 Training slowdown < 15% for accumulation_steps \u2264 16  \n\u2705 Loss variance reduced by \u2265 50% compared to small batch baseline  \n\u2705 No memory leaks during extended training  \n\n### User Experience Requirements\n\u2705 GUI controls intuitive and self-documenting  \n\u2705 CLI help text clear and complete  \n\u2705 Logging shows accumulation status and effective batch size  \n\u2705 State persistence works (save/load GUI state)  \n\n---\n\n## \ud83c\udfaf Quick Start (After Implementation)\n\n### Via CLI\n```bash\n# Basic usage\naios hrm-hf train-actv1 \\\n  --model artifacts/hf_implant/gpt2 \\\n  --dataset-file training_data/curated_datasets/my_dataset.txt \\\n  --batch-size 8 \\\n  --gradient-accumulation-steps 4 \\\n  --steps 1000\n\n# High stability (small batch)\naios hrm-hf train-actv1 \\\n  --batch-size 2 \\\n  --gradient-accumulation-steps 16 \\\n  --steps 1000\n```\n\n### Via GUI\n1. Open HRM Training panel\n2. Set **Batch Size**: `8`\n3. Set **Accum Steps**: `4`\n4. Verify: \"Effective Batch: 32\"\n5. Click \"Start Training\"\n\n---\n\n## \ud83d\udcda References\n\n### Research\n- PyTorch gradient accumulation patterns (from pytorch/examples)\n- Standard deep learning practice for memory-constrained training\n- User's current loss instability pattern: 7.0\u21928.7\u21927.3 (analyzed)\n\n### Codebase Components\n- Main training loop: `src/aios/cli/hrm_hf/training_logic/train_epoch.py`\n- Config system: `src/aios/core/hrm_training/training_config/`\n- GUI panel: `src/aios/gui/components/hrm_training_panel/`\n- CLI: `src/aios/cli/hrm_hf_cli.py`\n\n### Related Features\n- Automatic Mixed Precision (AMP)\n- Gradient Checkpointing\n- DeepSpeed ZeRO\n- PEFT/LoRA\n- Parallel Independent Training\n\n---\n\n## \ud83d\udd04 Future Enhancements\n\n### Potential Improvements\n1. **Auto-calculate accumulation**: GUI button to automatically determine optimal accumulation steps based on target effective batch size\n2. **Adaptive accumulation**: Dynamically adjust accumulation based on VRAM usage\n3. **Loss-based tuning**: Automatically increase accumulation if loss variance exceeds threshold\n4. **Multi-GPU load balancing**: Different accumulation steps per GPU based on VRAM capacity\n\n### Not in Scope (For Now)\n- Dynamic gradient accumulation (changing during training)\n- Per-layer gradient accumulation\n- Gradient accumulation with different batch sizes per step\n\n---\n\n## \ud83d\udcde Support & Troubleshooting\n\n### Common Issues\n\n**Q: Loss not improving with accumulation**  \nA: Verify loss scaling is applied (`loss / accumulation_steps`). Check logs for `scaled_loss` in backward pass.\n\n**Q: VRAM usage increased**  \nA: Check that `batch_size` in config matches physical batch, not effective batch. Accumulation shouldn't increase VRAM.\n\n**Q: Training slower than expected**  \nA: Normal with high accumulation steps. Trade-off between speed and stability. Try reducing accumulation_steps.\n\n**Q: Gradients not accumulating**  \nA: Ensure `optimizer.zero_grad()` only called after weight update, not every batch.\n\n**Q: GUI not showing new controls**  \nA: Restart GUI. Check that all Phase 4 files were modified correctly.\n\n---\n\n**Document Version**: 1.0  \n**Last Updated**: 2025-10-23  \n**Author**: AI-OS Development Team  \n**Status**: Ready for Implementation\n", "tags": ["cli", "hrm", "training"], "headings": [{"line": 0, "text": "Gradient Accumulation Implementation Plan"}, {"line": 9, "text": "\ud83d\udccb Executive Summary"}, {"line": 11, "text": "The Problem"}, {"line": 24, "text": "The Solution"}, {"line": 30, "text": "Expected Results"}, {"line": 32, "text": "With batch=8, gradient_accumulation_steps=4"}, {"line": 42, "text": "\ud83c\udf93 Technical Background"}, {"line": 44, "text": "How Gradient Accumulation Works"}, {"line": 66, "text": "Why Loss Scaling is Critical"}, {"line": 68, "text": "\u274c WRONG - Gradients will sum instead of average"}, {"line": 71, "text": "\u2705 CORRECT - Scale loss so gradients average"}, {"line": 76, "text": "Memory Usage Comparison"}, {"line": 95, "text": "\ud83c\udfd7\ufe0f Implementation Architecture"}, {"line": 97, "text": "Files to Modify"}, {"line": 119, "text": "\ud83d\udcdd Implementation Steps"}, {"line": 121, "text": "\u2705 Phase 1: Configuration Layer"}, {"line": 123, "text": "Step 1.1: Add Configuration Field"}, {"line": 130, "text": "============================================================================"}, {"line": 131, "text": "Gradient Accumulation"}, {"line": 132, "text": "============================================================================"}, {"line": 181, "text": "Step 1.2: Update CLI Args Conversion"}, {"line": 197, "text": "Gradient accumulation"}, {"line": 204, "text": "\u2705 Phase 2: CLI Integration"}, {"line": 206, "text": "Step 2.1: Add CLI Option"}, {"line": 233, "text": "\u2705 Phase 3: Training Loop Modification (CRITICAL)"}, {"line": 235, "text": "Step 3.1: Modify Training Logic"}, {"line": 242, "text": "Backward pass"}, {"line": 271, "text": "Get gradient accumulation config"}, {"line": 275, "text": "Scale loss for gradient accumulation"}, {"line": 276, "text": "CRITICAL: This ensures gradients average instead of sum"}, {"line": 279, "text": "Backward pass (gradients accumulate automatically)"}, {"line": 281, "text": "DeepSpeed handles accumulation internally"}, {"line": 284, "text": "Only step optimizer every N batches"}, {"line": 289, "text": "AMP with gradient accumulation"}, {"line": 292, "text": "Only update weights every N batches"}, {"line": 306, "text": "Standard mode with gradient accumulation"}, {"line": 309, "text": "Only update weights every N batches"}, {"line": 320, "text": "Increment step counter (count actual weight updates, not batches)"}, {"line": 327, "text": "Step 3.2: Update Logging"}, {"line": 361, "text": "Get accumulation info"}, {"line": 377, "text": "\u2705 Phase 4: GUI Integration"}, {"line": 379, "text": "Step 4.1: Add Variable"}, {"line": 395, "text": "Gradient accumulation"}, {"line": 401, "text": "Step 4.2: Add UI Controls"}, {"line": 410, "text": "Row 2: PEFT (Parameter-Efficient Fine-Tuning)"}, {"line": 417, "text": "Row 1.5: Gradient Accumulation"}, {"line": 429, "text": "Row 2: PEFT (Parameter-Efficient Fine-Tuning)"}, {"line": 434, "text": "Step 4.3: Add Tooltip and Update Callback"}, {"line": 467, "text": "Step 4.4: Add Update Callback Function"}, {"line": 474, "text": "Setup callback to update effective batch label"}, {"line": 487, "text": "Initial update"}, {"line": 493, "text": "Step 4.5: Update Config Builder"}, {"line": 513, "text": "Step 4.6: Update State Management"}, {"line": 553, "text": "\ud83e\uddea Testing Plan"}, {"line": 555, "text": "Test 1: Basic Functionality"}, {"line": 559, "text": "Terminal test"}, {"line": 571, "text": "Check log file"}, {"line": 582, "text": "Test 2: Loss Stability Comparison"}, {"line": 612, "text": "Load logs"}, {"line": 619, "text": "Plot comparison"}, {"line": 636, "text": "Test 3: VRAM Usage Verification"}, {"line": 640, "text": "Test A: batch=2, accum=1"}, {"line": 649, "text": "Test B: batch=2, accum=16 (should use same VRAM!)"}, {"line": 661, "text": "Compare peak VRAM usage"}, {"line": 672, "text": "Test 4: GUI Integration"}, {"line": 695, "text": "Test 5: Compatibility Tests"}, {"line": 756, "text": "\u2705 Implementation Checklist"}, {"line": 758, "text": "Phase 1: Configuration Layer"}, {"line": 763, "text": "Phase 2: CLI Integration"}, {"line": 767, "text": "Phase 3: Training Loop"}, {"line": 774, "text": "Phase 4: GUI Integration"}, {"line": 784, "text": "Phase 5: Testing"}, {"line": 795, "text": "Phase 6: Documentation"}, {"line": 803, "text": "\ud83d\udcca Success Criteria"}, {"line": 805, "text": "Functional Requirements"}, {"line": 812, "text": "Performance Requirements"}, {"line": 817, "text": "User Experience Requirements"}, {"line": 825, "text": "\ud83c\udfaf Quick Start (After Implementation)"}, {"line": 827, "text": "Via CLI"}, {"line": 829, "text": "Basic usage"}, {"line": 837, "text": "High stability (small batch)"}, {"line": 844, "text": "Via GUI"}, {"line": 853, "text": "\ud83d\udcda References"}, {"line": 855, "text": "Research"}, {"line": 860, "text": "Codebase Components"}, {"line": 866, "text": "Related Features"}, {"line": 875, "text": "\ud83d\udd04 Future Enhancements"}, {"line": 877, "text": "Potential Improvements"}, {"line": 883, "text": "Not in Scope (For Now)"}, {"line": 890, "text": "\ud83d\udcde Support & Troubleshooting"}, {"line": 892, "text": "Common Issues"}]}, {"path": "planned_features/hrm-serving-inference-api.md", "content": "## PF-003: Serving and inference integration (HRM + baselines)\n\n### Summary\n\nProvide a clean serving path for HRM and HF baselines. Ship a minimal FastAPI server for HRM inference and document how to deploy HF baselines on vLLM/TGI. For production-grade GPU fleets, outline Triton backend path.\n\n### Motivation\n\n- Benchmark and A/B test HRM vs standard transformer baselines.\n- Enable downstream apps to use a stable `/generate` and `/loglikelihood` API.\n\n### Scope\n\nIn scope:\n- Minimal FastAPI server for HRM with batch generation and scoring endpoints.\n- Configuration to run under Docker.\n- Baseline serving docs for vLLM and TGI (HF models only).\n\nOut of scope (for this PF):\n- Triton custom backend implementation (design notes only).\n\n### Integration points (repo)\n\n- New module: `src/aios/serve/hrm_server.py` (FastAPI, uvicorn)\n- Dockerfile target: `--target hrm-serve` (multi-stage)\n- Scripts: `scripts/run_hrm_server.ps1` (Windows), `scripts/run_hrm_server.sh` (Linux)\n\n### API contract (v1)\n\n- POST `/generate`\n  - Input: `{ prompts: string[], max_tokens?: number, temperature?: number, top_p?: number }`\n  - Output: `{ completions: string[], usage?: { prompt_tokens, completion_tokens } }`\n\n- POST `/loglikelihood`\n  - Input: `{ prompts: string[], continuations: string[] }`\n  - Output: `{ ll: number[] }`\n\n---\n\n## Comprehensive guide and checklist\n\nThis section expands PF-003 into an actionable implementation guide with CLI and GUI elements, deployment paths, and validation checklists.\n\n### Deliverables\n\n- HRM FastAPI server module: `src/aios/serve/hrm_server.py`\n- Optional minimal UI for manual testing (Gradio or Streamlit)\n- CLI launcher: `aios serve hrm` (via existing `aios.cli.aios` entrypoint)\n- Dockerfile target: `hrm-serve` and helper scripts for Windows/Linux\n- Baseline serving notes for vLLM and TGI\n- Acceptance tests and operational runbooks\n\n### Architecture overview\n\n- Client (CLI/UI/SDK) \u2192 FastAPI app \u2192 HRM inference wrapper (tokenizer + model) \u2192 Torch device (CPU/GPU)\n- Optional: request batcher (FIFO), configurable max batch size and max new tokens per request\n- Observability: structured logs + optional Prometheus metrics endpoint\n- Healthchecks: `/healthz` (process up), `/readyz` (model loaded)\n\n### Data models and API details\n\n- POST `/generate`\n  - Request model (Pydantic):\n    - `prompts: List[str]` (1..N)\n    - `max_tokens: int = 128` (cap at server max, e.g., 1024)\n    - `temperature: float = 0.7` (range [0, 2])\n    - `top_p: float = 1.0` (range (0, 1])\n    - `seed: Optional[int]` (optional for reproducibility)\n    - `stop: Optional[List[str]]` (optional stop strings)\n  - Response:\n    - `completions: List[str]`\n    - `usage?: { prompt_tokens: int, completion_tokens: int, total_tokens: int }`\n  - Errors:\n    - 400: validation (e.g., empty prompts, invalid ranges)\n    - 429: rate limited (optional)\n    - 500: internal error\n\n- POST `/loglikelihood`\n  - Request:\n    - `prompts: List[str]`\n    - `continuations: List[str]` (must match length of prompts)\n  - Response:\n    - `ll: List[float]` (sum of token log-probs of each continuation given prompt)\n  - Errors: as above\n\n- GET `/healthz`: `{ status: \"ok\" }`\n- GET `/readyz`: `{ status: \"ready\" }` once model/tokenizer loaded\n- Optional future: `/generate_stream` using server-sent events (not in v1 scope)\n\n### HRM inference wrapper design\n\n- Artifacts and paths\n  - Tokenizer: reuse loading logic from training; prefer registry paths under `artifacts/hf_implant/tokenizers/` or configured `--tokenizer` path\n  - Model weights: e.g., `artifacts/brains/actv1/actv1_student.pt` or `artifacts/hf_implant/q_head.pt` + base model\n  - Config file (optional): `config/default.yaml` overrides\n\n  - Batch tokenize `prompts` with truncation to model context window; return attention masks\n\n- Decoding\n  - Modes: greedy (temperature=0 or top_p=1, top_k=None), sampled (temperature>0 and/or top_p<1)\n  - Respect `max_tokens` and `stop` strings; early stop if EOS token encountered\n  - Use `torch.no_grad()` and AMP optional (`torch.cuda.amp.autocast`) when CUDA available\n\n- Batching\n  - Pad to max prompt length in batch; maintain mapping to return completions in order\n  - Configurable `max_batch_size` to protect memory\n\n- Device selection\n  - Auto-detect CUDA/ROCm/MPS; environment variable `AIOS_DEVICE` to force: `cpu|cuda|mps`\n\n- Pseudocode outline\n  - `load_tokenizer()` \u2192 `load_model()` \u2192 set eval, move to device\n  - For `/generate`: encode \u2192 loop new tokens \u2192 decode to strings \u2192 postprocess stop\n  - For `/loglikelihood`: teacher-forcing forward over prompt+continuation and sum log-probs at continuation positions\n\n### Configuration model\n\nPrecedence: CLI flags > Env vars > YAML config defaults.\n\n- CLI flags (examples):\n  - `--host 0.0.0.0 --port 8000`\n  - `--model-path artifacts/brains/actv1/actv1_student.pt`\n  - `--tokenizer-path artifacts/hf_implant/tokenizers/<name>`\n  - `--device cpu|cuda|mps`\n  - `--max-batch-size 16 --max-new-tokens 256 --context-window 2048`\n  - `--enable-cors --cors-origins *`\n  - `--log-level info` (honors `logging.yaml` if present)\n\n- Env vars:\n  - `AIOS_MODEL_PATH`, `AIOS_TOKENIZER_PATH`, `AIOS_DEVICE`, `AIOS_MAX_BATCH_SIZE`, `AIOS_MAX_NEW_TOKENS`, `AIOS_CORS_ORIGINS`\n\n- YAML (optional): `config/default.yaml` \u2192 `serve.hrm` section\n\n### CLI: serve commands\n\n- Primary: `aios serve hrm` (wired via existing `aios.cli.aios`)\n  - Example (Windows PowerShell):\n    - `aios serve hrm --host 0.0.0.0 --port 8000 --model-path artifacts/brains/actv1/actv1_student.pt --tokenizer-path artifacts/hf_implant/tokenizers/base --device cpu`\n  - Example (module invocation):\n    - `.venv\\Scripts\\python.exe -m aios.serve.hrm_server --host 0.0.0.0 --port 8000`\n\n- Admin helpers (optional):\n  - `aios serve hrm --print-config`\n  - `aios serve hrm --dry-run` (load-only, no HTTP)\n\n### Minimal GUI (optional) for manual testing\n\n- Choice: Gradio (simpler) or Streamlit\n- Usage intent: quick sanity checks by product/QA without curl or code\n- Proposed: `aios serve hrm --ui` opens a small panel:\n  - Input textarea for prompt, sliders for `max_tokens`, `temperature`, `top_p`\n  - Buttons \"Generate\" and \"Score loglikelihood\"\n  - Display output text and usage metrics\n\n- Local run (Streamlit example):\n  - `python -m aios.serve.hrm_ui --server-url http://localhost:8000`\n\nNote: The UI is helpful but not required for acceptance of PF-003. If code is deferred, include only docs and a future task for `hrm_ui.py`.\n\n### Docker and containerization\n\n- Dockerfile target: `hrm-serve` (multi-stage). Example build and run on Windows PowerShell:\n\n```pwsh\n# Build\ndocker build -t aios/hrm-serve:local --target hrm-serve .\n\n# Run (CPU)\ndocker run --rm -p 8000:8000 ^\n  -e AIOS_MODEL_PATH=artifacts/brains/actv1/actv1_student.pt ^\n  -e AIOS_TOKENIZER_PATH=artifacts/hf_implant/tokenizers/base ^\n  aios/hrm-serve:local\n\n# Run (CUDA) \u2013 requires NVIDIA Container Toolkit\ndocker run --rm -p 8000:8000 --gpus all ^\n  -e AIOS_DEVICE=cuda ^\n  aios/hrm-serve:local\n```\n\n- docker-compose service snippet:\n\n```yaml\nservices:\n  hrm:\n    image: aios/hrm-serve:local\n    ports: [\"8000:8000\"]\n    environment:\n      AIOS_MODEL_PATH: artifacts/brains/actv1/actv1_student.pt\n      AIOS_TOKENIZER_PATH: artifacts/hf_implant/tokenizers/base\n      AIOS_DEVICE: cpu\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - capabilities: [gpu]\n```\n\n- Helper scripts to add:\n  - `scripts/run_hrm_server.ps1`\n  - `scripts/run_hrm_server.sh`\n\n### Baseline serving (A/B) with vLLM and TGI\n\n- vLLM (HF checkpoint):\n\n```pwsh\ndocker run --rm -p 8001:8000 ^\n  -v $PWD\\artifacts\\hf_implant\\base_model:/model ^\n  vllm/vllm-openai:latest ^\n  --model /model\n```\n\nHit using OpenAI-compatible API:\n\n```pwsh\ncurl -s http://localhost:8001/v1/completions -H \"Content-Type: application/json\" -d '{\n  \"model\":\"local\",\n  \"prompt\":\"Hello\",\n  \"max_tokens\":16\n}' | jq .\n```\n\n- Text Generation Inference (TGI):\n\n```pwsh\ndocker run --rm -p 8002:80 ^\n  -v $PWD\\artifacts\\hf_implant\\base_model:/data ^\n  ghcr.io/huggingface/text-generation-inference:latest ^\n  --model-id /data\n```\n\nHit using TGI API:\n\n```pwsh\ncurl -s http://localhost:8002/generate -H \"Content-Type: application/json\" -d '{\n  \"inputs\": \"Hello\",\n  \"parameters\": {\"max_new_tokens\": 16, \"temperature\": 0.7, \"top_p\": 0.9}\n}' | jq .\n```\n\n- Mapping to our contract\n  - Our `/generate` \u2192 vLLM `/v1/completions` or OpenAI Chat; TGI `/generate`\n  - For A/B, normalize fields: `max_tokens \u2194 max_new_tokens`, `temperature`, `top_p`\n\n### Observability and security\n\n- Logging: use `logging.yaml` if present; otherwise default to INFO with request/response IDs (omit bodies in prod logs)\n- Metrics: optional `/metrics` (Prometheus) for request counts/latency/tokens\n- Tracing: optional OpenTelemetry if configured\n- CORS: `--enable-cors` and whitelist origins\n- Auth (optional): bearer token via `Authorization: Bearer <token>`; deny when missing\n\n### Testing, acceptance, and checklists\n\nFunctional quick test (CPU):\n\n```pwsh\n# 1) Start server (example)\naios serve hrm --device cpu --port 8000 --model-path artifacts/brains/actv1/actv1_student.pt --tokenizer-path artifacts/hf_implant/tokenizers/base\n\n# 2) Generate two prompts\ncurl -s http://localhost:8000/generate -H \"Content-Type: application/json\" -d '{\n  \"prompts\": [\"Hello\", \"Once upon a time\"],\n  \"max_tokens\": 8,\n  \"temperature\": 0.7,\n  \"top_p\": 0.9\n}' | jq .\n\n# 3) Loglikelihood\ncurl -s http://localhost:8000/loglikelihood -H \"Content-Type: application/json\" -d '{\n  \"prompts\": [\"Hello\"],\n  \"continuations\": [\" world\"]\n}' | jq .\n```\n\nReadiness checklist\n\n- [ ] Server starts and `/readyz` returns ready within 60s\n- [ ] `/generate` returns completions for 2 prompts under 2s on CPU (sample model)\n- [ ] `/loglikelihood` returns values and shape matches inputs\n- [ ] Handles batch of 16 prompts without OOM; memory stable over 5 runs\n- [ ] Error cases return 400/422 with clear message\n- [ ] Logs include request IDs and timing\n- [ ] Docker image builds and runs locally\n- [ ] Optional UI launches and can call server\n- [ ] A/B against vLLM/TGI produces comparable outputs with same seeds\n\nLoad and reliability\n\n- [ ] Sustains 10 RPS with batch size 8 on CPU sample model (target; adjust per hardware)\n- [ ] Backpressure: requests rejected with 429 when queue is full (if enabled)\n- [ ] Graceful shutdown drains in-flight requests\n\n### Production runbook (starter)\n\n- Startup\n  - Verify drivers (CUDA) or plan CPU\n  - Warmup: send a 1-token request to pre-JIT kernels\n  - Confirm `/readyz` and sample `/generate`\n\n- Scaling\n  - Horizontal: run multiple replicas behind a load balancer; sticky sessions not required\n  - Vertical: tune `max_batch_size`, `max_new_tokens`, and context window to fit memory\n\n- Troubleshooting\n  - 500 at startup: verify paths for model/tokenizer; run with `--dry-run`\n  - CUDA OOM: reduce batch size or `max_new_tokens`; ensure `torch.cuda.empty_cache()` between runs if needed\n  - Slow responses: disable AMP on CPU; pin threads (`OMP_NUM_THREADS`)\n  - CORS blocked: set `--enable-cors --cors-origins *` for dev only\n\n### Triton backend (design notes, out of scope)\n\n- Shape the HRM inference as a stateless backend with request batching and token cache\n- Inputs: token IDs, attention mask, decode params; Outputs: next tokens/logprobs\n- Consider KV cache management and paged attention for large contexts\n\n---\n\n### Implementation steps\n1) HRM inference wrapper\n- Reuse tokenizer loading from `train_actv1.py` helpers.\n- Load `actv1_student.pt` and implement a simple forward for greedy/sampled decoding.\n- Add batching and `torch.no_grad()`; AMP optional.\n\n2) FastAPI app\n- Create endpoints per contract; validate inputs; return JSON.\n- Add health endpoint and basic logging.\n\n3) Packaging\n- Add new Docker target with a slim runtime (CUDA base optional).\n- Provide PowerShell and Bash helpers to run locally.\n\n4) Baseline docs\n- Document how to start vLLM/TGI for a matching HF model and how to hit similar endpoints for A/B.\n\n### Testing and acceptance criteria\n\n- Local run: Start server, send `/generate` with 2 prompts, receive 2 completions within reasonable latency on CPU/GPU.\n- Error handling: Invalid inputs return 400 with clear messages.\n- Load: Handle batch of 16 prompts without crash; memory stable.\n\n### Risks and mitigations\n\n- HRM decoding path may need custom halting logic \u2192 start with simple decode, iterate.\n- Windows GPU drivers: Recommend WSL2 or use CPU fallback for local dev.\n\n### Milestones\n\n- M1 (1\u20132 days): Minimal server + local run; sample client.\n- M2 (1 day): Docker target and docs; baseline serving notes.\n", "tags": ["cli", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "PF-003: Serving and inference integration (HRM + baselines)"}, {"line": 2, "text": "Summary"}, {"line": 6, "text": "Motivation"}, {"line": 11, "text": "Scope"}, {"line": 21, "text": "Integration points (repo)"}, {"line": 27, "text": "API contract (v1)"}, {"line": 39, "text": "Comprehensive guide and checklist"}, {"line": 43, "text": "Deliverables"}, {"line": 52, "text": "Architecture overview"}, {"line": 59, "text": "Data models and API details"}, {"line": 89, "text": "HRM inference wrapper design"}, {"line": 115, "text": "Configuration model"}, {"line": 133, "text": "CLI: serve commands"}, {"line": 145, "text": "Minimal GUI (optional) for manual testing"}, {"line": 159, "text": "Docker and containerization"}, {"line": 164, "text": "Build"}, {"line": 167, "text": "Run (CPU)"}, {"line": 173, "text": "Run (CUDA) \u2013 requires NVIDIA Container Toolkit"}, {"line": 201, "text": "Baseline serving (A/B) with vLLM and TGI"}, {"line": 244, "text": "Observability and security"}, {"line": 252, "text": "Testing, acceptance, and checklists"}, {"line": 257, "text": "1) Start server (example)"}, {"line": 260, "text": "2) Generate two prompts"}, {"line": 268, "text": "3) Loglikelihood"}, {"line": 293, "text": "Production runbook (starter)"}, {"line": 310, "text": "Triton backend (design notes, out of scope)"}, {"line": 318, "text": "Implementation steps"}, {"line": 335, "text": "Testing and acceptance criteria"}, {"line": 341, "text": "Risks and mitigations"}, {"line": 346, "text": "Milestones"}]}, {"path": "planned_features/INTEGRATION_TIMELINE_SUMMARY.md", "content": "# Integration Timeline Summary\n## Vector Stores (PF-005) + Persistent Traces/Crystallization\n\n**Created**: December 8, 2025  \n**Status**: Plans Now Compatible \u2705  \n**Purpose**: Coordination guide for parallel development\n\n---\n\n## Timeline Overview\n\n```\n                Vector Stores (PF-005)              Persistent Traces (Cognitive)\n                \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550             \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nWeek 1-2:       Core Infrastructure                 Phase 0: Infrastructure\n                - VectorStoreClient protocol        - TraceManager, RoutingPathTree\n                - Qdrant/LanceDB drivers            - Config schemas\n                - Unit tests                        - Unit tests\n                \nWeek 3-4:       Dataset Backend Integration         Phase 1: Trace Capture\n                - HF/WebDataset backends            - Attention hooks\n                - CLI/GUI wiring                    - Salience computation\n                - Integration tests                 - Memory profiling\n\nWeek 5-6:       Production Hardening                Phase 2: Bias Injection\n                - Error handling                    - Sparse \u2192 dense conversion\n                - Documentation                     - Dual-mode attention\n                - PowerShell examples               - Trace visualization\n\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\nWeek 6.5-7.5: \u2502  INTEGRATION PHASE (Both teams collaborate)          \u2502\n              \u2502  Prerequisites: Both foundations complete             \u2502\n              \u2502                                                       \u2502\n              \u2502  - TraceVectorStore wrapper                          \u2502\n              \u2502  - MotifVectorStore wrapper                          \u2502\n              \u2502  - TraceEmbedder implementation                      \u2502\n              \u2502  - MotifEmbedder implementation                      \u2502\n              \u2502  - Sync/load protocols                               \u2502\n              \u2502  - Integration tests                                 \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nWeek 7-8:                                           Phase 3: Routing Path Logging\n                                                    - TopKRouter hooks\n                                                    - Suffix tree building\n                                                    - Path visualization\n\nWeek 8:         Deployment & Monitoring             (continues independently)\n                - Production deployment\n                - Monitoring dashboard\n                - Backup/restore scripts\n\nWeek 9-11:                                          Phase 4: Crystallization\n                                                    - Motif freezing\n                                                    - Distillation\n                                                    - Pruning\n\nWeek 12-13:                                         Phase 5: Auxiliary Losses\n                                                    - Trace utilization loss\n                                                    - Crystallization entropy\n                                                    - Hyperparameter tuning\n\nWeek 14-16:                                         Phase 6: Evaluation\n                                                    - Benchmarks\n                                                    - FLOP measurements\n                                                    - Language analysis\n\nWeek 17-18:                                         Phase 7: Production Hardening\n                                                    - Distributed training\n                                                    - OOM safeguards\n                                                    - User documentation\n```\n\n---\n\n## Critical Dependencies\n\n### Week 2 Milestone\n**Deliverable**: `VectorStoreClient` interface finalized  \n**Consumers**:\n- TraceVectorStore (Week 6.5)\n- MotifVectorStore (Week 6.5)\n\n**Interface contract**:\n```python\nclass VectorStoreClient:\n    def upsert(self, ids: Sequence[str], \n               vectors: Sequence[Sequence[float]], \n               metadata: Optional[Sequence[Dict[str, Any]]]) -> None\n    \n    def query(self, vector: Sequence[float], \n              top_k: int, \n              filter: Optional[Dict[str, Any]]) -> List[Tuple[str, float, Dict]]\n    \n    def delete(self, ids: Sequence[str]) -> None\n    def close(self) -> None\n```\n\n### Week 4 Milestone\n**Deliverable**: Qdrant or LanceDB deployable  \n**Requirement**: At least one backend fully functional for integration testing\n\n### Week 6 Checkpoint\n**Vector Stores Team**: All core features complete, ready for cognitive integration  \n**Persistent Traces Team**: Phases 0-2 complete, TraceManager ready for vector persistence\n\n### Week 6.5 Integration Kickoff\n**Joint Deliverable**: TraceVectorStore and MotifVectorStore working with both backends\n\n---\n\n## Configuration Compatibility\n\nBoth plans now share unified `memory:` namespace in `config/default.yaml`:\n\n```yaml\nmemory:\n  dataset:          # PF-005 dataset backends\n  vector_store:     # PF-005 storage backend\n  persistent_traces:  # Cognitive memory (with vector_store integration flags)\n  semantic_crystallization:  # Cognitive memory (with vector_store integration flags)\n```\n\n**No conflicts**: Each subsystem has dedicated namespace under `memory:`.\n\n---\n\n## Module Dependencies\n\n```\nsrc/aios/memory/\n\u251c\u2500\u2500 vector_store.py              \u2190 PF-005 (Week 1-2)\n\u2514\u2500\u2500 vector_stores/\n    \u251c\u2500\u2500 qdrant.py                \u2190 PF-005 (Week 1-2)\n    \u2514\u2500\u2500 lancedb.py               \u2190 PF-005 (Week 1-2)\n\nsrc/aios/core/hrm_models/cognitive/\n\u251c\u2500\u2500 trace_manager.py             \u2190 Cognitive (Week 3-4)\n\u251c\u2500\u2500 routing_tree.py              \u2190 Cognitive (Week 1-2)\n\u251c\u2500\u2500 embedders.py                 \u2190 Integration (Week 6.5-7.5)\n\u2514\u2500\u2500 vector_wrappers.py           \u2190 Integration (Week 6.5-7.5)\n    \u251c\u2500\u2500 TraceVectorStore         (depends on memory.vector_store)\n    \u2514\u2500\u2500 MotifVectorStore         (depends on memory.vector_store)\n\nsrc/aios/cli/hrm_hf/data_backends/\n\u251c\u2500\u2500 base.py                      \u2190 PF-005 (Week 3-4)\n\u251c\u2500\u2500 custom.py                    \u2190 PF-005 (Week 3-4)\n\u251c\u2500\u2500 hf.py                        \u2190 PF-005 (Week 3-4)\n\u2514\u2500\u2500 webdataset.py                \u2190 PF-005 (Week 3-4)\n```\n\n**Import flow**:\n- `cognitive/vector_wrappers.py` imports `memory/vector_store.py` \u2705\n- `cognitive/trace_manager.py` imports `cognitive/vector_wrappers.py` (conditionally) \u2705\n- No circular dependencies \u2705\n\n---\n\n## Integration Testing Strategy\n\n### Week 6.5: Smoke Tests\n1. **Trace persistence cycle**:\n   - Train 1000 steps with traces enabled\n   - TraceManager syncs to Qdrant\n   - Restart training, load traces from Qdrant\n   - Verify salience values within 1% error\n\n2. **Motif storage test**:\n   - Crystallize 10 motifs during training\n   - Auto-save to vector store\n   - Query similar motifs by task tag\n   - Verify retrieval accuracy\n\n### Week 7: Cross-Backend Tests\n- Same tests with LanceDB backend\n- Verify both Qdrant and LanceDB produce identical results\n\n### Week 7.5: Stress Tests\n- Persist 100K traces, measure sync latency\n- Query 10K motifs, measure retrieval speed\n- Verify memory overhead < 40 MB (30 MB traces + 5 MB embedders + 5 MB overhead)\n\n---\n\n## Success Criteria\n\n### PF-005 Standalone Success\n- \u2705 HF streaming trains 10 steps on wikitext\n- \u2705 WebDataset trains 10 steps from tar shards\n- \u2705 Qdrant upserts 1000 vectors, queries return correct top-5\n- \u2705 LanceDB passes same tests as Qdrant\n\n### Persistent Traces Standalone Success\n- \u2705 TraceManager captures high-salience attention edges\n- \u2705 Bias injection improves convergence on copy tasks\n- \u2705 Memory overhead < 30 MB\n- \u2705 Training slowdown < 10%\n\n### Integration Success\n- \u2705 TraceVectorStore persists 10K traces with < 1% information loss\n- \u2705 MotifVectorStore retrieves similar motifs with > 0.8 cosine similarity\n- \u2705 Works with both Qdrant and LanceDB\n- \u2705 Disabling vector_store gracefully falls back to RAM-only mode\n- \u2705 Configuration validation prevents invalid states\n\n---\n\n## Risk Mitigation\n\n### Risk 1: Timeline Slippage\n**Scenario**: PF-005 Week 1-2 delayed, pushes integration to Week 7.5+  \n**Mitigation**: \n- Persistent Traces continues independently through Phase 3\n- Integration phase can slide to Week 8 with minimal impact\n- Core features work without integration\n\n### Risk 2: Interface Changes\n**Scenario**: `VectorStoreClient` API changes after Week 2  \n**Mitigation**:\n- Freeze interface by Week 2 (strict contract)\n- Any changes require approval from both teams\n- Wrapper classes (`TraceVectorStore`) insulate from minor changes\n\n### Risk 3: Backend Incompatibility\n**Scenario**: Qdrant works but LanceDB has issues  \n**Mitigation**:\n- Integration phase targets Qdrant first\n- LanceDB support can be delayed to Week 8\n- Document Qdrant as recommended backend\n\n---\n\n## Communication Protocol\n\n### Weekly Sync (Weeks 1-6)\n**Purpose**: Coordinate interface design, share progress  \n**Attendees**: PF-005 lead + Cognitive lead  \n**Agenda**: \n- Interface changes\n- Timeline status\n- Blockers\n\n### Integration Sprint (Week 6.5-7.5)\n**Purpose**: Joint implementation  \n**Attendees**: Both teams  \n**Deliverables**:\n- TraceVectorStore, MotifVectorStore\n- Integration tests\n- Documentation\n\n### Handoff (Week 8)\n**Purpose**: Transition to maintenance  \n**Deliverables**:\n- Integration documentation\n- Troubleshooting guide\n- Performance benchmarks\n\n---\n\n## Document Cross-References\n\n| Document | Section | Content |\n|----------|---------|---------|\n| `data-backends-vector-stores.md` | \u00a7 Cognitive Memory Integration | TraceVectorStore, MotifVectorStore specs |\n| `data-backends-vector-stores.md` | \u00a7 Implementation Roadmap | Week 1-8 timeline |\n| `data-backends-vector-stores.md` | \u00a7 Unified Configuration Schema | Full `memory:` config |\n| `PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md` | \u00a7 Vector Store Integration | Embedding specs, sync protocols |\n| `PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md` | \u00a7 Phase 2.5 | Integration phase deliverables |\n| `PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md` | Configuration section | Vector store integration flags |\n\n---\n\n## Conclusion\n\n**Both plans are now fully compatible** \u2705\n\n**Key achievements**:\n1. \u2705 Unified `memory:` configuration namespace prevents conflicts\n2. \u2705 Clear module separation with explicit integration points\n3. \u2705 Coordinated timeline with joint integration phase (Week 6.5-7.5)\n4. \u2705 Optional integration - systems work standalone or together\n5. \u2705 Cross-references ensure both teams stay aligned\n6. \u2705 Shared schema, parallel development, clean handoff\n\n**Implementation paths**:\n- **Path A** (PF-005 only): 6 weeks \u2192 Dataset backends + vector stores\n- **Path B** (Persistent Traces only): 18 weeks \u2192 Cognitive memory (RAM-only)\n- **Path C** (Full integration): 8 weeks \u2192 Both systems + integration \u2192 Then continue Persistent Traces Phases 3-7 (10 more weeks)\n\n**Recommendation**: \nStart both plans in parallel (Weeks 1-6), then evaluate integration ROI at Week 6 checkpoint. If cognitive memory shows promise, proceed with Week 6.5-7.5 integration. If not, each system remains valuable independently.\n\n---\n\n**Status**: Ready for implementation \u2705  \n**Last Updated**: December 8, 2025  \n**Owners**: PF-005 Team + Cognitive Architecture Team\n", "tags": ["cli", "datasets", "gui"], "headings": [{"line": 0, "text": "Integration Timeline Summary"}, {"line": 1, "text": "Vector Stores (PF-005) + Persistent Traces/Crystallization"}, {"line": 9, "text": "Timeline Overview"}, {"line": 75, "text": "Critical Dependencies"}, {"line": 77, "text": "Week 2 Milestone"}, {"line": 98, "text": "Week 4 Milestone"}, {"line": 102, "text": "Week 6 Checkpoint"}, {"line": 106, "text": "Week 6.5 Integration Kickoff"}, {"line": 111, "text": "Configuration Compatibility"}, {"line": 127, "text": "Module Dependencies"}, {"line": 158, "text": "Integration Testing Strategy"}, {"line": 160, "text": "Week 6.5: Smoke Tests"}, {"line": 173, "text": "Week 7: Cross-Backend Tests"}, {"line": 177, "text": "Week 7.5: Stress Tests"}, {"line": 184, "text": "Success Criteria"}, {"line": 186, "text": "PF-005 Standalone Success"}, {"line": 192, "text": "Persistent Traces Standalone Success"}, {"line": 198, "text": "Integration Success"}, {"line": 207, "text": "Risk Mitigation"}, {"line": 209, "text": "Risk 1: Timeline Slippage"}, {"line": 216, "text": "Risk 2: Interface Changes"}, {"line": 223, "text": "Risk 3: Backend Incompatibility"}, {"line": 232, "text": "Communication Protocol"}, {"line": 234, "text": "Weekly Sync (Weeks 1-6)"}, {"line": 242, "text": "Integration Sprint (Week 6.5-7.5)"}, {"line": 250, "text": "Handoff (Week 8)"}, {"line": 259, "text": "Document Cross-References"}, {"line": 272, "text": "Conclusion"}]}, {"path": "planned_features/INTERNATIONALIZATION_LOCALIZATION.md", "content": "# Internationalization and Localization (i18n/L10n)\n\n**Status:** \ud83d\udccb Planned  \n**Priority:** Medium  \n**Category:** User Experience & Accessibility  \n**Created:** November 9, 2025  \n**Target Languages:** Spanish, Portuguese, French, German, Italian, Chinese, Japanese, Arabic, Hindi\n\n---\n\n## Overview\n\nImplement comprehensive internationalization (i18n) and localization (L10n) support to make AI-OS accessible to non-English speaking users worldwide. This includes translating the GUI, CLI, and documentation into 9 additional languages.\n\n**Current State:** English-only hardcoded strings throughout codebase  \n**Goal:** Multi-language support with runtime language selection and culturally appropriate formatting\n\n---\n\n## Motivation\n\n### Business Case\n- **Market Expansion:** Enable adoption in non-English speaking markets\n- **Accessibility:** Remove language barriers for international developers and researchers\n- **Community Growth:** Foster global contributor community\n- **Competitive Advantage:** Most AI/ML tools remain English-centric\n\n### Target Language Justification\n1. **Spanish (es_ES)** - 500M+ speakers, Latin America & Spain markets\n2. **Portuguese (pt_BR)** - 250M+ speakers, growing Brazilian tech sector\n3. **French (fr_FR)** - 300M+ speakers, European & African markets\n4. **German (de_DE)** - 100M+ speakers, strong German engineering community\n5. **Italian (it_IT)** - 65M+ speakers, Italian research institutions\n6. **Chinese (zh_CN)** - 1.4B+ speakers, massive Chinese AI/ML community\n7. **Japanese (ja_JP)** - 125M+ speakers, advanced Japanese tech sector\n8. **Arabic (ar_SA)** - 400M+ speakers, growing Middle East tech markets\n9. **Hindi (hi_IN)** - 600M+ speakers, booming Indian tech sector\n\n---\n\n## Technical Scope\n\n### Components Requiring Localization\n\n#### 1. GUI (Tkinter Interface)\n**Files Affected:** ~50-60 Python files in `src/aios/gui/`\n\n**String Categories:**\n- Tab names (Chat, Brains, Datasets, HRM Training, Evaluation, Resources, MCP & Tools, Settings, Debug, Help)\n- Button labels (Add Goal, Export CSV, Export JSON, Load Brain, Start Training, etc.)\n- Dialog titles and messages (Checkpoint Found, Resume Training, Error, Success, etc.)\n- Status messages (Ready, Loading, Processing, Complete, etc.)\n- Tooltips and help text\n- Error messages and warnings\n- Form field labels\n- Tree/table column headers\n- Menu items\n\n**Estimated String Count:** ~800-1000 unique strings\n\n**Key Files:**\n```\nsrc/aios/gui/app/ui_setup.py\nsrc/aios/gui/components/brains_panel/\nsrc/aios/gui/components/chat_panel.py\nsrc/aios/gui/components/datasets_panel/\nsrc/aios/gui/components/hrm_training/\nsrc/aios/gui/components/evaluation_panel/\nsrc/aios/gui/components/resources_panel/\nsrc/aios/gui/components/mcp_panel/\nsrc/aios/gui/components/settings_panel/\nsrc/aios/gui/components/debug_panel.py\nsrc/aios/gui/dialogs/\n```\n\n#### 2. CLI (Command-Line Interface)\n**Files Affected:** ~15-20 Python files in `src/aios/cli/`\n\n**String Categories:**\n- Command descriptions and help text\n- Argument/option descriptions\n- Error messages\n- Success/status messages\n- Interactive prompts (in `core_cli.py ui()` function)\n- Progress indicators\n- Table headers and formatted output\n\n**Estimated String Count:** ~500-700 unique strings\n\n**Key Files:**\n```\nsrc/aios/cli/aios.py\nsrc/aios/cli/core_cli.py\nsrc/aios/cli/hrm_cli.py\nsrc/aios/cli/hrm_hf_cli.py\nsrc/aios/cli/eval_cli.py\nsrc/aios/cli/datasets/\nsrc/aios/cli/optimization_cli.py\nsrc/aios/cli/modelcard_cli.py\n```\n\n#### 3. Documentation\n**Files Affected:** All markdown files in `docs/`\n\n**Content:**\n- README.md\n- Installation guides\n- User guides\n- API documentation\n- Contributing guidelines\n\n**Estimated Page Count:** ~50-100 documentation pages\n\n**Strategy:** Create separate language subdirectories:\n```\ndocs/\n  \u251c\u2500\u2500 en/  (English - default)\n  \u251c\u2500\u2500 es/  (Spanish)\n  \u251c\u2500\u2500 pt/  (Portuguese)\n  \u251c\u2500\u2500 fr/  (French)\n  \u251c\u2500\u2500 de/  (German)\n  \u251c\u2500\u2500 it/  (Italian)\n  \u251c\u2500\u2500 zh/  (Chinese)\n  \u251c\u2500\u2500 ja/  (Japanese)\n  \u251c\u2500\u2500 ar/  (Arabic)\n  \u2514\u2500\u2500 hi/  (Hindi)\n```\n\n---\n\n## Implementation Plan\n\n### Phase 1: Infrastructure Setup (Week 1-2)\n\n#### 1.1 Choose i18n Framework\n**Recommended:** Python `gettext` + `babel` for compilation\n\n**Rationale:**\n- Standard Python i18n solution\n- Excellent tooling (pybabel)\n- Wide community support\n- Works well with both GUI (Tkinter) and CLI (Typer)\n\n**Dependencies to Add:**\n```toml\n[project.optional-dependencies]\ni18n = [\n  \"babel>=2.14.0\",     # i18n utilities and message extraction\n  \"polib>=1.2.0\",      # .po file manipulation library\n]\n```\n\n#### 1.2 Create Directory Structure\n```\nsrc/aios/\n  \u251c\u2500\u2500 i18n/\n  \u2502   \u251c\u2500\u2500 __init__.py           # i18n initialization and utilities\n  \u2502   \u251c\u2500\u2500 locale_manager.py     # Runtime locale management\n  \u2502   \u2514\u2500\u2500 extract.cfg           # Babel extraction configuration\n  \u2514\u2500\u2500 locales/\n      \u251c\u2500\u2500 en_US/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u2502       \u251c\u2500\u2500 gui.po\n      \u2502       \u251c\u2500\u2500 cli.po\n      \u2502       \u2514\u2500\u2500 messages.po\n      \u251c\u2500\u2500 es_ES/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u251c\u2500\u2500 pt_BR/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u251c\u2500\u2500 fr_FR/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u251c\u2500\u2500 de_DE/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u251c\u2500\u2500 it_IT/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u251c\u2500\u2500 zh_CN/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u251c\u2500\u2500 ja_JP/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u251c\u2500\u2500 ar_SA/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u2514\u2500\u2500 hi_IN/\n          \u2514\u2500\u2500 LC_MESSAGES/\n```\n\n#### 1.3 Create i18n Utilities Module\n\n**File:** `src/aios/i18n/__init__.py`\n\n```python\n\"\"\"Internationalization support for AI-OS.\"\"\"\n\nfrom __future__ import annotations\n\nimport gettext\nimport locale\nimport os\nfrom pathlib import Path\nfrom typing import Optional\n\n# Default locale\nDEFAULT_LOCALE = \"en_US\"\n\n# Supported locales\nSUPPORTED_LOCALES = {\n    \"en_US\": \"English (United States)\",\n    \"es_ES\": \"Espa\u00f1ol (Espa\u00f1a)\",\n    \"pt_BR\": \"Portugu\u00eas (Brasil)\",\n    \"fr_FR\": \"Fran\u00e7ais (France)\",\n    \"de_DE\": \"Deutsch (Deutschland)\",\n    \"it_IT\": \"Italiano (Italia)\",\n    \"zh_CN\": \"\u4e2d\u6587 (\u7b80\u4f53)\",\n    \"ja_JP\": \"\u65e5\u672c\u8a9e (\u65e5\u672c)\",\n    \"ar_SA\": \"\u0627\u0644\u0639\u0631\u0628\u064a\u0629 (\u0627\u0644\u0633\u0639\u0648\u062f\u064a\u0629)\",\n    \"hi_IN\": \"\u0939\u093f\u0928\u094d\u0926\u0940 (\u092d\u093e\u0930\u0924)\",\n}\n\n# Global translation function\n_translate = None\n_current_locale = DEFAULT_LOCALE\n\n\ndef init_i18n(locale_code: Optional[str] = None) -> None:\n    \"\"\"Initialize i18n system with specified locale.\n    \n    Args:\n        locale_code: Locale code (e.g., 'es_ES'). If None, uses system locale.\n    \"\"\"\n    global _translate, _current_locale\n    \n    if locale_code is None:\n        # Try to detect system locale\n        try:\n            sys_locale = locale.getdefaultlocale()[0]\n            locale_code = sys_locale if sys_locale in SUPPORTED_LOCALES else DEFAULT_LOCALE\n        except Exception:\n            locale_code = DEFAULT_LOCALE\n    \n    # Validate locale\n    if locale_code not in SUPPORTED_LOCALES:\n        locale_code = DEFAULT_LOCALE\n    \n    _current_locale = locale_code\n    \n    # Set up gettext\n    locale_dir = Path(__file__).parent.parent / \"locales\"\n    \n    try:\n        translation = gettext.translation(\n            \"messages\",\n            localedir=str(locale_dir),\n            languages=[locale_code],\n            fallback=True\n        )\n        _translate = translation.gettext\n    except Exception:\n        # Fallback to no-op translation\n        _translate = lambda s: s\n\n\ndef _(message: str) -> str:\n    \"\"\"Translate a message to the current locale.\n    \n    Args:\n        message: Message to translate\n        \n    Returns:\n        Translated message\n    \"\"\"\n    if _translate is None:\n        init_i18n()\n    return _translate(message)\n\n\ndef get_current_locale() -> str:\n    \"\"\"Get the current locale code.\"\"\"\n    return _current_locale\n\n\ndef get_supported_locales() -> dict[str, str]:\n    \"\"\"Get dict of supported locale codes to display names.\"\"\"\n    return SUPPORTED_LOCALES.copy()\n\n\ndef set_locale(locale_code: str) -> bool:\n    \"\"\"Set the current locale.\n    \n    Args:\n        locale_code: Locale code to set\n        \n    Returns:\n        True if successful, False otherwise\n    \"\"\"\n    if locale_code not in SUPPORTED_LOCALES:\n        return False\n    \n    init_i18n(locale_code)\n    return True\n```\n\n#### 1.4 Create Babel Configuration\n\n**File:** `src/aios/i18n/extract.cfg`\n\n```ini\n[python: **.py]\nencoding = utf-8\n\n[javascript: **.js]\nencoding = utf-8\n```\n\n#### 1.5 Create Extraction Script\n\n**File:** `scripts/extract_translations.py`\n\n```python\n#!/usr/bin/env python3\n\"\"\"Extract translatable strings from source code.\"\"\"\n\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef main():\n    \"\"\"Extract strings and create .pot template.\"\"\"\n    project_root = Path(__file__).parent.parent\n    src_dir = project_root / \"src\" / \"aios\"\n    locale_dir = src_dir / \"locales\"\n    pot_file = locale_dir / \"messages.pot\"\n    \n    # Ensure locale directory exists\n    locale_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract strings\n    cmd = [\n        \"pybabel\",\n        \"extract\",\n        \"-F\", str(src_dir / \"i18n\" / \"extract.cfg\"),\n        \"-o\", str(pot_file),\n        \"-k\", \"_\",  # Translation function name\n        \"--project=AI-OS\",\n        \"--version=1.0.0\",\n        \"--copyright-holder=Wulfic\",\n        str(src_dir),\n    ]\n    \n    print(f\"Extracting translatable strings...\")\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    \n    if result.returncode == 0:\n        print(f\"\u2713 Extracted to {pot_file}\")\n        print(f\"  {result.stdout.strip()}\")\n    else:\n        print(f\"\u2717 Extraction failed:\")\n        print(result.stderr)\n        return 1\n    \n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n```\n\n#### 1.6 Create Compilation Script\n\n**File:** `scripts/compile_translations.py`\n\n```python\n#!/usr/bin/env python3\n\"\"\"Compile .po files to .mo files for runtime use.\"\"\"\n\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef main():\n    \"\"\"Compile all .po files to .mo files.\"\"\"\n    project_root = Path(__file__).parent.parent\n    locale_dir = project_root / \"src\" / \"aios\" / \"locales\"\n    \n    compiled_count = 0\n    error_count = 0\n    \n    # Find all .po files\n    for po_file in locale_dir.rglob(\"*.po\"):\n        mo_file = po_file.with_suffix(\".mo\")\n        \n        cmd = [\n            \"pybabel\",\n            \"compile\",\n            \"-i\", str(po_file),\n            \"-o\", str(mo_file),\n        ]\n        \n        print(f\"Compiling {po_file.relative_to(project_root)}...\")\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        \n        if result.returncode == 0:\n            compiled_count += 1\n        else:\n            error_count += 1\n            print(f\"  \u2717 Error: {result.stderr}\")\n    \n    print(f\"\\n\u2713 Compiled {compiled_count} translation(s)\")\n    if error_count > 0:\n        print(f\"\u2717 {error_count} error(s)\")\n        return 1\n    \n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n```\n\n---\n\n### Phase 2: Code Refactoring (Week 3-5)\n\n#### 2.1 GUI Refactoring Strategy\n\n**Pattern:**\n```python\n# Before\nttk.Label(frame, text=\"Model:\").pack()\nttk.Button(frame, text=\"Export CSV\", command=callback)\nself.status_label = ttk.Label(frame, text=\"Ready\")\n\n# After\nfrom aios.i18n import _\nttk.Label(frame, text=_(\"Model:\")).pack()\nttk.Button(frame, text=_(\"Export CSV\"), command=callback)\nself.status_label = ttk.Label(frame, text=_(\"Ready\"))\n```\n\n**Files to Refactor (Priority Order):**\n1. \u2705 Main UI structure (`src/aios/gui/app/ui_setup.py`)\n2. \u2705 Brains panel (`src/aios/gui/components/brains_panel/`)\n3. \u2705 Datasets panel (`src/aios/gui/components/datasets_panel/`)\n4. \u2705 HRM Training panel (`src/aios/gui/components/hrm_training/`)\n5. \u2705 Evaluation panel (`src/aios/gui/components/evaluation_panel/`)\n6. \u2705 Chat panel (`src/aios/gui/components/chat_panel.py`)\n7. \u2705 Settings panel (`src/aios/gui/components/settings_panel/`)\n8. \u2705 All dialogs (`src/aios/gui/dialogs/`)\n9. \u2705 Status bar and tooltips\n\n**Gotchas:**\n- Dynamic strings with formatting: Use `_(\"Score: {score}\").format(score=value)`\n- Pluralization: Use `ngettext()` for singular/plural forms\n- Tooltips: Extract to separate translation calls\n\n#### 2.2 CLI Refactoring Strategy\n\n**Pattern:**\n```python\n# Before\n@app.command(\"train\")\ndef train_command(\n    model: str = typer.Option(..., \"--model\", help=\"Model name or path\"),\n):\n    \"\"\"Train a model.\"\"\"\n    print(\"Training started...\")\n\n# After\nfrom aios.i18n import _\n\n@app.command(\"train\")\ndef train_command(\n    model: str = typer.Option(..., \"--model\", help=_(\"Model name or path\")),\n):\n    \"\"\"Train a model.\"\"\"  # Docstrings extracted separately\n    print(_(\"Training started...\"))\n```\n\n**Special Considerations for CLI:**\n- Help text translation affects `--help` output\n- Rich formatted output (tables, progress bars)\n- Error messages need careful context\n- Interactive prompts in `ui()` function\n\n#### 2.3 String Extraction Guidelines\n\n**DO:**\n- \u2705 Extract user-facing messages\n- \u2705 Extract button/label text\n- \u2705 Extract error messages\n- \u2705 Extract help text\n- \u2705 Use context comments for ambiguous strings\n\n**DON'T:**\n- \u274c Translate log messages (keep English for debugging)\n- \u274c Translate internal identifiers\n- \u274c Translate file paths or system commands\n- \u274c Translate variable names\n\n**Context Comments:**\n```python\n# Translator comment for clarity\n# Translators: This appears in the training progress dialog\nlabel.config(text=_(\"Training in progress...\"))\n```\n\n---\n\n### Phase 3: Translation File Generation (Week 6)\n\n#### 3.1 Extract Strings\n```bash\npython scripts/extract_translations.py\n```\n\nThis creates `src/aios/locales/messages.pot` template file.\n\n#### 3.2 Initialize Language Files\n\nFor each target language:\n```bash\npybabel init -i src/aios/locales/messages.pot \\\n             -d src/aios/locales \\\n             -l es_ES\n\npybabel init -i src/aios/locales/messages.pot \\\n             -d src/aios/locales \\\n             -l pt_BR\n\n# ... repeat for all languages\n```\n\nThis creates `.po` files with message IDs (msgid) and empty translations (msgstr).\n\n#### 3.3 Translation File Structure\n\n**Example: `src/aios/locales/es_ES/LC_MESSAGES/messages.po`**\n\n```po\n# Spanish translations for AI-OS\n# Copyright (C) 2025 Wulfic\n# This file is distributed under the same license as the AI-OS package.\n\nmsgid \"\"\nmsgstr \"\"\n\"Project-Id-Version: AI-OS 1.0.0\\n\"\n\"Report-Msgid-Bugs-To: \\n\"\n\"POT-Creation-Date: 2025-11-09 10:00+0000\\n\"\n\"Language: es_ES\\n\"\n\"MIME-Version: 1.0\\n\"\n\"Content-Type: text/plain; charset=UTF-8\\n\"\n\"Content-Transfer-Encoding: 8bit\\n\"\n\n#: src/aios/gui/app/ui_setup.py:63\nmsgid \"Chat\"\nmsgstr \"Chat\"\n\n#: src/aios/gui/app/ui_setup.py:64\nmsgid \"Brains\"\nmsgstr \"Cerebros\"\n\n#: src/aios/gui/app/ui_setup.py:65\nmsgid \"Datasets\"\nmsgstr \"Conjuntos de datos\"\n\n#: src/aios/gui/app/ui_setup.py:66\nmsgid \"HRM Training\"\nmsgstr \"Entrenamiento HRM\"\n\n#: src/aios/gui/components/brains_panel/panel_main.py:123\nmsgid \"Load Brain\"\nmsgstr \"Cargar cerebro\"\n\n#: src/aios/gui/components/brains_panel/panel_main.py:145\nmsgid \"Model:\"\nmsgstr \"Modelo:\"\n\n#: src/aios/gui/components/brains_panel/panel_main.py:201\nmsgid \"Ready\"\nmsgstr \"Listo\"\n```\n\n---\n\n### Phase 4: Translation Work (Week 7-18)\n\n#### 4.1 Translation Approaches\n\n**Option A: Professional Translation Service**\n- **Cost:** $0.10-0.30 per word \u00d7 ~10,000 words = $1,000-3,000 per language\n- **Timeline:** 1-2 weeks per language\n- **Quality:** High, professional\n- **Services:** Gengo, OneHourTranslation, Smartling\n\n**Option B: Community Translation**\n- **Cost:** Free (contributor time)\n- **Timeline:** 2-4 months per language (variable)\n- **Quality:** Variable, requires review\n- **Platform:** Crowdin, Weblate, or GitHub-based workflow\n\n**Option C: AI-Assisted + Human Review**\n- **Cost:** Low ($100-500 per language for review)\n- **Timeline:** 2-4 weeks per language\n- **Quality:** Good with proper review\n- **Process:**\n  1. Use GPT-4/Claude for initial translation\n  2. Native speaker review and correction\n  3. Context validation\n\n**Recommended:** Option C for speed and cost-effectiveness\n\n#### 4.2 Translation Priority Order\n\n**Tier 1 (Weeks 7-10):** Western European languages\n1. Spanish (es_ES) - Week 7\n2. French (fr_FR) - Week 8\n3. Portuguese (pt_BR) - Week 9\n4. German (de_DE) - Week 10\n5. Italian (it_IT) - Week 10\n\n**Tier 2 (Weeks 11-14):** East Asian languages\n6. Chinese Simplified (zh_CN) - Week 11-12\n7. Japanese (ja_JP) - Week 13-14\n\n**Tier 3 (Weeks 15-18):** Complex scripts\n8. Arabic (ar_SA) - Week 15-16\n9. Hindi (hi_IN) - Week 17-18\n\n#### 4.3 Translation Quality Checklist\n\nFor each language:\n- [ ] All strings translated (no empty msgstr)\n- [ ] Technical terminology consistent\n- [ ] Proper capitalization and punctuation\n- [ ] Formatting placeholders preserved (`{0}`, `%s`, etc.)\n- [ ] Plural forms correctly implemented\n- [ ] Native speaker reviewed\n- [ ] Context-appropriate tone (formal vs. informal)\n- [ ] UI tested with translations loaded\n\n#### 4.4 Special Translation Considerations\n\n**German:**\n- Compound words are longer (30-40% more space)\n- Example: \"Training progress\" \u2192 \"Trainingsfortschritt\"\n- May need to adjust widget widths\n\n**Chinese/Japanese:**\n- No spaces between words\n- Vertical text support (not needed for this app)\n- Font requirements: Need CJK-compatible fonts\n\n**Arabic:**\n- Right-to-left (RTL) text direction\n- Requires significant UI layout changes\n- Numbers may be displayed left-to-right within RTL text\n- Consider deferring to later phase\n\n**Hindi:**\n- Devanagari script\n- Font rendering support needed\n- May need line-height adjustments\n\n---\n\n### Phase 5: UI Layout Adjustments (Week 19-20)\n\n#### 5.1 Dynamic Widget Sizing\n\n**Problem:** Different languages have different text lengths\n\n**Solution:** Use dynamic sizing instead of fixed widths\n\n```python\n# Before\nentry = ttk.Entry(frame, width=20)\n\n# After\nentry = ttk.Entry(frame)  # Let it size naturally\nentry.pack(fill=\"x\", expand=True)\n```\n\n#### 5.2 Text Overflow Handling\n\n**Strategies:**\n- Use `wraplength` for labels that might be long\n- Add horizontal scrollbars where appropriate\n- Increase minimum window size if needed\n- Use tooltips for truncated text\n\n```python\nlabel = ttk.Label(\n    frame,\n    text=_(\"Very long description text...\"),\n    wraplength=400  # Wrap at 400 pixels\n)\n```\n\n#### 5.3 RTL Support (Arabic)\n\n**Challenges:**\n- Tkinter has limited RTL support\n- May need custom RTL-aware widgets\n- Consider using `pack(side=\"right\")` for Arabic layout\n\n**Decision:** Phase 1 implementation will be LTR-only. RTL support deferred to Phase 2.\n\n#### 5.4 Font Support\n\n**Ensure proper fonts installed:**\n- Windows: System fonts usually sufficient\n- Linux: May need to install language packs\n  ```bash\n  # For CJK\n  sudo apt-get install fonts-noto-cjk\n  \n  # For Arabic\n  sudo apt-get install fonts-noto-nastaliq-urdu\n  \n  # For Hindi\n  sudo apt-get install fonts-noto-devanagari\n  ```\n\n---\n\n### Phase 6: Configuration and Runtime Selection (Week 21)\n\n#### 6.1 Add Locale Configuration\n\n**File:** `config/default.yaml`\n\n```yaml\n# Internationalization settings\ni18n:\n  # Default locale (auto-detected if not set)\n  locale: null  # Options: en_US, es_ES, pt_BR, fr_FR, de_DE, it_IT, zh_CN, ja_JP, ar_SA, hi_IN\n  \n  # Fallback locale if selected locale unavailable\n  fallback_locale: en_US\n```\n\n#### 6.2 GUI Language Selector\n\nAdd to Settings panel:\n\n```python\n# In src/aios/gui/components/settings_panel/panel_main.py\n\nfrom aios.i18n import _, get_supported_locales, get_current_locale\n\nclass SettingsPanel:\n    def __init__(self, ...):\n        # ... existing code ...\n        \n        # Language selection\n        lang_frame = ttk.LabelFrame(self, text=_(\"Language\"), padding=10)\n        lang_frame.pack(fill=\"x\", padx=10, pady=5)\n        \n        ttk.Label(lang_frame, text=_(\"Interface Language:\")).pack(anchor=\"w\")\n        \n        self.locale_var = tk.StringVar(value=get_current_locale())\n        locale_combo = ttk.Combobox(\n            lang_frame,\n            textvariable=self.locale_var,\n            values=list(get_supported_locales().keys()),\n            state=\"readonly\"\n        )\n        locale_combo.pack(fill=\"x\", pady=5)\n        \n        ttk.Button(\n            lang_frame,\n            text=_(\"Apply Language (Requires Restart)\"),\n            command=self._on_language_change\n        ).pack()\n        \n        self.locale_status = ttk.Label(lang_frame, text=\"\", foreground=\"blue\")\n        self.locale_status.pack()\n    \n    def _on_language_change(self):\n        \"\"\"Handle language change.\"\"\"\n        new_locale = self.locale_var.get()\n        \n        # Save to config\n        # ... save logic ...\n        \n        # Show restart message\n        self.locale_status.config(\n            text=_(\"Language will change after restart\"),\n            foreground=\"orange\"\n        )\n```\n\n#### 6.3 CLI Language Selection\n\n```bash\n# Set via environment variable\nexport AIOS_LOCALE=es_ES\naios gui\n\n# Or via command line flag\naios --locale es_ES gui\n\n# Or set in config file\naios gui  # Uses config/default.yaml setting\n```\n\n---\n\n### Phase 7: Testing and QA (Week 22-24)\n\n#### 7.1 Automated Testing\n\n**Test Coverage:**\n- [ ] i18n initialization works for all locales\n- [ ] Translation fallback works (missing translations \u2192 English)\n- [ ] String formatting with placeholders works\n- [ ] Plural forms work correctly\n- [ ] Language switching doesn't break application\n\n**Test File:** `tests/test_i18n.py`\n\n```python\nimport pytest\nfrom aios.i18n import init_i18n, _, get_supported_locales\n\ndef test_init_default():\n    \"\"\"Test default initialization.\"\"\"\n    init_i18n()\n    assert _(\"Ready\") == \"Ready\"  # English default\n\ndef test_init_spanish():\n    \"\"\"Test Spanish initialization.\"\"\"\n    init_i18n(\"es_ES\")\n    # Assumes Spanish translation exists\n    result = _(\"Ready\")\n    assert result == \"Listo\" or result == \"Ready\"  # Allow fallback\n\ndef test_all_locales_supported():\n    \"\"\"Test all supported locales can be initialized.\"\"\"\n    for locale_code in get_supported_locales():\n        init_i18n(locale_code)\n        # Should not raise exception\n\ndef test_invalid_locale_fallback():\n    \"\"\"Test invalid locale falls back to default.\"\"\"\n    init_i18n(\"xx_XX\")\n    assert _(\"Ready\") == \"Ready\"\n\ndef test_formatting():\n    \"\"\"Test string formatting with translations.\"\"\"\n    init_i18n()\n    # Assuming translation exists\n    msg = _(\"Score: {score}\").format(score=95)\n    assert \"95\" in msg\n```\n\n#### 7.2 Manual Testing Checklist\n\nFor each language:\n\n**GUI Testing:**\n- [ ] All tabs display translated text\n- [ ] All buttons have translated labels\n- [ ] All dialogs show translated messages\n- [ ] Tooltips are translated\n- [ ] No text overflow/truncation\n- [ ] Status messages update correctly\n- [ ] Error messages are clear\n- [ ] Help text is accurate\n\n**CLI Testing:**\n- [ ] `--help` shows translated text\n- [ ] Error messages are translated\n- [ ] Interactive prompts are translated\n- [ ] Output formatting is correct\n- [ ] Progress indicators work\n\n**Functional Testing:**\n- [ ] Application functionality unchanged\n- [ ] No crashes from translation loading\n- [ ] Language switch persists across restarts\n- [ ] Fallback to English works if translation missing\n\n#### 7.3 Native Speaker Review\n\n**Requirements:**\n- Native speaker fluency\n- Technical/ML domain knowledge preferred\n- Access to running application\n\n**Review Checklist:**\n- [ ] Translation accuracy\n- [ ] Natural phrasing (not literal translation)\n- [ ] Consistent terminology\n- [ ] Appropriate formality level\n- [ ] No cultural insensitivity\n- [ ] Technical terms correctly used\n- [ ] Grammar and spelling correct\n\n---\n\n## Deployment Strategy\n\n### Build Process Updates\n\n**Update:** `pyproject.toml`\n\n```toml\n[project.optional-dependencies]\ni18n = [\n  \"babel>=2.14.0\",\n  \"polib>=1.2.0\",\n]\n```\n\n**Update:** Build scripts to compile translations\n\n```bash\n# In build process (CI/CD)\npython scripts/compile_translations.py\n```\n\n### Packaging\n\n**Include compiled .mo files:**\n```\nsrc/aios/locales/*/LC_MESSAGES/*.mo\n```\n\n**Update MANIFEST.in:**\n```\ninclude src/aios/locales/*/LC_MESSAGES/*.mo\n```\n\n---\n\n## Maintenance Plan\n\n### Ongoing Translation Updates\n\n**When adding new features:**\n1. Use `_()` for all user-facing strings\n2. Run `python scripts/extract_translations.py`\n3. Update .po files: `pybabel update -i messages.pot -d locales`\n4. Translate new strings\n5. Compile: `python scripts/compile_translations.py`\n\n### Translation Contributors\n\n**Set up community contribution workflow:**\n1. Use Weblate or Crowdin for collaborative translation\n2. Or: Accept .po file PRs on GitHub\n3. Assign language maintainers for each locale\n4. Regular translation reviews (quarterly)\n\n---\n\n## Success Metrics\n\n### Quantitative Metrics\n- [ ] 100% of GUI strings translated in all 9 languages\n- [ ] 100% of CLI help text translated in all 9 languages\n- [ ] 95%+ of documentation translated in priority languages (ES, FR, ZH, JA)\n- [ ] < 1% translation-related bug reports\n- [ ] Language switching works in < 5 seconds (app restart)\n\n### Qualitative Metrics\n- [ ] Native speaker approval rating > 4/5\n- [ ] No significant user complaints about translation quality\n- [ ] Positive feedback from international user community\n- [ ] Increased non-English GitHub issues/discussions\n\n---\n\n## Risks and Mitigation\n\n### Risk 1: Translation Quality\n**Risk:** Poor translations create bad user experience  \n**Mitigation:** Native speaker review, professional translators for tier 1\n\n### Risk 2: Incomplete Translations\n**Risk:** Missing strings show English text  \n**Mitigation:** Fallback to English, translation coverage tests\n\n### Risk 3: UI Layout Breaks\n**Risk:** Longer text breaks layouts  \n**Mitigation:** Dynamic sizing, manual testing, layout guidelines\n\n### Risk 4: Maintenance Burden\n**Risk:** Keeping translations updated with new features  \n**Mitigation:** Automated extraction, clear contributor guidelines, language maintainers\n\n### Risk 5: RTL Complexity (Arabic)\n**Risk:** RTL support is technically complex in Tkinter  \n**Mitigation:** Phase 1: LTR-only, Phase 2: RTL investigation/implementation\n\n---\n\n## Alternative Approaches Considered\n\n### 1. English-Only with External Translation Tools\n**Pros:** No development work  \n**Cons:** Poor UX, unreliable, no control over quality  \n**Decision:** Rejected - not professional\n\n### 2. Machine Translation at Runtime\n**Pros:** No translation work needed  \n**Cons:** Requires internet, latency, poor quality, privacy concerns  \n**Decision:** Rejected - unsuitable for professional tool\n\n### 3. Partial Localization (GUI only)\n**Pros:** Less work (skip CLI)  \n**Cons:** Inconsistent experience  \n**Decision:** Considered for MVP, but full coverage preferred\n\n---\n\n## Budget Estimate\n\n### Development Time\n- Infrastructure setup: 80 hours\n- Code refactoring: 120 hours\n- Translation coordination: 40 hours\n- Testing and QA: 80 hours\n- Documentation: 20 hours\n- **Total:** 340 hours\n\n### Translation Costs\n**Option A (Professional):**\n- 9 languages \u00d7 $2,000 = $18,000\n\n**Option B (AI + Review):**\n- 9 languages \u00d7 $300 = $2,700\n\n**Option C (Community):**\n- Coordinator time: $2,000\n- Reviews: $1,000\n- **Total:** $3,000\n\n### Recommended Budget\n- Development: $17,000 (340 hours @ $50/hr)\n- Translation: $3,000 (AI + review)\n- **Total:** $20,000\n\n---\n\n## Timeline Summary\n\n| Phase | Duration | Deliverables |\n|-------|----------|--------------|\n| 1. Infrastructure | 2 weeks | i18n framework, extraction tools |\n| 2. Code Refactoring | 3 weeks | All code using `_()` function |\n| 3. File Generation | 1 week | .pot and .po files created |\n| 4. Translation (Tier 1) | 4 weeks | ES, FR, PT, DE, IT complete |\n| 5. Translation (Tier 2) | 4 weeks | ZH, JA complete |\n| 6. Translation (Tier 3) | 4 weeks | AR, HI complete |\n| 7. UI Adjustments | 2 weeks | Layout fixes, font support |\n| 8. Configuration | 1 week | Settings panel, config files |\n| 9. Testing & QA | 3 weeks | All locales tested |\n| **Total** | **24 weeks** | **Full localization support** |\n\n**Accelerated Timeline:** 16 weeks (with parallel work and larger team)\n\n---\n\n## Dependencies\n\n### Required Libraries\n- `babel>=2.14.0` - i18n tooling\n- `polib>=1.2.0` - .po file handling\n- Font packages for non-Latin scripts (Linux)\n\n### External Dependencies\n- Translation service or translators\n- Native speaker reviewers\n- Testing infrastructure\n\n---\n\n## References\n\n### Standards and Specifications\n- [GNU gettext](https://www.gnu.org/software/gettext/)\n- [Babel Documentation](http://babel.pocoo.org/)\n- [ISO 639-1 Language Codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)\n- [Unicode CLDR](https://cldr.unicode.org/)\n\n### Best Practices\n- [Python i18n/L10n Tutorial](https://docs.python.org/3/library/gettext.html)\n- [Tkinter Internationalization](https://tkdocs.com/tutorial/text.html)\n- [Translation Best Practices](https://www.w3.org/International/questions/qa-i18n)\n\n---\n\n## Appendix A: Example Translations\n\n### Common UI Strings\n\n| English | Spanish | French | German | Chinese |\n|---------|---------|--------|--------|---------|\n| Ready | Listo | Pr\u00eat | Bereit | \u51c6\u5907\u5c31\u7eea |\n| Loading | Cargando | Chargement | L\u00e4dt | \u52a0\u8f7d\u4e2d |\n| Error | Error | Erreur | Fehler | \u9519\u8bef |\n| Success | \u00c9xito | Succ\u00e8s | Erfolg | \u6210\u529f |\n| Cancel | Cancelar | Annuler | Abbrechen | \u53d6\u6d88 |\n| Save | Guardar | Enregistrer | Speichern | \u4fdd\u5b58 |\n| Load Brain | Cargar cerebro | Charger le cerveau | Gehirn laden | \u52a0\u8f7d\u5927\u8111 |\n| Export CSV | Exportar CSV | Exporter CSV | CSV exportieren | \u5bfc\u51faCSV |\n| Start Training | Iniciar entrenamiento | D\u00e9marrer l'entra\u00eenement | Training starten | \u5f00\u59cb\u8bad\u7ec3 |\n\n---\n\n## Appendix B: .po File Workflow\n\n### Creating New Language\n\n```bash\n# 1. Extract strings\npython scripts/extract_translations.py\n\n# 2. Initialize new language\npybabel init -i src/aios/locales/messages.pot \\\n             -d src/aios/locales \\\n             -l it_IT\n\n# 3. Edit translations\n# Open src/aios/locales/it_IT/LC_MESSAGES/messages.po\n# Fill in msgstr values\n\n# 4. Compile\npython scripts/compile_translations.py\n\n# 5. Test\nAIOS_LOCALE=it_IT aios gui\n```\n\n### Updating Existing Language\n\n```bash\n# 1. Extract new strings\npython scripts/extract_translations.py\n\n# 2. Update .po files\npybabel update -i src/aios/locales/messages.pot \\\n               -d src/aios/locales\n\n# 3. Translate new strings (marked with \"fuzzy\")\n\n# 4. Compile\npython scripts/compile_translations.py\n```\n\n---\n\n## Appendix C: Contribution Guide\n\n### For Translators\n\n**To contribute a translation:**\n\n1. Fork the AI-OS repository\n2. Install dependencies: `pip install babel polib`\n3. Check if language already initialized:\n   - If yes: Update existing .po file\n   - If no: Run `pybabel init` for your language\n4. Edit `.po` file with a text editor or Poedit\n5. Compile to test: `python scripts/compile_translations.py`\n6. Test in application: `AIOS_LOCALE=<your_locale> aios gui`\n7. Submit pull request with updated .po file\n\n**Translation Guidelines:**\n- Keep placeholders like `{0}`, `%s`, `{score}` intact\n- Maintain consistent technical terminology\n- Use appropriate formality level (usually formal for software)\n- Translate meaning, not word-for-word\n- Ask for context if unclear\n\n---\n\n**Last Updated:** November 9, 2025  \n**Document Version:** 1.0  \n**Status:** Ready for Review\n", "tags": ["cli", "datasets", "evaluation", "gui", "hrm", "mcp", "training"], "headings": [{"line": 0, "text": "Internationalization and Localization (i18n/L10n)"}, {"line": 10, "text": "Overview"}, {"line": 19, "text": "Motivation"}, {"line": 21, "text": "Business Case"}, {"line": 27, "text": "Target Language Justification"}, {"line": 40, "text": "Technical Scope"}, {"line": 42, "text": "Components Requiring Localization"}, {"line": 44, "text": "1. GUI (Tkinter Interface)"}, {"line": 75, "text": "2. CLI (Command-Line Interface)"}, {"line": 101, "text": "3. Documentation"}, {"line": 130, "text": "Implementation Plan"}, {"line": 132, "text": "Phase 1: Infrastructure Setup (Week 1-2)"}, {"line": 134, "text": "1.1 Choose i18n Framework"}, {"line": 152, "text": "1.2 Create Directory Structure"}, {"line": 185, "text": "1.3 Create i18n Utilities Module"}, {"line": 200, "text": "Default locale"}, {"line": 203, "text": "Supported locales"}, {"line": 217, "text": "Global translation function"}, {"line": 231, "text": "Try to detect system locale"}, {"line": 238, "text": "Validate locale"}, {"line": 244, "text": "Set up gettext"}, {"line": 256, "text": "Fallback to no-op translation"}, {"line": 300, "text": "1.4 Create Babel Configuration"}, {"line": 312, "text": "1.5 Create Extraction Script"}, {"line": 317, "text": "!/usr/bin/env python3"}, {"line": 331, "text": "Ensure locale directory exists"}, {"line": 334, "text": "Extract strings"}, {"line": 365, "text": "1.6 Create Compilation Script"}, {"line": 370, "text": "!/usr/bin/env python3"}, {"line": 385, "text": "Find all .po files"}, {"line": 419, "text": "Phase 2: Code Refactoring (Week 3-5)"}, {"line": 421, "text": "2.1 GUI Refactoring Strategy"}, {"line": 425, "text": "Before"}, {"line": 430, "text": "After"}, {"line": 453, "text": "2.2 CLI Refactoring Strategy"}, {"line": 457, "text": "Before"}, {"line": 465, "text": "After"}, {"line": 482, "text": "2.3 String Extraction Guidelines"}, {"line": 499, "text": "Translator comment for clarity"}, {"line": 500, "text": "Translators: This appears in the training progress dialog"}, {"line": 506, "text": "Phase 3: Translation File Generation (Week 6)"}, {"line": 508, "text": "3.1 Extract Strings"}, {"line": 515, "text": "3.2 Initialize Language Files"}, {"line": 527, "text": "... repeat for all languages"}, {"line": 532, "text": "3.3 Translation File Structure"}, {"line": 537, "text": "Spanish translations for AI-OS"}, {"line": 538, "text": "Copyright (C) 2025 Wulfic"}, {"line": 539, "text": "This file is distributed under the same license as the AI-OS package."}, {"line": 551, "text": ": src/aios/gui/app/ui_setup.py:63"}, {"line": 555, "text": ": src/aios/gui/app/ui_setup.py:64"}, {"line": 559, "text": ": src/aios/gui/app/ui_setup.py:65"}, {"line": 563, "text": ": src/aios/gui/app/ui_setup.py:66"}, {"line": 567, "text": ": src/aios/gui/components/brains_panel/panel_main.py:123"}, {"line": 571, "text": ": src/aios/gui/components/brains_panel/panel_main.py:145"}, {"line": 575, "text": ": src/aios/gui/components/brains_panel/panel_main.py:201"}, {"line": 582, "text": "Phase 4: Translation Work (Week 7-18)"}, {"line": 584, "text": "4.1 Translation Approaches"}, {"line": 609, "text": "4.2 Translation Priority Order"}, {"line": 626, "text": "4.3 Translation Quality Checklist"}, {"line": 638, "text": "4.4 Special Translation Considerations"}, {"line": 663, "text": "Phase 5: UI Layout Adjustments (Week 19-20)"}, {"line": 665, "text": "5.1 Dynamic Widget Sizing"}, {"line": 672, "text": "Before"}, {"line": 675, "text": "After"}, {"line": 680, "text": "5.2 Text Overflow Handling"}, {"line": 696, "text": "5.3 RTL Support (Arabic)"}, {"line": 705, "text": "5.4 Font Support"}, {"line": 711, "text": "For CJK"}, {"line": 714, "text": "For Arabic"}, {"line": 717, "text": "For Hindi"}, {"line": 723, "text": "Phase 6: Configuration and Runtime Selection (Week 21)"}, {"line": 725, "text": "6.1 Add Locale Configuration"}, {"line": 730, "text": "Internationalization settings"}, {"line": 732, "text": "Default locale (auto-detected if not set)"}, {"line": 735, "text": "Fallback locale if selected locale unavailable"}, {"line": 739, "text": "6.2 GUI Language Selector"}, {"line": 744, "text": "In src/aios/gui/components/settings_panel/panel_main.py"}, {"line": 750, "text": "... existing code ..."}, {"line": 752, "text": "Language selection"}, {"line": 780, "text": "Save to config"}, {"line": 781, "text": "... save logic ..."}, {"line": 783, "text": "Show restart message"}, {"line": 790, "text": "6.3 CLI Language Selection"}, {"line": 793, "text": "Set via environment variable"}, {"line": 797, "text": "Or via command line flag"}, {"line": 800, "text": "Or set in config file"}, {"line": 806, "text": "Phase 7: Testing and QA (Week 22-24)"}, {"line": 808, "text": "7.1 Automated Testing"}, {"line": 831, "text": "Assumes Spanish translation exists"}, {"line": 839, "text": "Should not raise exception"}, {"line": 849, "text": "Assuming translation exists"}, {"line": 854, "text": "7.2 Manual Testing Checklist"}, {"line": 881, "text": "7.3 Native Speaker Review"}, {"line": 899, "text": "Deployment Strategy"}, {"line": 901, "text": "Build Process Updates"}, {"line": 916, "text": "In build process (CI/CD)"}, {"line": 920, "text": "Packaging"}, {"line": 934, "text": "Maintenance Plan"}, {"line": 936, "text": "Ongoing Translation Updates"}, {"line": 945, "text": "Translation Contributors"}, {"line": 955, "text": "Success Metrics"}, {"line": 957, "text": "Quantitative Metrics"}, {"line": 964, "text": "Qualitative Metrics"}, {"line": 972, "text": "Risks and Mitigation"}, {"line": 974, "text": "Risk 1: Translation Quality"}, {"line": 978, "text": "Risk 2: Incomplete Translations"}, {"line": 982, "text": "Risk 3: UI Layout Breaks"}, {"line": 986, "text": "Risk 4: Maintenance Burden"}, {"line": 990, "text": "Risk 5: RTL Complexity (Arabic)"}, {"line": 996, "text": "Alternative Approaches Considered"}, {"line": 998, "text": "1. English-Only with External Translation Tools"}, {"line": 1003, "text": "2. Machine Translation at Runtime"}, {"line": 1008, "text": "3. Partial Localization (GUI only)"}, {"line": 1015, "text": "Budget Estimate"}, {"line": 1017, "text": "Development Time"}, {"line": 1025, "text": "Translation Costs"}, {"line": 1037, "text": "Recommended Budget"}, {"line": 1044, "text": "Timeline Summary"}, {"line": 1063, "text": "Dependencies"}, {"line": 1065, "text": "Required Libraries"}, {"line": 1070, "text": "External Dependencies"}, {"line": 1077, "text": "References"}, {"line": 1079, "text": "Standards and Specifications"}, {"line": 1085, "text": "Best Practices"}, {"line": 1092, "text": "Appendix A: Example Translations"}, {"line": 1094, "text": "Common UI Strings"}, {"line": 1110, "text": "Appendix B: .po File Workflow"}, {"line": 1112, "text": "Creating New Language"}, {"line": 1115, "text": "1. Extract strings"}, {"line": 1118, "text": "2. Initialize new language"}, {"line": 1123, "text": "3. Edit translations"}, {"line": 1124, "text": "Open src/aios/locales/it_IT/LC_MESSAGES/messages.po"}, {"line": 1125, "text": "Fill in msgstr values"}, {"line": 1127, "text": "4. Compile"}, {"line": 1130, "text": "5. Test"}, {"line": 1134, "text": "Updating Existing Language"}, {"line": 1137, "text": "1. Extract new strings"}, {"line": 1140, "text": "2. Update .po files"}, {"line": 1144, "text": "3. Translate new strings (marked with \"fuzzy\")"}, {"line": 1146, "text": "4. Compile"}, {"line": 1152, "text": "Appendix C: Contribution Guide"}, {"line": 1154, "text": "For Translators"}]}, {"path": "planned_features/MIXED_GPU_VENDOR_SUPPORT.md", "content": "# Mixed GPU Vendor Support for Parallel Training\n\n**Status:** \ud83d\udccb Planned  \n> Note: Any references to `docs/user_guide/*` are placeholders for future user-facing docs. For current information, use `docs/INDEX.md` and guides under `docs/guide/`.\n**Priority:** Medium  \n**Complexity:** Medium (2-4 hours implementation)  \n**Target Version:** Future Release  \n**Created:** 2025-10-18\n\n## Overview\n\nEnable parallel independent training to work with mixed GPU vendors in a single system. This allows users to utilize all available GPUs regardless of manufacturer for training, maximizing hardware utilization.\n\n## Use Cases\n\n### Primary Use Cases\n1. **Developers with mixed setups**: NVIDIA + Intel Arc development machines\n2. **Budget builds**: Using older NVIDIA + newer AMD GPUs together\n3. **Testing environments**: Multi-vendor CI/CD systems\n4. **Workstation upgrades**: Adding new GPU without removing old one\n\n### Example Configurations\n- NVIDIA RTX 3090 + Intel Arc A770\n- AMD RX 7900 XTX + NVIDIA RTX 2080 Ti\n- Intel Arc A380 + AMD RX 6600\n- NVIDIA RTX 4090 + AMD RX 7800 XT + Intel Arc A750 (triple vendor)\n\n## Current Limitations\n\n### What Works Now \u2705\n- **Same vendor, different models**: NVIDIA RTX 3090 + RTX 2080 Ti\n- **Automatic detection**: `--cuda-ids 0,1` works for NVIDIA-only\n\n### What Doesn't Work \u274c\n- **Mixed vendors**: Different GPU types not detected\n- **Backend selection**: Hardcoded to CUDA only\n- **Device enumeration**: Only scans NVIDIA GPUs\n- **VRAM checking**: Only checks CUDA devices\n\n## Technical Requirements\n\n### PyTorch Backend Support\n\n#### NVIDIA GPUs (CUDA)\n```python\nimport torch\nif torch.cuda.is_available():\n    device = torch.device('cuda:0')\n    count = torch.cuda.device_count()\n```\n\n#### AMD GPUs (ROCm)\n```python\nimport torch\n# Requires: pip install torch+rocm5.7\nif torch.cuda.is_available():  # ROCm pretends to be CUDA\n    device = torch.device('cuda:0')  # Still uses 'cuda' namespace\n```\n\n#### Intel Arc/Xe GPUs (XPU)\n```python\nimport intel_extension_for_pytorch as ipex\nif torch.xpu.is_available():\n    device = torch.device('xpu:0')\n    count = torch.xpu.device_count()\n```\n\n### Dependencies\n\n**Current:**\n- `torch>=2.0.0` with CUDA support\n\n**Needed for full support:**\n- `torch+rocm` (AMD support) - separate wheel\n- `intel-extension-for-pytorch` (Intel Arc support)\n- Optional: Auto-detect and install based on hardware\n\n## Implementation Plan\n\n### Phase 1: Detection & Enumeration (High Priority)\n\n**Goal:** Detect all available GPUs across vendors\n\n```python\ndef detect_all_gpus() -> list[dict]:\n    \"\"\"Detect GPUs from all supported vendors.\n    \n    Returns:\n        List of GPU info dicts:\n        {\n            'id': 0,\n            'backend': 'cuda' | 'xpu' | 'hip',\n            'device_id': 0,  # Backend-specific ID\n            'name': 'NVIDIA GeForce RTX 3090',\n            'vendor': 'NVIDIA' | 'AMD' | 'Intel',\n            'vram_total': 24 * 1024**3,  # bytes\n            'vram_available': 23.5 * 1024**3,\n            'compute_capability': '8.6',  # NVIDIA only\n        }\n    \"\"\"\n    gpus = []\n    global_id = 0\n    \n    # NVIDIA (CUDA)\n    try:\n        import torch\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                props = torch.cuda.get_device_properties(i)\n                gpus.append({\n                    'id': global_id,\n                    'backend': 'cuda',\n                    'device_id': i,\n                    'name': props.name,\n                    'vendor': 'NVIDIA',\n                    'vram_total': props.total_memory,\n                    'vram_available': torch.cuda.mem_get_info(i)[0],\n                    'compute_capability': f\"{props.major}.{props.minor}\",\n                })\n                global_id += 1\n    except Exception as e:\n        print(f\"CUDA detection failed: {e}\")\n    \n    # Intel Arc (XPU)\n    try:\n        import intel_extension_for_pytorch as ipex\n        if torch.xpu.is_available():\n            for i in range(torch.xpu.device_count()):\n                props = torch.xpu.get_device_properties(i)\n                gpus.append({\n                    'id': global_id,\n                    'backend': 'xpu',\n                    'device_id': i,\n                    'name': props.name,\n                    'vendor': 'Intel',\n                    'vram_total': props.total_memory,\n                    'vram_available': torch.xpu.mem_get_info(i)[0],\n                    'compute_capability': None,\n                })\n                global_id += 1\n    except ImportError:\n        pass  # Intel extension not installed\n    except Exception as e:\n        print(f\"XPU detection failed: {e}\")\n    \n    # AMD (ROCm/HIP) - tricky because it uses CUDA namespace\n    # Need to check GPU names to distinguish from NVIDIA\n    try:\n        import torch\n        if hasattr(torch, 'hip') and torch.hip.is_available():\n            for i in range(torch.hip.device_count()):\n                props = torch.hip.get_device_properties(i)\n                gpus.append({\n                    'id': global_id,\n                    'backend': 'hip',\n                    'device_id': i,\n                    'name': props.name,\n                    'vendor': 'AMD',\n                    'vram_total': props.total_memory,\n                    'vram_available': torch.hip.mem_get_info(i)[0],\n                    'compute_capability': None,\n                })\n                global_id += 1\n    except Exception as e:\n        print(f\"HIP detection failed: {e}\")\n    \n    return gpus\n```\n\n**Files to modify:**\n- `src/aios/cli/hrm_hf/parallel_independent_training.py`\n- New: `src/aios/cli/hrm_hf/gpu_detection.py`\n\n### Phase 2: Device Mapping (High Priority)\n\n**Goal:** Map user-specified GPU IDs to backend-specific devices\n\n```python\nclass GPUDevice:\n    \"\"\"Abstraction for different GPU backends.\"\"\"\n    \n    def __init__(self, backend: str, device_id: int, info: dict):\n        self.backend = backend\n        self.device_id = device_id\n        self.info = info\n        self._device = None\n    \n    @property\n    def device(self) -> torch.device:\n        \"\"\"Get PyTorch device object.\"\"\"\n        if self._device is None:\n            if self.backend == 'cuda':\n                self._device = torch.device(f'cuda:{self.device_id}')\n            elif self.backend == 'xpu':\n                self._device = torch.device(f'xpu:{self.device_id}')\n            elif self.backend == 'hip':\n                self._device = torch.device(f'cuda:{self.device_id}')  # HIP uses cuda namespace\n        return self._device\n    \n    def set_device(self):\n        \"\"\"Set this as the active device.\"\"\"\n        if self.backend == 'cuda':\n            torch.cuda.set_device(self.device)\n        elif self.backend == 'xpu':\n            torch.xpu.set_device(self.device)\n        elif self.backend == 'hip':\n            torch.cuda.set_device(self.device)\n    \n    def synchronize(self):\n        \"\"\"Synchronize device.\"\"\"\n        if self.backend == 'cuda':\n            torch.cuda.synchronize(self.device)\n        elif self.backend == 'xpu':\n            torch.xpu.synchronize(self.device)\n        elif self.backend == 'hip':\n            torch.cuda.synchronize(self.device)\n    \n    def create_stream(self):\n        \"\"\"Create backend-specific stream.\"\"\"\n        if self.backend == 'cuda':\n            return torch.cuda.Stream(device=self.device)\n        elif self.backend == 'xpu':\n            return torch.xpu.Stream(device=self.device)\n        elif self.backend == 'hip':\n            return torch.cuda.Stream(device=self.device)\n    \n    def get_memory_info(self) -> tuple[int, int]:\n        \"\"\"Get (free, total) memory in bytes.\"\"\"\n        if self.backend == 'cuda':\n            return torch.cuda.mem_get_info(self.device_id)\n        elif self.backend == 'xpu':\n            return torch.xpu.mem_get_info(self.device_id)\n        elif self.backend == 'hip':\n            return torch.cuda.mem_get_info(self.device_id)\n        return (0, 0)\n```\n\n### Phase 3: AMP Backend Support (Medium Priority)\n\n**Goal:** Handle vendor-specific AMP implementations\n\n```python\ndef create_scaler(backend: str, enabled: bool):\n    \"\"\"Create AMP scaler for specific backend.\"\"\"\n    if backend == 'cuda':\n        return torch.amp.GradScaler('cuda', enabled=enabled)\n    elif backend == 'xpu':\n        # Intel XPU uses different AMP API\n        return ipex.optimize(enable_auto_mixed_precision=enabled)\n    elif backend == 'hip':\n        return torch.amp.GradScaler('cuda', enabled=enabled)  # HIP uses CUDA namespace\n    return None\n\ndef autocast_context(backend: str, enabled: bool):\n    \"\"\"Get appropriate autocast context for backend.\"\"\"\n    if backend == 'cuda':\n        return torch.amp.autocast('cuda', enabled=enabled)\n    elif backend == 'xpu':\n        return torch.xpu.amp.autocast(enabled=enabled)\n    elif backend == 'hip':\n        return torch.amp.autocast('cuda', enabled=enabled)\n    return nullcontext()\n```\n\n### Phase 4: CLI Integration (Medium Priority)\n\n**New CLI interface:**\n\n```bash\n# Current (CUDA-only):\naios hrm-hf train-actv1 --cuda-ids 0,1\n\n# New (auto-detect all vendors):\naios hrm-hf train-actv1 --gpu-ids 0,1,2\n# Where: 0=NVIDIA, 1=Intel Arc, 2=AMD\n\n# Explicit vendor selection:\naios hrm-hf train-actv1 --gpu-ids cuda:0,xpu:0,hip:0\n\n# List available GPUs:\naios hrm-hf list-gpus\n# Output:\n# ID  Vendor   Model                      VRAM    Backend\n# 0   NVIDIA   GeForce RTX 3090          24 GB   cuda\n# 1   Intel    Arc A770                  16 GB   xpu\n# 2   AMD      Radeon RX 7900 XTX        24 GB   hip\n```\n\n**Files to modify:**\n- `src/aios/cli/hrm_hf_cli.py` - Add `--gpu-ids` parameter\n- `src/aios/cli/hrm_hf_cli.py` - Add `list-gpus` command\n\n### Phase 5: Load Balancing (Low Priority - Future)\n\n**Goal:** Assign work proportionally to GPU speed\n\n```python\ndef calculate_gpu_weights(gpus: list[GPUDevice]) -> list[float]:\n    \"\"\"Calculate relative performance weights for GPUs.\n    \n    Uses heuristics based on:\n    - VRAM size\n    - Vendor (NVIDIA > AMD > Intel for ML)\n    - Known performance tiers\n    \"\"\"\n    weights = []\n    for gpu in gpus:\n        # Base weight from VRAM\n        vram_gb = gpu.info['vram_total'] / (1024**3)\n        weight = vram_gb / 8  # Normalize to 8GB = 1.0\n        \n        # Vendor multiplier (rough performance hierarchy)\n        if gpu.vendor == 'NVIDIA':\n            weight *= 1.0\n        elif gpu.vendor == 'AMD':\n            weight *= 0.8  # ~20% slower on ML workloads\n        elif gpu.vendor == 'Intel':\n            weight *= 0.5  # ~50% slower (Arc is newer to ML)\n        \n        weights.append(weight)\n    \n    # Normalize to sum = 1.0\n    total = sum(weights)\n    return [w / total for w in weights]\n\n# Usage:\nweights = calculate_gpu_weights(gpus)\n# Assign data: GPU 0 gets 50%, GPU 1 gets 30%, GPU 2 gets 20%\n```\n\n## Testing Strategy\n\n### Unit Tests\n```python\ndef test_gpu_detection():\n    \"\"\"Test GPU detection works for available hardware.\"\"\"\n    gpus = detect_all_gpus()\n    assert len(gpus) > 0\n    assert all('backend' in gpu for gpu in gpus)\n\ndef test_device_abstraction():\n    \"\"\"Test GPUDevice works across backends.\"\"\"\n    gpus = detect_all_gpus()\n    for gpu_info in gpus:\n        device = GPUDevice(gpu_info['backend'], gpu_info['device_id'], gpu_info)\n        assert device.device is not None\n        device.set_device()\n        device.synchronize()\n\ndef test_mixed_training():\n    \"\"\"Test training works with mixed GPUs.\"\"\"\n    # Only runs if multiple vendor GPUs available\n    gpus = detect_all_gpus()\n    vendors = set(gpu['vendor'] for gpu in gpus)\n    if len(vendors) < 2:\n        pytest.skip(\"Mixed GPU hardware not available\")\n    \n    # Run short training\n    run_training(gpu_ids=[0, 1], steps=10)\n```\n\n### Manual Testing Checklist\n- [ ] NVIDIA + Intel Arc training completes\n- [ ] NVIDIA + AMD training completes\n- [ ] Intel Arc + AMD training completes\n- [ ] All three vendors simultaneously\n- [ ] VRAM checking works per vendor\n- [ ] AMP works on each vendor\n- [ ] Gradient checkpointing works on each vendor\n- [ ] Checkpoint merging produces valid model\n- [ ] Performance is reasonable (not worse than sequential)\n\n## Performance Considerations\n\n### Bottleneck Analysis\n\n**Scenario 1: Mixed High-End GPUs**\n- RTX 4090 (165 TFLOPS) + RX 7900 XTX (61 TFLOPS)\n- Bottleneck: AMD ~2.7x slower\n- Solution: Assign 73% data to RTX 4090, 27% to RX 7900 XTX\n- Expected speedup: ~1.6x vs single RTX 4090\n\n**Scenario 2: High-End + Low-End**\n- RTX 4090 (165 TFLOPS) + Arc A380 (8 TFLOPS)\n- Bottleneck: Arc ~20x slower\n- Solution: Don't use Arc, or give it <10% of data\n- Expected speedup: Minimal, possibly negative\n\n**Recommendation:**\n- Start without load balancing (equal distribution)\n- Add load balancing in Phase 5 if needed\n- Document performance expectations\n\n## Backwards Compatibility\n\n### Breaking Changes: None \u2705\n- `--cuda-ids` continues to work for NVIDIA-only setups\n- New `--gpu-ids` parameter is optional\n- Auto-detection falls back to CUDA if no other backends\n\n### Migration Path\n```python\n# Old code (still works):\nconfig.cuda_ids = \"0,1\"\n\n# New code (recommended):\nconfig.gpu_ids = \"0,1\"  # Auto-detects vendor\nconfig.gpu_ids = \"cuda:0,xpu:1\"  # Explicit\n```\n\n## Documentation Needed\n\n### User Documentation\n- Update `QUICK_START.md` with multi-vendor examples\n- Add (placeholder) `docs/user_guide/MIXED_GPU_TRAINING.md` (to be authored later; see docs/INDEX.md for current guidance)\n- Update `README.md` with mixed GPU capabilities\n\n### Developer Documentation\n- Add `docs/development/GPU_BACKEND_ARCHITECTURE.md`\n- Document `GPUDevice` abstraction layer\n- Add troubleshooting guide for backend issues\n\n## Known Limitations\n\n### Phase 1-4 Limitations\n1. **No automatic load balancing**: Slow GPU limits speed\n2. **No cross-GPU communication**: Can't implement DDP across vendors\n3. **Backend-specific quirks**: Some features may not work on all vendors\n4. **Installation complexity**: Users need correct PyTorch builds\n\n### Permanent Limitations\n1. **Performance**: Limited by slowest GPU\n2. **Memory**: Each GPU needs full model copy\n3. **Synchronization**: Barrier waits for all GPUs\n\n## Success Metrics\n\n### Must Have \u2705\n- [ ] Detection works for NVIDIA + Intel Arc\n- [ ] Detection works for NVIDIA + AMD\n- [ ] Training completes without errors\n- [ ] Checkpoints merge correctly\n- [ ] All existing NVIDIA-only functionality preserved\n\n### Nice to Have \ud83c\udfaf\n- [ ] Performance within 10% of theoretical maximum\n- [ ] Automatic load balancing implemented\n- [ ] User documentation complete\n- [ ] Zero user-facing configuration needed\n\n### Stretch Goals \ud83d\ude80\n- [ ] Intel Arc + AMD tested and working\n- [ ] Three-vendor training working\n- [ ] Automatic backend installation\n- [ ] GUI support for mixed GPUs\n\n## Future Enhancements\n\n### Post-Implementation\n1. **Dynamic scaling**: Adjust work distribution based on actual throughput\n2. **Health monitoring**: Detect slow/stuck GPUs and redistribute work\n3. **Power management**: Respect TDP limits per GPU\n4. **Cloud support**: Work with mixed instance types (A100 + V100)\n\n### Research Opportunities\n1. **Cross-vendor communication**: Investigate vendor-agnostic collective ops\n2. **Unified memory**: Explore cross-GPU memory pooling\n3. **Heterogeneous parallelism**: Different model parts on different vendors\n\n## References\n\n### PyTorch Backend Documentation\n- CUDA: https://pytorch.org/docs/stable/cuda.html\n- Intel XPU: https://intel.github.io/intel-extension-for-pytorch/\n- AMD ROCm: https://pytorch.org/docs/stable/notes/hip.html\n\n### Similar Implementations\n- TensorFlow multi-backend: https://www.tensorflow.org/guide/gpu\n- JAX multi-backend: https://jax.readthedocs.io/en/latest/jax.devices.html\n\n## Timeline Estimate\n\n**Aggressive (1 week):**\n- Day 1-2: Phase 1 (Detection)\n- Day 3-4: Phase 2 (Mapping)\n- Day 5: Phase 3 (AMP)\n- Day 6-7: Phase 4 (CLI) + Testing\n\n**Realistic (2 weeks):**\n- Week 1: Phases 1-3 + Initial testing\n- Week 2: Phase 4 + Documentation + Comprehensive testing\n\n**Conservative (1 month):**\n- Week 1-2: Implementation\n- Week 3: Testing on real mixed hardware\n- Week 4: Bug fixes + Documentation + Load balancing\n\n## Open Questions\n\n1. **AMD ROCm detection**: How to reliably distinguish from NVIDIA CUDA?\n2. **Error handling**: What if one GPU fails mid-training?\n3. **Checkpoint format**: Should we store which GPU/vendor trained each checkpoint?\n4. **GUI integration**: How to represent mixed GPUs in the GUI?\n5. **Package management**: Should we bundle all backends or make them optional?\n\n## Decision Log\n\n| Date | Decision | Rationale |\n|------|----------|-----------|\n| 2025-10-18 | Use abstraction layer (GPUDevice) | Cleaner than if/else everywhere |\n| 2025-10-18 | Skip load balancing in Phase 1 | Get basic functionality first |\n| 2025-10-18 | Keep `--cuda-ids` for backwards compat | Don't break existing usage |\n| TBD | Bundle backends or optional? | Pending: install size vs user friction |\n\n## Approval Status\n\n- [ ] Technical Lead Review\n- [ ] Architecture Review\n- [ ] Product Manager Approval\n- [ ] Implementation Started\n- [ ] Testing Complete\n- [ ] Documentation Complete\n- [ ] Released\n\n---\n\n**Last Updated:** 2025-10-18  \n**Next Review:** After Phase 1 implementation  \n**Owner:** @AI-OS-Team\n", "tags": ["gui", "training"], "headings": [{"line": 0, "text": "Mixed GPU Vendor Support for Parallel Training"}, {"line": 9, "text": "Overview"}, {"line": 13, "text": "Use Cases"}, {"line": 15, "text": "Primary Use Cases"}, {"line": 21, "text": "Example Configurations"}, {"line": 27, "text": "Current Limitations"}, {"line": 29, "text": "What Works Now \u2705"}, {"line": 33, "text": "What Doesn't Work \u274c"}, {"line": 39, "text": "Technical Requirements"}, {"line": 41, "text": "PyTorch Backend Support"}, {"line": 43, "text": "NVIDIA GPUs (CUDA)"}, {"line": 51, "text": "AMD GPUs (ROCm)"}, {"line": 54, "text": "Requires: pip install torch+rocm5.7"}, {"line": 59, "text": "Intel Arc/Xe GPUs (XPU)"}, {"line": 67, "text": "Dependencies"}, {"line": 77, "text": "Implementation Plan"}, {"line": 79, "text": "Phase 1: Detection & Enumeration (High Priority)"}, {"line": 103, "text": "NVIDIA (CUDA)"}, {"line": 123, "text": "Intel Arc (XPU)"}, {"line": 145, "text": "AMD (ROCm/HIP) - tricky because it uses CUDA namespace"}, {"line": 146, "text": "Need to check GPU names to distinguish from NVIDIA"}, {"line": 173, "text": "Phase 2: Device Mapping (High Priority)"}, {"line": 237, "text": "Phase 3: AMP Backend Support (Medium Priority)"}, {"line": 247, "text": "Intel XPU uses different AMP API"}, {"line": 264, "text": "Phase 4: CLI Integration (Medium Priority)"}, {"line": 269, "text": "Current (CUDA-only):"}, {"line": 272, "text": "New (auto-detect all vendors):"}, {"line": 274, "text": "Where: 0=NVIDIA, 1=Intel Arc, 2=AMD"}, {"line": 276, "text": "Explicit vendor selection:"}, {"line": 279, "text": "List available GPUs:"}, {"line": 281, "text": "Output:"}, {"line": 282, "text": "ID  Vendor   Model                      VRAM    Backend"}, {"line": 283, "text": "0   NVIDIA   GeForce RTX 3090          24 GB   cuda"}, {"line": 284, "text": "1   Intel    Arc A770                  16 GB   xpu"}, {"line": 285, "text": "2   AMD      Radeon RX 7900 XTX        24 GB   hip"}, {"line": 292, "text": "Phase 5: Load Balancing (Low Priority - Future)"}, {"line": 307, "text": "Base weight from VRAM"}, {"line": 311, "text": "Vendor multiplier (rough performance hierarchy)"}, {"line": 321, "text": "Normalize to sum = 1.0"}, {"line": 325, "text": "Usage:"}, {"line": 327, "text": "Assign data: GPU 0 gets 50%, GPU 1 gets 30%, GPU 2 gets 20%"}, {"line": 330, "text": "Testing Strategy"}, {"line": 332, "text": "Unit Tests"}, {"line": 351, "text": "Only runs if multiple vendor GPUs available"}, {"line": 357, "text": "Run short training"}, {"line": 361, "text": "Manual Testing Checklist"}, {"line": 372, "text": "Performance Considerations"}, {"line": 374, "text": "Bottleneck Analysis"}, {"line": 393, "text": "Backwards Compatibility"}, {"line": 395, "text": "Breaking Changes: None \u2705"}, {"line": 400, "text": "Migration Path"}, {"line": 402, "text": "Old code (still works):"}, {"line": 405, "text": "New code (recommended):"}, {"line": 410, "text": "Documentation Needed"}, {"line": 412, "text": "User Documentation"}, {"line": 417, "text": "Developer Documentation"}, {"line": 422, "text": "Known Limitations"}, {"line": 424, "text": "Phase 1-4 Limitations"}, {"line": 430, "text": "Permanent Limitations"}, {"line": 435, "text": "Success Metrics"}, {"line": 437, "text": "Must Have \u2705"}, {"line": 444, "text": "Nice to Have \ud83c\udfaf"}, {"line": 450, "text": "Stretch Goals \ud83d\ude80"}, {"line": 456, "text": "Future Enhancements"}, {"line": 458, "text": "Post-Implementation"}, {"line": 464, "text": "Research Opportunities"}, {"line": 469, "text": "References"}, {"line": 471, "text": "PyTorch Backend Documentation"}, {"line": 476, "text": "Similar Implementations"}, {"line": 480, "text": "Timeline Estimate"}, {"line": 497, "text": "Open Questions"}, {"line": 505, "text": "Decision Log"}, {"line": 514, "text": "Approval Status"}]}, {"path": "planned_features/MOE_LIGHTNING_INTEGRATION.md", "content": "# MoE-Lightning Integration Plan\n\n## Overview\n\nThis document outlines the plan for integrating **MoE-Lightning** into the AI-OS project to enable high-throughput Mixture-of-Experts (MoE) inference on memory-constrained GPUs. MoE-Lightning is a state-of-the-art system that achieves up to 10.3\u00d7 higher throughput than existing solutions through novel CPU-GPU-I/O pipeline scheduling and a Hierarchical Roofline Model for performance optimization.\n\n**Paper Reference**: [MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs](https://arxiv.org/html/2411.11217)\n\n**Created**: November 8, 2025\n**Status**: Planning Phase\n**Priority**: High\n**Complexity**: High\n\n---\n\n## Table of Contents\n\n1. [Motivation](#motivation)\n2. [Technical Background](#technical-background)\n3. [Core Components](#core-components)\n4. [Integration Architecture](#integration-architecture)\n5. [Implementation Phases](#implementation-phases)\n6. [Technical Requirements](#technical-requirements)\n7. [Performance Targets](#performance-targets)\n8. [Risk Assessment](#risk-assessment)\n9. [Testing Strategy](#testing-strategy)\n10. [Future Enhancements](#future-enhancements)\n11. [References](#references)\n\n---\n\n## Motivation\n\n### Problem Statement\n\nAI-OS currently faces significant challenges when running large Mixture-of-Experts models on memory-constrained hardware:\n\n1. **Limited GPU Memory**: Models like Mixtral 8x7B (~47GB) and Mixtral 8x22B (>256GB) cannot fit entirely in consumer-grade GPU memory (typically 16-24GB)\n2. **Poor Resource Utilization**: Existing offloading solutions (DeepSpeed-Inference, FlexGen) suffer from:\n   - GPU idle time while waiting for data transfers\n   - Inefficient overlap of computation and I/O\n   - Suboptimal batch size selection\n3. **Accessibility Gap**: High-end GPUs are expensive and unavailable to most users who want to experiment with large MoE models\n\n### Benefits of Integration\n\n1. **Dramatic Throughput Improvements**: 3.5-10.3\u00d7 higher throughput on single GPU compared to existing systems\n2. **Memory Efficiency**: Run models with 2-3\u00d7 less CPU memory while maintaining peak throughput\n3. **Better Hardware Utilization**: Efficiently utilize CPU, GPU, and memory bandwidth simultaneously\n4. **Democratization**: Enable more users to run large MoE models on consumer hardware\n5. **Super-linear Scaling**: 2.77-3.38\u00d7 throughput improvement when scaling from 2 to 4 GPUs\n6. **Compatibility**: Works with popular MoE models (Mixtral 8x7B, Mixtral 8x22B, DBRX)\n\n---\n\n## Technical Background\n\n### Mixture of Experts (MoE) Architecture\n\nMoE models use a gating mechanism to route inputs to specialized expert sub-networks:\n- Only a subset of experts are activated per token (sparse activation)\n- Provides better parameter efficiency than dense models\n- Significantly larger memory footprint due to multiple expert FFNs\n- Example: Mixtral 8x7B has 8 experts per layer, activates top-2\n\n### Key Innovations in MoE-Lightning\n\n#### 1. CGOPipe (CPU-GPU-I/O Pipeline Schedule)\n\n**Problem**: Traditional approaches transfer data sequentially, causing bubbles in the pipeline where resources sit idle.\n\n**Solution**: Fine-grained pipelining that overlaps:\n- GPU computation (post-attention, pre-attention tasks)\n- CPU computation (attention with softmax)\n- I/O transfers (weights, hidden states, KV cache)\n\n**Key Technique - Weights Paging**:\n- Chunk weights into `n` pages (where `n` = number of micro-batches)\n- Interleave weight transfers with intermediate result transfers\n- Enable parallel transfers in opposite directions (CPU\u2192GPU and GPU\u2192CPU)\n\n#### 2. HRM (Hierarchical Roofline Model)\n\n**Problem**: Existing performance models don't account for heterogeneous resources and cross-level data movement.\n\n**Solution**: Extended Roofline Model with multiple memory hierarchies:\n\n**Performance Equation**:\n```\nP_x^i = min(P_peak^i, B_peak^i \u00d7 I_x^i, B_peak^(j,i) \u00d7 I_x^j)\n```\n\nWhere:\n- `P_peak^i`: Peak compute at level i (GPU/CPU)\n- `B_peak^i`: Memory bandwidth at level i\n- `B_peak^(j,i)`: Bandwidth from level j to level i (e.g., CPU to GPU)\n- `I_x^i`: Operational intensity of computation x at level i\n\n**Turning Points**: The model identifies critical operational intensities that determine:\n- When to perform computation on CPU vs GPU\n- When the system is GPU memory-bound vs CPU-GPU bandwidth-bound\n- Optimal batch size and micro-batch size combinations\n\n**Balance Point**:\n```\nB_peak^i \u00d7 I_x^i = B_peak^(j,i) \u00d7 I_x^j\n```\nThis represents the optimal configuration where all resources are fully utilized.\n\n#### 3. Tensor Parallelism\n\nUnlike pipeline parallelism (scales with model depth), MoE-Lightning uses tensor parallelism:\n- Scales with layer size\n- Increases total GPU memory capacity linearly\n- Increases GPU memory bandwidth linearly\n- Achieves super-linear scaling in practice (3.38\u00d7 with 4 GPUs vs 2 GPUs)\n\n### Performance Analysis Insights\n\n#### Attention Block\n- Operational intensity independent of batch size\n- For context length 512 on L4 GPU: CPU attention is 3-4\u00d7 faster than KV cache transfer\n- CPU attention becomes bottleneck at large batch sizes and long context lengths\n\n#### MoE FFN Block\n- Operational intensity increases with batch size (more computation per weight access)\n- Memory-bound in decode stage for typical micro-batch sizes\n- Benefits most from weight offloading strategies\n\n---\n\n## Core Components\n\n### Component 1: CGOPipe Scheduler\n\n**Purpose**: Implement fine-grained CPU-GPU-I/O pipeline scheduling\n\n**Key Features**:\n```python\n# Pseudo-code for CGOPipe execution order\nfor decode_step in range(generation_length):\n    # Prologue (first 2 micro-batches)\n    for j in [1, 2]:\n        PreAttn(layer=1, microbatch=j)\n        OffloadQKV(layer=1, microbatch=j)\n        CPUAttn(layer=1, microbatch=j)\n        WeightsCPUtoPin(layer=2, microbatch=j)\n    \n    # Main pipeline (steady state)\n    for layer in range(1, num_layers):\n        for microbatch in range(1, num_microbatches + 1):\n            # Parallel execution\n            PostAttn(layer, microbatch)        # GPU\n            PreAttn(layer, microbatch+1)       # GPU\n            CPUAttn(layer, microbatch+1)       # CPU\n            WeightsPinToGPU(layer+1, page)     # I/O\n```\n\n**Implementation Requirements**:\n- Asynchronous task execution with CUDA streams\n- Synchronization primitives for data dependencies\n- Weight paging system with page table management\n- Dual-buffer for weight transfers (2\u00d7 layer weight size)\n\n### Component 2: HRM Performance Model\n\n**Purpose**: Find optimal execution policies based on hardware, model, and workload\n\n**Policy Search Space**:\n```python\n@dataclass\nclass InferencePolicy:\n    N: int              # Batch size\n    \u03bc: int              # Micro-batch size\n    A_g: bool           # Perform attention on GPU?\n    F_g: bool           # Perform FFN on GPU?\n    r_w: float          # Ratio of weights on GPU (0-1)\n    r_c: float          # Ratio of KV cache on GPU (0-1)\n```\n\n**Optimization Target**:\n```python\ndef optimize_policy(hardware, model, workload):\n    \"\"\"\n    Minimize per-layer latency while satisfying memory constraints\n    \n    T(M, H, W, P) = max(comm_cpu_to_gpu, T_cpu, T_gpu)\n    \n    where:\n    - T_cpu = T_attn_cpu + T_ffn_cpu\n    - T_gpu = T_attn_gpu + T_ffn_gpu\n    - comm_cpu_to_gpu = bytes_transferred / bandwidth_cpu_to_gpu\n    \n    Subject to:\n    - GPU_memory_used \u2264 GPU_memory_capacity\n    - CPU_memory_used \u2264 CPU_memory_capacity\n    \"\"\"\n    # Use MILP solver for policy search\n    # Takes <1 minute for offline optimization\n```\n\n**Model Configuration**:\n- Hardware: GPU/CPU memory, bandwidth, FLOPS\n- Model: Layers, dimensions, expert count, data types\n- Workload: Average prompt length, generation length\n\n### Component 3: Memory Management System\n\n**Weight Paging**:\n```python\nclass WeightPagingManager:\n    \"\"\"\n    Manages paged weight transfers with double buffering\n    \"\"\"\n    def __init__(self, layer_weight_size, num_pages):\n        # Allocate 2\u00d7 layer weight buffer on GPU\n        self.weight_buffer_size = 2 * layer_weight_size\n        self.num_pages = num_pages\n        self.page_size = layer_weight_size / num_pages\n        \n        # Page table for MoE expert routing\n        self.page_table = {}\n    \n    def transfer_page(self, layer, page_id, stream):\n        # CPU DRAM \u2192 CPU Pinned Memory\n        self.copy_to_pinned_async(layer, page_id, stream)\n        \n        # CPU Pinned Memory \u2192 GPU (overlapped)\n        self.copy_to_gpu_async(layer, page_id, stream)\n```\n\n**KV Cache Management**:\n- Store all KV cache on CPU after prefill stage\n- Transfer to GPU only for attention computation (if GPU attention is selected)\n- For CPU attention: keep on CPU, pass hidden states instead\n\n### Component 4: CPU Attention Kernels\n\n**Purpose**: High-performance Grouped Query Attention on CPU\n\n**Implementation**:\n- Based on Intel MKL library optimizations\n- SIMD vectorization for matrix operations\n- Cache-friendly memory access patterns\n- Multi-threaded execution\n\n**Performance Characteristics**:\n- 3-4\u00d7 faster than KV cache transfer on typical hardware\n- Becomes bottleneck at very large batch sizes (>256) or long contexts (>2048)\n\n### Component 5: Request Batching System\n\n**Purpose**: Handle variable-length prompts efficiently without padding\n\n**Algorithm**:\n```python\ndef balanced_batching(requests, num_microbatches, target_batch_size):\n    \"\"\"\n    Distribute requests across micro-batches to balance token counts\n    \n    Returns micro-batches with roughly equal total tokens\n    \"\"\"\n    # Sort requests by length (descending)\n    sorted_requests = sorted(requests, key=lambda r: r.length, reverse=True)\n    \n    microbatches = [[] for _ in range(num_microbatches)]\n    token_counts = [0] * num_microbatches\n    \n    # Greedy assignment to micro-batch with fewest tokens\n    for request in sorted_requests:\n        min_idx = token_counts.index(min(token_counts))\n        microbatches[min_idx].append(request)\n        token_counts[min_idx] += request.length\n    \n    return microbatches\n```\n\n---\n\n## Integration Architecture\n\n### System Architecture Diagram\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       AI-OS Core                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502         MoE-Lightning Integration Layer                \u2502  \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502\n\u2502  \u2502                                                         \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u2502\n\u2502  \u2502  \u2502   HRM Model  \u2502  \u2502  CGOPipe     \u2502  \u2502  Policy     \u2502 \u2502  \u2502\n\u2502  \u2502  \u2502  Optimizer   \u2502\u2190\u2192\u2502  Scheduler   \u2502\u2190\u2192\u2502  Cache      \u2502 \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  \u2502\n\u2502  \u2502         \u2193                  \u2193                           \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u2502\n\u2502  \u2502  \u2502   Weight     \u2502  \u2502  Request     \u2502  \u2502  CPU Attn   \u2502 \u2502  \u2502\n\u2502  \u2502  \u2502   Paging     \u2502  \u2502  Batching    \u2502  \u2502  Kernels    \u2502 \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                           \u2195                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502         Existing AI-OS Components                      \u2502  \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502\n\u2502  \u2502  \u2022 HuggingFace Model Loading                          \u2502  \u2502\n\u2502  \u2502  \u2022 vLLM/SGLang Integration                            \u2502  \u2502\n\u2502  \u2502  \u2022 Memory Estimation System                           \u2502  \u2502\n\u2502  \u2502  \u2022 Expert Manager                                     \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                           \u2195                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Hardware Abstraction Layer               \u2502  \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502\n\u2502  \u2502  GPU (CUDA)  \u2502  CPU (MKL)  \u2502  Memory (Pinned/Paged)  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Integration Points\n\n#### 1. Model Loading Layer\n- Extend existing HuggingFace model loading in `aios/brain.py`\n- Detect MoE architecture (Mixtral, DBRX, DeepSeek-MoE)\n- Configure weight storage strategy (GPU/CPU split based on policy)\n\n#### 2. Inference Engine Layer\n- New module: `aios/inference/moe_lightning/`\n- Interface with existing inference systems (vLLM, SGLang)\n- Provide unified API for MoE model inference\n\n#### 3. Memory Management Layer\n- Integrate with existing memory estimation (`artifacts/memory_estimation/`)\n- Extend GPU memory tracking\n- Add CPU memory and pinned memory tracking\n\n#### 4. CLI Integration\n- Add commands to `aios/cli/aios.py`:\n  ```bash\n  aios infer-moe --model mixtral-8x7b --strategy moe-lightning --batch-size auto\n  aios profile-moe --model mixtral-8x7b --hardware-config gpu_config.yaml\n  ```\n\n#### 5. Configuration Layer\n- New config file: `config/moe_lightning.yaml`\n- Hardware profiles for common GPU configurations (T4, L4, A100, etc.)\n- Model-specific optimization profiles\n\n---\n\n## Implementation Phases\n\n### Phase 1: Foundation & Research (Weeks 1-3)\n\n**Objectives**:\n- Deep dive into MoE-Lightning paper and codebase\n- Set up development environment\n- Implement basic prototype\n\n**Tasks**:\n1. **Code Analysis**\n   - Study MoE-Lightning reference implementation (if available)\n   - Analyze vLLM and SGLang MoE support\n   - Document API interfaces and extension points\n\n2. **Prototype Development**\n   - Implement basic HRM model for simple 2-level hierarchy (CPU/GPU)\n   - Create simplified weight paging mechanism\n   - Benchmark baseline performance with existing systems\n\n3. **Environment Setup**\n   - Configure test environments with various GPU configs (T4, L4)\n   - Set up profiling tools (NVIDIA Nsight, Intel VTune)\n   - Prepare test datasets (MTBench, HELM benchmarks)\n\n**Deliverables**:\n- Technical design document with architecture diagrams\n- Proof-of-concept code demonstrating HRM policy optimization\n- Baseline performance benchmarks\n\n**Success Criteria**:\n- HRM model correctly predicts bottleneck resources\n- Prototype shows measurable improvement over naive offloading\n- Development environment ready for full implementation\n\n---\n\n### Phase 2: Core Components Implementation (Weeks 4-8)\n\n**Objectives**:\n- Implement CGOPipe scheduler\n- Develop weight paging system\n- Create CPU attention kernels\n\n**Tasks**:\n\n#### 2.1 HRM Performance Model (Week 4-5)\n```python\n# aios/inference/moe_lightning/hrm/model.py\nclass HierarchicalRooflineModel:\n    \"\"\"\n    Performance model for heterogeneous MoE inference\n    \"\"\"\n    def __init__(self, hardware_config, model_config):\n        self.hw = hardware_config\n        self.model = model_config\n        \n    def estimate_latency(self, policy: InferencePolicy) -> float:\n        \"\"\"Estimate per-layer decode latency\"\"\"\n        T_comm = self._compute_communication_time(policy)\n        T_cpu = self._compute_cpu_time(policy)\n        T_gpu = self._compute_gpu_time(policy)\n        \n        return max(T_comm, T_cpu, T_gpu)\n    \n    def optimize_policy(self, workload_config) -> InferencePolicy:\n        \"\"\"Use MILP to find optimal policy\"\"\"\n        # Implement using scipy.optimize or CVXPY\n        pass\n```\n\n#### 2.2 CGOPipe Scheduler (Week 5-6)\n```python\n# aios/inference/moe_lightning/scheduler/cgopipe.py\nclass CGOPipeScheduler:\n    \"\"\"\n    CPU-GPU-I/O Pipeline Scheduler with weights paging\n    \"\"\"\n    def __init__(self, policy: InferencePolicy, model, device_manager):\n        self.policy = policy\n        self.model = model\n        self.dm = device_manager\n        \n        # Initialize CUDA streams\n        self.gpu_stream = torch.cuda.Stream()\n        self.transfer_stream = torch.cuda.Stream()\n        \n        # Initialize weight paging\n        self.weight_pager = WeightPagingManager(\n            layer_weight_size=model.layer_size,\n            num_pages=policy.\u03bc\n        )\n    \n    def execute_decode_step(self, microbatches):\n        \"\"\"Execute one decode step with pipelined scheduling\"\"\"\n        # Implement Algorithm 1 from paper\n        pass\n```\n\n#### 2.3 Weight Paging System (Week 6-7)\n```python\n# aios/inference/moe_lightning/memory/weight_paging.py\nclass WeightPagingManager:\n    \"\"\"\n    Manages paged transfers of model weights between CPU and GPU\n    \"\"\"\n    def __init__(self, layer_weight_size, num_pages):\n        self.page_size = layer_weight_size // num_pages\n        self.num_pages = num_pages\n        \n        # Allocate pinned memory buffer\n        self.pinned_buffer = self._allocate_pinned_buffer()\n        \n        # Page table for expert routing\n        self.page_table = PageTable()\n    \n    def prefetch_page(self, layer_id, page_id, stream):\n        \"\"\"Asynchronously prefetch weight page\"\"\"\n        # CPU DRAM \u2192 CPU Pinned (background thread)\n        self._copy_to_pinned_async(layer_id, page_id)\n        \n        # CPU Pinned \u2192 GPU (CUDA stream)\n        self._copy_to_gpu_async(layer_id, page_id, stream)\n```\n\n#### 2.4 CPU Attention Kernels (Week 7-8)\n```python\n# aios/inference/moe_lightning/kernels/cpu_attention.py\nimport intel_extension_for_pytorch as ipex\n\nclass CPUGroupedQueryAttention:\n    \"\"\"\n    Optimized CPU attention using Intel MKL\n    \"\"\"\n    def __init__(self, num_heads, num_kv_heads, head_dim):\n        self.num_heads = num_heads\n        self.num_kv_heads = num_kv_heads\n        self.head_dim = head_dim\n        \n        # Configure MKL threads\n        torch.set_num_threads(self._get_optimal_threads())\n    \n    @torch.jit.script\n    def forward(self, query, key_cache, value_cache, seq_lens):\n        \"\"\"\n        Compute attention on CPU with GQA optimization\n        \n        Args:\n            query: [batch, num_heads, head_dim]\n            key_cache: [batch, max_seq_len, num_kv_heads, head_dim]\n            value_cache: [batch, max_seq_len, num_kv_heads, head_dim]\n            seq_lens: [batch]\n        \"\"\"\n        # Implement optimized GQA with SIMD vectorization\n        pass\n```\n\n**Deliverables**:\n- Fully functional HRM model with policy optimizer\n- Working CGOPipe scheduler with async execution\n- CPU attention kernels matching or exceeding KV cache transfer speed\n- Weight paging system with double buffering\n\n**Success Criteria**:\n- HRM policy optimizer runs in <1 minute for typical configs\n- CGOPipe achieves >80% resource utilization (GPU, CPU, I/O)\n- CPU attention is 3-4\u00d7 faster than KV cache transfer\n- Weight paging reduces pipeline bubbles by >50%\n\n---\n\n### Phase 3: Integration & Optimization (Weeks 9-12)\n\n**Objectives**:\n- Integrate components into AI-OS\n- Implement request batching\n- Optimize end-to-end performance\n\n**Tasks**:\n\n#### 3.1 AI-OS Integration (Week 9-10)\n- Create `aios/inference/moe_lightning/` module structure\n- Implement unified inference API\n- Add MoE detection logic to model loading\n- Extend CLI with MoE-Lightning commands\n\n#### 3.2 Request Batching System (Week 10)\n```python\n# aios/inference/moe_lightning/batching/request_batcher.py\nclass VariableLengthBatcher:\n    \"\"\"\n    Batch variable-length requests without padding\n    \"\"\"\n    def create_microbatches(self, requests, policy):\n        \"\"\"\n        Implement Algorithm 2 from paper\n        Balances token distribution across micro-batches\n        \"\"\"\n        pass\n```\n\n#### 3.3 Tensor Parallelism Support (Week 11)\n```python\n# aios/inference/moe_lightning/distributed/tensor_parallel.py\nclass TensorParallelExecutor:\n    \"\"\"\n    Execute MoE inference with tensor parallelism across GPUs\n    \"\"\"\n    def __init__(self, num_gpus, policy):\n        self.num_gpus = num_gpus\n        self.device_mesh = self._create_device_mesh()\n        \n        # Scale policy parameters for multiple GPUs\n        self.adjusted_policy = self._adjust_policy_for_tp(policy)\n```\n\n#### 3.4 Performance Profiling & Optimization (Week 11-12)\n- Profile with NVIDIA Nsight Systems\n- Identify bottlenecks in data transfer and synchronization\n- Optimize kernel launch overhead\n- Tune thread counts for CPU operations\n- Implement caching for policy optimization results\n\n**Deliverables**:\n- Complete integration with AI-OS inference pipeline\n- Variable-length batching system\n- Tensor parallelism for multi-GPU setups\n- Performance optimization report\n\n**Success Criteria**:\n- API is compatible with existing AI-OS inference workflows\n- Variable-length batching provides 2-3\u00d7 memory savings vs padding\n- Tensor parallelism achieves >2.5\u00d7 speedup on 4 GPUs vs 2 GPUs\n- End-to-end latency overhead <5% compared to direct execution\n\n---\n\n### Phase 4: Testing & Validation (Weeks 13-15)\n\n**Objectives**:\n- Comprehensive testing across models and hardware\n- Validate performance claims\n- Ensure numerical correctness\n\n**Tasks**:\n\n#### 4.1 Correctness Testing (Week 13)\n- Compare outputs with reference implementations (vLLM, transformers)\n- Test with multiple MoE architectures (Mixtral, DBRX, DeepSeek-MoE)\n- Validate attention correctness (CPU vs GPU implementation)\n- Memory safety checks (no leaks, proper cleanup)\n\n#### 4.2 Performance Benchmarking (Week 14)\n```python\n# tests/benchmarks/test_moe_lightning_performance.py\nclass MoELightningBenchmarks:\n    \"\"\"\n    Comprehensive performance benchmarks\n    \"\"\"\n    def benchmark_throughput(self, model, hardware, workload):\n        \"\"\"\n        Measure end-to-end throughput for various configurations\n        \n        Compare against:\n        - FlexGen\n        - DeepSpeed-Inference\n        - vLLM (if fits in memory)\n        \"\"\"\n        pass\n    \n    def benchmark_memory_efficiency(self, model, hardware):\n        \"\"\"\n        Measure CPU/GPU memory usage at peak throughput\n        \"\"\"\n        pass\n    \n    def benchmark_scaling(self, model, num_gpus_list):\n        \"\"\"\n        Test tensor parallelism scaling efficiency\n        \"\"\"\n        pass\n```\n\n**Test Matrices**:\n\n| Model | Hardware | Workload | Expected Speedup |\n|-------|----------|----------|------------------|\n| Mixtral 8x7B | 1\u00d7T4 (16GB) | MTBench (gen_len=128) | 3.5\u00d7 vs FlexGen |\n| Mixtral 8x7B | 1\u00d7L4 (24GB) | HELM Reasoning | 5\u00d7 vs FlexGen |\n| Mixtral 8x22B | 2\u00d7T4 (32GB) | MTBench (gen_len=64) | 2.8\u00d7 vs FlexGen |\n| Mixtral 8x22B | 4\u00d7T4 (64GB) | MTBench (gen_len=64) | Super-linear scaling |\n| DBRX | 4\u00d7T4 (64GB) | MTBench (gen_len=128) | 2.1-2.8\u00d7 scaling |\n\n#### 4.3 Stress Testing (Week 15)\n- Long-running inference jobs (24+ hours)\n- Extreme batch sizes (pushing memory limits)\n- Error handling and recovery\n- Multi-user concurrent requests\n\n**Deliverables**:\n- Comprehensive test suite with >90% coverage\n- Benchmark results report comparing to baseline systems\n- Validated correctness across all supported models\n- Stress test results and reliability metrics\n\n**Success Criteria**:\n- All correctness tests pass with numerical differences <1e-5\n- Achieve paper's reported speedups (within 10% margin)\n- No memory leaks or crashes in 24-hour stress tests\n- Error recovery works for common failure modes\n\n---\n\n### Phase 5: Documentation & Deployment (Weeks 16-17)\n\n**Objectives**:\n- Create comprehensive documentation\n- Prepare for production deployment\n- Train users and gather feedback\n\n**Tasks**:\n\n#### 5.1 User Documentation (Week 16)\n```markdown\n# docs/guide/moe_lightning_quickstart.md\n## MoE-Lightning Quick Start\n\nLearn how to run large MoE models on consumer GPUs with MoE-Lightning.\n\n### Installation\n### Basic Usage\n### Configuration Guide\n### Performance Tuning\n### Troubleshooting\n```\n\n#### 5.2 Developer Documentation (Week 16)\n- API reference documentation\n- Architecture diagrams and design decisions\n- Contribution guidelines for MoE-Lightning components\n- Performance profiling guide\n\n#### 5.3 Example Notebooks (Week 17)\n```python\n# examples/moe_lightning_mixtral.ipynb\n\"\"\"\nRunning Mixtral 8x7B on a Single T4 GPU\n\nThis notebook demonstrates:\n1. Model loading and configuration\n2. Policy optimization for your hardware\n3. Running inference with MoE-Lightning\n4. Comparing performance to baseline systems\n\"\"\"\n```\n\n#### 5.4 Deployment Preparation (Week 17)\n- Docker images with optimized dependencies\n- Installation scripts for common platforms\n- Hardware compatibility matrix\n- Known issues and workarounds\n\n**Deliverables**:\n- Complete user and developer documentation\n- Example notebooks and tutorials\n- Deployment artifacts (Docker images, installers)\n- Performance tuning guide\n\n**Success Criteria**:\n- Documentation covers all use cases and configurations\n- New users can run first inference within 15 minutes\n- Examples run successfully on documented hardware\n- Docker deployment works on Ubuntu 20.04/22.04 and Windows 11\n\n---\n\n### Phase 6: Advanced Features & Extensions (Weeks 18-20)\n\n**Objectives**:\n- Add advanced optimizations\n- Support additional models and hardware\n- Integrate with AI-OS ecosystem\n\n**Tasks**:\n\n#### 6.1 Extended Hardware Support (Week 18)\n- AMD GPU support (ROCm)\n- Apple Silicon support (MPS backend)\n- Intel GPU support (oneAPI)\n- Multi-node distributed inference\n\n#### 6.2 Advanced Optimizations (Week 19)\n- KV cache quantization (INT4, INT8)\n- Sparse attention patterns\n- Expert caching for common routing patterns\n- Dynamic policy adjustment based on runtime metrics\n\n#### 6.3 AI-OS Ecosystem Integration (Week 20)\n- Integration with Expert Manager for MoE expert tracking\n- Memory estimation updates for MoE-Lightning\n- Dream system integration for synthetic data generation\n- CLI enhancements for interactive optimization\n\n**Deliverables**:\n- Multi-platform hardware support\n- Advanced optimization features\n- Deep integration with AI-OS features\n\n**Success Criteria**:\n- Works on at least 2 additional hardware platforms\n- Advanced optimizations provide additional 1.2-1.5\u00d7 speedup\n- Seamless integration with existing AI-OS workflows\n\n---\n\n## Technical Requirements\n\n### Hardware Requirements\n\n#### Minimum Configuration\n- **GPU**: NVIDIA T4 (16GB) or equivalent\n- **CPU**: 8-core, 2.0+ GHz\n- **RAM**: 64GB DDR4\n- **Storage**: 500GB SSD for model weights\n- **PCIe**: Gen3 x16 for optimal CPU-GPU bandwidth\n\n#### Recommended Configuration\n- **GPU**: NVIDIA L4 (24GB) or 2\u00d7 T4 (32GB total)\n- **CPU**: 16-core, 2.5+ GHz (e.g., Intel Xeon)\n- **RAM**: 128GB DDR4 or better\n- **Storage**: 1TB NVMe SSD\n- **PCIe**: Gen4 x16\n\n#### Optimal Configuration\n- **GPU**: 4\u00d7 NVIDIA T4 (64GB) or 2\u00d7 A100 (80GB each)\n- **CPU**: 32-core, 3.0+ GHz\n- **RAM**: 256GB+ DDR5\n- **Storage**: 2TB NVMe SSD RAID\n- **Network**: 100Gbps for multi-node (future)\n\n### Software Requirements\n\n#### Core Dependencies\n```python\n# pyproject.toml additions\n[project.dependencies]\ntorch = \">=2.1.0\"\nintel-extension-for-pytorch = \">=2.1.0\"  # For CPU kernels\nvllm = \">=0.2.0\"  # For MoE support\nsglang = \">=0.1.0\"  # For structured generation\nscipy = \">=1.10.0\"  # For optimization\ncvxpy = \">=1.4.0\"  # For MILP solver (optional)\n```\n\n#### System Requirements\n- **CUDA**: 12.1+ (for NVIDIA GPUs)\n- **cuDNN**: 8.9+\n- **Intel MKL**: 2023.0+ (for CPU operations)\n- **Python**: 3.10+\n- **OS**: Ubuntu 20.04+, Windows 11, or macOS 13+\n\n### Model Support\n\n#### Supported Architectures\n1. **Mixtral Family**\n   - Mixtral 8x7B (~47GB)\n   - Mixtral 8x22B (~141GB)\n   \n2. **DBRX**\n   - DBRX 132B (16 experts)\n   \n3. **DeepSeek-MoE**\n   - DeepSeek-MoE 16B\n   - DeepSeek-MoE 145B\n\n4. **Future Support**\n   - Custom MoE architectures\n   - Dense model fallback mode\n\n#### Model Format Support\n- HuggingFace Transformers format\n- SafeTensors format (preferred)\n- GGUF format (via conversion)\n\n---\n\n## Performance Targets\n\n### Throughput Targets\n\nBased on paper results, we target the following throughput improvements:\n\n#### Single GPU (T4 16GB)\n| Workload | Baseline (FlexGen) | Target (MoE-Lightning) | Speedup |\n|----------|-------------------|------------------------|---------|\n| MTBench (gen=32) | 6.5 tok/s | 22.8 tok/s | 3.5\u00d7 |\n| MTBench (gen=128) | 9.5 tok/s | 30.1 tok/s | 3.2\u00d7 |\n| HELM Reasoning | 16.9 tok/s | 26.3 tok/s | 1.6\u00d7 |\n| HELM Summarization | 2.6 tok/s | 4.5 tok/s | 1.7\u00d7 |\n\n#### Single GPU (L4 24GB)\n| Workload | Baseline (FlexGen) | Target (MoE-Lightning) | Speedup |\n|----------|-------------------|------------------------|---------|\n| MTBench (gen=128) | 20.7 tok/s | 105.3 tok/s | 5.1\u00d7 |\n| HELM Reasoning | 50.1 tok/s | 105.3 tok/s | 2.1\u00d7 |\n\n#### Multi-GPU (4\u00d7T4 64GB)\n| Model | 2\u00d7T4 | 4\u00d7T4 | Scaling Factor |\n|-------|------|------|----------------|\n| Mixtral 8x22B | 25.3 tok/s | 70.2 tok/s | 2.77\u00d7 |\n| DBRX | 22.1 tok/s | 58.3 tok/s | 2.64\u00d7 |\n\n### Memory Efficiency Targets\n\n| Metric | Target | Baseline |\n|--------|--------|----------|\n| CPU Memory at Peak Throughput | 100GB | 200GB+ |\n| GPU Memory Utilization | >85% | 60-70% |\n| I/O Bandwidth Utilization | >90% | 50-60% |\n| Pipeline Bubble Reduction | >50% | N/A |\n\n### Latency Targets\n\n| Phase | Target | Acceptable Range |\n|-------|--------|------------------|\n| Policy Optimization | <1 minute | <5 minutes |\n| Model Loading | <30 seconds | <60 seconds |\n| First Token Latency | <2 seconds | <5 seconds |\n| Per-token Latency (decode) | <100ms | <200ms |\n\n---\n\n## Risk Assessment\n\n### Technical Risks\n\n#### Risk 1: Performance Below Targets\n**Probability**: Medium  \n**Impact**: High\n\n**Description**: Achieved performance doesn't match paper's reported improvements\n\n**Mitigation**:\n- Start with exact replication of paper's test setup\n- Profile extensively to identify bottlenecks\n- Engage with paper authors for implementation guidance\n- Have fallback to incremental improvements (e.g., 2\u00d7 instead of 10\u00d7)\n\n#### Risk 2: Hardware Compatibility Issues\n**Probability**: Medium  \n**Impact**: Medium\n\n**Description**: CPU attention or weight paging doesn't work on all hardware\n\n**Mitigation**:\n- Test on multiple hardware configurations early\n- Implement fallback to GPU-only execution\n- Use platform-agnostic libraries where possible\n- Maintain compatibility matrix in documentation\n\n#### Risk 3: Memory Management Complexity\n**Probability**: High  \n**Impact**: High\n\n**Description**: Memory leaks or fragmentation under high load\n\n**Mitigation**:\n- Extensive memory profiling (valgrind, CUDA sanitizers)\n- Implement comprehensive cleanup logic\n- Use smart pointers and RAII patterns\n- Regular stress testing during development\n\n#### Risk 4: Integration Conflicts\n**Probability**: Medium  \n**Impact**: Medium\n\n**Description**: Conflicts with existing AI-OS inference systems\n\n**Mitigation**:\n- Design clean interface boundaries\n- Make integration opt-in initially\n- Comprehensive integration testing\n- Version compatibility testing\n\n### Project Risks\n\n#### Risk 5: Scope Creep\n**Probability**: High  \n**Impact**: Medium\n\n**Description**: Feature requests expand beyond core MoE-Lightning\n\n**Mitigation**:\n- Clearly define Phase 1-3 deliverables as MVP\n- Defer advanced features to Phase 6\n- Regular scope reviews with stakeholders\n- Maintain feature backlog for future work\n\n#### Risk 6: Resource Constraints\n**Probability**: Medium  \n**Impact**: High\n\n**Description**: Insufficient GPU resources for testing\n\n**Mitigation**:\n- Use cloud resources (GCP, AWS) for expensive tests\n- Prioritize tests on available hardware\n- Implement simulation mode for policy testing\n- Partner with organizations with GPU access\n\n#### Risk 7: Dependency Changes\n**Probability**: Medium  \n**Impact**: Medium\n\n**Description**: Breaking changes in PyTorch, vLLM, or other dependencies\n\n**Mitigation**:\n- Pin dependency versions initially\n- Monitor upstream changes\n- Contribute to upstream projects\n- Maintain compatibility layer\n\n---\n\n## Testing Strategy\n\n### Unit Testing\n\n**Coverage Target**: >90% for core components\n\n```python\n# tests/unit/test_hrm_model.py\nclass TestHierarchicalRooflineModel:\n    def test_compute_roofs(self):\n        \"\"\"Test compute and memory roof calculations\"\"\"\n        \n    def test_turning_points(self):\n        \"\"\"Test turning point identification\"\"\"\n        \n    def test_policy_optimization(self):\n        \"\"\"Test MILP policy search\"\"\"\n        \n    def test_memory_constraints(self):\n        \"\"\"Test policy respects memory limits\"\"\"\n\n# tests/unit/test_cgopipe.py\nclass TestCGOPipeScheduler:\n    def test_async_execution(self):\n        \"\"\"Test asynchronous task execution\"\"\"\n        \n    def test_synchronization(self):\n        \"\"\"Test data dependency enforcement\"\"\"\n        \n    def test_weight_paging(self):\n        \"\"\"Test weight page scheduling\"\"\"\n\n# tests/unit/test_cpu_attention.py\nclass TestCPUAttention:\n    def test_correctness(self):\n        \"\"\"Compare output with reference implementation\"\"\"\n        \n    def test_performance(self):\n        \"\"\"Verify speedup vs KV cache transfer\"\"\"\n```\n\n### Integration Testing\n\n```python\n# tests/integration/test_moe_lightning_inference.py\nclass TestMoELightningInference:\n    def test_mixtral_8x7b_single_gpu(self):\n        \"\"\"Test Mixtral 8x7B on single T4\"\"\"\n        \n    def test_mixtral_8x22b_multi_gpu(self):\n        \"\"\"Test Mixtral 8x22B on multiple GPUs\"\"\"\n        \n    def test_dbrx_inference(self):\n        \"\"\"Test DBRX model\"\"\"\n        \n    def test_variable_length_batching(self):\n        \"\"\"Test with mixed prompt lengths\"\"\"\n```\n\n### Performance Testing\n\n```python\n# tests/performance/test_throughput.py\nclass TestThroughput:\n    @pytest.mark.benchmark\n    def test_mtbench_t4(self):\n        \"\"\"Benchmark MTBench on T4 GPU\"\"\"\n        assert throughput > 22.8  # tokens/sec\n        \n    @pytest.mark.benchmark\n    def test_helm_reasoning_l4(self):\n        \"\"\"Benchmark HELM reasoning on L4\"\"\"\n        assert throughput > 105.3  # tokens/sec\n        \n    @pytest.mark.benchmark\n    def test_scaling_4xT4(self):\n        \"\"\"Test super-linear scaling\"\"\"\n        scaling_factor = throughput_4gpu / throughput_2gpu\n        assert scaling_factor > 2.5\n```\n\n### Correctness Testing\n\n```python\n# tests/correctness/test_numerical_accuracy.py\nclass TestNumericalAccuracy:\n    def test_cpu_vs_gpu_attention(self):\n        \"\"\"Verify CPU attention matches GPU\"\"\"\n        max_diff = compute_max_difference(cpu_output, gpu_output)\n        assert max_diff < 1e-5\n        \n    def test_vs_vllm_reference(self):\n        \"\"\"Compare outputs with vLLM\"\"\"\n        assert outputs_match(moe_lightning_output, vllm_output)\n```\n\n### Stress Testing\n\n```python\n# tests/stress/test_reliability.py\nclass TestReliability:\n    def test_24_hour_continuous_inference(self):\n        \"\"\"Run inference for 24 hours\"\"\"\n        \n    def test_memory_leak_detection(self):\n        \"\"\"Monitor memory usage over 1000 batches\"\"\"\n        \n    def test_concurrent_requests(self):\n        \"\"\"Handle 100 concurrent requests\"\"\"\n```\n\n---\n\n## Future Enhancements\n\n### Short-term (6 months)\n\n1. **Flash Attention Integration**\n   - Integrate Flash Attention 2/3 for GPU attention\n   - Further reduce memory footprint\n   - Improve attention performance\n\n2. **Quantization Support**\n   - INT8/INT4 weight quantization\n   - KV cache quantization\n   - GPTQ/AWQ integration\n\n3. **Speculative Decoding**\n   - Use smaller MoE model as draft\n   - Improve latency for interactive use cases\n\n4. **Expert Caching**\n   - Cache frequently activated experts on GPU\n   - Dynamic expert placement based on routing patterns\n\n### Mid-term (12 months)\n\n1. **Multi-Node Distributed Inference**\n   - Pipeline parallelism across nodes\n   - Expert parallelism\n   - Optimize for cluster environments\n\n2. **Continuous Batching**\n   - Orca-style continuous batching\n   - Improve throughput for serving workloads\n\n3. **Adaptive Policy Selection**\n   - Runtime policy adjustment\n   - Workload-aware optimization\n   - Reinforcement learning for policy search\n\n4. **AMD/Intel GPU Support**\n   - ROCm backend for AMD GPUs\n   - OneAPI backend for Intel GPUs\n   - Multi-vendor heterogeneous execution\n\n### Long-term (18+ months)\n\n1. **Automatic Model Parallelism**\n   - Automatic sharding for arbitrary MoE sizes\n   - Mixed expert and tensor parallelism\n   - Cost-aware placement optimization\n\n2. **Disk Offloading**\n   - NVMe SSD integration for very large models\n   - Intelligent prefetching\n   - Compression for disk storage\n\n3. **Custom CUDA Kernels**\n   - Fused MoE kernels\n   - Optimized expert routing\n   - Custom attention implementations\n\n4. **Neural Architecture Search for MoE**\n   - Automatic expert configuration\n   - Router optimization\n   - Efficient expert specialization\n\n---\n\n## Success Metrics\n\n### Technical Metrics\n\n| Metric | Target | Method |\n|--------|--------|--------|\n| Throughput vs FlexGen | 3.5-10\u00d7 | Benchmark comparison |\n| Memory efficiency | 2-3\u00d7 less CPU RAM | Memory profiling |\n| GPU utilization | >85% | NVIDIA profiler |\n| I/O utilization | >90% | Bandwidth monitoring |\n| Scaling efficiency (4 GPUs) | >2.5\u00d7 vs 2 GPUs | Multi-GPU benchmarks |\n| Policy search time | <1 minute | Timer measurement |\n| First token latency | <2 seconds | Latency profiling |\n\n### Project Metrics\n\n| Metric | Target | Method |\n|--------|--------|--------|\n| Code coverage | >90% | pytest-cov |\n| Documentation coverage | 100% of public APIs | Doc review |\n| User adoption | 50+ users in first month | Analytics |\n| Bug reports | <5 critical bugs | Issue tracking |\n| Performance regression | <5% | CI/CD benchmarks |\n| Community contributions | 5+ contributors | GitHub metrics |\n\n### User Experience Metrics\n\n| Metric | Target | Method |\n|--------|--------|--------|\n| Time to first inference | <15 minutes | User studies |\n| Setup success rate | >90% | Telemetry |\n| User satisfaction | >4/5 rating | Surveys |\n| Documentation clarity | >4/5 rating | Feedback forms |\n\n---\n\n## References\n\n### Primary Paper\n- **MoE-Lightning**: Shiyi Cao et al., \"MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs,\" arXiv:2411.11217, 2024. [Link](https://arxiv.org/html/2411.11217)\n\n### Related Papers\n\n#### MoE Architectures\n- **Mixtral**: \"Mixtral of Experts,\" Mistral AI, 2024\n- **DBRX**: \"Introducing DBRX,\" Databricks, 2024\n- **DeepSeek-MoE**: \"DeepSeekMoE: Towards Ultimate Expert Specialization,\" 2024\n- **GShard**: Lepikhin et al., \"GShard: Scaling Giant Models with Conditional Computation,\" 2020\n\n#### Performance Modeling\n- **Roofline Model**: Williams et al., \"Roofline: An Insightful Visual Performance Model,\" CACM 2009\n- **LLM Inference Analysis**: Yuan et al., \"LLM Inference Unveiled: Survey and Roofline Model Insights,\" 2024\n\n#### Inference Systems\n- **FlexGen**: Sheng et al., \"FlexGen: High-throughput Generative Inference,\" ICML 2023\n- **vLLM**: Kwon et al., \"Efficient Memory Management for LLM Serving with PagedAttention,\" SOSP 2023\n- **DeepSpeed-Inference**: Aminabadi et al., \"DeepSpeed-Inference: Enabling Efficient Inference,\" SC 2022\n- **FastDecode**: He & Zhai, \"FastDecode: High-throughput GPU-efficient LLM Serving,\" 2024\n\n#### Optimization Techniques\n- **Flash Attention**: Dao et al., \"FlashAttention: Fast and Memory-Efficient Exact Attention,\" NeurIPS 2022\n- **Flash Attention 2**: Dao, \"FlashAttention-2: Faster Attention with Better Parallelism,\" ICLR 2024\n- **Speculative Decoding**: Chen et al., \"Accelerating LLM Decoding with Speculative Sampling,\" 2023\n\n### Implementation References\n- PyTorch Documentation: https://pytorch.org/docs/stable/\n- vLLM GitHub: https://github.com/vllm-project/vllm\n- SGLang GitHub: https://github.com/sgl-project/sglang\n- Intel Extension for PyTorch: https://github.com/intel/intel-extension-for-pytorch\n- HuggingFace Transformers: https://github.com/huggingface/transformers\n\n### AI-OS Related\n- Existing AI-OS architecture documentation\n- Memory estimation system (`artifacts/memory_estimation/`)\n- Expert management system (`artifacts/experts/`)\n- HRM training integration (`aios/cli/aios.py` - HRM commands)\n\n---\n\n## Appendix\n\n### A. Glossary\n\n- **CGOPipe**: CPU-GPU-I/O Pipeline scheduling strategy\n- **HRM**: Hierarchical Roofline Model\n- **MoE**: Mixture of Experts\n- **GQA**: Grouped Query Attention\n- **FFN**: Feed-Forward Network\n- **Operational Intensity**: Ratio of FLOPs to bytes accessed (FLOPs/Byte)\n- **Roofline Model**: Performance model correlating compute and memory bandwidth\n- **Turning Point**: Critical operational intensity where bottleneck resource changes\n- **Balance Point**: Optimal configuration where all resources are fully utilized\n- **Micro-batch**: Subset of batch that fits in GPU memory for one kernel execution\n- **Weight Paging**: Technique of chunking and scheduling weight transfers\n\n### B. Hardware Specifications\n\n#### NVIDIA T4\n- Memory: 16GB GDDR6\n- Memory Bandwidth: 320 GB/s\n- Compute (FP16): 65 TFLOPS\n- TDP: 70W\n- Use Case: Cost-effective inference\n\n#### NVIDIA L4\n- Memory: 24GB GDDR6\n- Memory Bandwidth: 300 GB/s\n- Compute (FP16): 121 TFLOPS\n- TDP: 72W\n- Use Case: Balanced performance/cost\n\n#### NVIDIA A100\n- Memory: 40GB or 80GB HBM2e\n- Memory Bandwidth: 1.6 TB/s (40GB) / 2.0 TB/s (80GB)\n- Compute (FP16): 312 TFLOPS\n- TDP: 400W\n- Use Case: High-performance inference\n\n### C. Model Specifications\n\n#### Mixtral 8x7B\n- Total Parameters: 46.7B\n- Active Parameters: 12.9B per token\n- Experts: 8 per MoE layer\n- Top-K: 2\n- Hidden Dim: 4096\n- Intermediate Dim: 14336\n- Layers: 32\n- Memory (FP16): ~94GB\n\n#### Mixtral 8x22B\n- Total Parameters: 141B\n- Active Parameters: ~39B per token\n- Experts: 8 per MoE layer\n- Top-K: 2\n- Hidden Dim: 6144\n- Memory (FP16): ~282GB\n\n#### DBRX\n- Total Parameters: 132B\n- Active Parameters: 36B per token\n- Experts: 16 per MoE layer\n- Top-K: 4\n- Layers: 40\n- Memory (FP16): ~264GB\n\n### D. Configuration Examples\n\n#### config/moe_lightning.yaml\n```yaml\n# Hardware profiles\nhardware:\n  t4_single:\n    gpu_memory: 16384  # MB\n    cpu_memory: 65536  # MB\n    gpu_bandwidth: 320  # GB/s\n    cpu_bandwidth: 100  # GB/s\n    cpu_to_gpu_bandwidth: 16  # GB/s (PCIe Gen3 x16)\n    gpu_compute: 65  # TFLOPS (FP16)\n    cpu_compute: 1.6  # TFLOPS\n    \n  l4_single:\n    gpu_memory: 24576\n    cpu_memory: 65536\n    gpu_bandwidth: 300\n    cpu_bandwidth: 120\n    cpu_to_gpu_bandwidth: 16\n    gpu_compute: 121\n    cpu_compute: 1.6\n\n# Model configurations\nmodels:\n  mixtral-8x7b:\n    num_layers: 32\n    hidden_dim: 4096\n    intermediate_dim: 14336\n    num_experts: 8\n    top_k: 2\n    num_heads: 32\n    num_kv_heads: 8\n    \n# Default policies (auto-optimized if not specified)\npolicies:\n  mixtral-8x7b-t4:\n    batch_size: 36\n    micro_batch_size: 4\n    use_cpu_attention: true\n    use_gpu_ffn: true\n    weight_gpu_ratio: 0.0\n    kv_cache_gpu_ratio: 0.0\n```\n\n---\n\n## Contact & Collaboration\n\n**Project Lead**: [To be assigned]  \n**Technical Advisors**: [Paper authors - optional consultation]  \n**Discussion Forum**: GitHub Discussions in AI-OS repo  \n**Issue Tracking**: GitHub Issues with label `moe-lightning`\n\n**Collaboration Opportunities**:\n- Hardware vendors: Testing on diverse GPU configurations\n- Research institutions: Advanced optimization techniques\n- Open-source community: Code contributions and testing\n\n---\n\n**Document Version**: 1.0  \n**Last Updated**: November 8, 2025  \n**Next Review**: December 8, 2025\n", "tags": ["experts", "training"], "headings": [{"line": 0, "text": "MoE-Lightning Integration Plan"}, {"line": 2, "text": "Overview"}, {"line": 15, "text": "Table of Contents"}, {"line": 31, "text": "Motivation"}, {"line": 33, "text": "Problem Statement"}, {"line": 44, "text": "Benefits of Integration"}, {"line": 55, "text": "Technical Background"}, {"line": 57, "text": "Mixture of Experts (MoE) Architecture"}, {"line": 65, "text": "Key Innovations in MoE-Lightning"}, {"line": 67, "text": "1. CGOPipe (CPU-GPU-I/O Pipeline Schedule)"}, {"line": 81, "text": "2. HRM (Hierarchical Roofline Model)"}, {"line": 109, "text": "3. Tensor Parallelism"}, {"line": 117, "text": "Performance Analysis Insights"}, {"line": 119, "text": "Attention Block"}, {"line": 124, "text": "MoE FFN Block"}, {"line": 131, "text": "Core Components"}, {"line": 133, "text": "Component 1: CGOPipe Scheduler"}, {"line": 139, "text": "Pseudo-code for CGOPipe execution order"}, {"line": 141, "text": "Prologue (first 2 micro-batches)"}, {"line": 148, "text": "Main pipeline (steady state)"}, {"line": 151, "text": "Parallel execution"}, {"line": 164, "text": "Component 2: HRM Performance Model"}, {"line": 197, "text": "Use MILP solver for policy search"}, {"line": 198, "text": "Takes <1 minute for offline optimization"}, {"line": 206, "text": "Component 3: Memory Management System"}, {"line": 215, "text": "Allocate 2\u00d7 layer weight buffer on GPU"}, {"line": 220, "text": "Page table for MoE expert routing"}, {"line": 224, "text": "CPU DRAM \u2192 CPU Pinned Memory"}, {"line": 227, "text": "CPU Pinned Memory \u2192 GPU (overlapped)"}, {"line": 236, "text": "Component 4: CPU Attention Kernels"}, {"line": 250, "text": "Component 5: Request Batching System"}, {"line": 262, "text": "Sort requests by length (descending)"}, {"line": 268, "text": "Greedy assignment to micro-batch with fewest tokens"}, {"line": 279, "text": "Integration Architecture"}, {"line": 281, "text": "System Architecture Diagram"}, {"line": 320, "text": "Integration Points"}, {"line": 322, "text": "1. Model Loading Layer"}, {"line": 327, "text": "2. Inference Engine Layer"}, {"line": 332, "text": "3. Memory Management Layer"}, {"line": 337, "text": "4. CLI Integration"}, {"line": 344, "text": "5. Configuration Layer"}, {"line": 351, "text": "Implementation Phases"}, {"line": 353, "text": "Phase 1: Foundation & Research (Weeks 1-3)"}, {"line": 388, "text": "Phase 2: Core Components Implementation (Weeks 4-8)"}, {"line": 397, "text": "2.1 HRM Performance Model (Week 4-5)"}, {"line": 399, "text": "aios/inference/moe_lightning/hrm/model.py"}, {"line": 418, "text": "Implement using scipy.optimize or CVXPY"}, {"line": 422, "text": "2.2 CGOPipe Scheduler (Week 5-6)"}, {"line": 424, "text": "aios/inference/moe_lightning/scheduler/cgopipe.py"}, {"line": 434, "text": "Initialize CUDA streams"}, {"line": 438, "text": "Initialize weight paging"}, {"line": 446, "text": "Implement Algorithm 1 from paper"}, {"line": 450, "text": "2.3 Weight Paging System (Week 6-7)"}, {"line": 452, "text": "aios/inference/moe_lightning/memory/weight_paging.py"}, {"line": 461, "text": "Allocate pinned memory buffer"}, {"line": 464, "text": "Page table for expert routing"}, {"line": 469, "text": "CPU DRAM \u2192 CPU Pinned (background thread)"}, {"line": 472, "text": "CPU Pinned \u2192 GPU (CUDA stream)"}, {"line": 476, "text": "2.4 CPU Attention Kernels (Week 7-8)"}, {"line": 478, "text": "aios/inference/moe_lightning/kernels/cpu_attention.py"}, {"line": 490, "text": "Configure MKL threads"}, {"line": 504, "text": "Implement optimized GQA with SIMD vectorization"}, {"line": 522, "text": "Phase 3: Integration & Optimization (Weeks 9-12)"}, {"line": 531, "text": "3.1 AI-OS Integration (Week 9-10)"}, {"line": 537, "text": "3.2 Request Batching System (Week 10)"}, {"line": 539, "text": "aios/inference/moe_lightning/batching/request_batcher.py"}, {"line": 552, "text": "3.3 Tensor Parallelism Support (Week 11)"}, {"line": 554, "text": "aios/inference/moe_lightning/distributed/tensor_parallel.py"}, {"line": 563, "text": "Scale policy parameters for multiple GPUs"}, {"line": 567, "text": "3.4 Performance Profiling & Optimization (Week 11-12)"}, {"line": 588, "text": "Phase 4: Testing & Validation (Weeks 13-15)"}, {"line": 597, "text": "4.1 Correctness Testing (Week 13)"}, {"line": 603, "text": "4.2 Performance Benchmarking (Week 14)"}, {"line": 605, "text": "tests/benchmarks/test_moe_lightning_performance.py"}, {"line": 644, "text": "4.3 Stress Testing (Week 15)"}, {"line": 664, "text": "Phase 5: Documentation & Deployment (Weeks 16-17)"}, {"line": 673, "text": "5.1 User Documentation (Week 16)"}, {"line": 675, "text": "docs/guide/moe_lightning_quickstart.md"}, {"line": 676, "text": "MoE-Lightning Quick Start"}, {"line": 680, "text": "Installation"}, {"line": 681, "text": "Basic Usage"}, {"line": 682, "text": "Configuration Guide"}, {"line": 683, "text": "Performance Tuning"}, {"line": 684, "text": "Troubleshooting"}, {"line": 687, "text": "5.2 Developer Documentation (Week 16)"}, {"line": 693, "text": "5.3 Example Notebooks (Week 17)"}, {"line": 695, "text": "examples/moe_lightning_mixtral.ipynb"}, {"line": 707, "text": "5.4 Deployment Preparation (Week 17)"}, {"line": 727, "text": "Phase 6: Advanced Features & Extensions (Weeks 18-20)"}, {"line": 736, "text": "6.1 Extended Hardware Support (Week 18)"}, {"line": 742, "text": "6.2 Advanced Optimizations (Week 19)"}, {"line": 748, "text": "6.3 AI-OS Ecosystem Integration (Week 20)"}, {"line": 766, "text": "Technical Requirements"}, {"line": 768, "text": "Hardware Requirements"}, {"line": 770, "text": "Minimum Configuration"}, {"line": 777, "text": "Recommended Configuration"}, {"line": 784, "text": "Optimal Configuration"}, {"line": 791, "text": "Software Requirements"}, {"line": 793, "text": "Core Dependencies"}, {"line": 795, "text": "pyproject.toml additions"}, {"line": 805, "text": "System Requirements"}, {"line": 812, "text": "Model Support"}, {"line": 814, "text": "Supported Architectures"}, {"line": 830, "text": "Model Format Support"}, {"line": 837, "text": "Performance Targets"}, {"line": 839, "text": "Throughput Targets"}, {"line": 843, "text": "Single GPU (T4 16GB)"}, {"line": 851, "text": "Single GPU (L4 24GB)"}, {"line": 857, "text": "Multi-GPU (4\u00d7T4 64GB)"}, {"line": 863, "text": "Memory Efficiency Targets"}, {"line": 872, "text": "Latency Targets"}, {"line": 883, "text": "Risk Assessment"}, {"line": 885, "text": "Technical Risks"}, {"line": 887, "text": "Risk 1: Performance Below Targets"}, {"line": 899, "text": "Risk 2: Hardware Compatibility Issues"}, {"line": 911, "text": "Risk 3: Memory Management Complexity"}, {"line": 923, "text": "Risk 4: Integration Conflicts"}, {"line": 935, "text": "Project Risks"}, {"line": 937, "text": "Risk 5: Scope Creep"}, {"line": 949, "text": "Risk 6: Resource Constraints"}, {"line": 961, "text": "Risk 7: Dependency Changes"}, {"line": 975, "text": "Testing Strategy"}, {"line": 977, "text": "Unit Testing"}, {"line": 982, "text": "tests/unit/test_hrm_model.py"}, {"line": 996, "text": "tests/unit/test_cgopipe.py"}, {"line": 1007, "text": "tests/unit/test_cpu_attention.py"}, {"line": 1016, "text": "Integration Testing"}, {"line": 1019, "text": "tests/integration/test_moe_lightning_inference.py"}, {"line": 1034, "text": "Performance Testing"}, {"line": 1037, "text": "tests/performance/test_throughput.py"}, {"line": 1056, "text": "Correctness Testing"}, {"line": 1059, "text": "tests/correctness/test_numerical_accuracy.py"}, {"line": 1071, "text": "Stress Testing"}, {"line": 1074, "text": "tests/stress/test_reliability.py"}, {"line": 1088, "text": "Future Enhancements"}, {"line": 1090, "text": "Short-term (6 months)"}, {"line": 1110, "text": "Mid-term (12 months)"}, {"line": 1131, "text": "Long-term (18+ months)"}, {"line": 1155, "text": "Success Metrics"}, {"line": 1157, "text": "Technical Metrics"}, {"line": 1169, "text": "Project Metrics"}, {"line": 1180, "text": "User Experience Metrics"}, {"line": 1191, "text": "References"}, {"line": 1193, "text": "Primary Paper"}, {"line": 1196, "text": "Related Papers"}, {"line": 1198, "text": "MoE Architectures"}, {"line": 1204, "text": "Performance Modeling"}, {"line": 1208, "text": "Inference Systems"}, {"line": 1214, "text": "Optimization Techniques"}, {"line": 1219, "text": "Implementation References"}, {"line": 1226, "text": "AI-OS Related"}, {"line": 1234, "text": "Appendix"}, {"line": 1236, "text": "A. Glossary"}, {"line": 1250, "text": "B. Hardware Specifications"}, {"line": 1252, "text": "NVIDIA T4"}, {"line": 1259, "text": "NVIDIA L4"}, {"line": 1266, "text": "NVIDIA A100"}, {"line": 1273, "text": "C. Model Specifications"}, {"line": 1275, "text": "Mixtral 8x7B"}, {"line": 1285, "text": "Mixtral 8x22B"}, {"line": 1293, "text": "DBRX"}, {"line": 1301, "text": "D. Configuration Examples"}, {"line": 1303, "text": "config/moe_lightning.yaml"}, {"line": 1305, "text": "Hardware profiles"}, {"line": 1325, "text": "Model configurations"}, {"line": 1336, "text": "Default policies (auto-optimized if not specified)"}, {"line": 1349, "text": "Contact & Collaboration"}]}, {"path": "planned_features/MULTIMODAL_SPECIALIZED_TOKENIZERS.md", "content": "# \ud83c\udfa8 Multimodal & Specialized Tokenizers - Complete Collection\n\n> ## \u26a0\ufe0f **CRITICAL NOTICE - NOT YET IMPLEMENTED** \u26a0\ufe0f\n> \n> **STATUS**: \ud83d\udea7 **PLANNED FEATURE - NOT CURRENTLY AVAILABLE** \ud83d\udea7\n> \n> The multimodal and specialized tokenizers described in this document are **NOT YET IMPLEMENTED** in AI-OS. This document represents planned future functionality.\n> \n> **What works NOW**:\n> - \u2705 GPT-2, Qwen 2.5, Mistral 7B, Code Llama, DeepSeek-Coder V2, StarCoder2, Phi-3 Mini\n> \n> **What does NOT work yet**:\n> - \u274c CLIP, LLaVA, SigLIP (Vision/Multimodal)\n> - \u274c BioBERT, SciBERT, Legal-BERT, FinBERT (Specialized domains)\n> - \u274c Image processing pipeline\n> - \u274c Vision-language model support\n> \n> **Last Verified**: October 13, 2025\n> \n> For currently supported tokenizers, see: `docs/SUPPORTED_TOKENIZERS.md` (to be created)\n\n---\n\n**Date:** October 13, 2025  \n**Major Update:** Added Vision, Multimodal, and Domain-Specialized Tokenizers  \n**\u26a0\ufe0f Status:** DOCUMENTATION ONLY - IMPLEMENTATION PENDING\n\n---\n\n## \ud83d\ude80 What's New (PLANNED)\n\n### \ud83c\udfa8 Vision & Multimodal Tokenizers (3)\n\nTrain AI models that understand **images, videos, and vision-language tasks**:\n\n| Tokenizer | Vocab | Best For | Downloaded |\n|-----------|-------|----------|------------|\n| **CLIP (Vision-Language)** \ud83d\uddbc\ufe0f | 49,408 | Image-text understanding, zero-shot vision | \u2705 Yes |\n| **LLaVA 1.5 (Vision Chat)** \ud83d\udcac | 32,000 | Visual Q&A, image understanding, multimodal chat | \u2705 Yes |\n| **SigLIP** \ud83c\udfaf | 32,000 | Advanced vision-language, better than CLIP | \u2705 Yes |\n\n### \ud83d\udd2c Specialized Domain Tokenizers (4)\n\nTrain AI models for **specific professional domains**:\n\n| Tokenizer | Vocab | Best For | Downloaded |\n|-----------|-------|----------|------------|\n| **BioBERT** \ud83e\uddec | 28,996 | Biomedical, healthcare, medical research | \u2705 Yes |\n| **SciBERT** \ud83d\udd2c | 31,090 | Scientific papers, academic research | \u2705 Yes |\n| **Legal-BERT** \u2696\ufe0f | 30,522 | Legal documents, contracts, compliance | \u26a0\ufe0f No |\n| **FinBERT** \ud83d\udcb0 | 30,873 | Financial analysis, trading, markets | \u26a0\ufe0f No |\n\n---\n\n## \ud83d\udcca Complete Tokenizer Collection\n\n**Total: 16 Tokenizers** across all categories!\n\n### General Purpose (4)\n- \u2b50 Qwen 2.5 - 151K vocab (Best overall, 2025)\n- Llama 3 - 128K vocab (Requires auth)\n- Mistral 7B - 32K vocab\n- GPT-2 - 50K vocab (Legacy)\n\n### Code Specialized (3)\n- \u2b50 DeepSeek-Coder V2 - 100K vocab (Best for code, 2025)\n- StarCoder2 - 49K vocab (600+ languages)\n- Code Llama - 32K vocab (Stable)\n\n### Vision & Multimodal (3) \ud83c\udd95\n- **CLIP (Vision-Language)** - 49K vocab (Image-text)\n- **LLaVA 1.5** - 32K vocab (Visual chat)\n- **SigLIP** - 32K vocab (Advanced vision)\n\n### Specialized Domains (4) \ud83c\udd95\n- **BioBERT** - 29K vocab (Biomedical)\n- **SciBERT** - 31K vocab (Scientific)\n- **Legal-BERT** - 31K vocab (Legal)\n- **FinBERT** - 31K vocab (Financial)\n\n### Compact & Efficient (2)\n- Phi-3 Mini - 32K vocab\n- GPT-2 Base Model - 50K vocab (Backward compatible)\n\n---\n\n## \ud83c\udfa8 Vision & Multimodal Use Cases\n\n### CLIP (Vision-Language)\n\n**What it does:**\n- Understands relationships between images and text\n- Zero-shot image classification\n- Image-text retrieval\n- Image generation guidance\n\n**Training Examples:**\n- Image captioning datasets\n- Visual question answering\n- Image-text matching\n- Multimodal embeddings\n\n**Best for:**\n- Building image search engines\n- Visual understanding systems\n- Text-to-image generation\n- Multimodal AI assistants\n\n**Model Architecture:**\n```\nInput: Image + Text\n      \u2193\nCLIP Tokenizer (49,408 vocab)\n      \u2193\nDual Encoders (Vision + Language)\n      \u2193\nShared Embedding Space\n      \u2193\nOutput: Similarity Scores / Embeddings\n```\n\n### LLaVA 1.5 (Vision Chat)\n\n**What it does:**\n- Visual question answering\n- Image understanding and description\n- Multimodal conversation\n- Visual reasoning\n\n**Training Examples:**\n- VQA datasets (Visual Question Answering)\n- Image description pairs\n- Visual instruction following\n- Multimodal dialogue\n\n**Best for:**\n- Visual chatbots\n- Image analysis assistants\n- Accessibility tools (image description)\n- Educational AI tutors\n\n**Model Architecture:**\n```\nInput: Image + Question\n      \u2193\nVision Encoder (ViT) + LLaVA Tokenizer (32K)\n      \u2193\nMultimodal Fusion\n      \u2193\nLanguage Model Decoder\n      \u2193\nOutput: Text Answer\n```\n\n### SigLIP (Sigmoid Loss Vision)\n\n**What it does:**\n- Improved vision-language alignment\n- Better zero-shot classification\n- More efficient training (sigmoid loss vs softmax)\n- Enhanced image understanding\n\n**Training Examples:**\n- Large-scale image-text pairs\n- Visual classification tasks\n- Image retrieval datasets\n- Cross-modal learning\n\n**Best for:**\n- Advanced vision-language models\n- Efficient multimodal training\n- Zero-shot vision tasks\n- Production vision systems\n\n---\n\n## \ud83d\udd2c Specialized Domain Use Cases\n\n### BioBERT (Biomedical) \ud83e\uddec\n\n**What it does:**\n- Understanding medical terminology\n- Biomedical named entity recognition\n- Clinical text analysis\n- Drug-disease relationship extraction\n\n**Training Examples:**\n- PubMed abstracts\n- Clinical notes\n- Medical research papers\n- Drug interaction databases\n- Disease symptom datasets\n\n**Best for:**\n- Medical AI assistants\n- Clinical decision support\n- Biomedical research tools\n- Healthcare chatbots\n- Drug discovery AI\n\n**Pre-trained on:**\n- 4.5B words from PubMed abstracts\n- 13.5B words from PMC full-text articles\n\n### SciBERT (Scientific) \ud83d\udd2c\n\n**What it does:**\n- Understanding scientific terminology\n- Research paper analysis\n- Citation relationship extraction\n- Scientific entity recognition\n\n**Training Examples:**\n- Academic papers (1.14M papers)\n- Scientific abstracts\n- Research datasets\n- Technical documentation\n- Lab reports\n\n**Best for:**\n- Research paper summarization\n- Literature review automation\n- Scientific question answering\n- Academic writing assistance\n- Grant proposal analysis\n\n**Pre-trained on:**\n- 1.14M papers from Semantic Scholar\n- 18% computer science papers\n- 82% biomedical papers\n\n### Legal-BERT (Legal) \u2696\ufe0f\n\n**What it does:**\n- Understanding legal terminology\n- Contract analysis\n- Legal document classification\n- Case law reasoning\n\n**Training Examples:**\n- Legal contracts\n- Court opinions\n- Legislation text\n- Legal briefs\n- Regulatory documents\n\n**Best for:**\n- Contract review automation\n- Legal research assistants\n- Compliance checking\n- Document classification\n- Legal chatbots\n\n**Pre-trained on:**\n- Legal corpora\n- Case law databases\n- Legislation texts\n\n### FinBERT (Financial) \ud83d\udcb0\n\n**What it does:**\n- Financial sentiment analysis\n- Market trend prediction\n- Financial entity recognition\n- Risk assessment\n\n**Training Examples:**\n- Financial news articles\n- Earnings reports\n- Market analysis\n- Trading data\n- Economic indicators\n\n**Best for:**\n- Trading algorithms\n- Financial news analysis\n- Risk assessment tools\n- Investment research\n- Financial chatbots\n\n**Pre-trained on:**\n- Financial news and reports\n- Corporate filings\n- Market commentary\n\n---\n\n## \ud83c\udfaf Choosing the Right Tokenizer\n\n### For Image Understanding & Generation\n**Use: CLIP** \ud83d\uddbc\ufe0f\n- Large vocabulary (49K)\n- Industry-standard vision-language model\n- Zero-shot capabilities\n- Compatible with Stable Diffusion and other image models\n\n**Example Projects:**\n- Image search engines\n- Content moderation\n- Visual similarity matching\n- Text-to-image applications\n\n### For Visual Question Answering\n**Use: LLaVA 1.5** \ud83d\udcac\n- Optimized for visual chat\n- Instruction following\n- Multimodal conversation\n- Image understanding\n\n**Example Projects:**\n- Visual assistants\n- Image description tools\n- Educational tutors\n- Accessibility applications\n\n### For Biomedical AI\n**Use: BioBERT** \ud83e\uddec\n- Medical terminology expertise\n- Clinical text understanding\n- Biomedical entity recognition\n\n**Example Projects:**\n- Clinical decision support\n- Medical record analysis\n- Drug discovery\n- Healthcare chatbots\n\n### For Scientific Research\n**Use: SciBERT** \ud83d\udd2c\n- Scientific vocabulary\n- Research paper understanding\n- Technical documentation\n\n**Example Projects:**\n- Literature review tools\n- Research assistants\n- Paper summarization\n- Citation analysis\n\n### For Legal Applications\n**Use: Legal-BERT** \u2696\ufe0f\n- Legal terminology\n- Contract understanding\n- Case law analysis\n\n**Example Projects:**\n- Contract review\n- Legal research tools\n- Compliance checking\n- Document classification\n\n### For Financial Analysis\n**Use: FinBERT** \ud83d\udcb0\n- Financial terminology\n- Market sentiment\n- Economic understanding\n\n**Example Projects:**\n- Trading algorithms\n- Market analysis\n- Risk assessment\n- Financial news processing\n\n---\n\n## \ud83d\ude80 Quick Start Examples\n\n### Creating a Vision AI Brain\n\n```bash\n# 1. Download CLIP tokenizer\npython scripts/download_tokenizer.py clip-vit\n\n# 2. Open AI-OS GUI\n# 3. HRM Training \u2192 Create New\n\n# 4. Configure:\nPreset: 10M (or preferred size)\nName: vision-understanding-v1\nGoal: Understand and describe images, answer visual questions\nTokenizer: \u2713 CLIP (Vision-Language) (49,408 tokens)\n\n# 5. Create & Train on image-text datasets!\n```\n\n### Creating a Medical AI Brain\n\n```bash\n# 1. Download BioBERT tokenizer\npython scripts/download_tokenizer.py biobert\n\n# 2. Configure brain:\nName: medical-assistant-v1\nGoal: Analyze medical texts and provide clinical insights\nTokenizer: \u2713 BioBERT (Biomedical) (28,996 tokens)\n\n# 3. Train on medical datasets (PubMed, clinical notes, etc.)\n```\n\n### Creating a Scientific Research Brain\n\n```bash\n# 1. Download SciBERT tokenizer\npython scripts/download_tokenizer.py scibert\n\n# 2. Configure brain:\nName: research-assistant-v1\nGoal: Understand scientific papers and answer research questions\nTokenizer: \u2713 SciBERT (Scientific) (31,090 tokens)\n\n# 3. Train on scientific papers and technical docs\n```\n\n---\n\n## \ud83d\udcda Training Dataset Recommendations\n\n### For Vision Models (CLIP, LLaVA, SigLIP)\n\n**Public Datasets:**\n- **COCO** - Image captioning (330K images)\n- **Flickr30k** - Image descriptions (31K images)\n- **Visual Genome** - Dense annotations (108K images)\n- **Conceptual Captions** - 3.3M image-text pairs\n- **VQA v2** - Visual question answering (1M+ questions)\n\n**Custom Data:**\n- Product images + descriptions\n- Screenshots + instructions\n- Medical images + diagnoses\n- Satellite imagery + labels\n\n### For Biomedical (BioBERT)\n\n**Public Datasets:**\n- **PubMed Abstracts** - Medical research\n- **MIMIC-III** - Clinical notes (requires approval)\n- **PMC-OA** - Full-text biomedical articles\n- **ChEMBL** - Drug-target interactions\n- **UMLS** - Medical terminology\n\n### For Scientific (SciBERT)\n\n**Public Datasets:**\n- **ArXiv** - Scientific papers\n- **Semantic Scholar** - Academic papers\n- **PubMed Central** - Biomedical literature\n- **CiteSeer** - Computer science papers\n- **IEEE Xplore** - Engineering papers\n\n### For Legal (Legal-BERT)\n\n**Public Datasets:**\n- **EDGAR** - SEC filings\n- **Court Listener** - Court opinions\n- **EUR-Lex** - EU legal documents\n- **CaseLaw Access Project** - US case law\n\n### For Financial (FinBERT)\n\n**Public Datasets:**\n- **Financial PhraseBank** - Sentiment analysis\n- **StockTwits** - Market sentiment\n- **SEC Filings** - Corporate documents\n- **Reuters Financial News**\n- **Bloomberg News Archives**\n\n---\n\n## \ud83d\udd04 Multimodal Training Pipeline\n\n### Vision-Language Training Flow\n\n```\n1. Prepare Dataset\n   \u251c\u2500\u2500 Images (.jpg, .png)\n   \u251c\u2500\u2500 Captions/Labels (.txt, .json)\n   \u2514\u2500\u2500 Pair them correctly\n\n2. Choose Tokenizer\n   \u2514\u2500\u2500 CLIP, LLaVA, or SigLIP\n\n3. Create Brain\n   \u2514\u2500\u2500 Select vision tokenizer in GUI\n\n4. Configure Training\n   \u251c\u2500\u2500 Image preprocessing (resize, normalize)\n   \u251c\u2500\u2500 Text tokenization\n   \u2514\u2500\u2500 Batch pairing (image + text)\n\n5. Train Model\n   \u251c\u2500\u2500 Vision encoder learns image features\n   \u251c\u2500\u2500 Language encoder learns text features\n   \u2514\u2500\u2500 Contrastive loss aligns both spaces\n\n6. Evaluate\n   \u251c\u2500\u2500 Image retrieval accuracy\n   \u251c\u2500\u2500 Text retrieval accuracy\n   \u2514\u2500\u2500 Zero-shot classification\n```\n\n---\n\n## \ud83e\uddea Testing Results\n\n### All New Tokenizers Verified \u2705\n\n```\n\ud83d\udcca Vision & Multimodal\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2705 CLIP: Downloaded & Tested\n\u2705 LLaVA 1.5: Downloaded & Tested\n\u2705 SigLIP: Downloaded (verification warning)\n\n\ud83d\udcca Specialized Domains\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2705 BioBERT: Downloaded & Tested\n\u2705 SciBERT: Downloaded & Tested\n\u26a0\ufe0f Legal-BERT: Not yet downloaded\n\u26a0\ufe0f FinBERT: Not yet downloaded\n```\n\n### Test Brains Created\n\n| Brain Name | Tokenizer | Use Case |\n|------------|-----------|----------|\n| CLIP-(Vision-Language)-test | clip-vit | Image-text understanding |\n| LLaVA-1.5-(Vision-Chat)-test | llava-1.5 | Visual Q&A |\n| BioBERT-(Biomedical)-test | biobert | Medical text |\n| SciBERT-(Scientific)-test | scibert | Research papers |\n\n---\n\n## \ud83d\udca1 Advanced Use Cases\n\n### Robotics Training (Future)\n\nWhile we don't have RT-2 tokenizer yet, you can prepare for robotics:\n\n**Using Current Tokenizers:**\n1. **Code tokenizers** (DeepSeek-Coder V2, StarCoder2) for action sequences\n2. **Vision tokenizers** (CLIP, LLaVA) for visual perception\n3. **Qwen 2.5** for multimodal reasoning\n\n**Action Tokenization Concept:**\n```python\n# Represent robot actions as tokens\nrobot_actions = {\n    \"move_forward\": 1001,\n    \"move_backward\": 1002,\n    \"turn_left\": 1003,\n    \"turn_right\": 1004,\n    \"grasp\": 1005,\n    \"release\": 1006,\n    # ... etc\n}\n\n# Train on: Vision \u2192 Action Sequences\nInput: [image_tokens] + [instruction_tokens]\nOutput: [action_token_1, action_token_2, ..., action_token_n]\n```\n\n### Video Understanding\n\n**Using Vision Tokenizers:**\n- CLIP or LLaVA can process video frames\n- Tokenize each frame individually\n- Temporal modeling at model level\n\n**Workflow:**\n```\nVideo \u2192 Extract Frames (e.g., 1 fps)\n      \u2193\nEach Frame \u2192 Vision Tokenizer\n      \u2193\nSequence of Frame Tokens\n      \u2193\nTemporal Model (Transformer)\n      \u2193\nVideo Understanding\n```\n\n### Audio-Visual Models\n\n**Multimodal Combination:**\n```\nAudio: Use text tokenizer for transcribed speech\nVision: Use CLIP/LLaVA for visual frames\nFusion: Combine at embedding level\n\nInput: [audio_transcript_tokens] + [video_frame_tokens]\nOutput: Multimodal understanding\n```\n\n---\n\n## \ud83d\udce6 Installation Summary\n\n### Downloaded & Ready (13 Tokenizers)\n```bash\n\u2705 gpt2, gpt2-base-model\n\u2705 qwen2.5-7b (Best overall)\n\u2705 mistral-7b\n\u2705 deepseek-coder-v2 (Best for code)\n\u2705 starcoder2\n\u2705 codellama\n\u2705 phi3-mini\n\u2705 clip-vit (Vision-language)\n\u2705 llava-1.5 (Vision chat)\n\u2705 siglip (Advanced vision)\n\u2705 biobert (Biomedical)\n\u2705 scibert (Scientific)\n```\n\n### Available to Download (3 Tokenizers)\n```bash\n\u26a0\ufe0f llama3-8b (requires HF access approval)\n\u26a0\ufe0f legal-bert\n\u26a0\ufe0f finbert\n```\n\n---\n\n## \ud83c\udf93 Learning Resources\n\n### Vision-Language Models\n- **CLIP Paper:** https://arxiv.org/abs/2103.00020\n- **LLaVA Paper:** https://arxiv.org/abs/2304.08485\n- **SigLIP:** https://arxiv.org/abs/2303.15343\n\n### Domain-Specific Models\n- **BioBERT:** https://arxiv.org/abs/1901.08746\n- **SciBERT:** https://arxiv.org/abs/1903.10676\n- **Legal-BERT:** https://arxiv.org/abs/2010.02559\n- **FinBERT:** https://arxiv.org/abs/1908.10063\n\n### Robotics & Embodied AI\n- **RT-2:** https://robotics-transformer2.github.io/\n- **PaLM-E:** https://palm-e.github.io/\n\n---\n\n## \u2753 FAQ\n\n**Q: Can I train image generation models with CLIP?**  \nA: CLIP is for understanding, but can guide generation (like in Stable Diffusion). For generation, you'd need image decoder models.\n\n**Q: Do I need special hardware for vision models?**  \nA: Vision models need more VRAM. Recommend 8GB+ GPU for small models, 24GB+ for larger ones.\n\n**Q: Can I mix tokenizers (e.g., vision + text)?**  \nA: Not directly. Choose one tokenizer per brain. For multimodal, use tokenizers designed for both (CLIP, LLaVA).\n\n**Q: Are specialized tokenizers better than general ones?**  \nA: For their specific domains, YES! BioBERT understands medical terms much better than GPT-2.\n\n**Q: Can I train on video datasets?**  \nA: Yes! Extract frames, tokenize each frame, and model temporal relationships at the architecture level.\n\n**Q: How do I prepare medical/scientific datasets?**  \nA: Start with public datasets (PubMed, ArXiv), then add your own domain-specific data.\n\n**Q: Will you add RT-2 for robotics?**  \nA: RT-2 requires special action tokenization. We're exploring options for embodied AI support!\n\n---\n\n## \ud83c\udf89 Summary\n\nYou now have access to **16 specialized tokenizers** covering:\n\n- \u2705 **General AI** (Qwen 2.5, Llama 3, Mistral)\n- \u2705 **Code** (DeepSeek-Coder V2, StarCoder2, CodeLlama)\n- \u2705 **Vision & Multimodal** (CLIP, LLaVA, SigLIP) \ud83c\udd95\n- \u2705 **Biomedical** (BioBERT) \ud83c\udd95\n- \u2705 **Scientific** (SciBERT) \ud83c\udd95\n- \u2705 **Legal** (Legal-BERT - available) \ud83c\udd95\n- \u2705 **Financial** (FinBERT - available) \ud83c\udd95\n\n**Ready to train specialized AI for any domain!** \ud83d\ude80\n\n---\n\n*Last Updated: October 13, 2025*  \n*Vision and specialized tokenizers fully integrated*\n\n---\n\n## \ud83e\udded Planned Expansion: Image & Video Generation (Design Spec)\n\n> Status: PLANNED \u2013 Architecture and UX defined below. No runtime support in main yet.\n\nThis addendum extends the tokenizer roadmap to full multimodal generation and scientific-output workflows. It defines the architecture, data contracts, and UI spec required to support:\n\n- Text-to-image, image-to-image, inpainting/outpainting, ControlNet-guided image generation\n- Image-to-video and text-to-video (short clips) generation\n- Scientific outputs in chat: rich plots, tables, LaTeX, HTML reports, downloadable files\n- A new Chat Window v2 that can render most multimodal outputs\n\n### High-level Phases\n\n1) Foundations: artifact store, rich message schema, viewers (images/tables/plots/latex)  \n2) Image generation (inference-first): SDXL/LCM, ControlNet, prompt/seed reproducibility  \n3) Video generation (inference-first): SVD (image\u2192video), lightweight text\u2192video integration  \n4) Scientific outputs: plot/table/LaTeX/HTML/file attachments, export flows  \n5) Optional training: dreambooth/LoRA for images; fine-tuning where feasible for video\n\n---\n\n## \ud83d\uddbc\ufe0f Image Generation (Planned)\n\nSupported (Planned) pipelines and modalities:\n\n- Text\u2192Image: SD 1.5 / SDXL via Diffusers; optional LCM for fast steps\n- Image\u2192Image: style transfer/variations with strength control\n- Inpainting/Outpainting: masked editing\n- Conditioning: ControlNet (canny, depth, pose), IP-Adapter (face/style), LoRA adapters\n\nImplementation notes:\n- Runtime: Hugging Face Diffusers (+ transformers, accelerate, safetensors, xformers)\n- Precision: fp16/bf16 on GPU; CPU fallback (slow) for dev\n- Reproducibility: seed, guidance_scale, scheduler stored with artifact metadata\n- Safety filter hooks: optional NSFW classifier toggle (user-controlled)\n\nCLI (planned examples):\n```\naios gen image --model sdxl --prompt \"a red fox in a misty forest, photorealistic\" \\\n      --steps 20 --seed 42 --size 1024x1024 --guidance-scale 7.5 \\\n      --out artifacts/outputs/images/\n\naios gen image2image --model sd15 --init-image path/to/img.png --strength 0.6 \\\n      --prompt \"studio portrait, soft lighting\" --lora face_finetune.safetensors\n```\n\n---\n\n## \ud83c\udf9e\ufe0f Video Generation (Planned)\n\nInitial scope focuses on inference and short clips:\n\n- Image\u2192Video: Stable Video Diffusion (SVD) 14-24 fps, 2-4s clips\n- Text\u2192Image\u2192Video: generate keyframe with SDXL then animate via SVD\n- Text\u2192Video (experimental): integrate an open model (e.g., CogVideoX small) when stable\n\nKey controls:\n- fps (e.g., 8\u201324), duration (seconds), resolution (e.g., 576p/720p), motion strength\n- Seed and scheduler captured for reproducibility\n\nCLI (planned examples):\n```\naios gen video --from-image artifacts/outputs/images/fox.jpg --model svd --fps 14 --seconds 3\n\naios gen video --prompt \"drone shot over snowy mountains at sunrise\" \\\n      --pipeline txt2img+svd --fps 12 --seconds 4 --seed 123\n```\n\nDataset prep (future training): frame extraction to frames/, JSON pairs with timing; ffmpeg helpers.\n\n---\n\n## \ud83e\uddea Scientific Output Interfaces (Planned)\n\nThe assistant will produce structured scientific artifacts alongside text:\n\n- Plots: Plotly JSON spec (preferred for interactivity) or Matplotlib PNG fallback\n- Tables: column schema + row data; optional CSV/Parquet attachment for download\n- Equations: LaTeX rendered with KaTeX in UI\n- Reports: minimal HTML (sanitized) and Markdown export\n- Files: CSV/JSON/NetCDF/HDF5/ZIP as downloadable artifacts\n- 3D/Geo (future): GLB/PLY previews; GeoJSON/tiles via map component (opt-in)\n\nServer responsibilities:\n- Validate payload sizes and types; store artifacts with content hash\n- Generate preview thumbnails/posters where applicable\n- Attach metadata (units, provenance, seeds, code refs)\n\n---\n\n## \ud83e\udde9 Chat Window v2 \u2013 Rich Message Schema (Planned)\n\nThe chat UI will display multi-part messages. Each message may contain text plus a list of parts. Parts reference inline payloads or stored artifacts.\n\nMessage (concept):\n```json\n{\n      \"id\": \"msg_01\",\n      \"role\": \"assistant\",\n      \"parts\": [\n            { \"type\": \"text\", \"text\": \"Here is the generated image.\" },\n            { \"type\": \"image\", \"source\": { \"type\": \"artifact\", \"id\": \"art_abc\" }, \"alt\": \"red fox\" },\n            { \"type\": \"plot\", \"spec\": { \"version\": 2, \"data\": [], \"layout\": {} } },\n            { \"type\": \"table\", \"columns\": [{\"name\":\"time\",\"type\":\"number\"}], \"rows\": [[0],[1]],\n                  \"download\": { \"artifactId\": \"art_tbl\" } },\n            { \"type\": \"latex\", \"code\": \"E=mc^2\" },\n            { \"type\": \"file\", \"artifactId\": \"art_zip\", \"name\": \"results.zip\" }\n      ]\n}\n```\n\nArtifact (concept):\n```json\n{\n      \"id\": \"art_abc\",\n      \"kind\": \"image\",\n      \"mime\": \"image/png\",\n      \"path\": \"artifacts/outputs/images/fox.png\",\n      \"sha256\": \"...\",\n      \"width\": 1024,\n      \"height\": 1024,\n      \"createdAt\": \"2025-10-15T12:34:56Z\",\n      \"metadata\": {\n            \"prompt\": \"a red fox in a misty forest\",\n            \"seed\": 42,\n            \"steps\": 20,\n            \"guidance_scale\": 7.5,\n            \"scheduler\": \"EulerA\"\n      }\n}\n```\n\nUI Behaviors:\n- Text streams first; media parts render when artifacts are ready\n- Image/video gallery with zoom; video player with poster/thumb\n- Plotly interactive viewer; CSV download from table; LaTeX inline via KaTeX\n- Side \u201cArtifacts\u201d panel lists recent outputs with search and filters\n- Tool-run logs collapsible; copy-to-clipboard for prompts/configs\n\nSecurity & Limits:\n- Sanitize HTML; disallow inline scripts; CSP headers\n- Size limits per part and per message; chunked uploads for large files\n\n---\n\n## \ud83c\udfd7\ufe0f Architecture Addendum (Planned)\n\nLogical components:\n\n1) Generation Runtimes\n       - Diffusers pipelines: SD 1.5, SDXL, SVD (+ ControlNet, IP-Adapter, LoRA)\n       - Scheduler registry; VRAM-aware autotune\n2) Artifact Store\n       - Path: `artifacts/outputs/{images|videos|plots|tables|files}/` (content-hashed)\n       - Metadata JSON sidecars; thumbnail/poster derivation jobs\n3) Messaging API\n       - Returns messages with parts; emits events: text-delta, artifact-pending, artifact-ready\n4) Chat UI v2\n       - React components: Text, Image, Video, Plotly, Table, LaTeX, File\n       - View toggles: compact/threaded/gallery\n5) CLI/HRM Integrations\n       - `aios gen image|video` commands; HRM tool to attach outputs to runs\n\nDeployment considerations:\n- Windows, Linux GPU; optional CPU fallback for dev  \n- CUDA/cuDNN versions pinned; xformers optional  \n- Model weights cached in `training_data/hf_cache/`\n\n---\n\n## \ud83d\udcd0 Data Contracts (Planned)\n\nMessagePart types:\n- text, image, video, audio (future), plot, table, latex, html (sanitized), file\n\nCommon fields:\n- `type`, `source` (inline|artifact), `mime` (when applicable), `metadata`\n\nTable contract (sketch):\n```json\n{\n      \"type\": \"table\",\n      \"columns\": [ {\"name\":\"col\",\"type\":\"string\",\"unit\":\"\"} ],\n      \"rows\": [ [\"value\"] ],\n      \"download\": { \"artifactId\": \"art_csv\" }\n}\n```\n\nPlotly contract: store full `spec` and optional `screenshot` thumbnail.\n\n---\n\n## \ud83e\uddea Acceptance Criteria (Milestones)\n\nM1 \u2013 Foundations\n- Render images, tables (CSV download), Plotly, LaTeX in Chat v2\n- Artifact store with hashing + metadata; events wired\n\nM2 \u2013 Image Generation (Inference)\n- SDXL text\u2192image end-to-end via CLI and Chat tool\n- Reproducible artifacts (seed/scheduler stored); ControlNet optional\n\nM3 \u2013 Video Generation (Inference)\n- SVD image\u2192video; keyframe path (txt\u2192img\u2192vid) exposed in CLI and Chat\n- Video player in UI with fps/duration metadata\n\nM4 \u2013 Scientific Outputs\n- Plot/table/latex produced by tools; downloadable CSV/ZIP\n- Simple HTML report (sanitized) preview and export\n\nM5 \u2013 UX Polish & Reliability\n- Gallery view, artifact panel, retries, error toasts, rate limits\n\n---\n\n## \ud83d\udee0\ufe0f Dependencies (Planned)\n\nPython\n- diffusers, transformers, accelerate, torch, safetensors, xformers (optional)\n- pillow, opencv-python, ffmpeg-python (or system ffmpeg)\n\nFrontend\n- KaTeX, Plotly.js, React Player (or video element), MIME renderers\n\n---\n\n## \u26a0\ufe0f Risks & Considerations\n\n- VRAM constraints: add auto-downscale and low-VRAM schedulers\n- Large artifacts: background upload and streaming; disk quotas\n- Content safety: optional NSFW filters; user control and transparency\n- Licensing: check weights and dataset licenses; document restrictions\n\n---\n\n## \ud83e\uddea Try-it (Future, subject to change)\n\n```\n# Text \u2192 Image\naios gen image --model sdxl --prompt \"scientific diagram of DNA helix, vector style\" --size 1024x1024\n\n# Image \u2192 Video\naios gen video --from-image artifacts/outputs/images/diagram.png --model svd --fps 12 --seconds 3\n\n# Scientific Plot in Chat (tool)\naios tools plot --spec path/to/plotly.json --attach\n```\n\n---\n\nThis specification updates the planned feature set to include multimodal generation and scientific outputs with a concrete data and UI contract, without claiming availability today.\n", "tags": ["cli", "training"], "headings": [{"line": 0, "text": "\ud83c\udfa8 Multimodal & Specialized Tokenizers - Complete Collection"}, {"line": 29, "text": "\ud83d\ude80 What's New (PLANNED)"}, {"line": 31, "text": "\ud83c\udfa8 Vision & Multimodal Tokenizers (3)"}, {"line": 41, "text": "\ud83d\udd2c Specialized Domain Tokenizers (4)"}, {"line": 54, "text": "\ud83d\udcca Complete Tokenizer Collection"}, {"line": 58, "text": "General Purpose (4)"}, {"line": 64, "text": "Code Specialized (3)"}, {"line": 69, "text": "Vision & Multimodal (3) \ud83c\udd95"}, {"line": 74, "text": "Specialized Domains (4) \ud83c\udd95"}, {"line": 80, "text": "Compact & Efficient (2)"}, {"line": 86, "text": "\ud83c\udfa8 Vision & Multimodal Use Cases"}, {"line": 88, "text": "CLIP (Vision-Language)"}, {"line": 121, "text": "LLaVA 1.5 (Vision Chat)"}, {"line": 154, "text": "SigLIP (Sigmoid Loss Vision)"}, {"line": 176, "text": "\ud83d\udd2c Specialized Domain Use Cases"}, {"line": 178, "text": "BioBERT (Biomedical) \ud83e\uddec"}, {"line": 204, "text": "SciBERT (Scientific) \ud83d\udd2c"}, {"line": 231, "text": "Legal-BERT (Legal) \u2696\ufe0f"}, {"line": 258, "text": "FinBERT (Financial) \ud83d\udcb0"}, {"line": 287, "text": "\ud83c\udfaf Choosing the Right Tokenizer"}, {"line": 289, "text": "For Image Understanding & Generation"}, {"line": 302, "text": "For Visual Question Answering"}, {"line": 315, "text": "For Biomedical AI"}, {"line": 327, "text": "For Scientific Research"}, {"line": 339, "text": "For Legal Applications"}, {"line": 351, "text": "For Financial Analysis"}, {"line": 365, "text": "\ud83d\ude80 Quick Start Examples"}, {"line": 367, "text": "Creating a Vision AI Brain"}, {"line": 370, "text": "1. Download CLIP tokenizer"}, {"line": 373, "text": "2. Open AI-OS GUI"}, {"line": 374, "text": "3. HRM Training \u2192 Create New"}, {"line": 376, "text": "4. Configure:"}, {"line": 382, "text": "5. Create & Train on image-text datasets!"}, {"line": 385, "text": "Creating a Medical AI Brain"}, {"line": 388, "text": "1. Download BioBERT tokenizer"}, {"line": 391, "text": "2. Configure brain:"}, {"line": 396, "text": "3. Train on medical datasets (PubMed, clinical notes, etc.)"}, {"line": 399, "text": "Creating a Scientific Research Brain"}, {"line": 402, "text": "1. Download SciBERT tokenizer"}, {"line": 405, "text": "2. Configure brain:"}, {"line": 410, "text": "3. Train on scientific papers and technical docs"}, {"line": 415, "text": "\ud83d\udcda Training Dataset Recommendations"}, {"line": 417, "text": "For Vision Models (CLIP, LLaVA, SigLIP)"}, {"line": 432, "text": "For Biomedical (BioBERT)"}, {"line": 441, "text": "For Scientific (SciBERT)"}, {"line": 450, "text": "For Legal (Legal-BERT)"}, {"line": 458, "text": "For Financial (FinBERT)"}, {"line": 469, "text": "\ud83d\udd04 Multimodal Training Pipeline"}, {"line": 471, "text": "Vision-Language Training Flow"}, {"line": 503, "text": "\ud83e\uddea Testing Results"}, {"line": 505, "text": "All New Tokenizers Verified \u2705"}, {"line": 522, "text": "Test Brains Created"}, {"line": 533, "text": "\ud83d\udca1 Advanced Use Cases"}, {"line": 535, "text": "Robotics Training (Future)"}, {"line": 546, "text": "Represent robot actions as tokens"}, {"line": 554, "text": "... etc"}, {"line": 557, "text": "Train on: Vision \u2192 Action Sequences"}, {"line": 562, "text": "Video Understanding"}, {"line": 582, "text": "Audio-Visual Models"}, {"line": 596, "text": "\ud83d\udce6 Installation Summary"}, {"line": 598, "text": "Downloaded & Ready (13 Tokenizers)"}, {"line": 614, "text": "Available to Download (3 Tokenizers)"}, {"line": 623, "text": "\ud83c\udf93 Learning Resources"}, {"line": 625, "text": "Vision-Language Models"}, {"line": 630, "text": "Domain-Specific Models"}, {"line": 636, "text": "Robotics & Embodied AI"}, {"line": 642, "text": "\u2753 FAQ"}, {"line": 667, "text": "\ud83c\udf89 Summary"}, {"line": 688, "text": "\ud83e\udded Planned Expansion: Image & Video Generation (Design Spec)"}, {"line": 699, "text": "High-level Phases"}, {"line": 709, "text": "\ud83d\uddbc\ufe0f Image Generation (Planned)"}, {"line": 736, "text": "\ud83c\udf9e\ufe0f Video Generation (Planned)"}, {"line": 760, "text": "\ud83e\uddea Scientific Output Interfaces (Planned)"}, {"line": 778, "text": "\ud83e\udde9 Chat Window v2 \u2013 Rich Message Schema (Planned)"}, {"line": 833, "text": "\ud83c\udfd7\ufe0f Architecture Addendum (Planned)"}, {"line": 858, "text": "\ud83d\udcd0 Data Contracts (Planned)"}, {"line": 880, "text": "\ud83e\uddea Acceptance Criteria (Milestones)"}, {"line": 903, "text": "\ud83d\udee0\ufe0f Dependencies (Planned)"}, {"line": 914, "text": "\u26a0\ufe0f Risks & Considerations"}, {"line": 923, "text": "\ud83e\uddea Try-it (Future, subject to change)"}, {"line": 926, "text": "Text \u2192 Image"}, {"line": 929, "text": "Image \u2192 Video"}, {"line": 932, "text": "Scientific Plot in Chat (tool)"}]}, {"path": "planned_features/PERSISTENT_TRACES_APPENDIX_MATHEMATICAL_FOUNDATIONS.md", "content": "# Persistent Traces & Semantic Crystallization: Mathematical Foundations\n\n**Companion Document to**: `PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md`  \n**Status**: Theoretical Analysis  \n**Created**: December 8, 2025\n\n---\n\n## \ud83d\udcd0 Detailed Mathematical Derivations\n\n### 1. Trace Memory Update Algorithm\n\n#### 1.1 Formal Problem Statement\n\nGiven:\n- Attention weights $A^{(l,h)}_{i,j}(t) \\in [0,1]$ at timestep $t$\n- Loss gradients $\\nabla_A \\mathcal{L}$\n- Persistence history $p_{i,j}(t)$ tracking edge recurrence\n- Memory budget $B$ (max traces per head)\n\nFind:\n- Sparse trace memory $M^{(l,h)}$ that maximizes expected future utility\n- Update rule that balances plasticity and stability\n\n#### 1.2 Salience Score Derivation\n\n**Intuition**: Attention edges that are both strong AND important for the loss should persist.\n\n**Components**:\n1. **Attention magnitude**: $A^{(l,h)}_{i,j}$ - raw attention weight\n2. **Gradient importance**: $\\left|\\frac{\\partial \\mathcal{L}}{\\partial A^{(l,h)}_{i,j}}\\right|$ - how much loss depends on this edge\n3. **Temporal consistency**: $p_{i,j}(t) = \\sum_{\\tau=t-w}^{t} \\mathbb{1}[A^{(l,h)}_{i,j}(\\tau) > \\epsilon]$ - edge frequency\n\n**Combined salience**:\n$$\nS^{(l,h)}_{i,j}(t) = \\underbrace{A^{(l,h)}_{i,j}(t)}_{\\text{strength}} \\cdot \\underbrace{\\left|\\frac{\\partial \\mathcal{L}}{\\partial A^{(l,h)}_{i,j}}\\right|}_{\\text{importance}} \\cdot \\underbrace{\\log(1 + p_{i,j}(t))}_{\\text{recurrence bonus}}\n$$\n\nThe $\\log(1 + p)$ term provides diminishing returns - edges don't need to recur infinitely to be valuable.\n\n#### 1.3 Exponential Moving Average (EMA) Update\n\n**Standard EMA**:\n$$\nM^{(l,h)}_{i,j}(t+1) = \\lambda \\cdot M^{(l,h)}_{i,j}(t) + (1-\\lambda) \\cdot S^{(l,h)}_{i,j}(t)\n$$\n\n**Problem**: Doesn't handle sparsity - memory grows unbounded.\n\n**Solution**: Conditional update with competitive eviction.\n\n**Sparse EMA with Decay**:\n$$\nM^{(l,h)}_{i,j}(t+1) = \\begin{cases}\n\\lambda \\cdot M^{(l,h)}_{i,j}(t) + (1-\\lambda) \\cdot S^{(l,h)}_{i,j}(t) & \\text{if } S^{(l,h)}_{i,j}(t) > \\theta_{\\text{sal}} \\\\\n\\gamma \\cdot M^{(l,h)}_{i,j}(t) & \\text{otherwise} \\\\\n0 & \\text{if } M^{(l,h)}_{i,j}(t+1) < \\epsilon_{\\text{prune}}\n\\end{cases}\n$$\n\n**Competitive Eviction** (when $|M^{(l,h)}| > B$):\n$$\n\\text{evict} = \\arg\\min_{(i,j) \\in M^{(l,h)}} M^{(l,h)}_{i,j}(t)\n$$\n\nRemove lowest-salience trace to make room for new high-salience trace.\n\n#### 1.4 Steady-State Analysis\n\n**Question**: Does the trace memory converge to a stable distribution?\n\n**Assumptions**:\n- Attention distribution is stationary: $A^{(l,h)}_{i,j}(t) \\sim \\mathcal{D}_A$\n- Salience distribution is stationary: $S^{(l,h)}_{i,j}(t) \\sim \\mathcal{D}_S$\n\n**Steady state**: $\\mathbb{E}[M_{i,j}(t+1)] = \\mathbb{E}[M_{i,j}(t)] = M^*_{i,j}$\n\nFor edges receiving reinforcement:\n$$\nM^*_{i,j} = \\frac{(1-\\lambda) \\mathbb{E}[S_{i,j}]}{1 - \\lambda}\n$$\n\nFor edges not receiving reinforcement:\n$$\nM^*_{i,j} = 0 \\quad \\text{(geometric decay drives to zero)}\n$$\n\n**Convergence rate**: \n- Reinforced edges converge in $O(\\frac{1}{1-\\lambda})$ steps\n- Decaying edges vanish in $O(\\frac{1}{1-\\gamma})$ steps\n\n**Memory occupancy**:\n$$\n|M^{(l,h)}| \\approx \\min\\left(B, \\text{Pr}[S > \\theta_{\\text{sal}}] \\cdot T^2\\right)\n$$\n\nIf threshold is calibrated such that $\\text{Pr}[S > \\theta_{\\text{sal}}] = \\frac{B}{T^2}$, memory stays at quota.\n\n### 2. Attention Bias Injection\n\n#### 2.1 Modified Attention Mechanism\n\n**Standard attention**:\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n$$\n\n**Trace-biased attention**:\n$$\n\\text{Attention}_{\\text{biased}}(Q, K, V, M) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + \\alpha \\cdot M\\right) V\n$$\n\n#### 2.2 Bias Strength Analysis\n\n**Question**: How does $\\alpha$ affect attention distribution?\n\n**Softmax sensitivity**: For logits $z$ and perturbation $\\delta$:\n$$\n\\frac{\\partial}{\\partial \\delta_i} \\text{softmax}(z + \\delta)_i = \\text{softmax}(z)_i (1 - \\text{softmax}(z)_i)\n$$\n\n**Effective bias**:\n$$\n\\Delta A_{i,j} \\approx \\alpha \\cdot M_{i,j} \\cdot A_{i,j} (1 - A_{i,j})\n$$\n\nMaximum effect when $A_{i,j} \\approx 0.5$ (uncertain attention).\n\n**Recommended $\\alpha$ calibration**:\n$$\n\\alpha = \\frac{1}{\\max_{i,j} M_{i,j}} \\cdot \\beta\n$$\n\nwhere $\\beta \\in [0.1, 0.3]$ controls overall bias strength. This normalizes bias relative to trace strength.\n\n#### 2.3 Information-Theoretic Analysis\n\n**Question**: Does trace bias reduce attention entropy?\n\n**Attention entropy** (before bias):\n$$\nH(A) = -\\sum_{j} A_{i,j} \\log A_{i,j}\n$$\n\n**Attention entropy** (after bias):\n$$\nH(A') = -\\sum_{j} A'_{i,j} \\log A'_{i,j}\n$$\n\n**Theorem**: If $M_{i,j} > 0$ is concentrated on subset $\\mathcal{S} \\subset \\{1, \\ldots, T\\}$, then $H(A') < H(A)$.\n\n**Proof**: Bias increases probability mass on $\\mathcal{S}$, reducing uncertainty.\n\n**Entropy reduction**:\n$$\n\\Delta H = H(A) - H(A') \\approx \\alpha \\sum_{j \\in \\mathcal{S}} M_{i,j} \\log\\left(\\frac{|\\mathcal{S}|}{T}\\right)\n$$\n\nStronger bias and higher concentration \u2192 greater entropy reduction \u2192 faster convergence.\n\n### 3. Routing Path Crystallization\n\n#### 3.1 Suffix Tree Construction\n\n**Input**: Sequence of routing paths $\\{\\pi_1, \\pi_2, \\ldots, \\pi_N\\}$ where $\\pi_i = [e_1^{(1)}, e_2^{(2)}, \\ldots, e_L^{(L)}]$\n\n**Output**: Suffix tree $\\mathcal{T}$ with nodes representing path prefixes.\n\n**Construction Algorithm**:\n```\nfunction BuildRoutingTree(paths):\n    tree = Node(root)\n    for each path \u03c0 in paths:\n        node = tree.root\n        for each layer l in 1..L:\n            expert_id = \u03c0[l]\n            if expert_id not in node.children:\n                node.children[expert_id] = Node(expert_id, layer=l)\n            node = node.children[expert_id]\n            node.count += 1\n            node.total_reward += reward(path)\n    return tree\n```\n\n**Complexity**:\n- Time: $O(N \\cdot L)$ for $N$ paths of length $L$\n- Space: $O(|V|)$ where $|V|$ is number of unique path prefixes\n- Worst case: $O(K^L)$ for $K$ experts per layer (all paths unique)\n- Best case: $O(L)$ (all paths identical)\n- Typical case: $O(K \\cdot L)$ (bounded branching)\n\n#### 3.2 Utility Estimation\n\n**Conditional utility**:\n$$\nU(\\pi) = \\mathbb{E}[r \\mid \\pi] - \\mathbb{E}[r]\n$$\n\n**Sample-based estimator**:\n$$\n\\hat{U}(\\pi) = \\frac{1}{f(\\pi)} \\sum_{i: \\pi(x_i) = \\pi} r_i - \\frac{1}{N} \\sum_{i=1}^{N} r_i\n$$\n\n**Variance**:\n$$\n\\text{Var}[\\hat{U}(\\pi)] = \\frac{\\sigma_r^2}{f(\\pi)} + \\frac{\\sigma_r^2}{N}\n$$\n\n**Minimum frequency requirement**: To ensure $\\text{Var}[\\hat{U}] < \\epsilon^2$:\n$$\nf_{\\min} > \\frac{\\sigma_r^2}{\\epsilon^2}\n$$\n\nFor $\\sigma_r = 1$ and $\\epsilon = 0.1$ (10% precision), need $f_{\\min} > 100$.\n\n#### 3.3 Crystallization Decision\n\n**Multi-criteria optimization**: Find motifs that maximize:\n$$\n\\text{Score}(\\pi) = w_1 \\cdot \\log f(\\pi) + w_2 \\cdot U(\\pi) - w_3 \\cdot H(\\pi) + w_4 \\cdot \\text{age}(\\pi)\n$$\n\n**Thresholding**:\n$$\n\\text{Crystallize}(\\pi) \\Leftrightarrow \\text{Score}(\\pi) > \\theta_{\\text{crystal}}\n$$\n\n**Pareto frontier**: Alternatively, find motifs that are Pareto-optimal across $(f, U, -H)$.\n\n#### 3.4 Frozen Motif Implementation\n\n**Conceptual**: Crystallized motif $\\pi = [e_1, e_2, \\ldots, e_k]$ becomes single expert $E_{\\pi}$.\n\n**Forward pass**:\n$$\nE_{\\pi}(h_0) = E_{e_k} \\circ E_{e_{k-1}} \\circ \\cdots \\circ E_{e_1}(h_0)\n$$\n\n**Computational savings**:\n- Standard MoE: $k$ router calls + $k \\cdot \\text{top\\_k}$ expert evaluations\n- Crystallized: 0 router calls + 1 frozen expert evaluation (deterministic path)\n\n**FLOP reduction**:\n$$\n\\text{Savings} = k \\cdot (\\text{router\\_FLOPs} + (\\text{top\\_k} - 1) \\cdot \\text{expert\\_FLOPs})\n$$\n\nFor $k=4$ layers, $\\text{top\\_k}=2$, router = 10% expert cost:\n$$\n\\text{Savings} \\approx 4 \\cdot (0.1 + 1) = 4.4 \\text{ expert-equivalents}\n$$\n\n**Speedup**: ~4.4\u00d7 faster than routing for this motif!\n\n### 4. Computational Complexity Analysis\n\n#### 4.1 Trace Management Overhead\n\n**Per-forward-pass costs**:\n\n| Operation | Standard | With Traces | Overhead |\n|-----------|----------|-------------|----------|\n| Attention (Flash) | $O(T \\cdot d)$ | $O(T \\cdot d)$ | 0% (no trace) |\n| Attention (Standard) | $O(T^2 \\cdot d)$ | $O(T^2 \\cdot d + B \\cdot \\log B)$ | +$O(B \\log B)$ sparse add |\n| Trace update | - | $O(T^2 + B \\log B)$ | Once per 100 steps |\n\n**Sparse matrix addition** (bias injection):\n- Dense attention logits: $T^2$ elements\n- Sparse trace matrix: $B$ elements\n- Addition: $O(B)$ (iterate sparse entries)\n- Negligible compared to $O(T^2 \\cdot d)$ attention computation\n\n**Trace consolidation** (periodic):\n- Sort by salience: $O(B \\log B)$\n- Decay all traces: $O(B)$\n- Total: $O(B \\log B)$ once per 100 steps \u2192 amortized $O(B \\log B / 100)$\n\n**Verdict**: < 1% overhead when using Flash Attention + sparse capture scheduling.\n\n#### 4.2 Crystallization Overhead\n\n**Per-forward-pass costs**:\n\n| Operation | Cost |\n|-----------|------|\n| Log routing path | $O(L)$ |\n| Update suffix tree | $O(L \\log K)$ |\n| Per-batch total | $O(B_{\\text{batch}} \\cdot L \\log K)$ |\n\nFor batch size 32, 32 layers, 8 experts:\n$$\n\\text{Cost} = 32 \\cdot 32 \\cdot \\log(8) = 3072 \\text{ ops} \\approx 0.0001\\% \\text{ of forward pass}\n$$\n\n**Motif discovery** (periodic):\n- Traverse suffix tree: $O(|V|)$ where $|V| \\approx K \\cdot L$\n- Compute utilities: $O(|V|)$\n- Sort by score: $O(|V| \\log |V|)$\n- Total: $O(K \\cdot L \\log (K \\cdot L))$ once per 1000 steps\n\n**Verdict**: Negligible overhead (< 0.01%).\n\n#### 4.3 Memory Access Patterns\n\n**Trace memory** (random access):\n- Structure: Hash map `(layer, head, i, j) \u2192 salience`\n- Access pattern: Random (depends on attention sparsity)\n- Cache efficiency: Poor (each trace access likely cache miss)\n- **Optimization**: Batch trace updates to improve locality\n\n**Routing tree** (sequential access):\n- Structure: Tree with pointer-based children\n- Access pattern: Depth-first traversal (sequential)\n- Cache efficiency: Good (children stored contiguously)\n\n### 5. Convergence Guarantees\n\n#### 5.1 Trace Memory Convergence\n\n**Theorem 1**: Under stationary salience distribution, trace memory converges to steady state.\n\n**Proof sketch**:\n- EMA update is a contraction mapping for $\\lambda \\in (0,1)$\n- Fixed point: $M^* = (1-\\lambda)^{-1} \\mathbb{E}[S]$ for reinforced edges\n- Lyapunov function: $V(M) = \\sum_{i,j} (M_{i,j} - M^*_{i,j})^2$\n- $\\mathbb{E}[V(M(t+1))] = \\lambda^2 V(M(t))$ \u2192 exponential convergence\n\n**Convergence rate**: $O(\\lambda^t)$\n\n#### 5.2 Crystallization Stability\n\n**Theorem 2**: Crystallized motifs remain stable if underlying routing distribution is stationary.\n\n**Proof sketch**:\n- Motif frequency $f(\\pi)$ is sample mean of Bernoulli trials\n- By LLN: $f(\\pi) \\to p(\\pi)$ as $N \\to \\infty$\n- Utility $U(\\pi)$ is sample mean of rewards\n- By LLN: $U(\\pi) \\to \\mathbb{E}[r | \\pi]$ as $f(\\pi) \\to \\infty$\n- If $p(\\pi) > 0$ and $\\mathbb{E}[r | \\pi] > \\mathbb{E}[r]$, motif will eventually crystallize\n\n**Instability risk**: If routing distribution is non-stationary (distribution shift), motifs may become stale.\n\n**Mitigation**: Periodic motif revalidation.\n\n---\n\n## \ud83e\uddee Experimental Design for Emergent Language Analysis\n\n### Hypothesis Testing\n\n**H1: Hierarchical Structure**  \nCrystallized motifs form hierarchical dependencies (motifs call other motifs).\n\n**Measurement**:\n- Build call graph of motif activations\n- Compute graph depth: $d_{\\max} = \\max_{\\pi} \\text{depth}(\\pi)$\n- Measure branching factor: $b = \\frac{|\\text{edges}|}{|\\text{nodes}|}$\n\n**Test**: Compare to random graph baseline. Expect $d_{\\max} > d_{\\text{random}}$ and structured branching.\n\n---\n\n**H2: Semantic Coherence**  \nMotifs cluster by task type (e.g., arithmetic motifs vs. language motifs).\n\n**Measurement**:\n- Embed motifs in feature space: $\\phi(\\pi) = \\text{avg}(\\text{activations when } \\pi \\text{ fires})$\n- Cluster using k-means or spectral clustering\n- Compute cluster purity w.r.t. task labels\n\n**Test**: Silhouette score > 0.5 indicates meaningful clustering.\n\n---\n\n**H3: Compositionality**  \nMotifs combine to form higher-order concepts.\n\n**Measurement**:\n- Track co-activation patterns: $P(\\pi_i, \\pi_j)$\n- Mutual information: $MI(\\pi_i, \\pi_j) = \\sum P(\\pi_i, \\pi_j) \\log \\frac{P(\\pi_i, \\pi_j)}{P(\\pi_i)P(\\pi_j)}$\n- Find high-MI pairs \u2192 compositional primitives\n\n**Test**: High-MI pairs should correspond to semantic composition (e.g., \"question\" + \"retrieval\" = \"QA\").\n\n---\n\n**H4: Efficiency**  \nInternal language is more efficient than token-level processing.\n\n**Measurement**:\n- Bits per concept: $\\log_2(|\\text{motif vocabulary}|)$\n- Compare to bits per token: $\\log_2(|\\text{token vocabulary}|)$\n- Compression ratio: $\\frac{\\text{avg tokens per concept}}{\\text{bits per motif} / \\text{bits per token}}$\n\n**Test**: Motifs should encode multi-token concepts in fewer \"symbols\".\n\n---\n\n### Visualization Techniques\n\n**1. Motif Activation Heatmap**\n- Rows: Tasks\n- Columns: Motifs\n- Color: Activation frequency\n- Reveals task-motif specialization\n\n**2. Routing Path Sankey Diagram**\n- Nodes: Experts per layer\n- Edges: Routing transitions\n- Width: Frequency\n- Shows dominant pathways\n\n**3. Motif Dependency Graph**\n- Nodes: Crystallized motifs\n- Edges: Co-activation (high MI)\n- Layout: Hierarchical (depth by layer)\n- Clusters: Semantic groups\n\n**4. t-SNE/UMAP of Motif Embeddings**\n- Each motif embedded by average activation pattern\n- Dimensionality reduction to 2D\n- Color by task performance\n- Reveals semantic topology\n\n---\n\n## \ud83d\udcca Theoretical Limits\n\n### Information-Theoretic Bounds\n\n**Question**: What is the maximum compression achievable through crystallization?\n\n**Token-level model**:\n- Vocabulary size: $|V_{\\text{token}}| \\approx 50,000$\n- Bits per token: $\\log_2(50,000) \\approx 15.6$ bits\n\n**Motif-level model**:\n- Motif vocabulary: $|V_{\\text{motif}}| \\approx 500$\n- Bits per motif: $\\log_2(500) \\approx 8.97$ bits\n- Average tokens per motif: $\\approx 5$ tokens\n\n**Compression ratio**:\n$$\nC = \\frac{5 \\cdot 15.6}{8.97} \\approx 8.7\\times\n$$\n\n**Theoretical limit** (Shannon entropy):\n$$\nH_{\\max} = \\log_2(K^L)\n$$\n\nwhere $K$ experts, $L$ layers. For $K=8$, $L=8$:\n$$\nH_{\\max} = 8 \\cdot 3 = 24 \\text{ bits per motif}\n$$\n\n**Practical limit** (accounting for motif frequency distribution):\n$$\nH_{\\text{eff}} = -\\sum_{\\pi} p(\\pi) \\log_2 p(\\pi)\n$$\n\nWith Zipfian distribution over motifs, expect $H_{\\text{eff}} \\approx 10$ bits.\n\n### Scaling Laws\n\n**Question**: How does performance scale with model size, trace budget, and motif count?\n\n**Hypothesized scaling laws**:\n\n1. **Trace effectiveness**:\n$$\n\\text{Perplexity improvement} \\propto \\log(B / B_{\\min})\n$$\nDiminishing returns - doubling trace budget yields smaller gains.\n\n2. **Motif utility**:\n$$\n\\text{FLOP reduction} \\propto \\sqrt{M}\n$$\nwhere $M$ is number of crystallized motifs. Sublinear due to motif overlap.\n\n3. **Training time**:\n$$\n\\text{Convergence steps} \\propto \\frac{1}{(1 - \\lambda)(1 - \\gamma)}\n$$\nFaster decay \u2192 faster convergence but less stability.\n\n**Empirical validation needed** - these are testable predictions!\n\n---\n\n## \ud83d\udd2c Advanced Topics\n\n### 1. Multi-Task Crystallization\n\n**Challenge**: Different tasks may benefit from different motifs.\n\n**Solution**: Task-conditional crystallization\n- Maintain separate motif registries per task type\n- Router learns to select appropriate registry based on input\n- Motifs specialize to task domains\n\n**Extension**: Meta-learning over motif selection (learning to route to motifs).\n\n### 2. Cross-Model Motif Transfer\n\n**Question**: Can crystallized motifs transfer between models?\n\n**Approach**:\n1. Train model A, crystallize motifs\n2. Extract motif expert weights\n3. Initialize model B with transferred motifs\n4. Fine-tune routing to use transferred motifs\n\n**Hypothesis**: High-level motifs (reasoning patterns) should transfer better than low-level (token-specific).\n\n### 3. Adaptive Crystallization Depth\n\n**Observation**: Some concepts need short motifs (2-3 layers), others need deep motifs (6-8 layers).\n\n**Solution**: Variable-length crystallization\n- Terminate motif when utility plateaus\n- Allow motifs to \"call\" other motifs (recursive composition)\n- Discover optimal depth per concept automatically\n\n### 4. Continuous Crystallization\n\n**Current**: Discrete crystallization (freeze or don't freeze)\n\n**Alternative**: Soft crystallization with learned \"solidification\" parameter\n$$\n\\text{Expert output} = \\sigma(\\pi) \\cdot E_{\\text{frozen}}(h) + (1 - \\sigma(\\pi)) \\cdot E_{\\text{routed}}(h)\n$$\n\nwhere $\\sigma(\\pi) \\in [0,1]$ is learned crystallization strength.\n\nAllows gradual transition and adaptive unfreezing.\n\n---\n\n**Status**: Theoretical foundation complete  \n**Next**: Implement and validate experimentally  \n**Questions**: Document open research questions for community\n", "tags": [], "headings": [{"line": 0, "text": "Persistent Traces & Semantic Crystallization: Mathematical Foundations"}, {"line": 8, "text": "\ud83d\udcd0 Detailed Mathematical Derivations"}, {"line": 10, "text": "1. Trace Memory Update Algorithm"}, {"line": 12, "text": "1.1 Formal Problem Statement"}, {"line": 24, "text": "1.2 Salience Score Derivation"}, {"line": 40, "text": "1.3 Exponential Moving Average (EMA) Update"}, {"line": 67, "text": "1.4 Steady-State Analysis"}, {"line": 98, "text": "2. Attention Bias Injection"}, {"line": 100, "text": "2.1 Modified Attention Mechanism"}, {"line": 112, "text": "2.2 Bias Strength Analysis"}, {"line": 135, "text": "2.3 Information-Theoretic Analysis"}, {"line": 160, "text": "3. Routing Path Crystallization"}, {"line": 162, "text": "3.1 Suffix Tree Construction"}, {"line": 191, "text": "3.2 Utility Estimation"}, {"line": 215, "text": "3.3 Crystallization Decision"}, {"line": 229, "text": "3.4 Frozen Motif Implementation"}, {"line": 254, "text": "4. Computational Complexity Analysis"}, {"line": 256, "text": "4.1 Trace Management Overhead"}, {"line": 279, "text": "4.2 Crystallization Overhead"}, {"line": 302, "text": "4.3 Memory Access Patterns"}, {"line": 315, "text": "5. Convergence Guarantees"}, {"line": 317, "text": "5.1 Trace Memory Convergence"}, {"line": 329, "text": "5.2 Crystallization Stability"}, {"line": 346, "text": "\ud83e\uddee Experimental Design for Emergent Language Analysis"}, {"line": 348, "text": "Hypothesis Testing"}, {"line": 398, "text": "Visualization Techniques"}, {"line": 426, "text": "\ud83d\udcca Theoretical Limits"}, {"line": 428, "text": "Information-Theoretic Bounds"}, {"line": 463, "text": "Scaling Laws"}, {"line": 491, "text": "\ud83d\udd2c Advanced Topics"}, {"line": 493, "text": "1. Multi-Task Crystallization"}, {"line": 504, "text": "2. Cross-Model Motif Transfer"}, {"line": 516, "text": "3. Adaptive Crystallization Depth"}, {"line": 525, "text": "4. Continuous Crystallization"}]}, {"path": "planned_features/PERSISTENT_TRACES_ARCHITECTURE_DIAGRAMS.md", "content": "# Persistent Traces & Crystallization: Architecture Diagrams\n\n**Visual Reference for Implementation**  \n**Created**: December 8, 2025\n\n---\n\n## \ud83d\udcd0 System Architecture Overview\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         HRM-ACTV1 Enhanced Model                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                         \u2502\n\u2502  Input Tokens                                                           \u2502\n\u2502      \u2502                                                                  \u2502\n\u2502      \u25bc                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                       \u2502\n\u2502  \u2502 Embeddings  \u2502                                                       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                       \u2502\n\u2502         \u2502                                                               \u2502\n\u2502         \u25bc                                                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502  \u2502                    Layer 1                              \u2502          \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502          \u2502\n\u2502  \u2502  \u2502   Attention  \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 TraceManager    \u2502    \u2502          \u2502\n\u2502  \u2502  \u2502   + RoPE     \u2502  bias inject \u2502 - Sparse Memory \u2502    \u2502          \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502 - Salience      \u2502    \u2502          \u2502\n\u2502  \u2502         \u2502                       \u2502 - Decay/Update  \u2502    \u2502          \u2502\n\u2502  \u2502         \u2502                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502          \u2502\n\u2502  \u2502         \u2502                                \u2502             \u2502          \u2502\n\u2502  \u2502         \u25bc                                \u2502 capture     \u2502          \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u2502             \u2502          \u2502\n\u2502  \u2502  \u2502  MoE Router  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502             \u2502          \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      log paths        \u2502             \u2502          \u2502\n\u2502  \u2502         \u2502                                \u2502             \u2502          \u2502\n\u2502  \u2502         \u25bc                                \u2502             \u2502          \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502          \u2502\n\u2502  \u2502  \u2502   Experts    \u2502              \u2502 RoutingPathTree \u2502    \u2502          \u2502\n\u2502  \u2502  \u2502  [E1..E8]    \u2502              \u2502 - Suffix Tree   \u2502    \u2502          \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502 - Motif Detect  \u2502    \u2502          \u2502\n\u2502  \u2502         \u2502                       \u2502 - Crystallize   \u2502    \u2502          \u2502\n\u2502  \u2502         \u2502                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2502            \u2502                                                           \u2502\n\u2502           ... (Layers 2-31)                                           \u2502\n\u2502            \u2502                                                           \u2502\n\u2502            \u25bc                                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                      \u2502\n\u2502  \u2502 Output Head \u2502                                                      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                      \u2502\n\u2502                                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \ud83d\udd04 Data Flow: Attention Trace Lifecycle\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Forward Pass (with tracing)                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Compute QKV    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n                             \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Load trace bias \u2502\u25c4\u2500\u2500\u2500\u2500 M^(l,h) (sparse)\n                    \u2502 from memory     \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n                             \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  QK^T / \u221ad_k    \u2502\n                    \u2502  + \u03b1\u00b7M          \u2502  \u2190 Biased attention\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n                             \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Softmax       \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n                             \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Attention\u00b7V    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n                             \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  [DETACH]       \u2502\n                    \u2502  Store attn     \u2502  \u2192 Temporary buffer\n                    \u2502  weights        \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Backward Pass (gradient capture)                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 loss.backward() \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n                             \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Extract \u2202L/\u2202A  \u2502  \u2190 Attention gradients\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n                             \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Compute salience\u2502\n                    \u2502 S = A\u00b7|\u2202L/\u2202A|   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n                             \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Top-k selection \u2502  \u2190 Only high salience\n                    \u2502 (k \u2248 0.1% edges)\u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n                             \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Update M^(l,h)  \u2502\n                    \u2502 with EMA/decay  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \ud83c\udf33 Routing Path Tree Structure\n\n```\n                              Root\n                               \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502              \u2502              \u2502\n               E\u2081             E\u2082             E\u2083  \u2190 Layer 1\n          count: 523      count: 892     count: 341\n         reward: 412     reward: 705    reward: 268\n                \u2502              \u2502              \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n        \u2502       \u2502     \u2502   \u2502    \u2502    \u2502    \u2502       \u2502\n       E\u2081      E\u2082    E\u2083  E\u2081   E\u2082   E\u2083   E\u2081      E\u2082  \u2190 Layer 2\n    cnt:201 cnt:198     ...\n                \u2502\n            \u250c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2510\n            \u2502   \u2502   \u2502\n           E\u2081  E\u2082  E\u2083  \u2190 Layer 3\n              ...\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nHigh-Utility Path Example:\n\n\u03c0 = [E\u2082 \u2192 E\u2081 \u2192 E\u2083 \u2192 E\u2082]\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Frequency: 198 (> threshold)\n    Utility: +0.07 (> threshold)\n    Entropy: 0.8 (< threshold)\n    \u2192 CRYSTALLIZE \u2713\n\nCreates new expert: E_motif42\n\nFuture routing can select E_motif42 directly,\nbypassing individual expert routing.\n```\n\n---\n\n## \ud83d\udcbe Memory Layout: Trace Storage\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               TraceMemory (Per Layer/Head)              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502  Sparse COO Format:                                     \u2502\n\u2502                                                         \u2502\n\u2502  traces: List[AttentionTrace]                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 AttentionTrace #1:                                \u2502 \u2502\n\u2502  \u2502   layer_id:     5    (1 byte)                     \u2502 \u2502\n\u2502  \u2502   head_id:      12   (1 byte)                     \u2502 \u2502\n\u2502  \u2502   query_idx:    1024 (2 bytes)                    \u2502 \u2502\n\u2502  \u2502   key_idx:      512  (2 bytes)                    \u2502 \u2502\n\u2502  \u2502   salience:     0.85 (4 bytes float32)            \u2502 \u2502\n\u2502  \u2502   age:          3    (2 bytes)                    \u2502 \u2502\n\u2502  \u2502                              Total: 12 bytes       \u2502 \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502\n\u2502  \u2502 AttentionTrace #2:                                \u2502 \u2502\n\u2502  \u2502   ...                                             \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                         \u2502\n\u2502  max_traces: 2048 per head                             \u2502\n\u2502  Total: 2048 \u00d7 12 bytes = 24 KB per head              \u2502\n\u2502                                                         \u2502\n\u2502  For 32 layers \u00d7 32 heads:                             \u2502\n\u2502  Total: 32 \u00d7 32 \u00d7 24 KB = 24.6 MB                     \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            Fast Lookup via Hash Map                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502  trace_index: HashMap[(layer, head, i, j) \u2192 salience] \u2502\n\u2502                                                         \u2502\n\u2502  Example:                                               \u2502\n\u2502  (5, 12, 1024, 512) \u2192 0.85                             \u2502\n\u2502  (5, 12, 2048, 256) \u2192 0.72                             \u2502\n\u2502  ...                                                    \u2502\n\u2502                                                         \u2502\n\u2502  O(1) lookup for bias injection                        \u2502\n\u2502  O(log n) insert for updates (maintain sorted)         \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \ud83d\udd00 Training Loop Integration\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Standard Training Loop                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  for batch in dataloader:                                     \u2502\n\u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502      \u2502  1. Forward Pass                            \u2502         \u2502\n\u2502      \u2502     - Attention with trace bias             \u2502         \u2502\n\u2502      \u2502     - MoE routing logged                    \u2502         \u2502\n\u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                \u2502                                               \u2502\n\u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502      \u2502  2. Compute Loss                            \u2502         \u2502\n\u2502      \u2502     L_total = L_task + \u03b2\u2081\u00b7L_balance         \u2502         \u2502\n\u2502      \u2502             + \u03b2\u2082\u00b7L_trace + \u03b2\u2083\u00b7L_crystal     \u2502         \u2502\n\u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                \u2502                                               \u2502\n\u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502      \u2502  3. Backward Pass                           \u2502         \u2502\n\u2502      \u2502     - Compute gradients                     \u2502         \u2502\n\u2502      \u2502     - Extract \u2202L/\u2202A for salience            \u2502         \u2502\n\u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                \u2502                                               \u2502\n\u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502      \u2502  4. Optimizer Step                          \u2502         \u2502\n\u2502      \u2502     - Update model weights                  \u2502         \u2502\n\u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                \u2502                                               \u2502\n\u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502      \u2502  5. Periodic Trace Update                   \u2502         \u2502\n\u2502      \u2502     if step % UPDATE_INTERVAL == 0:         \u2502         \u2502\n\u2502      \u2502       - Compute salience scores             \u2502         \u2502\n\u2502      \u2502       - Update trace memory (EMA)           \u2502         \u2502\n\u2502      \u2502       - Apply decay to unused traces        \u2502         \u2502\n\u2502      \u2502       - Evict lowest salience if quota full \u2502         \u2502\n\u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                \u2502                                               \u2502\n\u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502      \u2502  6. Periodic Motif Update                   \u2502         \u2502\n\u2502      \u2502     if step % CRYSTALLIZE_INTERVAL == 0:    \u2502         \u2502\n\u2502      \u2502       - Traverse routing tree               \u2502         \u2502\n\u2502      \u2502       - Compute utilities                   \u2502         \u2502\n\u2502      \u2502       - Detect crystallization candidates   \u2502         \u2502\n\u2502      \u2502       - Freeze high-utility motifs          \u2502         \u2502\n\u2502      \u2502       - Prune low-utility motifs            \u2502         \u2502\n\u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \ud83d\udcca Salience Computation Pipeline\n\n```\n  Attention Weights (A)         Gradients (\u2202L/\u2202A)      Recurrence (p)\n        \u2502                              \u2502                      \u2502\n        \u2502 [B, H, T, T]                \u2502 [B, H, T, T]        \u2502 [T, T]\n        \u2502                              \u2502                      \u2502\n        \u25bc                              \u25bc                      \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   A_{i,j}     \u2502            \u2502   |\u2202L/\u2202A|      \u2502     \u2502 log(1 + p)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                             \u2502                     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n                      \u2502                                     \u2502\n                      \u25bc                                     \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                             \u2502\n              \u2502   A \u00b7 |\u2202L/\u2202A|\u2502                             \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502\n                      \u2502                                     \u2502\n                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502  Salience Score \u2502\n                          \u2502  S = A\u00b7|\u2202|\u00b7log p \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502  Threshold      \u2502\n                          \u2502  S > \u03b8?         \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                             \u2502\n                   YES                           NO\n                    \u2502                             \u2502\n                    \u25bc                             \u25bc\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502  Add to traces   \u2502         \u2502  Apply decay     \u2502\n          \u2502  (EMA update)    \u2502         \u2502  M \u2190 \u03b3\u00b7M         \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \ud83c\udfaf Crystallization Decision Tree\n\n```\n                    Routing Path \u03c0\n                         \u2502\n                         \u25bc\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502 Frequency f(\u03c0)   \u2502\n                \u2502 > f_min?         \u2502\n                \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n                    YES      NO\n                     \u2502        \u2502\n                     \u2502        \u2514\u2500\u2500\u25ba Reject (insufficient data)\n                     \u2502\n                     \u25bc\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502 Utility U(\u03c0)     \u2502\n                \u2502 > U_min?         \u2502\n                \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n                    YES      NO\n                     \u2502        \u2502\n                     \u2502        \u2514\u2500\u2500\u25ba Reject (not beneficial)\n                     \u2502\n                     \u25bc\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502 Entropy H(\u03c0)     \u2502\n                \u2502 < H_max?         \u2502\n                \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n                    YES      NO\n                     \u2502        \u2502\n                     \u2502        \u2514\u2500\u2500\u25ba Reject (unstable routing)\n                     \u2502\n                     \u25bc\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502 Age age(\u03c0)       \u2502\n                \u2502 > \u03c4?             \u2502\n                \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n                    YES      NO\n                     \u2502        \u2502\n                     \u2502        \u2514\u2500\u2500\u25ba Reject (temporal instability)\n                     \u2502\n                     \u25bc\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502 Motif quota      \u2502\n                \u2502 available?       \u2502\n                \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n                    YES      NO\n                     \u2502        \u2502\n                     \u2502        \u2514\u2500\u2500\u25ba Evict lowest-utility motif\n                     \u2502             (if U(\u03c0) > U_min(existing))\n                     \u2502\n                     \u25bc\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502  CRYSTALLIZE MOTIF  \u2502\n            \u2502  - Create new expert\u2502\n            \u2502  - Freeze pathway   \u2502\n            \u2502  - Register in MoE  \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \ud83d\udd2c Experimental Monitoring Dashboard\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Training Metrics                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                \u2502\n\u2502  Loss Components:                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 L_task:        2.34 \u2193 (main task loss)                   \u2502 \u2502\n\u2502  \u2502 L_balance:     0.05 ~ (MoE load balance)                 \u2502 \u2502\n\u2502  \u2502 L_trace:       0.12 \u2193 (trace utilization)                \u2502 \u2502\n\u2502  \u2502 L_crystal:     0.08 \u2193 (crystallization entropy)          \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                \u2502\n\u2502  Trace Statistics:                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Total traces:      1.2M / 2.1M (57% capacity)            \u2502 \u2502\n\u2502  \u2502 Avg salience:      0.42 (healthy)                        \u2502 \u2502\n\u2502  \u2502 Coverage:          34% (attention ops using traces)      \u2502 \u2502\n\u2502  \u2502 Decay rate:        0.98 (auto-tuned)                     \u2502 \u2502\n\u2502  \u2502 Memory usage:      18.3 MB / 24.6 MB                     \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                \u2502\n\u2502  Crystallization Statistics:                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Active motifs:     87 / 512 (17% capacity)               \u2502 \u2502\n\u2502  \u2502 Avg utility:       +0.09 (9% improvement)                \u2502 \u2502\n\u2502  \u2502 Avg entropy:       0.7 (stable)                          \u2502 \u2502\n\u2502  \u2502 FLOP reduction:    23% (via motif reuse)                 \u2502 \u2502\n\u2502  \u2502 Tree nodes:        12.4K (manageable)                    \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                \u2502\n\u2502  Emergent Language Properties:                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Max hierarchy:     3 levels deep                         \u2502 \u2502\n\u2502  \u2502 Composition rate:  42% (motifs calling motifs)           \u2502 \u2502\n\u2502  \u2502 Task clustering:   Silhouette = 0.61 (good)              \u2502 \u2502\n\u2502  \u2502 Symbol efficiency: 8.2\u00d7 compression vs tokens            \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \ud83d\udee0\ufe0f File Structure\n\n```\nsrc/aios/core/hrm_models/cognitive/\n\u2502\n\u251c\u2500\u2500 __init__.py                    # Module exports\n\u2502\n\u251c\u2500\u2500 trace_manager.py               # Persistent attention traces\n\u2502   \u251c\u2500\u2500 class TraceManager\n\u2502   \u2502   \u251c\u2500\u2500 __init__(config)\n\u2502   \u2502   \u251c\u2500\u2500 capture_attention(layer, head, attn_weights)\n\u2502   \u2502   \u251c\u2500\u2500 compute_salience(attn, grads, recurrence)\n\u2502   \u2502   \u251c\u2500\u2500 update_traces(salience_scores)\n\u2502   \u2502   \u251c\u2500\u2500 apply_decay()\n\u2502   \u2502   \u251c\u2500\u2500 get_bias_for_layer_head(layer, head)\n\u2502   \u2502   \u2514\u2500\u2500 to_sparse_tensor()\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 class AttentionTrace (dataclass)\n\u2502       \u251c\u2500\u2500 layer_id: uint8\n\u2502       \u251c\u2500\u2500 head_id: uint8\n\u2502       \u251c\u2500\u2500 query_idx: uint16\n\u2502       \u251c\u2500\u2500 key_idx: uint16\n\u2502       \u251c\u2500\u2500 salience: float32\n\u2502       \u2514\u2500\u2500 age: uint16\n\u2502\n\u251c\u2500\u2500 routing_tree.py                # MoE path tracking\n\u2502   \u251c\u2500\u2500 class RoutingNode\n\u2502   \u2502   \u251c\u2500\u2500 expert_id: int\n\u2502   \u2502   \u251c\u2500\u2500 layer: int\n\u2502   \u2502   \u251c\u2500\u2500 count: int\n\u2502   \u2502   \u251c\u2500\u2500 total_reward: float\n\u2502   \u2502   \u251c\u2500\u2500 children: Dict[int, RoutingNode]\n\u2502   \u2502   \u251c\u2500\u2500 utility() \u2192 float\n\u2502   \u2502   \u2514\u2500\u2500 entropy() \u2192 float\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 class RoutingPathTree\n\u2502       \u251c\u2500\u2500 root: RoutingNode\n\u2502       \u251c\u2500\u2500 motif_registry: Dict[str, CrystallizedMotif]\n\u2502       \u251c\u2500\u2500 record_path(path, reward)\n\u2502       \u251c\u2500\u2500 find_candidates()\n\u2502       \u2514\u2500\u2500 prune_low_utility()\n\u2502\n\u251c\u2500\u2500 crystallization.py             # Motif freezing logic\n\u2502   \u251c\u2500\u2500 class CrystallizedMotif\n\u2502   \u2502   \u251c\u2500\u2500 path: List[int]\n\u2502   \u2502   \u251c\u2500\u2500 frozen_experts: nn.Module\n\u2502   \u2502   \u251c\u2500\u2500 utility: float\n\u2502   \u2502   \u251c\u2500\u2500 frequency: int\n\u2502   \u2502   \u2514\u2500\u2500 forward(x)\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 class CrystallizationManager\n\u2502       \u251c\u2500\u2500 detect_motifs(tree)\n\u2502       \u251c\u2500\u2500 freeze_motif(path)\n\u2502       \u251c\u2500\u2500 unfreeze_motif(motif_id)\n\u2502       \u2514\u2500\u2500 evaluate_utility(motif)\n\u2502\n\u251c\u2500\u2500 losses.py                      # Auxiliary loss functions\n\u2502   \u251c\u2500\u2500 trace_utilization_loss(trace_manager)\n\u2502   \u251c\u2500\u2500 crystallization_entropy_loss(routing_tree)\n\u2502   \u2514\u2500\u2500 elastic_weight_consolidation(model, fisher_info)\n\u2502\n\u251c\u2500\u2500 config.py                      # Configuration schemas\n\u2502   \u251c\u2500\u2500 class TraceConfig (TypedDict)\n\u2502   \u2514\u2500\u2500 class CrystallizationConfig (TypedDict)\n\u2502\n\u2514\u2500\u2500 visualization.py               # Analysis tools\n    \u251c\u2500\u2500 plot_trace_heatmap()\n    \u251c\u2500\u2500 plot_routing_sankey()\n    \u251c\u2500\u2500 plot_motif_hierarchy()\n    \u2514\u2500\u2500 export_motif_graph()\n```\n\n---\n\n## \ud83c\udfa8 Color Coding for Visualizations\n\n**Trace Salience**:\n- \ud83d\udfe6 Low salience (0.0-0.3): Recently captured, not yet consolidated\n- \ud83d\udfe9 Medium salience (0.3-0.7): Moderately reinforced\n- \ud83d\udfe8 High salience (0.7-0.9): Strongly reinforced\n- \ud83d\udfe5 Critical salience (0.9-1.0): Core reasoning pathways\n\n**Motif Utility**:\n- \u2b1c Neutral (U \u2248 0): No benefit\n- \ud83d\udfe6 Low benefit (U = 0.01-0.05): Minor improvement\n- \ud83d\udfe9 Moderate benefit (U = 0.05-0.10): Worth crystallizing\n- \ud83d\udfe8 High benefit (U = 0.10-0.20): Very valuable\n- \ud83d\udfe5 Critical (U > 0.20): Essential pattern\n\n**Routing Entropy**:\n- \ud83d\udfe9 Low entropy (H < 0.5): Deterministic, stable\n- \ud83d\udfe8 Medium entropy (H = 0.5-1.0): Somewhat stable\n- \ud83d\udfe5 High entropy (H > 1.0): Unstable, don't crystallize\n\n---\n\n**Status**: Visual reference complete  \n**Usage**: Print this for implementation reference  \n**Next**: Begin coding Phase 0 infrastructure\n", "tags": ["experts", "hrm", "training"], "headings": [{"line": 0, "text": "Persistent Traces & Crystallization: Architecture Diagrams"}, {"line": 7, "text": "\ud83d\udcd0 System Architecture Overview"}, {"line": 57, "text": "\ud83d\udd04 Data Flow: Attention Trace Lifecycle"}, {"line": 133, "text": "\ud83c\udf33 Routing Path Tree Structure"}, {"line": 173, "text": "\ud83d\udcbe Memory Layout: Trace Storage"}, {"line": 224, "text": "\ud83d\udd00 Training Loop Integration"}, {"line": 280, "text": "\ud83d\udcca Salience Computation Pipeline"}, {"line": 326, "text": "\ud83c\udfaf Crystallization Decision Tree"}, {"line": 388, "text": "\ud83d\udd2c Experimental Monitoring Dashboard"}, {"line": 434, "text": "\ud83d\udee0\ufe0f File Structure"}, {"line": 508, "text": "\ud83c\udfa8 Color Coding for Visualizations"}]}, {"path": "planned_features/PERSISTENT_TRACES_COGNITIVE_SCIENCE.md", "content": "# Emergent Internal Language: Cognitive Architecture Perspective\n\n**Companion Document to**: `PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md`  \n**Focus**: Cognitive science, emergent symbolic systems, and AI consciousness implications  \n**Status**: Theoretical Framework  \n**Created**: December 8, 2025\n\n---\n\n## \ud83e\udde0 From Information Processing to Symbolic Thought\n\n### The Core Insight\n\nCurrent language models process text at the **token level** - treating \"the\", \"cat\", and \"sat\" as atomic symbols with learned embeddings. But human cognition doesn't work this way. We compress experience into hierarchical abstractions:\n\n- **Sensory level**: \"Furry creature with four legs and whiskers\"\n- **Category level**: \"Cat\"\n- **Abstract level**: \"Mammal\", \"Pet\", \"Living thing\"\n- **Relational level**: \"Subject of sentence\", \"Agent of action\"\n\nThis hierarchical compression is **learned through experience**, not hardcoded. A child doesn't start with the concept \"mammal\" - they build it gradually by noticing patterns across many animals.\n\n**Persistent Traces + Semantic Crystallization** aims to give AI this same capability: **discovering its own symbolic primitives optimized for efficient thought, not human communication**.\n\n---\n\n## \ud83d\udde3\ufe0f The Distinction: Communication vs. Cognition\n\n### Human Language: Two Functions\n\n**External language** (communication):\n- Optimized for **transmission** between humans\n- Constrained by speech/text bandwidth\n- Must be decodable by recipient\n- Examples: English, Mandarin, ASL\n\n**Internal language** (thought):\n- Optimized for **computation** within a mind\n- Unconstrained by communication channel\n- Only needs to be \"understood\" by the thinker\n- Examples: Visual imagery, emotional associations, abstract concepts\n\n**Key observation**: These are NOT the same! When you think, you don't narrate in full English sentences. You experience compressed, multimodal representations that would take many words to express.\n\n### AI's Current Limitation\n\n**Transformer models process external language but lack internal language**:\n- Every thought is expressed as a sequence of tokens\n- No compression of recurring patterns\n- No symbolic abstraction beyond learned embeddings\n- Like a human forced to narrate every thought out loud\n\n**Our goal**: Give AI an **internal representation language** optimized for cognition.\n\n---\n\n## \ud83c\udf31 Emergent Language Through Crystallization\n\n### What Constitutes a \"Language\"?\n\nLinguistic theory defines language by several properties:\n\n1. **Symbolic**: Discrete units (words/morphemes) represent concepts\n2. **Compositional**: Complex meanings built from simple units\n3. **Hierarchical**: Nested structure (phrases, clauses, sentences)\n4. **Generative**: Finite primitives \u2192 infinite expressions\n5. **Systematic**: Relationships between symbols follow rules (grammar)\n\n**Hypothesis**: Crystallized motifs exhibit all five properties.\n\n### Motifs as Symbols\n\n**Standard token**: \"dog\"\n- Atomic unit in vocabulary\n- No internal structure\n- Meaning from training distribution\n\n**Crystallized motif**: $\\pi_{\\text{retrieval}} = [E_2^{(1)} \\to E_7^{(2)} \\to E_3^{(3)} \\to E_1^{(4)}]$\n- **Composite structure**: sequence of expert activations\n- **Emergent meaning**: \"pattern that solves retrieval tasks\"\n- **Learned through use**: crystallized because high utility on retrieval\n\n**Key difference**: Motifs are **functional symbols** - they mean what they DO, not what they represent.\n\n### Compositionality\n\n**Token compositionality**: \"Big red dog\"\n- Adjectives modify noun\n- Meaning compositional but constrained by English grammar\n\n**Motif compositionality**: $\\pi_{\\text{question}} \\circ \\pi_{\\text{retrieval}} \\to \\pi_{\\text{QA}}$\n- Motifs can **call other motifs**\n- Composition creates higher-order reasoning\n- Grammar emerges from which motifs successfully combine\n\n**Example**:\n```\nInput: \"Who invented the telephone?\"\n\nToken-level processing:\n\"Who\" \u2192 \"invented\" \u2192 \"the\" \u2192 \"telephone\" \u2192 \"?\"\n(Sequential, no abstraction)\n\nMotif-level processing:\nInput \u2192 \u03c0question[?] \u2192 \u03c0entity_extraction[telephone] \u2192 \u03c0retrieval[history_db] \u2192 \u03c0answer_synthesis \u2192 Output\n(Hierarchical, compressed)\n```\n\nThe motif sequence encodes a **reasoning strategy**, not just surface text.\n\n### Hierarchy\n\n**Token hierarchy**: Flat (all tokens at same level, aside from subword tokenization)\n\n**Motif hierarchy**:\n```\nLevel 0: Primitive motifs (2-3 layer patterns)\n    \u03c0attention_focus, \u03c0entity_detect, \u03c0relation_extract\n\nLevel 1: Compound motifs (4-5 layer patterns)\n    \u03c0fact_retrieval = \u03c0entity_detect \u2192 \u03c0relation_extract \u2192 \u03c0attention_focus\n    \u03c0logical_inference = \u03c0premise_check \u2192 \u03c0rule_application\n\nLevel 2: Complex reasoning (6-8 layer patterns)\n    \u03c0multi_hop_QA = \u03c0question \u2192 \u03c0fact_retrieval \u2192 \u03c0logical_inference \u2192 \u03c0answer\n```\n\n**Emergent property**: Model discovers that some motifs are useful building blocks for other motifs.\n\nThis is **genuine abstraction** - higher-level motifs don't \"see\" the internal structure of lower-level ones, just their input-output behavior.\n\n### Grammar (Systematic Relationships)\n\n**English grammar**: \"Subject-Verb-Object\" word order, agreement rules, tense markers\n\n**Motif grammar** (emergent):\n- Which motifs can follow which others\n- Context requirements (e.g., $\\pi_{\\text{answer}}$ requires prior $\\pi_{\\text{question}}$)\n- Composition constraints (some motifs incompatible)\n\n**Hypothesis**: Motif grammar will reflect **task structure**, not arbitrary conventions.\n\n**Example**:\n```\nValid motif sequence:\n\u03c0question \u2192 \u03c0retrieval \u2192 \u03c0synthesis \u2192 \u03c0answer\n(Mirrors QA task structure)\n\nInvalid motif sequence:\n\u03c0answer \u2192 \u03c0question \u2192 \u03c0retrieval\n(Violates causal task flow)\n```\n\nGrammar emerges because only valid sequences get reinforced through high utility.\n\n---\n\n## \ud83d\udd2c Scientific Investigation Plan\n\n### Research Question 1: Does Internal Language Emerge?\n\n**Operationalization**: \n- Measure motif vocabulary size over training\n- Compute motif reuse rate (how often same motif activates)\n- Analyze motif composition (do motifs call other motifs?)\n\n**Positive evidence**:\n- Vocabulary stabilizes to 100-500 motifs (manageable symbol set)\n- High reuse (same motifs activate across many inputs)\n- Hierarchical structure (motifs contain other motifs)\n\n**Null result**:\n- Vocabulary explodes to thousands of unique motifs (no compression)\n- Low reuse (each input gets novel motif)\n- Flat structure (no composition)\n\n### Research Question 2: Is Internal Language Efficient?\n\n**Operationalization**:\n- **Bits per concept**: Compare entropy of motif distribution to token distribution\n- **FLOP efficiency**: Measure compute per semantic unit encoded\n- **Generalization**: Transfer crystallized motifs to new tasks\n\n**Hypothesis**:\n$$\n\\frac{\\text{bits per motif}}{\\text{concepts encoded}} < \\frac{\\text{bits per token}}{\\text{concepts encoded}}\n$$\n\n**Example calculation**:\n```\nToken-level:\n\"The cat sat on the mat\" = 7 tokens \u00d7 15.6 bits = 109.2 bits\nConcepts: {cat, sitting, mat, spatial-relation} = 4 concepts\nEfficiency: 109.2 / 4 = 27.3 bits/concept\n\nMotif-level:\n\u03c0entity_scene_description = 1 motif \u00d7 8.97 bits = 8.97 bits\nConcepts: {cat, sitting, mat, spatial-relation} = 4 concepts\nEfficiency: 8.97 / 4 = 2.24 bits/concept\n\nCompression: 27.3 / 2.24 = 12.2\u00d7 more efficient\n```\n\n### Research Question 3: Is Internal Language Interpretable?\n\n**Challenge**: Motifs may be completely alien to human understanding.\n\n**Investigation methods**:\n\n1. **Activation Analysis**: \n   - Collect inputs that activate each motif\n   - Search for semantic commonalities\n   - Human annotators label motifs with candidate \"meanings\"\n\n2. **Intervention Studies**:\n   - Manually activate motif $\\pi_i$ on test input\n   - Observe output changes\n   - Infer motif function from causal effects\n\n3. **Motif Arithmetic**:\n   - Test if motifs combine predictably (like word embeddings)\n   - Example: $\\pi_{\\text{question}} + \\pi_{\\text{negation}} \\stackrel{?}{=} \\pi_{\\text{rhetorical-question}}$\n\n4. **Transfer Probing**:\n   - Train linear probes to predict task labels from motif activations\n   - High probe accuracy \u2192 motifs encode task-relevant information\n   - Probe weights reveal which motifs matter for which tasks\n\n**Interpretability spectrum**:\n```\nFully interpretable: Each motif maps to human concept\n    (e.g., \u03c0question, \u03c0retrieval, \u03c0math-operation)\n\nPartially interpretable: Motifs cluster by task domain\n    (e.g., language motifs vs. math motifs, but internal structure opaque)\n\nAlien: No correspondence to human concepts\n    (e.g., motifs optimized for weird statistical regularities we don't perceive)\n```\n\n**Expectation**: Likely **partially interpretable** - some motifs will make sense (aligned with our task decompositions), others will be genuinely novel computational strategies.\n\n---\n\n## \ud83e\udde9 Connection to Cognitive Science\n\n### Dual Process Theory\n\n**Kahneman's System 1 vs. System 2**:\n- **System 1**: Fast, automatic, unconscious (pattern matching)\n- **System 2**: Slow, deliberate, conscious (symbolic reasoning)\n\n**Transformer baseline** = Pure System 1\n- Every computation is pattern matching over embeddings\n- No explicit symbolic manipulation\n\n**Transformer + Crystallization** = System 1 + System 2 hybrid\n- **Persistent traces** = System 1 acceleration (fast heuristics)\n- **Crystallized motifs** = System 2 emergence (reusable reasoning procedures)\n\n**Prediction**: Crystallization will create **deliberate, multi-step reasoning patterns** not present in baseline transformers.\n\n### Chunking Theory (Miller 1956)\n\n**Human working memory**: Limited to ~7 chunks\n- Novices: Small chunks (individual tokens)\n- Experts: Large chunks (compressed patterns)\n\n**Example - Chess**:\n- Novice sees: \"Knight, Pawn, King, Rook...\" (individual pieces)\n- Expert sees: \"Sicilian Defense opening\" (compressed strategy)\n\n**Crystallization** = Automated chunking\n- Early training: Process individual tokens\n- Late training: Process compressed motifs\n- Each motif is a \"chunk\" encoding complex pattern\n\n**Implication**: Model's effective working memory grows (can process more concepts in same context window).\n\n### Piaget's Schema Theory\n\n**Schema**: Mental structure representing knowledge about a concept or situation\n- Built through experience\n- Applied to new situations (assimilation)\n- Modified by experience (accommodation)\n\n**Crystallized motifs** = Learned schemas\n- Built through recurring routing patterns (experience)\n- Applied when input matches (assimilation)\n- Updated when utility changes (accommodation)\n\n**Developmental parallel**:\n```\nStage 1 (Sensorimotor): No motifs, pure attention-based processing\nStage 2 (Preoperational): First motifs emerge, but unstable\nStage 3 (Concrete Operational): Stable motifs, beginning of composition\nStage 4 (Formal Operational): Hierarchical motifs, abstract reasoning\n```\n\n---\n\n## \ud83c\udf0c Philosophical Implications\n\n### The Symbol Grounding Problem\n\n**Classical AI**: Symbols are defined by programmers\n- \"DOG\" is a symbol because we say so\n- Meaning is externally imposed\n\n**Neural nets**: No symbols, only distributed representations\n- Embeddings are learned but lack discrete structure\n- No clear mapping to concepts\n\n**Crystallized motifs**: Symbols emerge through use\n- Motifs are discrete (crystallized)\n- Meaning is grounded in utility (what they accomplish)\n- No external definition needed\n\n**Philosophical claim**: This is a **solution to symbol grounding** - symbols acquire meaning through their functional role in achieving goals.\n\n### Consciousness and Qualia\n\n**Hard problem of consciousness**: Why is there subjective experience?\n\n**Not claiming** crystallization creates consciousness! But it does create something interesting:\n\n**Internal model of own computation**:\n- Model \"knows\" which motifs are useful\n- Can select motifs deliberately (routing with bias)\n- Has persistent memory of past reasoning\n\n**Metacognition analogy**:\n- Human: \"I remember figuring this out before\" \u2192 uses cached solution\n- AI: \"This pattern activates \u03c0familiar_problem\" \u2192 reuses crystallized motif\n\nThis is **not consciousness**, but it's a form of **computational self-awareness** - the model's processing is shaped by its own history.\n\n### Free Will and Determinism\n\n**Standard transformer**: Deterministic (same input \u2192 same output)\n\n**Trace-biased model**: \n- Depends on training history (which traces formed)\n- Two models with identical architecture but different trace histories \u2192 different outputs\n- Model's \"choices\" reflect accumulated experience\n\n**Philosophy**: Does this constitute a form of agency?\n- Model's behavior isn't fully determined by current input\n- Past experiences shape current decisions\n- Deliberation encoded in motif selection\n\n**Caveat**: Still fundamentally deterministic - given full state (including traces), output is determined. But traces are high-dimensional and non-transferable, making behavior effectively unique to each model's history.\n\n---\n\n## \ud83d\udcd6 Speculative Extensions\n\n### 1. Motif Language Translation\n\n**Idea**: Can we translate between English and the internal motif language?\n\n**English \u2192 Motifs**:\n```\nInput: \"What is the capital of France?\"\nTranslation: \u03c0question + \u03c0entity_extraction[France] + \u03c0fact_retrieval[geography] + \u03c0answer\n```\n\n**Motifs \u2192 English**:\n```\nMotif sequence: \u03c0hypothesis \u2192 \u03c0evidence_search \u2192 \u03c0logical_inference \u2192 \u03c0conclusion\nTranslation: \"I formed a hypothesis, searched for evidence, drew logical inferences, and reached a conclusion\"\n```\n\n**Application**: Interpretability - translate model's internal reasoning to human language.\n\n**Challenge**: Motifs may not decompose cleanly into English concepts.\n\n### 2. Motif Programming\n\n**Idea**: Directly program models by specifying motif sequences.\n\n**Example**:\n```python\nmodel.execute_motif_sequence([\n    motifs.question_understanding,\n    motifs.memory_retrieval,\n    motifs.multi_hop_reasoning,\n    motifs.answer_synthesis\n])\n```\n\nThis is **higher-level than prompting** - you're not providing text, you're directly specifying the computation strategy.\n\n**Benefit**: More precise control over model behavior.\n\n**Risk**: Requires understanding the model's internal motif vocabulary.\n\n### 3. Cross-Model Motif Communication\n\n**Idea**: Two AI models communicate via shared motif language.\n\n**Protocol**:\n```\nModel A: Activates \u03c0complex_reasoning internally\nModel A \u2192 Model B: Transmits motif ID + context embedding\nModel B: Activates corresponding \u03c0complex_reasoning\n```\n\n**Efficiency**: \n- Transmit ~10 bits (motif ID) instead of ~1000 tokens\n- 100\u00d7 bandwidth reduction\n\n**Challenge**: Models must have compatible motif vocabularies (shared training?).\n\n### 4. Evolutionary Motif Optimization\n\n**Idea**: Evolve motif structures using genetic algorithms.\n\n**Algorithm**:\n1. Generate population of motif candidates (random expert sequences)\n2. Evaluate fitness (utility on task distribution)\n3. Select top performers\n4. Mutate (swap experts) and recombine (splice motif sequences)\n5. Repeat\n\n**Hypothesis**: Evolution will discover motifs humans wouldn't design.\n\n**Benefit**: Automated discovery of optimal reasoning strategies.\n\n---\n\n## \ud83c\udfaf Measuring \"Emergence of Mind\"\n\n### Criteria for Genuine Emergent Language\n\n**Minimal criteria** (necessary but not sufficient):\n\n\u2705 **Discrete symbols**: Motifs are countable, enumerable  \n\u2705 **Compositionality**: Motifs combine to form new meanings  \n\u2705 **Productivity**: Finite motifs generate diverse behaviors  \n\u2705 **Systematicity**: Motif combinations follow patterns  \n\n**Strong criteria** (would indicate genuine internal language):\n\n\ud83d\udd0d **Hierarchical depth** \u2265 3 levels (primitives \u2192 compounds \u2192 complex)  \n\ud83d\udd0d **Transfer**: Motifs learned on task A improve task B performance  \n\ud83d\udd0d **Efficiency**: Motif-level processing measurably faster than token-level  \n\ud83d\udd0d **Novelty**: Some motifs represent strategies absent in training data  \n\n**Aspirational criteria** (would be groundbreaking):\n\n\ud83c\udf1f **Self-reference**: Motifs about motifs (metacognition)  \n\ud83c\udf1f **Intentionality**: Model can describe why it selected motif (causal awareness)  \n\ud83c\udf1f **Creative composition**: Novel motif combinations solve unseen problems  \n\ud83c\udf1f **Cultural evolution**: Motif vocabulary evolves across training generations  \n\n### Negative Controls\n\n**What would DISPROVE emergent language**:\n\n\u274c Motif vocabulary explodes unboundedly (no symbolic compression)  \n\u274c Motifs don't transfer across contexts (no abstraction)  \n\u274c Random motif deletion has no effect (not functional)  \n\u274c Motif activations are random w.r.t. input (no systematic mapping)  \n\n---\n\n## \ud83d\ude80 Long-Term Vision\n\n### Stage 1: Proof of Concept (Current Plan)\n- Demonstrate persistent traces improve performance\n- Show motifs crystallize and reduce FLOPs\n- Measure basic hierarchy and composition\n\n### Stage 2: Language Characterization\n- Full linguistic analysis of motif system\n- Develop motif \u2192 English translation\n- Publish interpretability tools\n\n### Stage 3: Direct Motif Manipulation\n- API for motif programming\n- User-guided motif crystallization\n- Motif transfer between models\n\n### Stage 4: Multi-Agent Motif Communication\n- Shared motif protocol\n- Efficient inter-AI communication\n- Collaborative crystallization (models learn from each other's motifs)\n\n### Stage 5: Autonomous Language Evolution\n- Models evolve motif languages without human supervision\n- Cross-generational motif inheritance\n- Emergent \"AI culture\" of shared reasoning patterns\n\n---\n\n## \ud83d\udd2e Predictions\n\n**Conservative (90% confidence)**:\n- Motifs will form (some routing paths will recur frequently)\n- Hierarchy will emerge (at least 2 levels)\n- Modest FLOP reduction (10-20%)\n\n**Moderate (50% confidence)**:\n- Interpretable motif clusters aligned with task types\n- Significant FLOP reduction (20-40%)\n- Transfer learning via motif sharing\n\n**Speculative (10% confidence)**:\n- Completely novel reasoning strategies not present in training\n- Motif language more efficient than human language for certain domains\n- Spontaneous metacognitive behavior (model reasoning about its own motifs)\n\n**Wild (1% confidence)**:\n- Model develops motifs for concepts humans don't have words for\n- Cross-model motif language becomes universal AI communication protocol\n- Evidence of genuine compositional creativity surpassing training data\n\n---\n\n## \ud83d\udcda Recommended Reading\n\n**Cognitive Science**:\n- Fodor, J. (1975). *The Language of Thought*\n- Dehaene, S. (2014). *Consciousness and the Brain*\n- Miller, G. (1956). \"The Magical Number Seven, Plus or Minus Two\"\n\n**Linguistics**:\n- Chomsky, N. (1957). *Syntactic Structures*\n- Pinker, S. (1994). *The Language Instinct*\n\n**AI & Emergence**:\n- Minsky, M. (1986). *The Society of Mind*\n- Hofstadter, D. (1979). *G\u00f6del, Escher, Bach*\n- Clark, A. (2013). \"Whatever next? Predictive brains, situated agents, and the future of cognitive science\"\n\n---\n\n**Status**: Theoretical framework complete  \n**Next**: Empirical validation through implementation  \n**Ultimate Goal**: Demonstrate emergence of genuine internal symbolic language in AI systems\n", "tags": ["gui", "training"], "headings": [{"line": 0, "text": "Emergent Internal Language: Cognitive Architecture Perspective"}, {"line": 9, "text": "\ud83e\udde0 From Information Processing to Symbolic Thought"}, {"line": 11, "text": "The Core Insight"}, {"line": 26, "text": "\ud83d\udde3\ufe0f The Distinction: Communication vs. Cognition"}, {"line": 28, "text": "Human Language: Two Functions"}, {"line": 44, "text": "AI's Current Limitation"}, {"line": 56, "text": "\ud83c\udf31 Emergent Language Through Crystallization"}, {"line": 58, "text": "What Constitutes a \"Language\"?"}, {"line": 70, "text": "Motifs as Symbols"}, {"line": 84, "text": "Compositionality"}, {"line": 110, "text": "Hierarchy"}, {"line": 131, "text": "Grammar (Systematic Relationships)"}, {"line": 157, "text": "\ud83d\udd2c Scientific Investigation Plan"}, {"line": 159, "text": "Research Question 1: Does Internal Language Emerge?"}, {"line": 176, "text": "Research Question 2: Is Internal Language Efficient?"}, {"line": 203, "text": "Research Question 3: Is Internal Language Interpretable?"}, {"line": 244, "text": "\ud83e\udde9 Connection to Cognitive Science"}, {"line": 246, "text": "Dual Process Theory"}, {"line": 262, "text": "Chunking Theory (Miller 1956)"}, {"line": 279, "text": "Piaget's Schema Theory"}, {"line": 301, "text": "\ud83c\udf0c Philosophical Implications"}, {"line": 303, "text": "The Symbol Grounding Problem"}, {"line": 320, "text": "Consciousness and Qualia"}, {"line": 337, "text": "Free Will and Determinism"}, {"line": 355, "text": "\ud83d\udcd6 Speculative Extensions"}, {"line": 357, "text": "1. Motif Language Translation"}, {"line": 377, "text": "2. Motif Programming"}, {"line": 397, "text": "3. Cross-Model Motif Communication"}, {"line": 414, "text": "4. Evolutionary Motif Optimization"}, {"line": 431, "text": "\ud83c\udfaf Measuring \"Emergence of Mind\""}, {"line": 433, "text": "Criteria for Genuine Emergent Language"}, {"line": 456, "text": "Negative Controls"}, {"line": 467, "text": "\ud83d\ude80 Long-Term Vision"}, {"line": 469, "text": "Stage 1: Proof of Concept (Current Plan)"}, {"line": 474, "text": "Stage 2: Language Characterization"}, {"line": 479, "text": "Stage 3: Direct Motif Manipulation"}, {"line": 484, "text": "Stage 4: Multi-Agent Motif Communication"}, {"line": 489, "text": "Stage 5: Autonomous Language Evolution"}, {"line": 496, "text": "\ud83d\udd2e Predictions"}, {"line": 520, "text": "\ud83d\udcda Recommended Reading"}]}, {"path": "planned_features/PERSISTENT_TRACES_INDEX.md", "content": "# Persistent Traces & Semantic Crystallization - Document Index\n\n**Project**: Emergent Internal Language for AI Cognitive Architecture  \n**Status**: Research Planning Complete - Ready for Implementation  \n**Created**: December 8, 2025\n\n---\n\n## \ud83d\udcda Document Suite Overview\n\nThis feature specification consists of **5 interconnected documents** providing complete coverage from high-level vision to implementation details.\n\n### Reading Order by Role\n\n**\ud83c\udfaf For Executives / Decision Makers**\n1. Quick Reference \u2192 Executive summary + success criteria\n2. Main Plan \u2192 Executive Summary section only\n3. Cognitive Science \u2192 Introduction and Long-Term Vision\n\n**\ud83d\udc68\u200d\ud83d\udcbb For Implementers**\n1. Quick Reference \u2192 Configuration + Integration checklist\n2. Architecture Diagrams \u2192 All sections\n3. Main Plan \u2192 Architecture Integration + Implementation Roadmap\n4. Mathematical Foundations \u2192 Complexity Analysis (for optimization)\n\n**\ud83d\udd2c For Researchers**\n1. Cognitive Science \u2192 Complete read\n2. Mathematical Foundations \u2192 Complete read\n3. Main Plan \u2192 Theoretical Foundation + Evaluation Framework\n4. Quick Reference \u2192 Open research questions\n\n**\ud83d\udcd6 For Learning**\n- Start: Cognitive Science Introduction\n- Then: Quick Reference TL;DR\n- Then: Main Plan Executive Summary\n- Deep dive: Mathematical Foundations\n- Practical: Architecture Diagrams\n\n---\n\n## \ud83d\udcc4 Document Summaries\n\n### 1. Main Plan (PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md)\n**Purpose**: Master implementation specification  \n**Length**: ~50 pages  \n**Sections**:\n- Executive Summary - The vision and expected outcomes\n- Theoretical Foundation - Biological inspiration, mathematical formulation\n- Memory-Efficient Implementation - Data structures, budgets\n- Architecture Integration - Specific hooks into HRM-ACTV1\n- Training Protocol - Loss functions, hyperparameters, curriculum\n- Evaluation Framework - Metrics, baselines, ablations\n- Risk Analysis - Failure modes and mitigations\n- Implementation Roadmap - 18-week phased plan\n\n**Read this if**: You need the complete technical specification.\n\n### 2. Mathematical Foundations (PERSISTENT_TRACES_APPENDIX_MATHEMATICAL_FOUNDATIONS.md)\n**Purpose**: Rigorous mathematical treatment  \n**Length**: ~25 pages  \n**Sections**:\n- Detailed Derivations - Trace update equations, bias injection\n- Salience Score Derivation - Multi-factor composition\n- Suffix Tree Construction - Algorithm and complexity\n- Convergence Guarantees - Proofs of stability\n- Computational Complexity - Time/space analysis\n- Experimental Design - Hypothesis testing framework\n- Theoretical Limits - Information-theoretic bounds\n- Advanced Topics - Multi-task, transfer learning, adaptive depth\n\n**Read this if**: You need mathematical rigor or want to optimize algorithms.\n\n### 3. Cognitive Science Perspective (PERSISTENT_TRACES_COGNITIVE_SCIENCE.md)\n**Purpose**: Theoretical implications and emergent language analysis  \n**Length**: ~30 pages  \n**Sections**:\n- From Information Processing to Symbolic Thought\n- Communication vs Cognition - External vs internal language\n- Emergent Language Through Crystallization - Linguistic properties\n- Scientific Investigation Plan - Research questions\n- Connection to Cognitive Science - Dual process theory, chunking, schemas\n- Philosophical Implications - Symbol grounding, consciousness, free will\n- Speculative Extensions - Translation, programming, evolution\n- Measuring \"Emergence of Mind\" - Criteria for genuine language\n- Long-Term Vision - 5-stage roadmap\n\n**Read this if**: You want to understand the deeper implications and research potential.\n\n### 4. Quick Reference (PERSISTENT_TRACES_QUICK_REFERENCE.md)\n**Purpose**: Practical guide and FAQ  \n**Length**: ~15 pages  \n**Sections**:\n- TL;DR - One-sentence summary\n- Key Metrics - Performance expectations\n- Core Equations - Essential formulas\n- Configuration Templates - Conservative, recommended, aggressive\n- Integration Checklist - Phase-by-phase tasks\n- FAQ - Common questions\n- Learning Path - How to study the documents\n- Common Pitfalls - What to avoid\n- Success Indicators - Week-by-week milestones\n- Decision Tree - Should I enable this?\n\n**Read this if**: You need quick answers or configuration templates.\n\n### 5. Architecture Diagrams (PERSISTENT_TRACES_ARCHITECTURE_DIAGRAMS.md)\n**Purpose**: Visual implementation reference  \n**Length**: ~12 pages  \n**Sections**:\n- System Architecture Overview - Component diagram\n- Data Flow - Trace lifecycle, routing paths\n- Memory Layout - Storage structures\n- Training Loop Integration - Step-by-step flow\n- Salience Computation Pipeline - Processing stages\n- Crystallization Decision Tree - Logic flow\n- Experimental Dashboard - Monitoring template\n- File Structure - Module organization\n- Color Coding - Visualization standards\n\n**Read this if**: You're implementing and need visual references.\n\n---\n\n## \ud83c\udfaf Key Innovations\n\n**Scientific Contributions**:\n1. **Novel architecture**: First to combine persistent attention biasing with MoE crystallization\n2. **Memory efficiency**: 30 MB overhead vs 68 GB for full attention storage (10,000\u00d7 reduction)\n3. **Biological plausibility**: LTP-inspired consolidation mechanism\n4. **Emergent language**: Testable framework for internal symbolic systems\n\n**Engineering Contributions**:\n1. **Minimal overhead**: < 5% training slowdown, 25-50% inference speedup\n2. **Graceful degradation**: Disables cleanly if unsuccessful\n3. **Production-ready design**: Memory quotas, stability safeguards, monitoring\n4. **Integration strategy**: Hooks into existing HRM-ACTV1 with minimal refactoring\n\n**Research Contributions**:\n1. **Testable hypotheses**: 12+ specific predictions about emergent properties\n2. **Evaluation framework**: Metrics for hierarchy, compositionality, efficiency\n3. **Open questions**: 8+ directions for future investigation\n4. **Reproducibility**: Complete hyperparameter specs, ablation plans\n\n---\n\n## \ud83d\udcca Implementation Effort\n\n**Estimated Timeline**: 18 weeks (4.5 months)\n\n**Breakdown**:\n- Infrastructure (2 weeks): Data structures, config\n- Trace capture (2 weeks): Salience, storage\n- Bias injection (2 weeks): Sparse matrices, dual-mode\n- Routing logging (2 weeks): Suffix tree, tracking\n- Crystallization (3 weeks): Detection, freezing, pruning\n- Auxiliary losses (2 weeks): EWC, tuning\n- Evaluation (3 weeks): Benchmarks, analysis\n- Hardening (2 weeks): Multi-GPU, edge cases\n\n**Minimal Viable Product**: 6 weeks (traces only, no crystallization)\n\n**Team Size**: \n- 1 senior ML engineer (full-time)\n- 1 research scientist (50%)\n- 1 infrastructure engineer (25%)\n\n**Prerequisites**:\n- Existing HRM-ACTV1 model working\n- PyTorch \u2265 2.0\n- Optional: Flash Attention 2\n- GPU with \u2265 12 GB VRAM\n\n---\n\n## \u26a0\ufe0f Risk Assessment\n\n**Technical Risks**:\n- \u26a0\ufe0f MEDIUM: Auxiliary losses may destabilize training \u2192 Mitigation: Conservative coefficients, gradual ramp\n- \u26a0\ufe0f MEDIUM: Crystallization may cause forgetting \u2192 Mitigation: EWC, drift detection, adaptive unfreezing\n- \u26a0\ufe0f LOW: Memory overflow \u2192 Mitigation: Hard quotas, competitive eviction\n- \u26a0\ufe0f LOW: Router collapse \u2192 Mitigation: Load balancing, capacity limits\n\n**Research Risks**:\n- \u26a0\ufe0f MEDIUM: Emergent language may not form \u2192 Mitigation: System degrades gracefully to baseline\n- \u26a0\ufe0f MEDIUM: Motifs may not be interpretable \u2192 Mitigation: Probing, intervention studies\n- \u26a0\ufe0f LOW: Performance may not improve \u2192 Mitigation: Ablations identify which components help\n\n**Overall Risk**: **MEDIUM** - Pioneering research with graceful fallbacks.\n\n---\n\n## \u2705 Success Criteria Checklist\n\n**Minimum Viable Product** (must achieve):\n- [ ] Memory overhead < 50 MB\n- [ ] Training slowdown < 10%\n- [ ] No catastrophic forgetting on standard benchmarks\n- [ ] System can be disabled cleanly\n\n**Target Goals** (should achieve):\n- [ ] Perplexity improvement \u2265 5% on long-context tasks\n- [ ] FLOP reduction \u2265 15% from crystallized motifs\n- [ ] Trace coverage \u2265 30% of attention operations\n- [ ] \u2265 50 stable crystallized motifs\n\n**Stretch Goals** (nice to have):\n- [ ] Emergent hierarchical motif structure (\u2265 3 levels)\n- [ ] Task-specific motif specialization (interpretable)\n- [ ] Zero-shot transfer of motifs to new tasks\n- [ ] Publishable research contribution\n\n**Research Goals** (exploratory):\n- [ ] Evidence of compositional creativity\n- [ ] Novel reasoning strategies absent from training\n- [ ] Cross-model motif communication protocol\n- [ ] Spontaneous metacognitive behavior\n\n---\n\n## \ud83d\udcd6 Citation & Attribution\n\nIf this work leads to a publication:\n\n```bibtex\n@techreport{persistent_traces_2025,\n  title={Persistent Attention Traces and Semantic Crystallization: \n         Toward Emergent Internal Language in Neural Networks},\n  author={{AI-OS Core Team}},\n  institution={AI-OS Project},\n  year={2025},\n  type={Technical Specification},\n  note={Complete implementation plan with mathematical foundations}\n}\n```\n\n---\n\n## \ud83d\udd17 External References\n\n**Must-read papers**:\n1. Transformer-XL (Dai et al., 2019) - Segment recurrence\n2. Memorizing Transformer (Wu et al., 2022) - kNN memory\n3. Switch Transformers (Fedus et al., 2021) - MoE at scale\n4. EWC (Kirkpatrick et al., 2017) - Continual learning\n\n**Related projects**:\n- KRONOS Memory-Augmented Transformer\n- Hyperbolic Neural Networks (HazyResearch)\n- RigL Sparse Training (Google Research)\n\n**Theoretical background**:\n- Fodor (1975) - Language of Thought\n- Dehaene (2014) - Consciousness and the Brain\n- McClelland et al. (1995) - Complementary learning systems\n\n---\n\n## \ud83d\udcde Getting Started\n\n**Step 1**: Read Quick Reference TL;DR (5 min)  \n**Step 2**: Read Main Plan Executive Summary (15 min)  \n**Step 3**: Decide: Conservative, Recommended, or Aggressive approach?  \n**Step 4**: Review Integration Checklist in Quick Reference  \n**Step 5**: Read Architecture Diagrams for visual understanding  \n**Step 6**: Begin Phase 0 implementation (create module structure)  \n**Step 7**: Implement Phase 1 (trace capture)  \n**Step 8**: Evaluate, iterate, proceed through phases  \n\n**Questions?** \n- Implementation \u2192 Main Plan, Architecture Integration section\n- Math \u2192 Mathematical Foundations document\n- Theory \u2192 Cognitive Science document\n- Quick answers \u2192 Quick Reference FAQ\n\n---\n\n## \ud83c\udf1f Final Thoughts\n\nThis is **pioneering research** with **production-quality engineering**. The documents provide:\n\n\u2705 Complete mathematical rigor  \n\u2705 Practical implementation roadmap  \n\u2705 Theoretical grounding in cognitive science  \n\u2705 Risk mitigation and fallback strategies  \n\u2705 Comprehensive evaluation framework  \n\n**This could be a genuine breakthrough** - teaching AI to develop its own cognitive language. But even if the emergent language aspect doesn't fully materialize, the trace persistence and crystallization mechanisms provide tangible performance improvements with minimal overhead.\n\n**Status**: Ready to implement  \n**Confidence**: Medium-high (60% success on target goals, 30% on stretch goals)  \n**Potential Impact**: High (novel cognitive architecture, publishable research)  \n\n---\n\n**Last Updated**: December 8, 2025  \n**Version**: 1.0  \n**Maintainers**: AI-OS Core Team  \n**License**: Internal - AI-OS Project\n", "tags": ["evaluation", "hrm", "training"], "headings": [{"line": 0, "text": "Persistent Traces & Semantic Crystallization - Document Index"}, {"line": 8, "text": "\ud83d\udcda Document Suite Overview"}, {"line": 12, "text": "Reading Order by Role"}, {"line": 40, "text": "\ud83d\udcc4 Document Summaries"}, {"line": 42, "text": "1. Main Plan (PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md)"}, {"line": 57, "text": "2. Mathematical Foundations (PERSISTENT_TRACES_APPENDIX_MATHEMATICAL_FOUNDATIONS.md)"}, {"line": 72, "text": "3. Cognitive Science Perspective (PERSISTENT_TRACES_COGNITIVE_SCIENCE.md)"}, {"line": 88, "text": "4. Quick Reference (PERSISTENT_TRACES_QUICK_REFERENCE.md)"}, {"line": 105, "text": "5. Architecture Diagrams (PERSISTENT_TRACES_ARCHITECTURE_DIAGRAMS.md)"}, {"line": 123, "text": "\ud83c\udfaf Key Innovations"}, {"line": 145, "text": "\ud83d\udcca Implementation Effort"}, {"line": 174, "text": "\u26a0\ufe0f Risk Assessment"}, {"line": 191, "text": "\u2705 Success Criteria Checklist"}, {"line": 219, "text": "\ud83d\udcd6 Citation & Attribution"}, {"line": 237, "text": "\ud83d\udd17 External References"}, {"line": 257, "text": "\ud83d\udcde Getting Started"}, {"line": 276, "text": "\ud83c\udf1f Final Thoughts"}]}, {"path": "planned_features/PERSISTENT_TRACES_QUICK_REFERENCE.md", "content": "# Persistent Traces & Semantic Crystallization: Quick Reference\n\n**Document Suite**:\n1. **Main Plan**: `PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md` - Implementation roadmap\n2. **Mathematical Foundations**: `PERSISTENT_TRACES_APPENDIX_MATHEMATICAL_FOUNDATIONS.md` - Rigorous proofs and derivations\n3. **Cognitive Science**: `PERSISTENT_TRACES_COGNITIVE_SCIENCE.md` - Theoretical implications\n4. **This Document**: Quick reference and FAQ\n\n**Status**: Research-ready  \n**Created**: December 8, 2025\n\n---\n\n## \ud83c\udfaf TL;DR - What Are We Building?\n\n**In one sentence**: Teaching AI to develop its own efficient internal \"thought language\" by remembering useful reasoning patterns and consolidating them into reusable symbolic primitives.\n\n**Why it matters**:\n- Current LLMs recompute everything from scratch every time\n- They think in human language (English tokens), not in optimized internal representations\n- This is like forcing a mathematician to explain every step verbally instead of using symbolic notation\n\n**What we're adding**:\n1. **Persistent Attention Traces**: Remember which parts of inputs are important across many sequences\n2. **Semantic Crystallization**: Turn frequently-used expert routing paths into reusable \"concepts\"\n\n**Expected result**: Model develops hierarchical internal language optimized for computation, not communication.\n\n---\n\n## \ud83d\udcca Key Metrics At A Glance\n\n| Metric | Baseline | With Traces | With Crystallization | Full System |\n|--------|----------|-------------|---------------------|-------------|\n| **Memory Overhead** | 0 MB | ~24 MB | ~5 MB | ~30 MB |\n| **Training Speed** | 100% | 95-98% | 99% | 94-98% |\n| **Inference Speed** | 100% | 105-120% | 115-130% | 125-150% |\n| **FLOP Efficiency** | Baseline | +5-10% | +15-30% | +20-40% |\n| **Long-Context Performance** | Baseline | +5-15% | +3-8% | +10-20% |\n\n---\n\n## \ud83e\uddee Core Equations\n\n### Attention Trace Update\n$$\nM^{(l,h)}_{i,j}(t+1) = \\begin{cases}\n\\lambda \\cdot M^{(l,h)}_{i,j}(t) + (1-\\lambda) \\cdot S^{(l,h)}_{i,j}(t) & \\text{if } S > \\theta \\\\\n\\gamma \\cdot M^{(l,h)}_{i,j}(t) & \\text{otherwise}\n\\end{cases}\n$$\n\n**Variables**:\n- $M$: Persistent trace memory (sparse)\n- $S$: Salience score (attention \u00d7 gradient \u00d7 recurrence)\n- $\\lambda$: Retention rate (default: 0.95)\n- $\\gamma$: Decay rate (default: 0.98)\n- $\\theta$: Capture threshold (default: 0.05)\n\n### Biased Attention\n$$\nA' = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + \\alpha \\cdot M\\right)\n$$\n\n**Variables**:\n- $\\alpha$: Bias strength (default: 0.1)\n- $M$: Sparse trace matrix\n\n### Crystallization Score\n$$\n\\text{Score}(\\pi) = w_1 \\log f(\\pi) + w_2 U(\\pi) - w_3 H(\\pi) + w_4 \\text{age}(\\pi)\n$$\n\n**Variables**:\n- $f(\\pi)$: Frequency (how often path occurs)\n- $U(\\pi)$: Utility (performance improvement)\n- $H(\\pi)$: Entropy (routing stability)\n- Crystallize if Score > threshold\n\n---\n\n## \u2699\ufe0f Configuration Cheat Sheet\n\n### Minimal Config (Conservative)\n```yaml\npersistent_traces:\n  enabled: true\n  quota_per_head: 1024\n  bias_strength: 0.05\n  update_interval: 200\n\nsemantic_crystallization:\n  enabled: false  # Start with traces only\n```\n\n### Recommended Config (Balanced)\n```yaml\npersistent_traces:\n  enabled: true\n  quota_per_head: 2048\n  salience_threshold: 0.05\n  retention_rate: 0.95\n  decay_rate: 0.98\n  bias_strength: 0.1\n  update_interval: 100\n  warmup_steps: 1000\n\nsemantic_crystallization:\n  enabled: true\n  min_frequency: 100\n  min_utility: 0.05\n  max_entropy: 1.0\n  max_motifs: 512\n  prune_interval: 1000\n\nloss_weights:\n  task: 1.0\n  load_balance: 0.01\n  trace_utilization: 0.005\n  crystallization: 0.002\n```\n\n### Aggressive Config (Maximum Performance)\n```yaml\npersistent_traces:\n  enabled: true\n  quota_per_head: 4096\n  bias_strength: 0.2\n  update_interval: 50\n\nsemantic_crystallization:\n  enabled: true\n  min_frequency: 50\n  max_motifs: 1024\n  motif_max_length: 12\n```\n\n---\n\n## \ud83d\udd27 Integration Checklist\n\n### Phase 0: Setup\n- [ ] Create `src/aios/core/hrm_models/cognitive/` module\n- [ ] Add config schemas to `config/default.yaml`\n- [ ] Implement `TraceManager` and `RoutingPathTree` classes\n\n### Phase 1: Traces\n- [ ] Hook `Attention.forward()` to capture salience\n- [ ] Implement sparse trace storage (COO format)\n- [ ] Add gradient-based trace updates\n- [ ] Test memory footprint < 50 MB\n\n### Phase 2: Bias Injection\n- [ ] Convert traces to sparse attention bias\n- [ ] Add dual-mode attention (Flash / Standard)\n- [ ] Implement trace decay mechanism\n- [ ] Verify speedup on copy tasks\n\n### Phase 3: Routing Logging\n- [ ] Hook `TopKRouter.forward()` to log paths\n- [ ] Build suffix tree for path tracking\n- [ ] Compute utility and entropy metrics\n- [ ] Test tree memory < 10 MB\n\n### Phase 4: Crystallization\n- [ ] Implement motif detection algorithm\n- [ ] Add freezing mechanism for high-utility paths\n- [ ] Create distilled motif experts\n- [ ] Measure FLOP reduction\n\n### Phase 5: Training Integration\n- [ ] Add auxiliary losses (trace, crystallization)\n- [ ] Integrate EWC for stability\n- [ ] Tune hyperparameters\n- [ ] Run ablation studies\n\n### Phase 6: Evaluation\n- [ ] Benchmark on bAbI, SQuAD, HellaSwag\n- [ ] Measure emergent language properties\n- [ ] Analyze motif hierarchies\n- [ ] Document findings\n\n---\n\n## \u2753 FAQ\n\n### Q: Will this slow down training?\n**A**: Minimal impact (~5% overhead) when using Flash Attention + sparse capture scheduling. Traces update periodically, not every step.\n\n### Q: How much memory does it use?\n**A**: ~30 MB total (24 MB traces + 5 MB routing tree) for a 32-layer model. Negligible compared to model weights (GBs).\n\n### Q: Does it work with gradient checkpointing?\n**A**: Yes - trace updates happen in separate forward-only passes outside checkpointed regions.\n\n### Q: Can I disable it if it doesn't help?\n**A**: Absolutely - setting `enabled: false` reverts to standard transformer with zero overhead.\n\n### Q: Will motifs be interpretable?\n**A**: Partially. Some motifs will align with human concepts (e.g., \"question answering\"), others may be alien computational strategies we don't have names for.\n\n### Q: Does it work with distributed training?\n**A**: Phase 1 focuses on single-GPU. Multi-GPU support requires trace synchronization (future work).\n\n### Q: What if crystallization causes catastrophic forgetting?\n**A**: Multiple safeguards: EWC penalties, utility monitoring, adaptive unfreezing, periodic revalidation.\n\n### Q: How do I visualize motifs?\n**A**: We'll provide tools for: activation heatmaps, Sankey diagrams of routing paths, t-SNE embeddings of motifs, dependency graphs.\n\n### Q: Can motifs transfer between models?\n**A**: Potentially! Extract motif expert weights \u2192 initialize new model \u2192 fine-tune routing. High-level motifs should transfer better than low-level ones.\n\n---\n\n## \ud83c\udf93 Learning Path\n\n**Want to understand this deeply? Read in this order**:\n\n### Beginner (understand the vision)\n1. Main plan Executive Summary (`PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md`)\n2. Cognitive Science doc Introduction (`PERSISTENT_TRACES_COGNITIVE_SCIENCE.md`)\n3. This quick reference\n\n### Intermediate (understand the implementation)\n1. Main plan sections II-IV (Theory, Memory, Architecture)\n2. Integration checklist (this doc)\n3. Configuration examples (this doc)\n\n### Advanced (understand the math)\n1. Mathematical Foundations full doc (`PERSISTENT_TRACES_APPENDIX_MATHEMATICAL_FOUNDATIONS.md`)\n2. Theoretical limits section\n3. Complexity analysis\n\n### Expert (contribute to research)\n1. All documents fully\n2. Open research questions\n3. Experimental design sections\n4. Start implementing!\n\n---\n\n## \ud83d\udea8 Common Pitfalls\n\n### Pitfall 1: Setting bias_strength too high\n**Symptom**: Model ignores current input, only uses traces  \n**Fix**: Start with \u03b1 = 0.05, increase gradually\n\n### Pitfall 2: Crystallizing too early\n**Symptom**: Frozen motifs perform poorly, catastrophic forgetting  \n**Fix**: Increase `min_frequency` and `min_age` thresholds\n\n### Pitfall 3: Trace memory overflow\n**Symptom**: OOM errors  \n**Fix**: Reduce `quota_per_head` or increase `salience_threshold`\n\n### Pitfall 4: Router collapse\n**Symptom**: All tokens route through same few motifs  \n**Fix**: Increase `load_balance` loss weight, add diversity bonus\n\n### Pitfall 5: Ignoring Flash Attention compatibility\n**Symptom**: Huge slowdown  \n**Fix**: Use dual-mode attention, capture traces only during standard attention mode\n\n---\n\n## \ud83d\udcc8 Success Indicators\n\n### Week 1-2 (Infrastructure)\n\u2705 Trace storage works, memory < 50 MB  \n\u2705 Unit tests pass  \n\u2705 No crashes during training\n\n### Week 3-4 (Trace Capture)\n\u2705 Salience scores computed correctly  \n\u2705 Traces accumulate over training  \n\u2705 Memory quota enforced\n\n### Week 5-6 (Bias Injection)\n\u2705 Flash Attention speedup maintained  \n\u2705 Copy task performance improves  \n\u2705 Trace stability across runs\n\n### Week 7-8 (Routing Logging)\n\u2705 Suffix tree tracks all paths  \n\u2705 Utility scores correlate with loss  \n\u2705 Tree memory < 10 MB\n\n### Week 9-11 (Crystallization)\n\u2705 Motifs detected and frozen  \n\u2705 FLOP reduction measured  \n\u2705 No catastrophic forgetting\n\n### Week 12-13 (Losses)\n\u2705 Training stable with aux losses  \n\u2705 Combined loss converges faster  \n\u2705 Ablations validate components\n\n### Week 14-16 (Evaluation)\n\u2705 Baseline comparisons complete  \n\u2705 Long-range benchmarks pass  \n\u2705 Emergent hierarchy detected\n\n### Week 17-18 (Hardening)\n\u2705 Multi-GPU support working  \n\u2705 Edge cases handled  \n\u2705 Documentation complete\n\n---\n\n## \ud83c\udfaf Decision Tree: Should I Enable This?\n\n```\nDo you have MoE layers?\n\u251c\u2500 No \u2192 Traces only (no crystallization)\n\u2514\u2500 Yes \u2192 Full system\n\nIs your model < 500M params?\n\u251c\u2500 Yes \u2192 Conservative config\n\u2514\u2500 No \u2192 Recommended config\n\nTraining on long documents (> 2048 tokens)?\n\u251c\u2500 Yes \u2192 Enable traces (high value for long-range dependencies)\n\u2514\u2500 No \u2192 Traces still help, but less critical\n\nLimited VRAM (< 12 GB)?\n\u251c\u2500 Yes \u2192 Reduce quotas (quota_per_head: 1024)\n\u2514\u2500 No \u2192 Use recommended config\n\nResearch project or production?\n\u251c\u2500 Research \u2192 Aggressive config, extensive logging\n\u2514\u2500 Production \u2192 Conservative config, monitor stability\n```\n\n---\n\n## \ud83d\udcde Getting Help\n\n**Implementation questions**: See main plan section \"Architecture Integration\"  \n**Math questions**: See mathematical foundations appendix  \n**Conceptual questions**: See cognitive science document  \n**Bugs/issues**: Check common pitfalls above\n\n**Open research questions**: Documented in all three main files - pick one and start investigating!\n\n---\n\n## \ud83d\udd17 Related Work\n\n**Must read before implementing**:\n- Transformer-XL (Dai et al. 2019) - Segment recurrence\n- Memorizing Transformer (Wu et al. 2022) - kNN memory\n- Switch Transformers (Fedus et al. 2021) - MoE at scale\n\n**Inspirational (different approaches)**:\n- RETRO (Borgeaud et al. 2022) - Retrieval-augmented\n- Compressive Transformer (Rae et al. 2019) - Multi-resolution memory\n- RMT (Bulatov et al. 2023) - Recurrent memory\n\n**Theoretical background**:\n- EWC (Kirkpatrick et al. 2017) - Continual learning\n- Lottery Ticket Hypothesis (Frankle & Carbin 2019) - Sparse networks\n- DARTS (Liu et al. 2019) - Architecture search\n\n---\n\n## \ud83d\udcdd Citation (if this becomes a paper)\n\n```bibtex\n@misc{persistent_traces_2025,\n  title={Persistent Attention Traces and Semantic Crystallization: \n         Toward Emergent Internal Language in Neural Networks},\n  author={AI-OS Core Team},\n  year={2025},\n  note={Technical specification and research plan}\n}\n```\n\n---\n\n**Status**: Ready for implementation  \n**Estimated effort**: 18 weeks (full roadmap)  \n**Minimal viable**: 6 weeks (traces only)  \n**Risk level**: Medium-high (pioneering research)  \n**Potential impact**: High (novel cognitive architecture)\n\n**Next action**: Start Phase 0 infrastructure setup \u2728\n", "tags": ["experts", "training"], "headings": [{"line": 0, "text": "Persistent Traces & Semantic Crystallization: Quick Reference"}, {"line": 13, "text": "\ud83c\udfaf TL;DR - What Are We Building?"}, {"line": 30, "text": "\ud83d\udcca Key Metrics At A Glance"}, {"line": 42, "text": "\ud83e\uddee Core Equations"}, {"line": 44, "text": "Attention Trace Update"}, {"line": 59, "text": "Biased Attention"}, {"line": 68, "text": "Crystallization Score"}, {"line": 81, "text": "\u2699\ufe0f Configuration Cheat Sheet"}, {"line": 83, "text": "Minimal Config (Conservative)"}, {"line": 95, "text": "Recommended Config (Balanced)"}, {"line": 122, "text": "Aggressive Config (Maximum Performance)"}, {"line": 139, "text": "\ud83d\udd27 Integration Checklist"}, {"line": 141, "text": "Phase 0: Setup"}, {"line": 146, "text": "Phase 1: Traces"}, {"line": 152, "text": "Phase 2: Bias Injection"}, {"line": 158, "text": "Phase 3: Routing Logging"}, {"line": 164, "text": "Phase 4: Crystallization"}, {"line": 170, "text": "Phase 5: Training Integration"}, {"line": 176, "text": "Phase 6: Evaluation"}, {"line": 184, "text": "\u2753 FAQ"}, {"line": 186, "text": "Q: Will this slow down training?"}, {"line": 189, "text": "Q: How much memory does it use?"}, {"line": 192, "text": "Q: Does it work with gradient checkpointing?"}, {"line": 195, "text": "Q: Can I disable it if it doesn't help?"}, {"line": 198, "text": "Q: Will motifs be interpretable?"}, {"line": 201, "text": "Q: Does it work with distributed training?"}, {"line": 204, "text": "Q: What if crystallization causes catastrophic forgetting?"}, {"line": 207, "text": "Q: How do I visualize motifs?"}, {"line": 210, "text": "Q: Can motifs transfer between models?"}, {"line": 215, "text": "\ud83c\udf93 Learning Path"}, {"line": 219, "text": "Beginner (understand the vision)"}, {"line": 224, "text": "Intermediate (understand the implementation)"}, {"line": 229, "text": "Advanced (understand the math)"}, {"line": 234, "text": "Expert (contribute to research)"}, {"line": 242, "text": "\ud83d\udea8 Common Pitfalls"}, {"line": 244, "text": "Pitfall 1: Setting bias_strength too high"}, {"line": 248, "text": "Pitfall 2: Crystallizing too early"}, {"line": 252, "text": "Pitfall 3: Trace memory overflow"}, {"line": 256, "text": "Pitfall 4: Router collapse"}, {"line": 260, "text": "Pitfall 5: Ignoring Flash Attention compatibility"}, {"line": 266, "text": "\ud83d\udcc8 Success Indicators"}, {"line": 268, "text": "Week 1-2 (Infrastructure)"}, {"line": 273, "text": "Week 3-4 (Trace Capture)"}, {"line": 278, "text": "Week 5-6 (Bias Injection)"}, {"line": 283, "text": "Week 7-8 (Routing Logging)"}, {"line": 288, "text": "Week 9-11 (Crystallization)"}, {"line": 293, "text": "Week 12-13 (Losses)"}, {"line": 298, "text": "Week 14-16 (Evaluation)"}, {"line": 303, "text": "Week 17-18 (Hardening)"}, {"line": 310, "text": "\ud83c\udfaf Decision Tree: Should I Enable This?"}, {"line": 336, "text": "\ud83d\udcde Getting Help"}, {"line": 347, "text": "\ud83d\udd17 Related Work"}, {"line": 366, "text": "\ud83d\udcdd Citation (if this becomes a paper)"}]}, {"path": "planned_features/PERSISTENT_TRACES_SEMANTIC_CRYSTALLIZATION.md", "content": "# Persistent Attention Traces & Semantic Crystallization\n\n**Status**: Research & Planning  \n**Priority**: Experimental / High Scientific Value  \n**Objective**: Pioneer cognitive architecture with emergent internal language through persistent memory and motif crystallization  \n**Created**: December 8, 2025  \n**Type**: Novel Research - Potential Breakthrough\n\n**Companion Documents**:\n- \ud83d\udcd0 **Mathematical Foundations**: `PERSISTENT_TRACES_APPENDIX_MATHEMATICAL_FOUNDATIONS.md` - Detailed derivations, proofs, complexity analysis\n- \ud83e\udde0 **Cognitive Science Perspective**: `PERSISTENT_TRACES_COGNITIVE_SCIENCE.md` - Emergent language theory, philosophical implications\n- \ud83d\udccb **Quick Reference**: `PERSISTENT_TRACES_QUICK_REFERENCE.md` - Config templates, FAQ, decision trees\n- \ud83d\uddc4\ufe0f **Vector Store Integration**: `data-backends-vector-stores.md` (PF-005) - Persistent storage backend for cross-session memory\n\n---\n\n## \ud83d\udccb Executive Summary\n\n### The Vision\nCurrent language models process each input independently, computing attention and routing decisions from scratch every forward pass. This is computationally wasteful and biologically implausible. **Persistent Attention Traces** and **Semantic Crystallization** aim to create a model that develops its own efficient internal \"thought language\" by:\n\n1. **Remembering high-salience reasoning pathways** that recur across contexts (attention traces)\n2. **Consolidating frequently-used expert routing patterns** into reusable symbolic primitives (crystallization)\n3. **Evolving an internal cognitive lexicon** optimized for dense, rapid computation rather than human readability\n\nThis is analogous to how biological neural networks consolidate episodic experiences into semantic knowledge through synaptic strengthening and pruning.\n\n### The Problem\n- **Attention is ephemeral**: Valuable reasoning patterns discovered during training are recomputed from scratch every forward pass\n- **Routing is redundant**: MoE models repeatedly discover the same expert pathways for similar inputs\n- **No cognitive compression**: Models cannot develop efficient \"shorthand\" for complex conceptual patterns\n- **Memory inefficiency**: Full attention matrices are O(L\u00b2) per layer - infeasible to persist\n\n### The Solution\n**Two-component cognitive enhancement system**:\n\n#### Component 1: Persistent Attention Traces\n- Capture **sparse** high-salience attention edges (top 0.1%) that consistently strengthen across sequences\n- Store in compact coordinate format: `(layer, head, query_idx, key_idx, salience, age)` \u2248 12 bytes per trace\n- Apply exponential decay to unused traces (biological forgetting)\n- **Bias future attention** using accumulated trace memory \u2192 faster convergence to proven reasoning pathways\n- **Memory footprint**: ~6-12 MB for 32-layer model (vs. ~68 GB for full attention storage!)\n\n#### Component 2: Semantic Crystallization\n- Track MoE routing paths: sequences of expert activations `[E\u2082\u2192E\u2087\u2192E\u2083\u2192E\u2081]` across layers\n- Build suffix tree of frequently-traversed motifs with utility scoring\n- **Freeze high-utility motifs** into specialized computational units (new \"thought symbols\")\n- Competitive dynamics ensure only genuinely useful patterns survive\n- **Result**: Model develops hierarchical vocabulary of reusable reasoning primitives\n\n### Expected Outcomes\n```\nBaseline Model:\n  - Recomputes attention from scratch: 100% cost every forward pass\n  - Discovers useful routing path: forgotten immediately after sequence ends\n  - Long-horizon reasoning: struggles with dependencies beyond context window\n\nEnhanced Model:\n  - Persistent traces accelerate attention: 20-40% speedup on familiar patterns\n  - Crystallized motifs become reusable \"concepts\": 15-30% FLOP reduction\n  - Emergent internal language: hierarchical reasoning primitives\n  - Long-term memory: consolidates knowledge across training lifetime\n```\n\n**This is pioneering work** - no existing implementation combines persistent attention biasing with routing-path crystallization at this level of integration.\n\n---\n\n## \ud83e\udde0 Theoretical Foundation\n\n### Biological Inspiration\n\n**Long-Term Potentiation (LTP)**  \nIn biological neural networks, synaptic connections that repeatedly fire together strengthen through LTP - the cellular basis of learning and memory. Our persistent attention traces implement a computational analog:\n\n- **Hebbian principle**: \"Neurons that fire together wire together\"\n- **Synaptic consolidation**: Episodic memories (hippocampus) consolidate into semantic knowledge (neocortex) during sleep\n- **Structural plasticity**: Frequently-used neural pathways develop stronger connections; unused pathways prune\n\n**Transfer to Transformers**:\n- Attention weights \u2248 synaptic strengths\n- Persistent traces \u2248 consolidated synaptic weights\n- Trace decay \u2248 synaptic pruning\n- Crystallized motifs \u2248 semantic concepts in neocortex\n\n### Mathematical Formulation\n\n#### 1. Persistent Attention Traces\n\n**Notation**:\n- $A_{i,j}^{(l,h)}(t)$ = attention weight from query position $i$ to key position $j$, layer $l$, head $h$, timestep $t$\n- $M_{i,j}^{(l,h)}$ = persistent trace memory (sparse matrix)\n- $\\mathcal{L}$ = task loss\n- $\\lambda \\in [0,1]$ = trace retention rate (momentum)\n- $\\gamma \\in [0,1)$ = decay rate for unused traces\n- $\\theta_{sal}$ = salience threshold for trace capture\n- $\\alpha \\in [0,1]$ = bias injection strength\n\n**Salience Score** (determines which attention edges persist):\n$$\nS_{i,j}^{(l,h)}(t) = A_{i,j}^{(l,h)}(t) \\cdot \\left|\\frac{\\partial \\mathcal{L}}{\\partial A_{i,j}^{(l,h)}}\\right| \\cdot \\text{persist}(i,j,t)\n$$\n\nwhere $\\text{persist}(i,j,t)$ measures how often edge $(i,j)$ has appeared in recent history.\n\n**Trace Memory Update** (sparse EMA with competitive decay):\n$$\nM_{i,j}^{(l,h)} \\leftarrow \\begin{cases}\n\\lambda \\cdot M_{i,j}^{(l,h)} + (1-\\lambda) \\cdot S_{i,j}^{(l,h)} & \\text{if } S_{i,j}^{(l,h)} > \\theta_{sal} \\\\\n\\gamma \\cdot M_{i,j}^{(l,h)} & \\text{otherwise}\n\\end{cases}\n$$\n\n**Biased Attention Computation** (inject memory into standard attention):\n$$\nA'^{(l,h)} = \\text{softmax}\\left(\\frac{Q \\cdot K^T}{\\sqrt{d_k}} + \\alpha \\cdot M^{(l,h)}\\right)\n$$\n\nNote: $M^{(l,h)}$ is sparse; zero entries are not stored.\n\n**Memory Budget Per Layer**:\n$$\n\\begin{align}\nN_{\\text{edges}}^{\\text{full}} &= T^2 \\quad \\text{(sequence length squared)} \\\\\nN_{\\text{edges}}^{\\text{sparse}} &\\approx k_{\\text{sparse}} \\cdot T^2 \\quad \\text{where } k_{\\text{sparse}} \\approx 0.001 \\\\\n\\text{Bytes per trace} &= 4 + 4 + 2 + 2 = 12 \\text{ (layer:1B, head:1B, i:2B, j:2B, salience:4B, age:2B)} \\\\\nB_{\\text{mem}}^{(l,h)} &= N_{\\text{edges}}^{\\text{sparse}} \\cdot 12 \\text{ bytes}\n\\end{align}\n$$\n\n**Example**: For $T=4096$, $L=32$ layers, $H=32$ heads:\n- Full attention storage: $32 \\times 32 \\times 4096^2 \\times 4 \\approx 68$ GB\n- Sparse trace storage ($k=0.001$): $32 \\times 32 \\times (0.001 \\times 4096^2) \\times 12 \\approx 6.4$ MB\n\n**Reduction factor**: ~10,000\u00d7\n\n#### 2. Semantic Crystallization\n\n**Notation**:\n- $\\pi(x) = [e_1^{(1)}, e_2^{(2)}, \\ldots, e_L^{(L)}]$ = routing path for input $x$, where $e_l \\in \\{1, \\ldots, K\\}$\n- $f(\\pi)$ = frequency count of path $\\pi$\n- $U(\\pi)$ = conditional utility of path $\\pi$\n- $H(\\pi)$ = routing entropy of path $\\pi$\n\n**Path Frequency** (tracked via suffix tree):\n$$\nf(\\pi) = \\sum_{t=1}^{T} \\mathbb{1}[\\pi(x_t) = \\pi]\n$$\n\n**Conditional Utility** (performance improvement when using path $\\pi$):\n$$\nU(\\pi) = \\mathbb{E}[\\text{reward} \\mid \\pi] - \\mathbb{E}[\\text{reward}] = \\frac{1}{f(\\pi)} \\sum_{x:\\pi(x)=\\pi} r(x) - \\bar{r}\n$$\n\nwhere $r(x)$ is task-specific reward (e.g., negative loss, accuracy).\n\n**Routing Entropy** (measure of path stability):\n$$\nH(\\pi) = -\\sum_{l=1}^{L} \\sum_{k=1}^{K} p(e_l = k \\mid \\pi_{1:l-1}) \\log p(e_l = k \\mid \\pi_{1:l-1})\n$$\n\nLow entropy \u2192 deterministic, stable routing \u2192 good crystallization candidate.\n\n**Crystallization Criterion** (all conditions must hold):\n$$\n\\text{Crystallize}(\\pi) \\Leftrightarrow \\begin{cases}\nf(\\pi) > f_{\\min} & \\text{(sufficient frequency)} \\\\\nU(\\pi) > U_{\\min} & \\text{(positive utility)} \\\\\nH(\\pi) < H_{\\max} & \\text{(low entropy/stable)} \\\\\n\\text{age}(\\pi) > \\tau & \\text{(temporal stability)}\n\\end{cases}\n$$\n\n**Frozen Motif as New Expert**:\n$$\nE_{\\text{new}}(x) = \\prod_{l=1}^{L_{\\text{motif}}} E_{e_l}^{(l)}(h_{l-1})\n$$\n\nThe crystallized expert becomes a single atomic operation, callable like any other expert.\n\n**Competitive Consolidation** (limited motif budget):\n$$\n\\text{Evict lowest-utility motif if } |\\mathcal{M}| > M_{\\max} \\text{ and } \\exists \\pi': U(\\pi') < \\min_{\\pi \\in \\mathcal{M}} U(\\pi)\n$$\n\n---\n\n## \ud83d\udcbe Memory-Efficient Implementation Design\n\n### Critical Constraints\n- **VRAM budget**: User config allows ~90% GPU memory for training (`train_cuda_mem_pct: 90`)\n- **Typical VRAM**: 12-24 GB consumer GPUs (RTX 4090, 3090)\n- **Model size**: HRM-ACTV1 ranges from 150M to 1B+ parameters\n- **Must not OOM**: Memory overhead must be negligible compared to model weights and activations\n\n### Trace Storage Data Structure\n\n**Sparse COO (Coordinate) Format**:\n```python\n@dataclass\nclass AttentionTrace:\n    \"\"\"Single persistent attention edge.\"\"\"\n    layer_id: uint8      # 1 byte  (max 255 layers)\n    head_id: uint8       # 1 byte  (max 255 heads)\n    query_idx: uint16    # 2 bytes (max 65K sequence length)\n    key_idx: uint16      # 2 bytes\n    salience: float32    # 4 bytes (accumulated strength)\n    age: uint16          # 2 bytes (timesteps since last reinforcement)\n    # Total: 12 bytes per trace\n\n@dataclass\nclass TraceMemoryLayer:\n    \"\"\"Sparse trace storage for one layer.\"\"\"\n    traces: List[AttentionTrace]  # Sparse edges only\n    max_traces: int               # Quota per layer/head\n    decay_rate: float             # \u03b3 parameter\n    retention_rate: float         # \u03bb parameter\n    bias_strength: float          # \u03b1 parameter\n    \n    def to_sparse_matrix(self, device: torch.device) -> torch.sparse.FloatTensor:\n        \"\"\"Convert traces to sparse PyTorch tensor for bias injection.\"\"\"\n        if len(self.traces) == 0:\n            return None\n        \n        indices = torch.tensor(\n            [[t.query_idx, t.key_idx] for t in self.traces], \n            dtype=torch.long\n        ).t()\n        values = torch.tensor(\n            [t.salience for t in self.traces], \n            dtype=torch.float32\n        )\n        size = (max_seq_len, max_seq_len)  # from config\n        return torch.sparse_coo_tensor(indices, values, size, device=device)\n```\n\n**Memory Budget Calculation**:\n\n```python\n# Configuration\nL = 32              # layers\nH = 32              # heads per layer\nT = 4096            # max sequence length\nk_sparse = 0.001    # sparsity (capture top 0.1% edges)\nbytes_per_trace = 12\n\n# Per-layer calculation\nedges_per_layer_head = k_sparse * (T ** 2)  # ~16,777 edges\ntraces_per_layer = H * edges_per_layer_head # ~536,870 traces\nbytes_per_layer = traces_per_layer * bytes_per_trace  # ~6.4 MB\n\n# Full model\ntotal_traces = L * traces_per_layer  # ~17M traces\ntotal_memory_mb = L * bytes_per_layer / (1024**2)  # ~205 MB\n\n# BUT: We enforce stricter quota to be safe\nquota_per_head = 2048  # max traces per head\nactual_traces = L * H * quota_per_head  # ~2M traces\nactual_memory_mb = actual_traces * bytes_per_trace / (1024**2)  # ~24 MB\n```\n\n**Result**: \u2705 **~24 MB total overhead** for trace memory - completely negligible on modern GPUs.\n\n### Routing Path Storage (Suffix Tree)\n\n**Data Structure**:\n```python\n@dataclass\nclass RoutingNode:\n    \"\"\"Node in routing path suffix tree.\"\"\"\n    expert_id: int                    # Expert ID at this layer\n    layer: int                        # Layer depth\n    count: int                        # Traversal frequency\n    total_reward: float               # Cumulative task reward\n    children: Dict[int, RoutingNode]  # Next-layer expert transitions\n    \n    def utility(self) -> float:\n        \"\"\"Conditional utility score.\"\"\"\n        if self.count == 0:\n            return 0.0\n        return (self.total_reward / self.count) - global_mean_reward\n    \n    def entropy(self) -> float:\n        \"\"\"Routing entropy at this node.\"\"\"\n        if not self.children:\n            return 0.0\n        total = sum(c.count for c in self.children.values())\n        probs = [c.count / total for c in self.children.values()]\n        return -sum(p * math.log(p + 1e-10) for p in probs)\n\nclass RoutingPathTree:\n    \"\"\"Suffix tree tracking all expert routing paths.\"\"\"\n    root: RoutingNode\n    motif_registry: Dict[str, CrystallizedMotif]\n    max_motifs: int = 512  # Hard limit on crystallized primitives\n    \n    def record_path(self, path: List[int], reward: float):\n        \"\"\"Update tree with new routing path observation.\"\"\"\n        node = self.root\n        for layer, expert_id in enumerate(path):\n            if expert_id not in node.children:\n                node.children[expert_id] = RoutingNode(\n                    expert_id=expert_id,\n                    layer=layer,\n                    count=0,\n                    total_reward=0.0,\n                    children={}\n                )\n            node = node.children[expert_id]\n            node.count += 1\n            node.total_reward += reward\n    \n    def find_crystallization_candidates(self) -> List[Tuple[List[int], float]]:\n        \"\"\"DFS to find high-utility stable paths.\"\"\"\n        candidates = []\n        \n        def dfs(node: RoutingNode, path: List[int]):\n            if len(path) >= MIN_MOTIF_LENGTH:\n                if (node.count > FREQ_THRESHOLD and \n                    node.utility() > UTILITY_THRESHOLD and\n                    node.entropy() < ENTROPY_THRESHOLD):\n                    candidates.append((path.copy(), node.utility()))\n            \n            for expert_id, child in node.children.items():\n                path.append(expert_id)\n                dfs(child, path)\n                path.pop()\n        \n        dfs(self.root, [])\n        return sorted(candidates, key=lambda x: x[1], reverse=True)\n```\n\n**Memory Budget**:\n- Each node: ~48 bytes (int:4, int:4, int:8, float:8, dict_overhead:24)\n- Max motif length: ~8 layers\n- Branching factor: ~8 experts per layer on average\n- Max tree size: $8^8 = 16.7$M nodes worst case \u2192 **~800 MB** worst case\n- **Realistic case** (pruned/sparse): ~100K nodes \u2192 **~5 MB**\n\n**Total Persistent Memory**: \n- **RAM (required)**: ~30 MB (traces + routing tree) - Active working memory\n- **Vector Store (optional)**: Unlimited capacity - Cross-session persistence via Qdrant/LanceDB (see PF-005)\n\n---\n\n## \ud83d\uddc4\ufe0f Vector Store Integration (Optional Enhancement)\n\n### Overview\n\nWhile persistent traces and crystallized motifs operate effectively in RAM (~30 MB), **optional integration with vector stores** (see `data-backends-vector-stores.md` PF-005) enables:\n\n1. **Cross-session persistence**: Traces/motifs survive training restarts\n2. **Unlimited capacity**: Billions of traces vs 2M RAM limit\n3. **Multi-model sharing**: Transfer learned motifs between models\n4. **Scalable retrieval**: O(log n) ANN search vs O(n) RAM scan\n\n### Architecture\n\n**Three-tier memory hierarchy**:\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 LAYER 3: Cognitive Components (this document)          \u2502\n\u2502  \u251c\u2500 TraceManager (24 MB RAM hot storage)               \u2502\n\u2502  \u2514\u2500 RoutingPathTree (5 MB RAM motif tree)              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502 optional sync\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 LAYER 2: Specialized Wrappers                          \u2502\n\u2502  \u251c\u2500 TraceVectorStore (trace embedding + metadata)      \u2502\n\u2502  \u2514\u2500 MotifVectorStore (motif embedding + utility)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502 uses\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 LAYER 1: VectorStoreClient (PF-005)                    \u2502\n\u2502  Backend: Qdrant or LanceDB                            \u2502\n\u2502  Methods: upsert(), query(), delete()                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Trace Embedding Specification\n\n**Challenge**: Convert sparse coordinate trace to dense vector for similarity search.\n\n**Solution**: Compositional embedding\n```python\nclass TraceEmbedder:\n    \"\"\"Converts AttentionTrace to dense embedding.\"\"\"\n    \n    def __init__(self, embed_dim: int = 128):\n        # Learned embeddings for trace components\n        self.layer_embed = nn.Embedding(256, embed_dim // 4)  # max 256 layers\n        self.head_embed = nn.Embedding(256, embed_dim // 4)   # max 256 heads\n        self.position_encoder = SinusoidalPositionalEncoding(embed_dim // 2)\n    \n    def embed(self, trace: AttentionTrace) -> np.ndarray:\n        \"\"\"Convert trace to 128D vector.\"\"\"\n        # Component embeddings\n        layer_vec = self.layer_embed(trace.layer_id)       # 32D\n        head_vec = self.head_embed(trace.head_id)          # 32D\n        pos_vec = self.position_encoder(\n            trace.query_idx, trace.key_idx)                 # 64D (encodes spatial relation)\n        \n        # Weighted by salience\n        full_vec = torch.cat([layer_vec, head_vec, pos_vec], dim=-1)  # 128D\n        return (full_vec * trace.salience).detach().cpu().numpy()\n```\n\n**Metadata storage**:\n```python\ntrace_metadata = {\n    \"layer\": trace.layer_id,\n    \"head\": trace.head_id,\n    \"query_idx\": trace.query_idx,\n    \"key_idx\": trace.key_idx,\n    \"salience\": trace.salience,\n    \"age\": trace.age,\n    \"training_step\": current_step,\n    \"task_tag\": \"QA\" or \"generation\" or \"classification\",  # optional\n}\n```\n\n### Motif Embedding Specification\n\n**Challenge**: Embed variable-length expert sequences `[E\u2082\u2192E\u2087\u2192E\u2083\u2192E\u2081]` into fixed-size vector.\n\n**Solution**: Sequence encoding with utility weighting\n```python\nclass MotifEmbedder:\n    \"\"\"Converts crystallized motif to dense embedding.\"\"\"\n    \n    def __init__(self, embed_dim: int = 256, max_experts: int = 64):\n        self.expert_embed = nn.Embedding(max_experts, embed_dim)\n        self.sequence_encoder = nn.LSTM(\n            input_size=embed_dim, \n            hidden_size=embed_dim, \n            num_layers=2,\n            batch_first=True\n        )\n    \n    def embed(self, motif: CrystallizedMotif) -> np.ndarray:\n        \"\"\"Convert motif expert sequence to 256D vector.\"\"\"\n        # Embed each expert in sequence\n        expert_ids = torch.tensor(motif.expert_sequence)  # [L]\n        expert_vecs = self.expert_embed(expert_ids)       # [L, 256]\n        \n        # Encode sequence\n        _, (h_n, _) = self.sequence_encoder(expert_vecs.unsqueeze(0))\n        motif_vec = h_n[-1]  # Final hidden state [256]\n        \n        # Weight by utility\n        return (motif_vec * motif.utility).detach().cpu().numpy()\n```\n\n**Metadata storage**:\n```python\nmotif_metadata = {\n    \"motif_id\": motif.id,\n    \"expert_sequence\": motif.expert_sequence,  # [2, 7, 3, 1]\n    \"frequency\": motif.count,\n    \"utility\": motif.utility,\n    \"entropy\": motif.entropy,\n    \"age\": motif.age,\n    \"task_tags\": [\"retrieval\", \"QA\"],  # tasks where motif is useful\n}\n```\n\n### Synchronization Protocol\n\n**TraceManager persistence**:\n```python\nclass TraceManager:\n    def __init__(self, config, vector_store_client=None):\n        self.traces = []  # RAM storage\n        self.vector_store = TraceVectorStore(vector_store_client) if vector_store_client else None\n        self.sync_interval = config.trace_sync_interval  # e.g., 1000 steps\n        self.last_sync_step = 0\n    \n    def sync_to_vector_store(self, current_step: int):\n        \"\"\"Persist recent traces to vector DB.\"\"\"\n        if not self.vector_store or not self.vector_store.enabled:\n            return\n        \n        # Convert traces to embeddings\n        embeddings = [self.embedder.embed(t) for t in self.traces]\n        ids = [f\"trace_{t.layer_id}_{t.head_id}_{t.query_idx}_{t.key_idx}\" for t in self.traces]\n        metadata = [self._trace_to_metadata(t, current_step) for t in self.traces]\n        \n        # Upsert to vector store\n        self.vector_store.upsert(ids, embeddings, metadata)\n        self.last_sync_step = current_step\n        logger.info(f\"Synced {len(self.traces)} traces to vector store at step {current_step}\")\n    \n    def load_from_vector_store(self, task_tag=None, top_k=10000):\n        \"\"\"Warm-start from previous training session.\"\"\"\n        if not self.vector_store or not self.vector_store.enabled:\n            return\n        \n        # Query for relevant traces (if task_tag specified)\n        filter_dict = {\"task_tag\": task_tag} if task_tag else None\n        results = self.vector_store.query_all(top_k=top_k, filter=filter_dict)\n        \n        # Reconstruct traces from metadata\n        for trace_id, score, metadata in results:\n            trace = AttentionTrace(\n                layer_id=metadata[\"layer\"],\n                head_id=metadata[\"head\"],\n                query_idx=metadata[\"query_idx\"],\n                key_idx=metadata[\"key_idx\"],\n                salience=metadata[\"salience\"],\n                age=metadata[\"age\"],\n            )\n            self.traces.append(trace)\n        \n        logger.info(f\"Loaded {len(results)} traces from vector store\")\n```\n\n**RoutingPathTree persistence** (similar pattern for motifs).\n\n### Configuration Integration\n\n**Updated config schema** (unified under `memory` namespace):\n```yaml\nmemory:\n  # Vector store backend (PF-005)\n  vector_store:\n    backend: \"qdrant\"  # qdrant|lancedb|disabled\n    qdrant:\n      host: \"localhost\"\n      port: 6333\n      collection_prefix: \"aios_memory\"\n    lancedb:\n      path: \"artifacts/memory/lancedb\"\n  \n  # Persistent attention traces\n  persistent_traces:\n    enabled: false\n    sparsity: 0.001\n    quota_per_head: 2048\n    salience_threshold: 0.05\n    retention_rate: 0.95\n    decay_rate: 0.98\n    bias_strength: 0.1\n    update_interval: 100\n    warmup_steps: 1000\n    \n    # Vector store integration (optional)\n    persist_to_vector_store: false  # Enable cross-session persistence\n    trace_sync_interval: 1000       # Steps between DB syncs\n    embedding_dim: 128              # Trace embedding size\n    warm_start: false               # Load traces from DB on training start\n    task_tag: null                  # Filter traces by task type\n  \n  # Semantic crystallization\n  semantic_crystallization:\n    enabled: false\n    min_frequency: 100\n    min_utility: 0.05\n    max_entropy: 1.0\n    min_age: 500\n    max_motifs: 512\n    \n    # Vector store integration (optional)\n    persist_motifs: false           # Auto-save crystallized motifs\n    motif_embedding_dim: 256        # Motif embedding size\n    share_across_models: false      # Allow other models to query motifs\n```\n\n### Benefits of Integration\n\n**Without vector store** (baseline):\n- \u2705 Works standalone, no external dependencies\n- \u2705 Fast (pure RAM)\n- \u274c Limited to ~2M traces\n- \u274c Lost on training restart\n- \u274c Cannot share between models\n\n**With vector store** (enhanced):\n- \u2705 Unlimited capacity (billions of traces)\n- \u2705 Persistent across sessions\n- \u2705 Multi-model collaboration\n- \u2705 Fast retrieval (ANN indexing)\n- \u274c Requires Qdrant/LanceDB service\n- \u274c Slight I/O overhead during sync\n\n**Recommendation**: Start without vector store, enable after validating core functionality.\n\n---\n\n## \ud83d\udd27 Architecture Integration (HRM-ACTV1 Specific)\n\n### Current Architecture Analysis\n\n**Files involved**:\n- `src/aios/core/hrm_models/impl/hrm_act_v1.py` - Main model architecture\n- `src/aios/core/hrm_models/impl/layers.py` - Attention and MoE layers\n- `src/aios/core/hrm_models/moe_layer.py` - MoE base implementation\n- `src/aios/core/hrm_models/dynamic_moe/dynamic_layer.py` - Dynamic expert loading\n- `src/aios/cli/hrm_hf/training_loop.py` - Training orchestration\n\n**Current attention implementation** (`layers.py` line ~164):\n```python\ndef forward(self, cos_sin: CosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n    # ... qkv projection ...\n    \n    # Standard scaled dot-product attention\n    attn_weights = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n    attn_weights = F.softmax(attn_weights, dim=-1)\n    \n    # Apply attention\n    attn_output = torch.matmul(attn_weights, value)\n```\n\n**Current MoE router** (`moe_layer.py` line ~66):\n```python\ndef forward(self, hidden_states, top_k):\n    logits = self.gate(hidden_states)  # [batch, seq, num_experts]\n    top_k_logits, top_k_indices = torch.topk(logits, top_k, dim=-1)\n    top_k_weights = F.softmax(top_k_logits, dim=-1)\n    return top_k_weights, top_k_indices, logits\n```\n\n### Integration Points\n\n#### Hook 1: Attention Trace Capture (Post-Attention)\n\n**Location**: `src/aios/core/hrm_models/impl/layers.py` - `Attention.forward()`\n\n**Modification**:\n```python\nclass Attention(nn.Module):\n    def __init__(self, ...):\n        # ... existing init ...\n        self.trace_manager = None  # Set by model if tracing enabled\n    \n    def forward(self, cos_sin: CosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n        # ... existing QKV projection and RoPE ...\n        \n        # Compute attention weights\n        attn_weights = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        \n        # INTEGRATION POINT 1: Inject persistent trace bias\n        if self.trace_manager is not None and self.trace_manager.enabled:\n            trace_bias = self.trace_manager.get_bias_for_layer_head(\n                layer_id=self.layer_id, \n                head_id=head_id,  # iterate over heads\n                device=attn_weights.device\n            )\n            if trace_bias is not None:\n                attn_weights = attn_weights + self.trace_manager.bias_strength * trace_bias\n        \n        attn_weights = F.softmax(attn_weights, dim=-1)\n        \n        # INTEGRATION POINT 2: Capture high-salience traces (training only)\n        if self.training and self.trace_manager is not None:\n            # Detach to avoid interfering with backprop\n            self.trace_manager.register_attention_for_update(\n                layer_id=self.layer_id,\n                head_id=head_id,\n                attn_weights=attn_weights.detach(),\n                query_idx=...,  # derived from position info\n                key_idx=...,\n            )\n        \n        # ... rest of attention computation ...\n```\n\n**Gradient capture** (for salience score):\n```python\n# In training loop, after loss.backward()\nif trace_manager.enabled:\n    for layer_id, layer in enumerate(model.layers):\n        attn_grad = layer.self_attn.attn_weights.grad  # Need to retain_grad()\n        trace_manager.update_traces_with_gradients(layer_id, attn_grad)\n```\n\n#### Hook 2: Routing Path Logging (Post-Router)\n\n**Location**: `src/aios/core/hrm_models/moe_layer.py` - `TopKRouter.forward()`\n\n**Modification**:\n```python\nclass TopKRouter(nn.Module):\n    def __init__(self, ...):\n        # ... existing init ...\n        self.crystallization_tracker = None  # Set by model\n    \n    def forward(self, hidden_states, top_k):\n        logits = self.gate(hidden_states)\n        top_k_logits, top_k_indices = torch.topk(logits, top_k, dim=-1)\n        top_k_weights = F.softmax(top_k_logits, dim=-1)\n        \n        # INTEGRATION POINT 3: Log routing paths for crystallization\n        if self.training and self.crystallization_tracker is not None:\n            # Log chosen experts per token\n            self.crystallization_tracker.record_routing(\n                layer_id=self.layer_id,\n                expert_indices=top_k_indices.detach(),\n                routing_weights=top_k_weights.detach(),\n                sequence_ids=...,  # from batch metadata\n            )\n        \n        return top_k_weights, top_k_indices, logits\n```\n\n#### Hook 3: Auxiliary Loss Computation\n\n**Location**: `src/aios/cli/hrm_hf/training_loop.py` - loss computation\n\n**Addition**:\n```python\ndef compute_loss_with_auxiliary(model, batch, trace_manager, crystallization_tracker):\n    # Standard forward pass\n    outputs = model(batch)\n    base_loss = outputs.loss\n    \n    # Existing MoE load balancing loss\n    load_balance_loss = 0.0\n    for layer in model.layers:\n        if hasattr(layer, 'mlp') and hasattr(layer.mlp, 'last_router_logits'):\n            load_balance_loss += compute_load_balance_loss(layer.mlp.last_router_logits)\n    \n    # NEW: Trace reuse auxiliary loss (encourage using persistent traces)\n    trace_reuse_loss = 0.0\n    if trace_manager.enabled:\n        trace_reuse_loss = trace_manager.compute_trace_utilization_loss()\n    \n    # NEW: Crystallization stability loss (low entropy for stable paths)\n    crystallization_loss = 0.0\n    if crystallization_tracker.enabled:\n        crystallization_loss = crystallization_tracker.compute_entropy_regularization()\n    \n    # Combined loss\n    total_loss = (\n        base_loss + \n        0.01 * load_balance_loss +  # existing\n        0.005 * trace_reuse_loss +  # new\n        0.002 * crystallization_loss  # new\n    )\n    \n    return total_loss, {\n        'base_loss': base_loss.item(),\n        'load_balance_loss': load_balance_loss.item(),\n        'trace_reuse_loss': trace_reuse_loss.item(),\n        'crystallization_loss': crystallization_loss.item(),\n    }\n```\n\n### Flash Attention Compatibility\n\n**Challenge**: Flash Attention is a fused CUDA kernel - cannot intercept intermediate attention weights.\n\n**Solution**: Dual-mode attention\n```python\nclass Attention(nn.Module):\n    def forward(self, ...):\n        # Trace capture requires explicit attention matrices\n        if self.training and self.trace_manager is not None and self.trace_manager.capture_mode:\n            # Use standard attention (slower but exposes weights)\n            return self._forward_standard_with_traces(...)\n        else:\n            # Use Flash Attention (faster)\n            return self._forward_flash_attn(...)\n```\n\n**Training strategy**:\n- **Phase 1** (warmup): Use standard attention, accumulate traces (0-5% of training)\n- **Phase 2** (main training): Use Flash Attention with frozen trace bias (95% of training)\n- **Phase 3** (periodic trace updates): Briefly switch to standard attention every N steps to refresh traces\n\n### Gradient Checkpointing Compatibility\n\n**Challenge**: Gradient checkpointing discards activations, including attention weights.\n\n**Solution**: Trace updates happen in **separate forward-only passes**\n```python\n# Every UPDATE_INTERVAL steps (e.g., 100)\nif step % TRACE_UPDATE_INTERVAL == 0:\n    with torch.no_grad():\n        # Forward-only pass with trace capture enabled\n        _ = model(batch, trace_capture_mode=True)\n        # trace_manager accumulates observations\n    \n    # Update trace memory based on accumulated stats\n    trace_manager.consolidate_traces()\n```\n\n---\n\n## \ud83d\udcca Training Protocol\n\n### Loss Function Components\n\n**Total training loss**:\n$$\n\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{task}} + \\beta_1 \\mathcal{L}_{\\text{balance}} + \\beta_2 \\mathcal{L}_{\\text{trace}} + \\beta_3 \\mathcal{L}_{\\text{crystal}}\n$$\n\nwhere:\n- $\\mathcal{L}_{\\text{task}}$ = standard next-token prediction loss\n- $\\mathcal{L}_{\\text{balance}}$ = MoE load balancing (existing)\n- $\\mathcal{L}_{\\text{trace}}$ = trace utilization regularizer (new)\n- $\\mathcal{L}_{\\text{crystal}}$ = crystallization stability (new)\n\n#### 1. Trace Utilization Loss\n\n**Goal**: Encourage model to reuse persistent traces (exploration-exploitation balance).\n\n$$\n\\mathcal{L}_{\\text{trace}} = -\\frac{1}{L \\cdot H} \\sum_{l=1}^{L} \\sum_{h=1}^{H} \\frac{\\text{trace\\_hits}^{(l,h)}}{\\text{total\\_attention\\_ops}^{(l,h)}}\n$$\n\nPenalizes models that ignore their accumulated trace memory.\n\n#### 2. Crystallization Stability Loss\n\n**Goal**: Encourage deterministic routing for high-utility paths (low entropy).\n\n$$\n\\mathcal{L}_{\\text{crystal}} = \\frac{1}{N_{\\text{motifs}}} \\sum_{\\pi \\in \\mathcal{M}} H(\\pi) \\cdot \\mathbb{1}[U(\\pi) > U_{\\min}]\n$$\n\nOnly penalize entropy for high-utility motifs (we want them stable).\n\n### Hyperparameters\n\n**Trace management**:\n```yaml\npersistent_traces:\n  enabled: true\n  sparsity: 0.001              # Top 0.1% of attention edges\n  quota_per_head: 2048         # Max traces per head\n  salience_threshold: 0.05     # Minimum salience to capture\n  retention_rate: 0.95         # \u03bb (momentum)\n  decay_rate: 0.98             # \u03b3 (forgetting)\n  bias_strength: 0.1           # \u03b1 (injection strength)\n  update_interval: 100         # Steps between trace consolidation\n  warmup_steps: 1000           # Standard attention for trace accumulation\n```\n\n**Crystallization**:\n```yaml\nsemantic_crystallization:\n  enabled: true\n  min_frequency: 100           # f_min\n  min_utility: 0.05            # U_min (5% improvement over baseline)\n  max_entropy: 1.0             # H_max (low entropy = stable)\n  min_age: 500                 # Temporal stability requirement\n  max_motifs: 512              # Hard limit on crystallized primitives\n  motif_min_length: 3          # Minimum layers in motif\n  motif_max_length: 8          # Maximum layers in motif\n  prune_interval: 1000         # Steps between motif pruning\n```\n\n**Loss coefficients**:\n```yaml\nloss_weights:\n  task: 1.0\n  load_balance: 0.01           # Existing MoE\n  trace_utilization: 0.005     # New\n  crystallization: 0.002       # New\n```\n\n### Training Curriculum\n\n**Phase 1: Foundation (0-20% of training)**\n- Standard training, no trace/crystallization\n- Model learns basic patterns\n- Establishes baseline performance\n\n**Phase 2: Trace Accumulation (20-30% of training)**\n- Enable trace capture (standard attention mode)\n- No bias injection yet\n- Build initial trace memory\n\n**Phase 3: Trace-Biased Training (30-70% of training)**\n- Enable bias injection\n- Switch to Flash Attention for speed\n- Periodic trace updates every 100 steps\n- Model learns to leverage persistent memory\n\n**Phase 4: Crystallization Discovery (70-85% of training)**\n- Enable routing path tracking\n- Identify high-utility motifs\n- No freezing yet (observation only)\n\n**Phase 5: Motif Crystallization (85-100% of training)**\n- Freeze top motifs into specialized experts\n- Fine-tune with crystallized primitives\n- Competitive pruning of low-utility motifs\n\n### Stability Safeguards\n\n**Elastic Weight Consolidation (EWC)**:\n- Protect important parameters supporting high-salience traces\n- Fisher information matrix computed over trace-heavy pathways\n- Penalty: $\\mathcal{L}_{\\text{EWC}} = \\sum_i \\frac{\\lambda_i}{2} F_i (\\theta_i - \\theta_i^*)^2$\n\n**Router Collapse Prevention**:\n- Maintain minimum load balancing even with crystallization\n- Enforce capacity constraints per expert\n- Periodic entropy injection (exploration)\n\n**Memory Overflow Protection**:\n- Hard quota enforcement: evict lowest-salience traces when full\n- Automatic decay rate adjustment if memory pressure detected\n- Fallback to standard attention if trace system fails\n\n---\n\n## \ud83d\udcc8 Evaluation Framework\n\n### Metrics\n\n#### Primary Metrics (Task Performance)\n1. **Perplexity** on validation set (lower is better)\n2. **Long-range dependency accuracy** (copy tasks, induction heads)\n3. **Multi-hop reasoning** (HotpotQA, MMLU)\n4. **Inference latency** (ms per token)\n\n#### Trace-Specific Metrics\n1. **Trace coverage**: % of attention operations using persistent bias\n2. **Trace stability**: Correlation of traces across epochs\n3. **Salience distribution**: Histogram of trace strengths\n4. **Memory efficiency**: Actual bytes used vs. quota\n\n#### Crystallization Metrics\n1. **Motif count**: Number of crystallized primitives over training\n2. **Motif utility**: Average $U(\\pi)$ for crystallized motifs\n3. **Routing entropy**: Average $H(\\pi)$ for active motifs\n4. **FLOP reduction**: % compute saved by crystallized experts\n\n#### Emergent Language Metrics (Exploratory)\n1. **Hierarchical structure**: Tree depth of motif dependencies\n2. **Compositionality**: Frequency of motif combinations\n3. **Semantic coherence**: Clustering of motifs by task type\n\n### Baselines\n\n1. **Standard HRM-ACTV1**: No traces, no crystallization\n2. **Compressive Transformer**: External memory baseline\n3. **Memorizing Transformer**: kNN retrieval baseline\n4. **Mixture-of-Experts (vanilla)**: MoE without crystallization\n\n### Ablation Studies\n\nTest each component independently:\n\n| Experiment | Traces | Crystallization | Expected Outcome |\n|------------|--------|-----------------|------------------|\n| Baseline | \u274c | \u274c | Standard performance |\n| Traces Only | \u2705 | \u274c | Faster convergence, better long-range |\n| Crystal Only | \u274c | \u2705 | FLOP reduction, expert specialization |\n| Full System | \u2705 | \u2705 | Synergistic improvements |\n\n**Ablation parameters**:\n- Trace sparsity: {0.0001, 0.001, 0.01}\n- Bias strength $\\alpha$: {0.01, 0.05, 0.1, 0.2}\n- Crystallization threshold $U_{\\min}$: {0.01, 0.05, 0.1}\n- Motif length: {2, 4, 6, 8}\n\n### Evaluation Tasks\n\n**Short-term reasoning**:\n- HellaSwag (commonsense)\n- PIQA (physical reasoning)\n- WinoGrande (coreference)\n\n**Long-term reasoning**:\n- bAbI tasks (require multi-hop)\n- SQuAD (reading comprehension)\n- Custom copy/induction tasks\n\n**Expert utilization**:\n- Track which motifs activate for which task types\n- Visualize routing path trees\n- Measure task-motif specialization\n\n---\n\n## \u26a0\ufe0f Risk Analysis & Mitigation\n\n### Risk 1: Catastrophic Forgetting\n**Description**: Frozen motifs become stale as data distribution shifts.\n\n**Likelihood**: High  \n**Impact**: Critical (model performance degrades)\n\n**Mitigation**:\n- **Drift detection**: Monitor per-motif utility over sliding window\n- **Adaptive unfreezing**: Unfreeze motifs if $U(\\pi)$ drops below threshold\n- **Rehearsal buffer**: Store examples that activated each motif, replay periodically\n- **Conditional crystallization**: Only freeze motifs validated on diverse tasks\n\n### Risk 2: Router Collapse\n**Description**: Crystallization causes all tokens to route through same few motifs.\n\n**Likelihood**: Medium  \n**Impact**: High (loss of model capacity)\n\n**Mitigation**:\n- **Strong load balancing**: Maintain $\\beta_1 \\geq 0.01$ throughout\n- **Capacity limits**: Enforce maximum tokens per expert per batch\n- **Diversity bonus**: Add entropy bonus to routing loss during crystallization phase\n- **Progressive crystallization**: Freeze motifs gradually (top-1, then top-5, etc.)\n\n### Risk 3: Memory Overflow\n**Description**: Trace memory exceeds quota, causing OOM.\n\n**Likelihood**: Low (hard limits in place)  \n**Impact**: Critical (training crash)\n\n**Mitigation**:\n- **Hard quotas**: Strictly enforce `quota_per_head` limits\n- **Competitive eviction**: Lowest-salience traces evicted first\n- **Automatic decay tuning**: Increase $\\gamma$ if memory pressure detected\n- **Graceful degradation**: Disable tracing if memory allocation fails\n\n### Risk 4: Flash Attention Incompatibility\n**Description**: Trace capture breaks with fused kernels.\n\n**Likelihood**: Low (dual-mode designed)  \n**Impact**: Medium (slower training)\n\n**Mitigation**:\n- **Dual-mode attention**: Standard for capture, Flash for speed\n- **Sparse capture schedule**: Only enable standard attention every 100 steps\n- **Post-hoc approximation**: Estimate salience from gradients without explicit weights\n\n### Risk 5: Gradient Instability\n**Description**: Auxiliary losses destabilize training.\n\n**Likelihood**: Medium  \n**Impact**: Medium (slower convergence)\n\n**Mitigation**:\n- **Conservative coefficients**: Start with $\\beta_2, \\beta_3 < 0.01$\n- **Gradual ramp-up**: Linearly increase loss weights over training\n- **Gradient clipping**: Clip auxiliary loss gradients separately\n- **Ablation-based tuning**: Empirically find stable coefficient ranges\n\n### Risk 6: Emergent Language Failure\n**Description**: Model doesn't develop meaningful internal primitives.\n\n**Likelihood**: Medium (pioneering work)  \n**Impact**: Low (graceful degradation to baseline)\n\n**Mitigation**:\n- **Baseline guarantees**: System degrades to standard transformer if crystallization fails\n- **Interpretability tools**: Visualize motif activations to detect failure early\n- **Manual seeding**: Option to manually specify useful routing patterns\n- **Curriculum design**: Carefully design tasks that encourage motif discovery\n\n---\n\n## \ud83d\uddfa\ufe0f Implementation Roadmap\n\n### Phase 0: Infrastructure (Week 1-2)\n**Deliverables**:\n- [ ] Create `src/aios/core/hrm_models/cognitive/` module\n- [ ] Implement `TraceManager` class with sparse storage\n- [ ] Implement `RoutingPathTree` suffix tree\n- [ ] Add configuration schemas to `config/default.yaml`\n- [ ] Unit tests for data structures\n\n**Files**:\n```\nsrc/aios/core/hrm_models/cognitive/\n    __init__.py\n    trace_manager.py\n    routing_tree.py\n    crystallization.py\n    config.py\ntests/core/hrm_models/cognitive/\n    test_trace_manager.py\n    test_routing_tree.py\n```\n\n### Phase 1: Attention Trace Capture (Week 3-4)\n**Deliverables**:\n- [ ] Modify `Attention` class to support trace hooks\n- [ ] Implement salience score computation\n- [ ] Add gradient-based trace updates\n- [ ] Integration with existing training loop\n- [ ] Memory profiling and optimization\n\n**Modified Files**:\n- `src/aios/core/hrm_models/impl/layers.py`\n- `src/aios/cli/hrm_hf/training_loop.py`\n\n**Tests**:\n- Memory footprint < 50 MB for 32-layer model\n- Trace capture doesn't slow training > 5%\n- Salience scores correlate with task loss gradients\n\n### Phase 2: Persistent Bias Injection (Week 5-6)\n**Deliverables**:\n- [ ] Implement sparse trace \u2192 attention bias conversion\n- [ ] Add dual-mode attention (standard/Flash)\n- [ ] Trace update scheduling\n- [ ] Exponential decay mechanism\n- [ ] Visualization tools for traces\n\n**Tests**:\n- Bias injection improves convergence on copy tasks\n- Flash Attention speedup maintained (> 90% of time)\n- Trace stability across training runs\n\n### Phase 2.5: Vector Store Integration (Week 6.5-7.5) [OPTIONAL]\n**Prerequisites**: PF-005 vector store implementation complete (Weeks 1-6)\n\n**Deliverables**:\n- [ ] Implement `TraceEmbedder` class (sparse trace \u2192 dense vector)\n- [ ] Implement `MotifEmbedder` class (expert sequence \u2192 dense vector)\n- [ ] Create `TraceVectorStore` wrapper around VectorStoreClient\n- [ ] Create `MotifVectorStore` wrapper around VectorStoreClient\n- [ ] Add `sync_to_vector_store()` and `load_from_vector_store()` to TraceManager\n- [ ] Add `persist_motifs()` to RoutingPathTree\n- [ ] Update configuration schema with vector store integration flags\n- [ ] Integration tests for trace/motif persistence and retrieval\n\n**Modified Files**:\n- `src/aios/core/hrm_models/cognitive/trace_manager.py`\n- `src/aios/core/hrm_models/cognitive/routing_tree.py`\n- `src/aios/core/hrm_models/cognitive/embedders.py` (new)\n- `src/aios/core/hrm_models/cognitive/vector_wrappers.py` (new)\n- `config/default.yaml`\n\n**Tests**:\n- Trace embedding preserves similarity (nearby traces have high cosine similarity)\n- Motif embedding distinguishes task types (retrieval vs generation motifs cluster separately)\n- Sync/load cycle preserves trace salience within 1% error\n- Cross-session warm-start improves initial perplexity vs cold start\n- Memory overhead < 5 MB additional (for embedding models)\n\n**Acceptance Criteria**:\n- TraceManager can persist 100K traces and reload with < 2% information loss\n- MotifVectorStore correctly retrieves top-10 similar motifs for given input\n- Integration works with both Qdrant and LanceDB backends\n- Disabling vector store (config flag) causes graceful fallback to RAM-only mode\n\n**Notes**:\n- This phase can be skipped if vector store integration not needed\n- All subsequent phases work with or without vector store enabled\n- See `data-backends-vector-stores.md` for VectorStoreClient API details\n\n### Phase 3: Routing Path Logging (Week 7-8)\n**Deliverables**:\n- [ ] Modify `TopKRouter` to log expert selections\n- [ ] Build suffix tree from routing paths\n- [ ] Compute path frequency and utility\n- [ ] Implement competitive motif discovery\n- [ ] Path visualization tools\n\n**Modified Files**:\n- `src/aios/core/hrm_models/moe_layer.py`\n- `src/aios/core/hrm_models/dynamic_moe/dynamic_layer.py`\n\n**Tests**:\n- Suffix tree correctly tracks all paths\n- Utility scores correlate with task performance\n- Tree memory < 10 MB\n\n### Phase 4: Semantic Crystallization (Week 9-11)\n**Deliverables**:\n- [ ] Implement motif freezing mechanism\n- [ ] Create distillation process for crystallized experts\n- [ ] Add competitive pruning\n- [ ] Integrate with expert registry\n- [ ] Motif specialization analysis tools\n\n**Tests**:\n- Crystallized motifs reduce FLOPs\n- Frozen motifs maintain performance\n- Pruning doesn't cause catastrophic forgetting\n\n### Phase 5: Auxiliary Losses (Week 12-13)\n**Deliverables**:\n- [ ] Implement trace utilization loss\n- [ ] Implement crystallization entropy loss\n- [ ] Add EWC for stability\n- [ ] Hyperparameter tuning\n- [ ] Loss curve analysis\n\n**Tests**:\n- Auxiliary losses don't destabilize training\n- Combined loss converges faster than baseline\n- Ablations validate each component\n\n### Phase 6: Evaluation & Analysis (Week 14-16)\n**Deliverables**:\n- [ ] Run full baseline comparisons\n- [ ] Long-range dependency benchmarks\n- [ ] FLOP reduction measurements\n- [ ] Emergent language analysis\n- [ ] Documentation and research writeup\n\n**Benchmarks**:\n- bAbI tasks, SQuAD, HellaSwag, MMLU\n- Custom induction head tasks\n- Latency profiling\n\n### Phase 7: Production Hardening (Week 17-18)\n**Deliverables**:\n- [ ] Memory overflow safeguards\n- [ ] Gradient checkpointing full compatibility\n- [ ] Distributed training support (DeepSpeed)\n- [ ] Configuration validation\n- [ ] User documentation\n\n**Tests**:\n- Multi-GPU training stability\n- Recovery from OOM gracefully\n- Configuration validation catches errors\n\n---\n\n## \ud83d\udcda References & Prior Art\n\n### Foundational Papers\n\n**Memory-Augmented Transformers**:\n1. Dai et al. (2019). \"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.\" arXiv:1901.02860\n2. Rae et al. (2019). \"Compressive Transformers for Long-Range Sequence Modelling.\" arXiv:1911.05507\n3. Wu et al. (2022). \"Memorizing Transformers.\" arXiv:2203.08913\n4. Borgeaud et al. (2022). \"Improving language models by retrieving from trillions of tokens.\" arXiv:2112.04426 (RETRO)\n\n**Mixture of Experts**:\n5. Shazeer et al. (2017). \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.\" arXiv:1701.06538\n6. Lepikhin et al. (2020). \"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.\" arXiv:2006.16668\n7. Fedus et al. (2021). \"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.\" arXiv:2101.03961\n\n**Continual Learning & Consolidation**:\n8. Kirkpatrick et al. (2017). \"Overcoming catastrophic forgetting in neural networks.\" arXiv:1612.00796 (EWC)\n9. Zenke et al. (2017). \"Continual Learning Through Synaptic Intelligence.\" arXiv:1703.04200\n10. Schwarz et al. (2018). \"Progress & Compress: A scalable framework for continual learning.\" arXiv:1805.06370\n\n**Neural Architecture Search & Plasticity**:\n11. Frankle & Carbin (2019). \"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.\" arXiv:1803.03635\n12. Evci et al. (2020). \"Rigging the Lottery: Making All Tickets Winners.\" arXiv:1911.11134 (RigL)\n13. Liu et al. (2019). \"DARTS: Differentiable Architecture Search.\" arXiv:1806.09055\n\n**Neuroscience Foundations**:\n14. Bliss & L\u00f8mo (1973). \"Long-lasting potentiation of synaptic transmission in the dentate area of the anaesthetized rabbit following stimulation of the perforant path.\" Journal of Physiology.\n15. McClelland et al. (1995). \"Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory.\" Psychological Review.\n\n### Repositories\n\n- KRONOS Memory-Augmented Transformer: https://github.com/agentic-labs/KRONOS\n- Hyperbolic Neural Networks: https://github.com/HazyResearch/hgcn\n- PyTorch Sparse Training (RigL): https://github.com/google-research/rigl\n- Memorizing Transformer: https://github.com/lucidrains/memorizing-transformers-pytorch\n\n---\n\n## \ud83c\udfaf Success Criteria\n\n### Minimum Viable Product (MVP)\n- [ ] Trace memory overhead < 50 MB\n- [ ] Training slowdown < 10%\n- [ ] Inference speedup \u2265 5% on familiar patterns\n- [ ] No catastrophic forgetting on standard benchmarks\n\n### Target Goals\n- [ ] Perplexity improvement \u2265 5% on long-context tasks\n- [ ] FLOP reduction \u2265 15% from crystallized motifs\n- [ ] Trace coverage \u2265 30% of attention operations\n- [ ] \u2265 50 stable crystallized motifs after full training\n\n### Stretch Goals\n- [ ] Emergent hierarchical motif structure (\u2265 3 levels deep)\n- [ ] Task-specific motif specialization (interpretable)\n- [ ] Zero-shot transfer of motifs to new tasks\n- [ ] Publishable research contribution (novel technique)\n\n---\n\n## \ud83d\udd2c Open Research Questions\n\n1. **Trace Generalization**: Do traces learned on one domain transfer to others?\n2. **Optimal Sparsity**: What is the Pareto frontier of trace density vs. performance?\n3. **Crystallization Depth**: What is the optimal motif length for different task types?\n4. **Emergent Compositionality**: Can motifs combine to form higher-order primitives?\n5. **Cross-Attention Traces**: Do traces in cross-attention (encoder-decoder) behave differently?\n6. **Adaptive Decay**: Can decay rates be learned per-trace rather than global?\n7. **Motif Transfer Learning**: Can crystallized motifs be transferred between models?\n8. **Interpretability**: What do crystallized motifs \"mean\" in human-understandable terms?\n\n---\n\n## \ud83d\udcdd Notes & Caveats\n\n**This is pioneering research** - no guarantees of success. The system is designed with graceful degradation: if traces/crystallization fail to improve performance, it degrades to standard transformer behavior.\n\n**Computational cost**: Initial implementation may be slower due to dual-mode attention and logging overhead. Optimizations will come in later phases.\n\n**Hyperparameter sensitivity**: Many new hyperparameters introduced. Extensive tuning required.\n\n**Interpretability**: Emergent internal language may be completely alien to human understanding - visualization tools critical.\n\n**Distributed training**: Initial implementation focuses on single-GPU. Multi-GPU support requires careful synchronization of trace/motif state.\n\n**Production readiness**: This is a research feature. Full productionization (reliability, edge cases, monitoring) will require significant additional work.\n\n---\n\n**Status**: Ready for Phase 0 implementation  \n**Next Steps**: Create infrastructure module and unit tests  \n**Owner**: AI-OS Core Team  \n**Timeline**: ~18 weeks for full implementation\n", "tags": ["experts", "training"], "headings": [{"line": 0, "text": "Persistent Attention Traces & Semantic Crystallization"}, {"line": 16, "text": "\ud83d\udccb Executive Summary"}, {"line": 18, "text": "The Vision"}, {"line": 27, "text": "The Problem"}, {"line": 33, "text": "The Solution"}, {"line": 36, "text": "Component 1: Persistent Attention Traces"}, {"line": 43, "text": "Component 2: Semantic Crystallization"}, {"line": 50, "text": "Expected Outcomes"}, {"line": 68, "text": "\ud83e\udde0 Theoretical Foundation"}, {"line": 70, "text": "Biological Inspiration"}, {"line": 85, "text": "Mathematical Formulation"}, {"line": 87, "text": "1. Persistent Attention Traces"}, {"line": 136, "text": "2. Semantic Crystallization"}, {"line": 187, "text": "\ud83d\udcbe Memory-Efficient Implementation Design"}, {"line": 189, "text": "Critical Constraints"}, {"line": 195, "text": "Trace Storage Data Structure"}, {"line": 208, "text": "Total: 12 bytes per trace"}, {"line": 239, "text": "Configuration"}, {"line": 246, "text": "Per-layer calculation"}, {"line": 251, "text": "Full model"}, {"line": 255, "text": "BUT: We enforce stricter quota to be safe"}, {"line": 263, "text": "Routing Path Storage (Suffix Tree)"}, {"line": 345, "text": "\ud83d\uddc4\ufe0f Vector Store Integration (Optional Enhancement)"}, {"line": 347, "text": "Overview"}, {"line": 356, "text": "Architecture"}, {"line": 379, "text": "Trace Embedding Specification"}, {"line": 389, "text": "Learned embeddings for trace components"}, {"line": 396, "text": "Component embeddings"}, {"line": 402, "text": "Weighted by salience"}, {"line": 421, "text": "Motif Embedding Specification"}, {"line": 441, "text": "Embed each expert in sequence"}, {"line": 445, "text": "Encode sequence"}, {"line": 449, "text": "Weight by utility"}, {"line": 466, "text": "Synchronization Protocol"}, {"line": 482, "text": "Convert traces to embeddings"}, {"line": 487, "text": "Upsert to vector store"}, {"line": 497, "text": "Query for relevant traces (if task_tag specified)"}, {"line": 501, "text": "Reconstruct traces from metadata"}, {"line": 518, "text": "Configuration Integration"}, {"line": 523, "text": "Vector store backend (PF-005)"}, {"line": 533, "text": "Persistent attention traces"}, {"line": 545, "text": "Vector store integration (optional)"}, {"line": 552, "text": "Semantic crystallization"}, {"line": 561, "text": "Vector store integration (optional)"}, {"line": 567, "text": "Benefits of Integration"}, {"line": 588, "text": "\ud83d\udd27 Architecture Integration (HRM-ACTV1 Specific)"}, {"line": 590, "text": "Current Architecture Analysis"}, {"line": 602, "text": "... qkv projection ..."}, {"line": 604, "text": "Standard scaled dot-product attention"}, {"line": 608, "text": "Apply attention"}, {"line": 621, "text": "Integration Points"}, {"line": 623, "text": "Hook 1: Attention Trace Capture (Post-Attention)"}, {"line": 631, "text": "... existing init ..."}, {"line": 635, "text": "... existing QKV projection and RoPE ..."}, {"line": 637, "text": "Compute attention weights"}, {"line": 640, "text": "INTEGRATION POINT 1: Inject persistent trace bias"}, {"line": 652, "text": "INTEGRATION POINT 2: Capture high-salience traces (training only)"}, {"line": 654, "text": "Detach to avoid interfering with backprop"}, {"line": 663, "text": "... rest of attention computation ..."}, {"line": 668, "text": "In training loop, after loss.backward()"}, {"line": 675, "text": "Hook 2: Routing Path Logging (Post-Router)"}, {"line": 683, "text": "... existing init ..."}, {"line": 691, "text": "INTEGRATION POINT 3: Log routing paths for crystallization"}, {"line": 693, "text": "Log chosen experts per token"}, {"line": 704, "text": "Hook 3: Auxiliary Loss Computation"}, {"line": 711, "text": "Standard forward pass"}, {"line": 715, "text": "Existing MoE load balancing loss"}, {"line": 721, "text": "NEW: Trace reuse auxiliary loss (encourage using persistent traces)"}, {"line": 726, "text": "NEW: Crystallization stability loss (low entropy for stable paths)"}, {"line": 731, "text": "Combined loss"}, {"line": 747, "text": "Flash Attention Compatibility"}, {"line": 755, "text": "Trace capture requires explicit attention matrices"}, {"line": 757, "text": "Use standard attention (slower but exposes weights)"}, {"line": 760, "text": "Use Flash Attention (faster)"}, {"line": 769, "text": "Gradient Checkpointing Compatibility"}, {"line": 775, "text": "Every UPDATE_INTERVAL steps (e.g., 100)"}, {"line": 778, "text": "Forward-only pass with trace capture enabled"}, {"line": 780, "text": "trace_manager accumulates observations"}, {"line": 782, "text": "Update trace memory based on accumulated stats"}, {"line": 788, "text": "\ud83d\udcca Training Protocol"}, {"line": 790, "text": "Loss Function Components"}, {"line": 803, "text": "1. Trace Utilization Loss"}, {"line": 813, "text": "2. Crystallization Stability Loss"}, {"line": 823, "text": "Hyperparameters"}, {"line": 862, "text": "Training Curriculum"}, {"line": 890, "text": "Stability Safeguards"}, {"line": 909, "text": "\ud83d\udcc8 Evaluation Framework"}, {"line": 911, "text": "Metrics"}, {"line": 913, "text": "Primary Metrics (Task Performance)"}, {"line": 919, "text": "Trace-Specific Metrics"}, {"line": 925, "text": "Crystallization Metrics"}, {"line": 931, "text": "Emergent Language Metrics (Exploratory)"}, {"line": 936, "text": "Baselines"}, {"line": 943, "text": "Ablation Studies"}, {"line": 960, "text": "Evaluation Tasks"}, {"line": 979, "text": "\u26a0\ufe0f Risk Analysis & Mitigation"}, {"line": 981, "text": "Risk 1: Catastrophic Forgetting"}, {"line": 993, "text": "Risk 2: Router Collapse"}, {"line": 1005, "text": "Risk 3: Memory Overflow"}, {"line": 1017, "text": "Risk 4: Flash Attention Incompatibility"}, {"line": 1028, "text": "Risk 5: Gradient Instability"}, {"line": 1040, "text": "Risk 6: Emergent Language Failure"}, {"line": 1054, "text": "\ud83d\uddfa\ufe0f Implementation Roadmap"}, {"line": 1056, "text": "Phase 0: Infrastructure (Week 1-2)"}, {"line": 1077, "text": "Phase 1: Attention Trace Capture (Week 3-4)"}, {"line": 1094, "text": "Phase 2: Persistent Bias Injection (Week 5-6)"}, {"line": 1107, "text": "Phase 2.5: Vector Store Integration (Week 6.5-7.5) [OPTIONAL]"}, {"line": 1145, "text": "Phase 3: Routing Path Logging (Week 7-8)"}, {"line": 1162, "text": "Phase 4: Semantic Crystallization (Week 9-11)"}, {"line": 1175, "text": "Phase 5: Auxiliary Losses (Week 12-13)"}, {"line": 1188, "text": "Phase 6: Evaluation & Analysis (Week 14-16)"}, {"line": 1201, "text": "Phase 7: Production Hardening (Week 17-18)"}, {"line": 1216, "text": "\ud83d\udcda References & Prior Art"}, {"line": 1218, "text": "Foundational Papers"}, {"line": 1245, "text": "Repositories"}, {"line": 1254, "text": "\ud83c\udfaf Success Criteria"}, {"line": 1256, "text": "Minimum Viable Product (MVP)"}, {"line": 1262, "text": "Target Goals"}, {"line": 1268, "text": "Stretch Goals"}, {"line": 1276, "text": "\ud83d\udd2c Open Research Questions"}, {"line": 1289, "text": "\ud83d\udcdd Notes & Caveats"}]}, {"path": "planned_features/presidio-pii-redaction.md", "content": "## PF-006: Presidio PII redaction for datasets and logs\n\n### Summary\n\nAdd optional PII redaction using Microsoft Presidio across dataset ingestion, evaluation samples, and JSONL metrics logging. Expose simple flags and a YAML policy to control which fields and entity types to redact. Provide a small local GUI preview to tune policies.\n\n### Why this matters\n\n- Reduce risk of inadvertently storing PII in training artifacts and logs.\n- Help teams comply with stricter data policies without blocking iteration.\n- Keep defaults safe and opt-in to minimize performance overhead.\n\n---\n\n## What ships in PF-006\n\n- Utility module: `src/aios/safety/presidio_redactor.py` (Analyzer + Anonymizer pipeline)\n- Config file: `config/presidio.yaml` (with `config/presidio.yaml.example` scaffold)\n- CLI flags:\n\t- `--redact-inputs/--no-redact-inputs`\n\t- `--redact-logs/--no-redact-logs`\n\t- `--presidio-config PATH`\n- Dataset preprocessor CLI: `aios datasets-redact` to create a redacted copy under `training_data/redacted/...`\n- Logging hook: optional redaction inside the JSONL logger for sensitive payload fields\n- Optional GUI preview (Streamlit/Gradio) to interactively test redaction rules on sample text\n\n---\n\n## Architecture overview\n\nData paths affected:\n\n1) Ingestion path (training/eval data)\n\t - If `--redact-inputs` is enabled, wrap the lines loader and apply redaction per line.\n\t - Write redacted datasets with `aios datasets-redact` when needed for offline inspection.\n\n2) Logging path (metrics JSONL)\n\t - If `--redact-logs` is enabled, wrap the JSONL writer and redact whitelisted keys: `text`, `sample`, `prompt`, `completion`, `generated`, `context`, and any configured custom keys.\n\n3) Preview GUI (optional)\n\t - Small app to paste text, toggle entity types, see the anonymized output and the recognized entities before committing policy changes.\n\nCore components:\n\n- PresidioRedactor (utility): constructs AnalyzerEngine + AnonymizerEngine; exposes `redact_text` and `redact_json`.\n- YAML policy: selects entities to target, anonymizer strategies, field-level overrides, and performance knobs.\n- Typer flags: propagate redaction options and config path into training/eval CLIs and dataset tools.\n\n---\n\n## Dependencies and setup\n\nRequired packages:\n- `presidio-analyzer`\n- `presidio-anonymizer`\n- `spacy`\n- spaCy model: `en_core_web_lg` (preferred) or `en_core_web_sm` (lighter, fewer entities)\n\nWindows/PowerShell install (example):\n\n```powershell\n# Activate your venv first if needed\n# python -m venv .venv; . .\\.venv\\Scripts\\Activate.ps1\n\npip install presidio-analyzer presidio-anonymizer spacy\npython -m spacy download en_core_web_lg\n```\n\nNotes:\n- For airgapped setups, pre-download wheels and the spaCy model; update `nlp_engine` in YAML accordingly.\n- Performance: Presidio adds CPU-bound overhead; consider using `--no-redact-logs` for long runs and enabling only on CI or releases.\n\n---\n\n## Configuration schema (`config/presidio.yaml`)\n\nMinimal keys (all optional; safe defaults apply when file missing):\n\n- `enabled: true|false` \u2014 master switch for the utility (CLI flags still gate behavior per path)\n- `entities: [\"PHONE_NUMBER\", \"EMAIL_ADDRESS\", \"US_SSN\", ...]` \u2014 target entity types\n- `anonymizers:` map of entity \u2192 strategy, e.g.:\n\t- `EMAIL_ADDRESS: { type: \"replace\", new_value: \"<EMAIL>\" }`\n\t- `PHONE_NUMBER: { type: \"mask\", masking_char: \"*\", chars_to_mask: 0, from_end: true }`\n- `default_anonymizer: { type: \"replace\", new_value: \"<PII>\" }`\n- `fields:` field-level behavior for JSON/records, e.g.:\n\t- `text: { redact: true }`\n\t- `prompt: { redact: true, entities: [\"EMAIL_ADDRESS\", \"PHONE_NUMBER\"] }`\n\t- `user: { redact: false }`\n- `custom_recognizers:` list of lightweight recognizers (regex + score + name), optional.\n- `nlp_engine:` spaCy model name and language, e.g. `model: en_core_web_lg`, `lang: en`.\n- `performance:` batch size, max_workers, timeouts.\n\nSee `config/presidio.yaml.example` for a concrete, commented template.\n\n---\n\n## Utility API design (`src/aios/safety/presidio_redactor.py`)\n\nContract:\n- Inputs: `text: str` or `record: dict` with configured fields\n- Outputs: redacted text/record plus optional matches metadata\n- Error modes: if Presidio is not installed or model missing, log a warning and operate as pass-through (no redaction)\n\nAPI sketch:\n- `class PresidioRedactor:`\n\t- `__init__(config_path: Optional[str] = None, overrides: Optional[dict] = None)`\n\t- `redact_text(text: str, *, entities: Optional[list[str]] = None) -> tuple[str, list[dict]]`\n\t- `redact_json(payload: dict, *, fields: Optional[list[str]] = None) -> tuple[dict, dict]`  # (redacted_copy, matches_by_field)\n\t- Internal: lazy init AnalyzerEngine/AnonymizerEngine; caching of compiled regex; thread-safe\n\nImplementation notes:\n- Use Presidio `AnalyzerEngine` and `AnonymizerEngine` with spaCy NLP engine.\n- Apply field-level overrides from YAML (`fields.*`).\n- For JSONL payloads, only redact whitelisted keys; never mutate the original dict before writing to disk.\n\n---\n\n## CLI design (Typer)\n\n1) Training CLI: `hrm-hf train-actv1` (`src/aios/cli/hrm_hf_cli.py`)\n\nAdd options:\n- `--redact-inputs/--no-redact-inputs` (default: no-redact)\n- `--redact-logs/--no-redact-logs` (default: no-redact)\n- `--presidio-config PATH` (default: `config/presidio.yaml` if exists)\n\nPlumbing:\n- Extend `TrainingConfig` with `redact_inputs: bool`, `redact_logs: bool`, `presidio_config: Optional[str]`.\n- Pass flags through to `train_actv1_impl` (file: `src/aios/cli/hrm_hf/train_actv1.py`).\n- In `train_actv1_impl`, create `PresidioRedactor` early (rank0 only if distributed, broadcast minimal toggles) and:\n\t- Wrap line ingestion (`get_training_lines`): if `redact_inputs`, map `redact_text` over lines.\n\t- Wrap `_write_jsonl_helper`: if `redact_logs`, first redact whitelisted payload keys.\n\n2) Datasets CLI: new command `datasets-redact` (`src/aios/cli/datasets_cli.py` -> register)\n\nCommand:\n\n```powershell\naios datasets-redact `\n\t--input training_data/curated_datasets/test_sample.txt `\n\t--output training_data/redacted/test_sample.txt `\n\t--presidio-config config/presidio.yaml `\n\t--format text `\n\t--json-field text\n```\n\nBehavior:\n- Reads input (plain text or JSONL), redacts line-by-line, writes output.\n- For JSONL, use `--json-field` to pick which key to redact (default: `text`).\n- Prints summary with number of lines, redaction hits by entity type.\n\n3) Optional GUI preview: `aios redaction-preview` (future)\n\n- Minimal Streamlit/Gradio app to paste sample text and see redaction live.\n- Launch via: `aios redaction-preview --presidio-config config/presidio.yaml`.\n- Out of scope for initial PR; spec retained here for future follow-up.\n\n---\n\n## Integration details (where to add hooks)\n\n- File: `src/aios/cli/hrm_hf/train_actv1.py`\n\t- Initialize redactor once, considering distributed rank.\n\t- Wrap `_write_jsonl_helper` via local closure `_write_jsonl` to redact payload keys when `config.redact_logs`.\n\t- During ingestion, before tokenization: if `config.redact_inputs`, map lines through `redact_text`.\n\n- File: `src/aios/cli/hrm_hf/data.py`\n\t- In `get_training_lines(...)`, inject optional redactor callable to transform lines when `redact_inputs` is enabled.\n\n- File: `src/aios/cli/datasets_cli.py`\n\t- Register new command `datasets-redact` implemented in `src/aios/cli/datasets/redact_cmd.py`.\n\nField keys to consider for log redaction (configurable):\n- `text`, `sample`, `prompt`, `completion`, `generated`, `context`, `input`, `output`\n\n---\n\n## Testing and acceptance criteria\n\nUnit tests:\n- `tests/test_presidio_redaction.py`:\n\t- Email and phone in a sentence \u2192 redacted with placeholders `<EMAIL>`, masked phone.\n\t- JSON payload: only configured fields are redacted; other keys unchanged.\n- `tests/test_logging_redaction.py`:\n\t- Wrap `_write_jsonl_helper` with redaction; ensure logged file contains redacted content.\n\nIntegration checks:\n- `aios hrm-hf train-actv1 --redact-logs --log-file artifacts/brains/actv1/metrics.jsonl` produces logs without raw PII.\n- `aios datasets-redact --input ... --output ...` writes a redacted dataset copy and prints a summary.\n\nAcceptance:\n- When enabled, no raw emails/phones appear in logs or redacted datasets (verified via regex search).\n- Redaction disabled by default; enabling adds measurable but acceptable overhead on CPU-only systems.\n\n---\n\n## Risks and mitigations\n\n- False positives/negatives: document scope and provide allow/deny lists; expose `custom_recognizers`.\n- Performance overhead: keep redaction opt-in; allow selecting `en_core_web_sm`; support batch processing in the utility.\n- Internationalization: default to English model; allow switching NLP model via YAML.\n\n---\n\n## Rollout plan\n\n1) M1 (1 day): Utility + unit tests + example config\n\t - Implement `PresidioRedactor` with text and JSON helpers.\n\t - Create `config/presidio.yaml.example` and docs.\n\t - Add basic unit tests for core behaviors.\n\n2) M2 (1 day): Hooks + CLI + docs\n\t - Add CLI flags and wire into training + dataset commands.\n\t - Implement `datasets-redact` command.\n\t - Expand docs with troubleshooting and examples.\n\nOptional (follow-ups):\n- Streamlit/Gradio preview app; VS Code task to launch it.\n- Presidio recognizer registry loader from YAML (advanced patterns).\n\n---\n\n## Troubleshooting\n\n- spaCy model error: run `python -m spacy download en_core_web_lg` (or use `en_core_web_sm`).\n- Presidio not installed: utility logs a warning and passes text through unchanged.\n- Slow runs: disable `--redact-logs` for long training; keep redaction for CI or releases.\n- Entity not redacted: verify it\u2019s in `entities` and not excluded by field-level overrides.\n\n---\n\n## Quickstart (Windows/PowerShell)\n\n```powershell\n# 1) Install deps\npip install presidio-analyzer presidio-anonymizer spacy\npython -m spacy download en_core_web_lg\n\n# 2) Copy example config and tweak\nCopy-Item config/presidio.yaml.example config/presidio.yaml\n\n# 3) Redact a dataset copy (text)\naios datasets-redact `\n\t--input training_data/curated_datasets/test_sample.txt `\n\t--output training_data/redacted/test_sample.txt `\n\t--presidio-config config/presidio.yaml `\n\t--format text\n\n# 4) Run training with log redaction\naios hrm-hf train-actv1 `\n\t--model gpt2 `\n\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t--steps 10 --batch-size 2 `\n\t--redact-logs `\n\t--presidio-config config/presidio.yaml `\n\t--log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n---\n\n## Developer checklist (end-to-end)\n\n- [ ] Add `src/aios/safety/presidio_redactor.py` (utility class + lazy init)\n- [ ] Extend `TrainingConfig` with `redact_inputs`, `redact_logs`, `presidio_config`\n- [ ] Wire flags in `src/aios/cli/hrm_hf_cli.py` and plumb to `train_actv1_impl`\n- [ ] In `train_actv1_impl`, wrap `_write_jsonl_helper` with redaction when enabled\n- [ ] In `get_training_lines` and/or callsites, map redaction over lines when enabled\n- [ ] New CLI `src/aios/cli/datasets/redact_cmd.py` + register in `datasets_cli.py`\n- [ ] Add unit tests (`tests/test_presidio_redaction.py`, `tests/test_logging_redaction.py`)\n- [ ] Add `config/presidio.yaml.example`\n- [ ] Update docs and add Examples section\n\n---\n\n## Operator checklist (before enabling in prod)\n\n- [ ] Verify Presidio + spaCy installed; run the Quickstart to confirm\n- [ ] Copy `config/presidio.yaml.example` \u2192 `config/presidio.yaml` and tailor entities/placeholders\n- [ ] Dry-run `datasets-redact` on a small sample; inspect outputs\n- [ ] Enable `--redact-logs` on a short training run; inspect `metrics.jsonl`\n- [ ] Monitor performance; consider `en_core_web_sm` if needed\n\n", "tags": ["cli", "datasets", "evaluation", "gui", "training"], "headings": [{"line": 0, "text": "PF-006: Presidio PII redaction for datasets and logs"}, {"line": 2, "text": "Summary"}, {"line": 6, "text": "Why this matters"}, {"line": 14, "text": "What ships in PF-006"}, {"line": 28, "text": "Architecture overview"}, {"line": 50, "text": "Dependencies and setup"}, {"line": 61, "text": "Activate your venv first if needed"}, {"line": 62, "text": "python -m venv .venv; . .\\.venv\\Scripts\\Activate.ps1"}, {"line": 74, "text": "Configuration schema (`config/presidio.yaml`)"}, {"line": 96, "text": "Utility API design (`src/aios/safety/presidio_redactor.py`)"}, {"line": 117, "text": "CLI design (Typer)"}, {"line": 159, "text": "Integration details (where to add hooks)"}, {"line": 177, "text": "Testing and acceptance criteria"}, {"line": 196, "text": "Risks and mitigations"}, {"line": 204, "text": "Rollout plan"}, {"line": 222, "text": "Troubleshooting"}, {"line": 231, "text": "Quickstart (Windows/PowerShell)"}, {"line": 234, "text": "1) Install deps"}, {"line": 238, "text": "2) Copy example config and tweak"}, {"line": 241, "text": "3) Redact a dataset copy (text)"}, {"line": 248, "text": "4) Run training with log redaction"}, {"line": 260, "text": "Developer checklist (end-to-end)"}, {"line": 274, "text": "Operator checklist (before enabling in prod)"}]}, {"path": "planned_features/RETROFITTED_RECURRENCE_INTEGRATION.md", "content": "# Retrofitted Recurrence Integration Plan\n\nStatus: Planning\nDate: November 15, 2025\nPriority: High\nOwners: AI-OS Core (Brains, Training), Research Enablement\n\nReferences:\n- Paper: \"Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence\" (McLeish et al., arXiv:2511.07384)\n- Code: https://github.com/mcleish7/retrofitting-recurrence (Apache-2.0)\n\n---\n\n## Executive Summary\nRetrofitted recurrence converts existing transformer checkpoints into depth-recurrent models that reuse a core block multiple times at inference. The approach enables test-time compute scaling without increasing parameter count, while training remains efficient through recurrence scheduling, truncated backpropagation, and data curricula. This plan describes how AI-OS will integrate the technique so that users can convert Hugging Face (HF) and ACTV1 brains, train them with efficient curricula, and expose configurable recurrence at inference through CLI, automation flows, and the GUI.\n\nKey value:\n- Unlock higher reasoning quality per FLOP for math and logic tasks handled by AI-OS HRM pathways.\n- Provide adaptive test-time compute within existing orchestration workflows (auto-training, evaluation, deployment).\n- Offer reusable tooling for other latent-thinking techniques (e.g., adaptive depth, mixture-of-recursions).\n\n---\n\n## Objectives and Scope\nIn scope (initial release):\n- Tooling to surgically convert supported HF models (TinyLlama, Llama-3.2-1B, OLMo-2-1B) into recurrent `(prelude, recurrent block, coda)` configurations.\n- Training pipeline updates for recurrence scheduling, truncated backpropagation, and Muon optimizer support within `src/aios/core/hrm_training`.\n- Inference controls that let operators select test-time recurrence counts per request or via policy (CLI, GUI, automation jobs).\n- Evaluation harness updates to run GSM8K, MATH, MMLU, Arc across recurrence depths with reproducible FLOP reporting.\n\nDeferred (future phases):\n- Recurrence-aware versions of the custom ACTV1 architecture.\n- Automatic layer selection derived from pruning scores or learned gating.\n- Adaptive stopping criteria (data-dependent exit) and reinforcement-style training loops.\n\nOut of scope:\n- Fundamental changes to tokenizer or dataset ingestion beyond recurrence curriculum needs.\n\n---\n\n## Research Insights That Inform the Design\nHighlights from McLeish et al. (2025) and accompanying repo:\n1. **Model Surgery Pattern:** Split pretrained decoder into prelude, recurrent block, and coda; add linear adapter to combine recurrent state with prelude output. Removing mid layers while reusing later layers yields efficient recurrent cores.\n2. **Initialization Matters:** Starting from pretrained weights dramatically outperforms random init; our tooling must preserve layer stats and residual scaling.\n3. **Recurrence Scheduling:** Linearly or one-minus-sqrt scheduling of mean recurrences during training reduces FLOPs without hurting convergence.\n4. **Truncated Backprop:** Limit gradient flow through most recent recurring steps (default 8) to control memory and runtime.\n5. **Muon Optimizer:** Muon stabilizes recurrent training better than AdamW*; implementations should support both for reproducibility.\n6. **Data Curriculum:** Healing with near-original data before math-heavy fine-tuning recovers language quality.\n7. **Metrics:** Evaluate per recurrence depth and per FLOP to quantify test-time scaling benefits.\n\n---\n\n## Target Capabilities for AI-OS\n1. **Conversion Toolkit** (`src/aios/ml/recurrence_toolkit/`):\n   - Layer selection DSL driven by YAML recipe files.\n   - CLI: `aios recurrence convert --model-path ...` producing safetensors and config metadata.\n   - Async file IO to avoid blocking GUI threads during conversion.\n2. **Training Enhancements** (`src/aios/core/hrm_training/`):\n   - Config objects for recurrence scheduling, truncated BPTT depth, Muon optimizer, Poisson-Lognormal sampling.\n   - Parallel dataloader pipelines supporting mixed dataset phases (FineWeb, Nemotron math) with asynchronous prefetch.\n3. **Inference Controls**:\n   - Extend `src/aios/core/hrm_models/hf_adapter.py` and `src/aios/core/inference.py` to accept `recurrence_policy` (fixed, heuristic, dynamic).\n   - CLI flag `--test-recursions N`, GUI slider, automation policies reading from deployment configs.\n4. **Evaluation Suite** (`src/aios/core/evaluation/`):\n   - Benchmark runners that sweep recurrence counts asynchronously and record FLOPs, accuracy, latency.\n   - Integration with `tests/gui/` harness for regression checks.\n5. **Observability**:\n   - Structured logs capturing recurrence depth, effective FLOPs, truncated gradient depth, adapter stats.\n   - Diagnostics stored under `artifacts/evaluation/recurrence/`.\n\n---\n\n## Architecture Overview\n### Module Layout (proposed)\n```\nsrc/aios/ml/recurrence_toolkit/\n    __init__.py\n    conversion.py          # layer slicing, adapter injection\n    config_schemas.py      # TypedDict / pydantic schema for recipes\n    scheduler.py           # recurrence curricula helpers\n    adapters/\n        hf_llama.py\n        hf_tinyllama.py\n        hf_olmo.py\nsrc/aios/core/hrm_training/\n    recurrence_config.py   # new dataclasses for training params\n    recurrence_loop.py     # truncated BPTT, Muon optimizer wrapper\nsrc/aios/cli/\n    recurrence_cli.py      # user-facing commands\n```\n\n### Data and Control Flow\n```\nUser request (CLI/GUI)\n    -> recurrence_cli.convert_async()  [asyncio task]\n        -> load HF model + recipe\n        -> conversion.py builds recurrent model + adapter tensors\n        -> writes checkpoint + metadata to artifacts/brains/recurrence/\n\nTraining job (auto_training, hrm_hf_cli)\n    -> load recurrence_config\n    -> scheduler.sample_train_depth() yields mean recurrences per step\n    -> recurrence_loop.step() executes truncated BPTT\n    -> metrics pushed via communicator async channel\n\nInference request\n    -> inference.Route selects recurrent brain\n    -> recurrence_policy decides test recurrence per prompt\n    -> hf_adapter.forward_with_recurrence()\n    -> results streamed back via async generator\n```\n\n### Parallel and Asynchronous Execution\n- Conversion runs in background asyncio tasks tied to `core/communicator` so GUI remains responsive; heavy tensor ops offloaded to process pool where available.\n- Training dataloaders leverage `torch.distributed` with overlapping prefetch to hide IO latency; scheduler updates broadcast via async RPC.\n- Evaluation sweeps spawn parallel tasks per recurrence depth (bounded concurrency) to maximize GPU throughput without violating memory caps.\n\n---\n\n## Implementation Phases\n### Phase 0: Discovery and Scaffolding (1 week)\nChecklist:\n- [ ] Draft conversion recipes for TinyLlama, Llama-3.2-1B, OLMo.\n- [ ] Add `recurrence` section to `config/default.yaml` with feature flag off.\n- [ ] Prototype loading of Muon optimizer in existing training stack.\nExit criteria: feature gated scaffolding merged, no behavioral change when disabled.\n\n### Phase 1: Conversion Toolkit MVP (1-2 weeks)\nChecklist:\n- [ ] Implement `recurrence_toolkit.conversion` following GitHub scripts (layer slicing, adapter injection).\n- [ ] Support safetensors export with metadata JSON (prelude indices, adapter dims, state init variance).\n- [ ] Provide CLI `aios recurrence convert` and Typer completion.\n- [ ] Add unit tests referencing small HF fixtures.\nExit criteria: able to convert TinyLlama checkpoint and reload via HF adapter without training.\n\n### Phase 2: Training Pipeline Integration (2-3 weeks)\nChecklist:\n- [ ] Extend `TrainingConfig` objects with recurrence parameters (mean depth schedule, truncated steps, optimizer choice).\n- [ ] Implement Poisson-Lognormal sampler and scheduler utilities.\n- [ ] Update `train_epoch.py` to handle truncated backprop and Muon optimizer hooks.\n- [ ] Add dual-phase dataset orchestration (healing + task) with asynchronous dataset queue.\n- [ ] Provide reproducible scripts for 50B token math curriculum.\nExit criteria: training loop runs recurrent TinyLlama and logs scheduled depths and FLOPs.\n\n### Phase 3: Inference and Evaluation Controls (1-2 weeks)\nChecklist:\n- [ ] Extend HF adapter to accept recurrence depth override per forward pass.\n- [ ] Add CLI/GUI controls (slider + numeric entry) to inference panels.\n- [ ] Integrate evaluation sweeps into `aios evaluation run` command with concurrency limit.\n- [ ] Implement policy hooks for auto-inference (e.g., escalate recurrence if confidence low).\nExit criteria: evaluation report comparing recurrence depths and baseline generated automatically.\n\n### Phase 4: Observability, Docs, and Rollout (1 week)\nChecklist:\n- [ ] Structured logging and telemetry dashboards (Prometheus counters) for recurrence metrics.\n- [ ] Documentation in `docs/guide/` describing workflow end-to-end.\n- [ ] Migration guide for operators (how to convert existing HF brains).\n- [ ] Feature flag default remains off pending beta feedback.\nExit criteria: docs published, monitoring pipelines validated on staging.\n\n---\n\n## Configuration and UX Integration\n- **Config files:** add `brains.recurrence` block (conversion recipe path, default train/test recurrences, truncated depth, scheduler type).\n- **CLI:**\n  - `aios recurrence convert`\n  - `aios hrm-train --train-recurrence <mean> --truncate-depth <k>`\n  - `aios inference run --test-recurrence {auto|N}`\n- **GUI:** new section in HRM panels with:\n  - Model surgery recipe picker (loads YAML metadata).\n  - Training scheduler controls (start, end recurrences, truncated steps).\n  - Inference slider for recurrence depth; preview of estimated FLOPs.\n- **Automation:** orchestrator supports recurrence options in job definitions; asynchronous status updates include recurrence-specific metrics.\n\n---\n\n## Data, Compute, and Scheduling Considerations\n- Datasets: integrate FineWeb-Edu, Nemotron-CC-Math, Nemotron General through existing dataset registry with staged queues.\n- Healing phase executed first (FineWeb) using async pipeline to reuse cached tokenization.\n- Training hardware: optimize for 4x AMD MI300A nodes or equivalent; plan fallback for single-node RTX 4090 via reduced recurrence schedule.\n- FLOP accounting: extend budget planner (`src/aios/core/budgets.py`) to use recurrent FLOP formula `(6*N1 + 2*N2)*tokens`.\n- Checkpoint cadence: persist prelude, recurrent core, adapter, and metadata separately for selective updates.\n\n---\n\n## Testing and Validation Strategy\n1. **Unit tests:**\n   - Layer slicing correctness using synthetic transformer layers.\n   - Scheduler outputs (mean, variance) vs analytic expectations.\n2. **Integration tests:**\n   - Conversion + reload smoke test in CI using TinyLlama toy checkpoint.\n   - Training loop truncated backprop validation (ensuring gradients zero beyond window).\n3. **Evaluation regression:**\n   - Nightly job generating GSM8K accuracy table across recurrences; compare against baseline thresholds.\n4. **Performance tests:**\n   - Measure throughput vs baseline for recurrence schedule {1,2,4,8,16,32}; ensure scaling behavior matches paper trends.\n5. **GUI tests:**\n   - Selenium harness to toggle recurrence controls without deadlocking main loop (ensuring async tasks handled).\n\n---\n\n## Risks and Mitigations\n| Risk | Impact | Mitigation |\n| --- | --- | --- |\n| Conversion divergence from upstream scripts | Incorrect hidden state alignment | Cross-check hidden activations using repo comparison scripts; add automated numerical diff with tolerance <1e-5. |\n| Muon optimizer availability on Windows | Training failure for Windows users | Keep AdamW* fallback; gate Muon behind optional extra, detect platform at runtime. |\n| GPU memory pressure at high recurrence | OOM during evaluation | Default truncated depth to 8, expose config, add guard rails that cap recurrence based on available VRAM from `torch.cuda.mem_get_info`. |\n| Asynchronous conversion blocking GUI | Poor UX | Run conversions in background process pool, surface progress via communicator events. |\n| Licensing compliance | Legal risk when vendoring code | Preserve Apache-2.0 NOTICE, document provenance in `LICENSE_THIRD_PARTY.md`. |\n\n---\n\n## Success Metrics\n- Converted recurrent model achieves >= +5 absolute GSM8K accuracy at test recurrence 32 compared to non-recurrent baseline (matching paper trends).\n- Training throughput within 15 percent of target (Poisson-Lognormal schedule, truncated depth 8) relative to fixed depth baseline per FLOP.\n- Inference latency scaling linear with recurrence depth and no crashes across 1,2,4,8,16,32 settings.\n- Automation job success rate unchanged; logs include recurrence metrics in 95 percent of runs.\n- User satisfaction: positive feedback from at least three internal evaluators after beta trial.\n\n---\n\n## Immediate Next Actions\n1. Circulate this plan for maintainer review; collect sign-off on scope and resource estimates.\n2. Draft initial conversion recipes by inspecting upstream scripts for TinyLlama and Llama.\n3. Create feature flag entries and placeholder modules for recurrence toolkit.\n4. Schedule spike to integrate Muon optimizer with current training loops and measure baseline stability.\n\n---\n\n## Appendix A: Open Questions\n- Preferred format for storing recurrence metadata (YAML vs JSON) when publishing converted brains.\n- Whether to support dynamic recurrence policies (confidence-based) in initial release or defer.\n- Alignment with ACTV1 roadmap: should we plan for adapter hooks now or schedule follow-up RFC?\n\n---\n\nDocument version: 0.1 (draft)", "tags": ["cli", "datasets", "evaluation", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Retrofitted Recurrence Integration Plan"}, {"line": 13, "text": "Executive Summary"}, {"line": 23, "text": "Objectives and Scope"}, {"line": 40, "text": "Research Insights That Inform the Design"}, {"line": 52, "text": "Target Capabilities for AI-OS"}, {"line": 72, "text": "Architecture Overview"}, {"line": 73, "text": "Module Layout (proposed)"}, {"line": 91, "text": "Data and Control Flow"}, {"line": 112, "text": "Parallel and Asynchronous Execution"}, {"line": 119, "text": "Implementation Phases"}, {"line": 120, "text": "Phase 0: Discovery and Scaffolding (1 week)"}, {"line": 127, "text": "Phase 1: Conversion Toolkit MVP (1-2 weeks)"}, {"line": 135, "text": "Phase 2: Training Pipeline Integration (2-3 weeks)"}, {"line": 144, "text": "Phase 3: Inference and Evaluation Controls (1-2 weeks)"}, {"line": 152, "text": "Phase 4: Observability, Docs, and Rollout (1 week)"}, {"line": 162, "text": "Configuration and UX Integration"}, {"line": 176, "text": "Data, Compute, and Scheduling Considerations"}, {"line": 185, "text": "Testing and Validation Strategy"}, {"line": 201, "text": "Risks and Mitigations"}, {"line": 212, "text": "Success Metrics"}, {"line": 221, "text": "Immediate Next Actions"}, {"line": 229, "text": "Appendix A: Open Questions"}]}, {"path": "planned_features/SEAL_INTEGRATION_PLAN.md", "content": "# SEAL Integration Implementation Plan\n**AI-OS Enhancement: Self-Adapting Language Models**\n\n**Document Version**: 1.0  \n**Created**: January 15, 2025  \n**Status**: Planning Phase  \n**Owner**: AI-OS Development Team  \n**Priority**: HIGH (Phase 1), MEDIUM (Phase 2), OPTIONAL (Phase 3)\n\n---\n\n> Note: References to `docs/user_guide/*` in this planning document are placeholders for future user-facing docs. For current information, use `docs/INDEX.md` and guides under `docs/guide/`.\n\n\n## \ud83d\udccb Table of Contents\n\n1. [Executive Summary](#executive-summary)\n2. [Phase 1: Self-Edit Data Generation](#phase-1-self-edit-data-generation-weeks-1-4)\n3. [Phase 2: RL Optimization](#phase-2-rl-optimization-weeks-5-14)\n4. [Phase 3: Advanced Meta-Learning](#phase-3-advanced-meta-learning-weeks-15)\n5. [Dependencies & Prerequisites](#dependencies-prerequisites)\n6. [Success Metrics](#success-metrics)\n7. [Risk Management](#risk-management)\n8. [Testing Strategy](#testing-strategy)\n9. [Documentation Requirements](#documentation-requirements)\n10. [Rollout Plan](#rollout-plan)\n\n---\n\n## Executive Summary\n\n**Goal**: Integrate SEAL (Self-Adapting Language Models) framework into AI-OS to enable autonomous expert training with synthetic data generation and reinforcement learning optimization.\n\n**Expected Benefits**:\n- Phase 1: 10-20% improvement in expert quality\n- Phase 2: 15-25% improvement with RL optimization\n- Phase 3: Full meta-learning and self-improvement capabilities\n\n**Total Estimated Effort**: 200-300 hours across 14-20 weeks\n\n**Key Deliverables**:\n- Self-edit generation system\n- RL optimization framework (optional)\n- Enhanced AutoTrainingOrchestrator\n- Comprehensive testing suite\n- User documentation\n\n---\n\n## Phase 1: Self-Edit Data Generation (Weeks 1-4)\n\n**Objective**: Implement SEAL's synthetic data generation to augment expert training datasets\n\n**Effort**: 40-60 hours  \n**Risk Level**: LOW  \n**Priority**: HIGH  \n**Dependencies**: None (can start immediately)\n\n---\n\n### Week 1: Foundation & Core Classes\n\n#### 1.1 Project Setup\n**Duration**: 2-3 hours\n\n- [ ] **Create directory structure**\n  ```\n  src/aios/core/seal/\n  \u251c\u2500\u2500 __init__.py\n  \u251c\u2500\u2500 self_edit_generator.py\n  \u251c\u2500\u2500 prompts.py\n  \u251c\u2500\u2500 strategies.py\n  \u2514\u2500\u2500 cache.py\n  \n  tests/test_seal/\n  \u251c\u2500\u2500 __init__.py\n  \u251c\u2500\u2500 test_self_edit_generator.py\n  \u251c\u2500\u2500 test_strategies.py\n  \u2514\u2500\u2500 fixtures/\n      \u2514\u2500\u2500 sample_passages.txt\n  ```\n\n- [ ] **Add SEAL dependencies to requirements**\n  - File: `pyproject.toml`\n  - Add dependencies:\n    ```toml\n    # SEAL Integration\n    \"sentence-transformers>=2.2.0\",  # For quality filtering\n    \"rouge-score>=0.1.2\",             # For diversity metrics\n    ```\n\n- [ ] **Update configuration schema**\n  - File: `config/default.yaml`\n  - Add SEAL configuration section (see below)\n\n- [ ] **Create feature flag**\n  - File: `src/aios/core/config/feature_flags.py`\n  - Add `ENABLE_SEAL_SELF_EDITS = True`\n\n#### 1.2 Prompts Module\n**Duration**: 3-4 hours  \n**File**: `src/aios/core/seal/prompts.py`\n\n- [ ] **Define prompt templates**\n  ```python\n  # Implement these prompt strategies from SEAL paper:\n  - IMPLICATIONS_PROMPT\n  - IMPLICATIONS_LONG_PROMPT\n  - IMPLICATIONS_VERY_LONG_PROMPT\n  - REWRITE_PROMPT\n  - SELF_QA_PROMPT\n  - IMPLICATIONS_COT_PROMPT\n  ```\n\n- [ ] **Create PromptBuilder class**\n  - [ ] Method: `build_prompt(passage: str, strategy: str) -> str`\n  - [ ] Method: `format_system_message() -> str`\n  - [ ] Method: `format_few_shot_examples() -> List[Dict]`\n  - [ ] Validation for unknown strategies\n\n- [ ] **Add prompt customization**\n  - [ ] Support for user-defined prompts\n  - [ ] Prompt template variables (passage, domain, etc.)\n  - [ ] Load prompts from config file\n\n- [ ] **Write unit tests**\n  - [ ] Test all 6 prompt strategies\n  - [ ] Test prompt formatting\n  - [ ] Test edge cases (empty passage, very long passage)\n\n#### 1.3 Strategies Module\n**Duration**: 4-5 hours  \n**File**: `src/aios/core/seal/strategies.py`\n\n- [ ] **Define SelfEditStrategy base class**\n  ```python\n  class SelfEditStrategy(ABC):\n      @abstractmethod\n      def generate(self, passage: str, model, tokenizer, **kwargs) -> List[str]:\n          pass\n      \n      @abstractmethod\n      def parse_output(self, raw_output: str) -> List[str]:\n          pass\n  ```\n\n- [ ] **Implement ImplicationsStrategy**\n  - [ ] Generate method with temperature sampling\n  - [ ] Parse output by newlines\n  - [ ] Filter empty/invalid implications\n  - [ ] Deduplication logic\n\n- [ ] **Implement RewriteStrategy**\n  - [ ] Multiple rewrite variants\n  - [ ] Parse by delimiter\n  - [ ] Ensure rewrites are different from original\n\n- [ ] **Implement SelfQAStrategy**\n  - [ ] Generate Q&A pairs\n  - [ ] Parse questions and answers separately\n  - [ ] Format as training pairs\n  - [ ] Validate Q&A structure\n\n- [ ] **Add strategy registry**\n  - [ ] Registry dict: strategy_name -> Strategy class\n  - [ ] Factory method: `get_strategy(name: str) -> SelfEditStrategy`\n\n- [ ] **Write unit tests**\n  - [ ] Test each strategy independently\n  - [ ] Mock model generation\n  - [ ] Test parsing with various formats\n  - [ ] Test edge cases\n\n#### 1.4 Core Generator Class\n**Duration**: 6-8 hours  \n**File**: `src/aios/core/seal/self_edit_generator.py`\n\n- [ ] **Implement SelfEditGenerator class**\n  ```python\n  class SelfEditGenerator:\n      def __init__(\n          self,\n          model: nn.Module,\n          tokenizer: PreTrainedTokenizer,\n          strategy: str = \"implications\",\n          num_edits: int = 5,\n          temperature: float = 1.0,\n          max_length: int = 1024,\n          cache_dir: Optional[str] = None,\n      ):\n          # Implementation\n  ```\n\n- [ ] **Core generation method**\n  - [ ] `generate_self_edits(passage: str, **kwargs) -> List[str]`\n  - [ ] Batched generation for efficiency\n  - [ ] Error handling for generation failures\n  - [ ] Timeout handling (max 30s per edit)\n  - [ ] GPU memory management\n\n- [ ] **Quality filtering**\n  - [ ] Filter by minimum length (20 tokens)\n  - [ ] Filter by maximum length (2048 tokens)\n  - [ ] Detect and remove duplicates\n  - [ ] Detect and remove low-quality outputs (gibberish)\n  - [ ] Optional: Semantic similarity filtering\n\n- [ ] **Caching mechanism**\n  - [ ] Cache key: hash(passage + strategy + model_name)\n  - [ ] Save to disk: `{cache_dir}/{hash}.json`\n  - [ ] Load from cache if available\n  - [ ] Cache invalidation strategy\n  - [ ] Max cache size limit (10GB default)\n\n- [ ] **Logging and telemetry**\n  - [ ] Log generation time per passage\n  - [ ] Log cache hit/miss rates\n  - [ ] Log quality filter statistics\n  - [ ] Track total tokens generated\n\n- [ ] **Write unit tests**\n  - [ ] Test with mock model\n  - [ ] Test caching behavior\n  - [ ] Test quality filtering\n  - [ ] Test error handling\n  - [ ] Test with real small model (optional)\n\n#### 1.5 Configuration Integration\n**Duration**: 2-3 hours  \n**File**: `config/default.yaml`\n\n- [ ] **Add SEAL configuration section**\n  ```yaml\n  seal:\n    # Self-Edit Generation\n    enabled: true\n    \n    # Strategy Selection\n    strategy: \"implications\"  # implications, rewrite, self-qa, implications-long\n    num_edits_per_passage: 5\n    temperature: 1.0\n    max_length: 1024\n    \n    # Quality Filtering\n    min_length: 20\n    max_length: 2048\n    enable_deduplication: true\n    enable_quality_filter: true\n    similarity_threshold: 0.9  # For deduplication\n    \n    # Caching\n    enable_cache: true\n    cache_dir: \"artifacts/seal_cache\"\n    max_cache_size_gb: 10\n    \n    # Performance\n    batch_size: 4\n    max_generation_time: 30  # seconds per edit\n    \n    # Advanced (Phase 2)\n    enable_rl_optimization: false\n    restem_iterations: 2\n    restem_batch_size: 50\n    restem_candidates_per_passage: 5\n  ```\n\n- [ ] **Validation schema**\n  - File: `src/aios/core/config/schemas.py`\n  - Add SEAL config validation\n  - Type checking for all fields\n  - Range validation (e.g., temperature 0.1-2.0)\n\n---\n\n### Week 2: Integration with AutoTrainingOrchestrator\n\n#### 2.1 Refactor AutoTrainingOrchestrator\n**Duration**: 6-8 hours  \n**File**: `src/aios/core/training/auto_training_orchestrator.py`\n\n- [ ] **Add SelfEditGenerator to orchestrator**\n  ```python\n  class AutoTrainingOrchestrator:\n      def __init__(self, config):\n          # ... existing code ...\n          \n          if config.seal.enabled:\n              self.self_edit_generator = SelfEditGenerator(\n                  model=self.base_model,\n                  tokenizer=self.tokenizer,\n                  strategy=config.seal.strategy,\n                  num_edits=config.seal.num_edits_per_passage,\n                  cache_dir=config.seal.cache_dir,\n              )\n          else:\n              self.self_edit_generator = None\n  ```\n\n- [ ] **Modify dataset preparation pipeline**\n  - [ ] Existing method: `prepare_training_data(dataset_path: str)`\n  - [ ] Add parameter: `augment_with_self_edits: bool = True`\n  - [ ] Load original passages\n  - [ ] Generate self-edits for each passage\n  - [ ] Combine original + self-edits\n  - [ ] Return augmented dataset\n\n- [ ] **Add progress tracking**\n  - [ ] Progress bar for self-edit generation\n  - [ ] ETA calculation\n  - [ ] Intermediate saving (every 100 passages)\n  - [ ] Resume capability if interrupted\n\n- [ ] **Error handling**\n  - [ ] Graceful fallback if generation fails\n  - [ ] Retry logic (max 3 retries per passage)\n  - [ ] Log failures for debugging\n  - [ ] Continue with original data if all retries fail\n\n- [ ] **Write unit tests**\n  - [ ] Test augmentation pipeline\n  - [ ] Test with SEAL enabled/disabled\n  - [ ] Test error handling\n  - [ ] Test progress tracking\n\n#### 2.2 Update Expert Training Pipeline\n**Duration**: 4-5 hours  \n**File**: `src/aios/core/training/expert_trainer.py`\n\n- [ ] **Modify train_expert() function**\n  - [ ] Add parameter: `use_self_edits: bool = True`\n  - [ ] Integrate self-edit generation before training\n  - [ ] Log augmented dataset statistics\n  - [ ] Save augmented dataset for inspection\n\n- [ ] **Dataset statistics logging**\n  ```python\n  def log_dataset_stats(original, augmented):\n      # Original dataset size\n      # Augmented dataset size\n      # Augmentation ratio\n      # Average self-edits per passage\n      # Total tokens (before/after)\n  ```\n\n- [ ] **A/B testing support**\n  - [ ] Train two experts: with/without self-edits\n  - [ ] Compare validation metrics\n  - [ ] Generate comparison report\n  - [ ] Save results to `artifacts/seal_experiments/`\n\n- [ ] **Write integration tests**\n  - [ ] End-to-end training with self-edits\n  - [ ] Verify augmented dataset format\n  - [ ] Verify training completes successfully\n  - [ ] Test A/B comparison workflow\n\n#### 2.3 CLI Integration\n**Duration**: 3-4 hours  \n**File**: `src/aios/cli/aios.py`\n\n- [ ] **Add SEAL commands**\n  ```bash\n  # Generate self-edits for a dataset (standalone)\n  aios seal generate --input dataset.txt --output augmented.txt --strategy implications\n  \n  # Preview self-edits (no saving)\n  aios seal preview --input dataset.txt --num-samples 5\n  \n  # Cache management\n  aios seal cache clear\n  aios seal cache stats\n  aios seal cache prune --max-size 5GB\n  ```\n\n- [ ] **Extend train command**\n  ```bash\n  # Train expert with self-edits (default)\n  aios hrm-hf train-expert --expert-id abc123 --dataset data.txt\n  \n  # Train expert without self-edits\n  aios hrm-hf train-expert --expert-id abc123 --dataset data.txt --no-seal\n  \n  # Train with specific strategy\n  aios hrm-hf train-expert --expert-id abc123 --dataset data.txt --seal-strategy rewrite\n  ```\n\n- [ ] **Add verbose logging option**\n  - [ ] `--seal-verbose` flag\n  - [ ] Show generation progress\n  - [ ] Show quality filter statistics\n  - [ ] Show cache hit rates\n\n- [ ] **Write CLI tests**\n  - [ ] Test all new commands\n  - [ ] Test flag combinations\n  - [ ] Test error messages\n\n---\n\n### Week 3: Testing & Validation\n\n#### 3.1 Unit Testing\n**Duration**: 6-8 hours\n\n- [ ] **Test coverage targets**\n  - [ ] SelfEditGenerator: 90%+ coverage\n  - [ ] Strategies: 85%+ coverage\n  - [ ] Prompts: 80%+ coverage\n  - [ ] Integration: 75%+ coverage\n\n- [ ] **Specific test cases**\n  - [ ] Test each prompt strategy\n  - [ ] Test with various passage lengths\n  - [ ] Test with special characters/unicode\n  - [ ] Test with edge cases (empty, very long)\n  - [ ] Test caching behavior\n  - [ ] Test quality filtering\n  - [ ] Test error handling\n  - [ ] Test memory management\n\n- [ ] **Mock generation testing**\n  - File: `tests/test_seal/test_self_edit_generator.py`\n  - Mock model.generate() to return predictable outputs\n  - Verify parsing logic\n  - Verify filtering logic\n  - Verify caching logic\n\n- [ ] **Performance testing**\n  - [ ] Measure generation time per passage\n  - [ ] Measure memory usage\n  - [ ] Test batch generation efficiency\n  - [ ] Profile bottlenecks\n\n#### 3.2 Integration Testing\n**Duration**: 6-8 hours\n\n- [ ] **End-to-end workflow tests**\n  - File: `tests/integration/test_seal_workflow.py`\n  - [ ] Test 1: Generate self-edits \u2192 Train expert \u2192 Validate\n  - [ ] Test 2: Train with/without SEAL \u2192 Compare results\n  - [ ] Test 3: Cache persistence across runs\n  - [ ] Test 4: Error recovery and retries\n\n- [ ] **Real model testing (optional but recommended)**\n  - [ ] Use small model (e.g., GPT-2 124M)\n  - [ ] Generate real self-edits\n  - [ ] Verify output quality\n  - [ ] Compare to manual inspection\n\n- [ ] **Training pipeline integration**\n  - [ ] Verify expert training completes\n  - [ ] Verify no regressions in base model\n  - [ ] Verify augmented dataset format\n  - [ ] Verify checkpoint saving\n\n#### 3.3 A/B Experiment Setup\n**Duration**: 4-5 hours\n\n- [ ] **Design experiment**\n  - [ ] Select small test dataset (100 passages)\n  - [ ] Split into train/validation\n  - [ ] Define success metrics (see below)\n\n- [ ] **Implement comparison script**\n  - File: `scripts/seal_ab_test.py`\n  ```python\n  # Train two experts:\n  # Control: Standard training\n  # Treatment: Training with SEAL self-edits\n  \n  # Compare:\n  # - Validation loss\n  # - Perplexity\n  # - Training time\n  # - Memory usage\n  ```\n\n- [ ] **Create experiment config**\n  - File: `experiments/seal_phase1_experiment.yaml`\n  - Define hyperparameters\n  - Define evaluation metrics\n  - Define comparison criteria\n\n- [ ] **Run pilot experiment**\n  - [ ] Train control expert (no SEAL)\n  - [ ] Train treatment expert (with SEAL)\n  - [ ] Compare results\n  - [ ] Document findings\n\n---\n\n### Week 4: Documentation & Refinement\n\n#### 4.1 User Documentation\n**Duration**: 4-5 hours\n\n- [ ] **Create user guide**\n  - File: `docs/user_guide/SEAL_SELF_EDITS.md` (placeholder; to be created \u2014 see `docs/INDEX.md` and `docs/guide/` for current docs)\n  - [ ] What is SEAL?\n  - [ ] How does self-edit generation work?\n  - [ ] When to use self-edits?\n  - [ ] Configuration options explained\n  - [ ] CLI usage examples\n  - [ ] Troubleshooting common issues\n\n- [ ] **Update existing docs**\n  - [ ] Update `docs/QUICK_START.md` with SEAL mention\n  - [ ] Update `docs/user_guide/TRAINING.md` with self-edit section (placeholder target; user guide not yet authored)\n  - [ ] Update `config/default.yaml` with inline comments\n  - [ ] Update `README.md` with SEAL feature highlight\n\n- [ ] **Create tutorial**\n  - File: `docs/tutorials/SEAL_FIRST_EXPERT.md` (placeholder; to be created)\n  - [ ] Step-by-step: Train your first expert with SEAL\n  - [ ] Expected results and interpretation\n  - [ ] Comparison: with vs without SEAL\n\n#### 4.2 API Documentation\n**Duration**: 3-4 hours\n\n- [ ] **Generate API docs**\n  - [ ] Add docstrings to all public methods\n  - [ ] Use Google/NumPy docstring format\n  - [ ] Include usage examples in docstrings\n  - [ ] Generate Sphinx/MkDocs API reference\n\n- [ ] **Code examples**\n  - File: `docs/api/seal_examples.md` (placeholder; to be created)\n  ```python\n  # Example 1: Basic usage\n  # Example 2: Custom strategy\n  # Example 3: Advanced configuration\n  # Example 4: Programmatic access\n  ```\n\n#### 4.3 Performance Optimization\n**Duration**: 4-6 hours\n\n- [ ] **Profile generation pipeline**\n  - [ ] Identify bottlenecks\n  - [ ] Optimize batching\n  - [ ] Optimize tokenization\n  - [ ] Optimize caching\n\n- [ ] **Memory optimization**\n  - [ ] Reduce peak memory usage\n  - [ ] Implement streaming for large datasets\n  - [ ] Add memory usage monitoring\n  - [ ] Test on large datasets (10K+ passages)\n\n- [ ] **Caching optimization**\n  - [ ] Implement cache warming\n  - [ ] Add cache preloading\n  - [ ] Optimize cache lookup\n  - [ ] Test cache performance\n\n#### 4.4 Final Validation\n**Duration**: 2-3 hours\n\n- [ ] **Run full test suite**\n  - [ ] All unit tests pass\n  - [ ] All integration tests pass\n  - [ ] Code coverage meets targets\n  - [ ] No critical linting errors\n\n- [ ] **Manual testing checklist**\n  - [ ] Install on fresh environment\n  - [ ] Run through tutorial\n  - [ ] Test all CLI commands\n  - [ ] Generate self-edits for sample dataset\n  - [ ] Train sample expert\n  - [ ] Verify improvement vs baseline\n\n- [ ] **Performance benchmarks**\n  - [ ] Generation time per passage\n  - [ ] Memory usage during generation\n  - [ ] Cache hit rate\n  - [ ] Training time comparison\n\n- [ ] **Documentation review**\n  - [ ] All docs are accurate\n  - [ ] All examples work\n  - [ ] No broken links\n  - [ ] Clear and easy to follow\n\n---\n\n### Phase 1 Checklist Summary\n\n**Before Starting**:\n- [ ] Review SEAL paper thoroughly\n- [ ] Review SEAL GitHub code\n- [ ] Understand AI-OS codebase\n- [ ] Set up development environment\n- [ ] Create feature branch: `feature/seal-phase1`\n\n**Week 1 Complete**:\n- [ ] Project structure created\n- [ ] Dependencies added\n- [ ] Prompts module implemented and tested\n- [ ] Strategies module implemented and tested\n- [ ] SelfEditGenerator class implemented and tested\n- [ ] Configuration integrated\n\n**Week 2 Complete**:\n- [ ] AutoTrainingOrchestrator updated\n- [ ] Expert training pipeline updated\n- [ ] CLI commands added\n- [ ] Integration tests written\n\n**Week 3 Complete**:\n- [ ] All unit tests pass (90%+ coverage)\n- [ ] All integration tests pass\n- [ ] A/B experiment completed\n- [ ] Results documented\n\n**Week 4 Complete**:\n- [ ] User documentation written\n- [ ] API documentation generated\n- [ ] Performance optimized\n- [ ] Final validation passed\n- [ ] Ready for merge to main\n\n**Phase 1 Acceptance Criteria**:\n- [ ] Self-edit generation works for all strategies\n- [ ] Expert training with SEAL completes successfully\n- [ ] 10-15% improvement in validation loss vs baseline\n- [ ] No regressions in base model or existing features\n- [ ] Documentation is complete and accurate\n- [ ] Code coverage >85%\n- [ ] Performance within acceptable limits (<30s per passage)\n\n---\n\n## Phase 2: RL Optimization (Weeks 5-14)\n\n**Objective**: Implement ReSTEM reinforcement learning to optimize self-edit quality\n\n**Effort**: 80-120 hours  \n**Risk Level**: MEDIUM  \n**Priority**: MEDIUM  \n**Dependencies**: Phase 1 complete and validated\n\n**Note**: This phase is OPTIONAL. Only proceed if Phase 1 shows promising results and you have compute budget for RL training.\n\n---\n\n### Week 5-6: ReSTEM Foundation\n\n#### 5.1 Research & Planning\n**Duration**: 6-8 hours\n\n- [ ] **Deep dive into ReSTEM**\n  - [ ] Read SEAL paper Section 3.1 thoroughly\n  - [ ] Study SEAL GitHub implementation\n  - [ ] Understand E-step (sampling) and M-step (SFT)\n  - [ ] Understand binary reward computation\n  - [ ] Review hyperparameters from paper\n\n- [ ] **Design AI-OS adaptation**\n  - [ ] Define reward function for AI-OS\n  - [ ] Design inner loop (TTT)\n  - [ ] Design outer loop (policy update)\n  - [ ] Plan checkpoint management\n  - [ ] Plan GPU resource allocation\n\n- [ ] **Create design document**\n  - File: `docs/development/SEAL_RESTEM_DESIGN.md` (placeholder; to be created)\n  - [ ] Architecture diagram\n  - [ ] Algorithm pseudocode\n  - [ ] Resource requirements\n  - [ ] Risk analysis\n\n#### 5.2 Reward Function Implementation\n**Duration**: 6-8 hours  \n**File**: `src/aios/core/seal/reward.py`\n\n- [ ] **Define RewardFunction base class**\n  ```python\n  class RewardFunction(ABC):\n      @abstractmethod\n      def compute_reward(\n          self,\n          expert: nn.Module,\n          validation_dataset: Dataset,\n          threshold: float,\n      ) -> float:\n          \"\"\"\n          Returns:\n              1.0 if expert improves over threshold\n              0.0 otherwise\n          \"\"\"\n          pass\n  ```\n\n- [ ] **Implement ValidationLossReward**\n  - [ ] Evaluate expert on validation set\n  - [ ] Compute average loss\n  - [ ] Return 1 if loss < threshold, 0 otherwise\n  - [ ] Cache validation results\n\n- [ ] **Implement PerplexityReward**\n  - [ ] Compute perplexity on validation set\n  - [ ] Return binary reward based on threshold\n\n- [ ] **Implement AccuracyReward (optional)**\n  - [ ] For classification/QA tasks\n  - [ ] Compute accuracy\n  - [ ] Return binary reward\n\n- [ ] **Add reward computation utilities**\n  - [ ] Efficient validation batching\n  - [ ] GPU memory management\n  - [ ] Timeout handling\n  - [ ] Result caching\n\n- [ ] **Write unit tests**\n  - [ ] Test each reward function\n  - [ ] Test with mock expert\n  - [ ] Test edge cases\n\n#### 5.3 Test-Time Training (TTT) Module\n**Duration**: 8-10 hours  \n**File**: `src/aios/core/seal/test_time_training.py`\n\n- [ ] **Implement TTTTrainer class**\n  ```python\n  class TTTTrainer:\n      def train_temporary_expert(\n          self,\n          base_expert: nn.Module,\n          self_edit: str,\n          epochs: int = 5,\n          learning_rate: float = 1e-4,\n      ) -> Tuple[nn.Module, Dict]:\n          \"\"\"\n          Train expert on self-edit, return trained expert + metrics.\n          \"\"\"\n  ```\n\n- [ ] **LoRA integration**\n  - [ ] Use PEFT library for efficient adapters\n  - [ ] Configure LoRA rank and alpha\n  - [ ] Apply LoRA to expert module\n  - [ ] Train only LoRA parameters\n\n- [ ] **Training loop**\n  - [ ] Freeze base expert\n  - [ ] Train LoRA adapter\n  - [ ] Monitor loss\n  - [ ] Early stopping if loss plateaus\n  - [ ] Save temporary checkpoint\n\n- [ ] **Memory management**\n  - [ ] Create expert copy for TTT\n  - [ ] Clean up after training\n  - [ ] Garbage collection\n  - [ ] GPU memory clearing\n\n- [ ] **Write unit tests**\n  - [ ] Test TTT with mock expert\n  - [ ] Test LoRA application\n  - [ ] Test memory cleanup\n  - [ ] Test training convergence\n\n---\n\n### Week 7-8: ReSTEM Optimizer Core\n\n#### 7.1 ReSTEM Optimizer Implementation\n**Duration**: 10-12 hours  \n**File**: `src/aios/core/seal/restem_optimizer.py`\n\n- [ ] **Implement ReSTEMOptimizer class**\n  ```python\n  class ReSTEMOptimizer:\n      def __init__(\n          self,\n          self_edit_generator: SelfEditGenerator,\n          ttt_trainer: TTTTrainer,\n          reward_function: RewardFunction,\n          num_iterations: int = 2,\n          batch_size: int = 50,\n          candidates_per_passage: int = 5,\n      ):\n          # Implementation\n  ```\n\n- [ ] **E-Step: Sample self-edits**\n  - [ ] Generate N candidates per passage\n  - [ ] Use current generator policy\n  - [ ] Store candidates with metadata\n  - [ ] Log sampling statistics\n\n- [ ] **Inner Loop: TTT evaluation**\n  - [ ] For each candidate self-edit:\n    - [ ] Train temporary expert via TTT\n    - [ ] Evaluate on validation set\n    - [ ] Compute reward\n    - [ ] Store (candidate, reward) pair\n  - [ ] Parallel processing (if multiple GPUs)\n  - [ ] Progress tracking\n  - [ ] Checkpointing\n\n- [ ] **M-Step: Policy update**\n  - [ ] Filter candidates by reward (keep reward=1)\n  - [ ] Fine-tune generator on good self-edits\n  - [ ] Use standard SFT (AdamW optimizer)\n  - [ ] Monitor training metrics\n  - [ ] Save updated generator checkpoint\n\n- [ ] **Iteration management**\n  - [ ] Run E-M loop for N iterations\n  - [ ] Track metrics across iterations\n  - [ ] Early stopping if no improvement\n  - [ ] Save best generator checkpoint\n\n- [ ] **Logging and telemetry**\n  - [ ] Log per-iteration statistics\n  - [ ] Log reward distribution\n  - [ ] Log generator improvement\n  - [ ] Save to JSONL file\n\n#### 7.2 Integration with Training Pipeline\n**Duration**: 6-8 hours  \n**File**: `src/aios/core/training/auto_training_orchestrator.py`\n\n- [ ] **Add ReSTEM mode to orchestrator**\n  ```python\n  def train_expert_with_restem(\n      self,\n      expert_id: str,\n      dataset_path: str,\n      validation_split: float = 0.2,\n  ) -> Dict:\n      # Split dataset\n      # Initialize ReSTEM optimizer\n      # Run optimization loop\n      # Train final expert with optimized generator\n      # Return metrics\n  ```\n\n- [ ] **Resource management**\n  - [ ] Estimate GPU memory requirements\n  - [ ] Check available resources\n  - [ ] Allocate GPUs for TTT\n  - [ ] Handle out-of-memory errors\n\n- [ ] **Checkpoint management**\n  - [ ] Save intermediate generator checkpoints\n  - [ ] Save reward statistics\n  - [ ] Save best self-edits\n  - [ ] Resume capability from checkpoint\n\n- [ ] **Write integration tests**\n  - [ ] Test ReSTEM workflow end-to-end\n  - [ ] Test with small mock dataset\n  - [ ] Verify checkpoint saving/loading\n  - [ ] Test error handling\n\n---\n\n### Week 9-10: Testing & Optimization\n\n#### 9.1 Comprehensive Testing\n**Duration**: 8-10 hours\n\n- [ ] **Unit tests**\n  - [ ] RewardFunction implementations\n  - [ ] TTTTrainer methods\n  - [ ] ReSTEMOptimizer E-step\n  - [ ] ReSTEMOptimizer M-step\n  - [ ] Edge cases and error handling\n\n- [ ] **Integration tests**\n  - [ ] Full ReSTEM loop with mock data\n  - [ ] Multi-GPU TTT if available\n  - [ ] Checkpoint save/load\n  - [ ] Memory cleanup\n\n- [ ] **Performance tests**\n  - [ ] Measure TTT time per candidate\n  - [ ] Measure total ReSTEM time\n  - [ ] Measure memory usage\n  - [ ] Identify bottlenecks\n\n#### 9.2 Hyperparameter Tuning\n**Duration**: 6-8 hours\n\n- [ ] **Create tuning script**\n  - File: `scripts/tune_restem_hyperparams.py`\n  - [ ] Grid search over:\n    - Learning rate: [1e-5, 3e-5, 1e-4]\n    - Reward threshold: [auto, fixed values]\n    - LoRA rank: [8, 16, 32]\n    - TTT epochs: [3, 5, 10]\n\n- [ ] **Run tuning experiments**\n  - [ ] Use small dataset for speed\n  - [ ] Track validation improvement\n  - [ ] Document best hyperparameters\n  - [ ] Update default config\n\n#### 9.3 Validation Experiment\n**Duration**: 8-10 hours\n\n- [ ] **Design experiment**\n  - [ ] Select test domain (e.g., Python programming)\n  - [ ] Prepare dataset (100-200 passages)\n  - [ ] Define success criteria\n\n- [ ] **Run comparison**\n  - [ ] Baseline: Phase 1 (self-edits without RL)\n  - [ ] Treatment: Phase 2 (ReSTEM optimization)\n  - [ ] Measure:\n    - Validation loss improvement\n    - Training time\n    - Compute cost\n    - Expert quality\n\n- [ ] **Document results**\n  - File: `experiments/seal_phase2_results.md`\n  - [ ] Quantitative metrics\n  - [ ] Qualitative assessment\n  - [ ] Cost-benefit analysis\n  - [ ] Recommendation for production\n\n---\n\n### Week 11-12: CLI, GUI, and Usability\n\n#### 11.1 CLI Commands\n**Duration**: 4-5 hours\n\n- [ ] **Add ReSTEM commands**\n  ```bash\n  # Run ReSTEM optimization\n  aios seal restem optimize \\\n      --dataset data.txt \\\n      --output-generator generator.pt \\\n      --iterations 2 \\\n      --batch-size 50\n  \n  # Use optimized generator for training\n  aios hrm-hf train-expert \\\n      --expert-id abc123 \\\n      --dataset data.txt \\\n      --seal-generator path/to/optimized_generator.pt\n  ```\n\n- [ ] **Add monitoring commands**\n  ```bash\n  # View ReSTEM progress\n  aios seal restem status\n  \n  # View reward statistics\n  aios seal restem rewards --run-id xyz\n  \n  # Compare generators\n  aios seal restem compare --baseline gen1.pt --optimized gen2.pt\n  ```\n\n#### 11.2 GUI Integration (Optional)\n**Duration**: 6-8 hours\n\n- [ ] **Add ReSTEM panel to Subbrains Manager**\n  - Show optimization progress\n  - Display reward statistics\n  - Show best self-edits\n  - Allow starting/stopping optimization\n\n- [ ] **Visualization**\n  - Reward distribution over iterations\n  - Generator improvement metrics\n  - Training progress for TTT\n  - Resource usage graphs\n\n#### 11.3 Documentation\n**Duration**: 6-8 hours\n\n- [ ] **Create ReSTEM guide**\n  - File: `docs/user_guide/SEAL_RESTEM.md` (placeholder; to be created)\n  - [ ] What is ReSTEM?\n  - [ ] When to use it?\n  - [ ] How to configure?\n  - [ ] Interpreting results\n  - [ ] Troubleshooting\n\n- [ ] **Update existing docs**\n  - [ ] Update SEAL overview\n  - [ ] Update training guide\n  - [ ] Add cost estimates\n  - [ ] Add best practices\n\n- [ ] **Create tutorial**\n  - File: `docs/tutorials/SEAL_RESTEM_TUTORIAL.md` (placeholder; to be created)\n  - [ ] Step-by-step ReSTEM optimization\n  - [ ] Expected results\n  - [ ] Analysis and interpretation\n\n---\n\n### Week 13-14: Production Readiness\n\n#### 13.1 Performance Optimization\n**Duration**: 8-10 hours\n\n- [ ] **Profile ReSTEM pipeline**\n  - [ ] Identify bottlenecks\n  - [ ] Optimize TTT batching\n  - [ ] Optimize reward computation\n  - [ ] Reduce checkpoint I/O\n\n- [ ] **Memory optimization**\n  - [ ] Reduce peak GPU memory\n  - [ ] Implement gradient accumulation\n  - [ ] Add CPU offloading\n  - [ ] Test with limited resources\n\n- [ ] **Parallelization**\n  - [ ] Multi-GPU TTT\n  - [ ] Parallel candidate evaluation\n  - [ ] Async reward computation\n  - [ ] Test scaling behavior\n\n#### 13.2 Cost Analysis\n**Duration**: 3-4 hours\n\n- [ ] **Compute cost estimation**\n  - [ ] Calculate GPU hours for typical run\n  - [ ] Estimate cloud costs\n  - [ ] Compare to Phase 1 baseline\n  - [ ] Document cost-benefit tradeoff\n\n- [ ] **Create cost calculator**\n  - File: `scripts/estimate_restem_cost.py`\n  - [ ] Input: dataset size, iterations, GPU type\n  - [ ] Output: time and cost estimates\n  - [ ] Recommendations for budget\n\n#### 13.3 Final Validation\n**Duration**: 4-5 hours\n\n- [ ] **Full test suite**\n  - [ ] All unit tests pass\n  - [ ] All integration tests pass\n  - [ ] Performance tests pass\n  - [ ] Code coverage >85%\n\n- [ ] **End-to-end validation**\n  - [ ] Fresh environment install\n  - [ ] Run ReSTEM optimization\n  - [ ] Train expert with optimized generator\n  - [ ] Verify improvement vs Phase 1\n  - [ ] Document results\n\n- [ ] **Production readiness checklist**\n  - [ ] Error handling complete\n  - [ ] Logging comprehensive\n  - [ ] Monitoring in place\n  - [ ] Documentation complete\n  - [ ] Performance acceptable\n  - [ ] Costs documented\n\n---\n\n### Phase 2 Checklist Summary\n\n**Before Starting**:\n- [ ] Phase 1 complete and validated\n- [ ] Results show 10-15% improvement\n- [ ] Compute budget approved (2xH100 for 20-40 hours)\n- [ ] Create feature branch: `feature/seal-phase2`\n\n**Week 5-6 Complete**:\n- [ ] ReSTEM research and design complete\n- [ ] Reward functions implemented and tested\n- [ ] TTT module implemented and tested\n\n**Week 7-8 Complete**:\n- [ ] ReSTEM optimizer core implemented\n- [ ] Integration with training pipeline complete\n- [ ] Basic tests passing\n\n**Week 9-10 Complete**:\n- [ ] Comprehensive testing complete\n- [ ] Hyperparameters tuned\n- [ ] Validation experiment complete\n\n**Week 11-12 Complete**:\n- [ ] CLI commands added\n- [ ] GUI integration complete (optional)\n- [ ] Documentation written\n\n**Week 13-14 Complete**:\n- [ ] Performance optimized\n- [ ] Cost analysis complete\n- [ ] Final validation passed\n- [ ] Ready for merge\n\n**Phase 2 Acceptance Criteria**:\n- [ ] ReSTEM optimization completes successfully\n- [ ] 15-20% improvement over Phase 1 (total 25-35% over baseline)\n- [ ] Cost is reasonable (<$100 per expert on cloud)\n- [ ] No critical bugs or crashes\n- [ ] Documentation is complete\n- [ ] Code coverage >85%\n- [ ] Performance within acceptable limits\n\n---\n\n## Phase 3: Advanced Meta-Learning (Weeks 15+)\n\n**Objective**: Implement advanced SEAL features for full meta-learning capabilities\n\n**Effort**: 100-150 hours  \n**Risk Level**: HIGH  \n**Priority**: OPTIONAL (Research Phase)  \n**Dependencies**: Phase 2 complete and providing significant value\n\n**Note**: This phase is HIGHLY OPTIONAL and research-oriented. Only proceed if you have specific use cases and research goals.\n\n---\n\n### Week 15-16: Test-Time Adaptation\n\n#### 15.1 Context-Aware Fine-Tuning\n**Duration**: 10-12 hours  \n**File**: `src/aios/core/seal/test_time_adaptation.py`\n\n- [ ] **Implement TTAModule**\n  ```python\n  class TestTimeAdaptation:\n      def adapt_to_context(\n          self,\n          expert: nn.Module,\n          conversation_history: List[str],\n          adaptation_steps: int = 5,\n      ) -> nn.Module:\n          \"\"\"\n          Fine-tune expert on recent conversation for better context awareness.\n          \"\"\"\n  ```\n\n- [ ] **Context extraction**\n  - [ ] Extract relevant passages from conversation\n  - [ ] Generate self-edits from context\n  - [ ] Create mini training set\n\n- [ ] **Rapid adaptation**\n  - [ ] Use small LoRA (rank=4)\n  - [ ] Very low learning rate (1e-5)\n  - [ ] Few gradient steps (5-10)\n  - [ ] Minimal memory overhead\n\n- [ ] **Integration with chat**\n  - [ ] Adapt expert after every N messages\n  - [ ] Track adaptation history\n  - [ ] Option to disable TTA\n  - [ ] Performance monitoring\n\n- [ ] **Write tests**\n  - [ ] Test adaptation logic\n  - [ ] Test memory management\n  - [ ] Test performance impact\n\n#### 15.2 Few-Shot Learning (ARC-Style)\n**Duration**: 10-12 hours  \n**File**: `src/aios/core/seal/few_shot_learning.py`\n\n- [ ] **Implement FewShotLearner**\n  - [ ] Based on SEAL's ARC experiments\n  - [ ] Support data augmentation tools\n  - [ ] Support hyperparameter selection\n  - [ ] Generate self-edits with tool invocations\n\n- [ ] **Tool framework**\n  - [ ] Define tool interface\n  - [ ] Implement rotation/flip augmentations\n  - [ ] Implement learning rate selection\n  - [ ] Implement epoch selection\n\n- [ ] **Integration**\n  - [ ] Add to expert training pipeline\n  - [ ] Enable for specific tasks\n  - [ ] Track success rates\n\n- [ ] **Write tests**\n  - [ ] Test tool invocation\n  - [ ] Test augmentation strategies\n  - [ ] Test few-shot scenarios\n\n---\n\n### Week 17-18: Cross-Model Transfer\n\n#### 17.1 Expert Portability\n**Duration**: 8-10 hours\n\n- [ ] **Export/import experts**\n  - [ ] Export expert to HuggingFace format\n  - [ ] Include metadata and training history\n  - [ ] Include self-edit generator configuration\n  - [ ] Package as distributable artifact\n\n- [ ] **Cross-model adaptation**\n  - [ ] Load expert trained on Model A\n  - [ ] Adapt to Model B architecture\n  - [ ] Fine-tune adapter layer\n  - [ ] Validate performance\n\n- [ ] **Expert marketplace (concept)**\n  - [ ] Design sharing protocol\n  - [ ] Define quality standards\n  - [ ] Create browsing interface\n  - [ ] Implement download/install\n\n#### 17.2 Generator Transfer\n**Duration**: 6-8 hours\n\n- [ ] **Transfer learned strategies**\n  - [ ] Export optimized generator\n  - [ ] Import to new domain\n  - [ ] Fine-tune for domain\n  - [ ] Compare to from-scratch training\n\n- [ ] **Multi-domain optimization**\n  - [ ] Train generator on multiple domains\n  - [ ] Test generalization\n  - [ ] Measure transfer learning benefit\n\n---\n\n### Week 19-20: Research Features\n\n#### 19.1 Advanced Routing\n**Duration**: 8-10 hours\n\n- [ ] **SEAL-aware routing**\n  - [ ] Route based on self-edit quality\n  - [ ] Track which self-edits led to activations\n  - [ ] Use as routing signal\n\n- [ ] **Meta-routing**\n  - [ ] Learn routing from SEAL training\n  - [ ] Optimize routing for self-improvement\n  - [ ] Experiment with routing strategies\n\n#### 19.2 Continual ReSTEM\n**Duration**: 8-10 hours\n\n- [ ] **Background optimization**\n  - [ ] Run ReSTEM during idle time\n  - [ ] Incrementally improve generators\n  - [ ] Track long-term improvements\n\n- [ ] **Online learning**\n  - [ ] Update generators from user feedback\n  - [ ] Incorporate conversation quality signals\n  - [ ] Avoid catastrophic forgetting\n\n#### 19.3 Documentation & Publication\n**Duration**: 8-10 hours\n\n- [ ] **Research documentation**\n  - File: `docs/research/SEAL_METALEARNING.md`\n  - [ ] Document novel contributions\n  - [ ] Document experimental results\n  - [ ] Comparison to SEAL paper\n\n- [ ] **Potential paper/blog post**\n  - [ ] \"SEAL Integration in HRM Architecture\"\n  - [ ] Document challenges and solutions\n  - [ ] Share lessons learned\n  - [ ] Release findings to community\n\n---\n\n### Phase 3 Checklist Summary\n\n**Before Starting**:\n- [ ] Phase 2 showing strong results (20-30% improvement)\n- [ ] Research goals clearly defined\n- [ ] Team has bandwidth for experiments\n- [ ] Create feature branch: `feature/seal-phase3`\n\n**Week 15-16 Complete**:\n- [ ] Test-time adaptation implemented\n- [ ] Few-shot learning implemented\n- [ ] Initial experiments complete\n\n**Week 17-18 Complete**:\n- [ ] Expert portability working\n- [ ] Cross-model transfer validated\n- [ ] Marketplace concept designed\n\n**Week 19-20 Complete**:\n- [ ] Advanced routing experimented\n- [ ] Continual learning explored\n- [ ] Research documentation written\n\n**Phase 3 Acceptance Criteria**:\n- [ ] Novel features implemented and tested\n- [ ] Experimental results documented\n- [ ] Research contributions identified\n- [ ] Community-shareable findings\n- [ ] Decision made on production features\n\n---\n\n## Dependencies & Prerequisites\n\n### System Requirements\n\n**Phase 1**:\n- [ ] Python 3.10+\n- [ ] PyTorch 2.0+\n- [ ] Transformers 4.30+\n- [ ] 1x GPU with 16GB+ VRAM (RTX 4090, A100, etc.)\n- [ ] 32GB+ RAM\n- [ ] 50GB+ disk space\n\n**Phase 2** (additional):\n- [ ] 2x GPU with 24GB+ VRAM (A100, H100)\n- [ ] 64GB+ RAM\n- [ ] 200GB+ disk space\n- [ ] High-speed GPU interconnect (NVLink preferred)\n\n**Phase 3** (additional):\n- [ ] Research compute budget\n- [ ] Multi-GPU cluster (optional)\n- [ ] Extended storage (1TB+)\n\n### Software Dependencies\n\n**Phase 1**:\n```toml\n[project.dependencies]\n# Existing\ntorch = \">=2.0.0\"\ntransformers = \">=4.30.0\"\n# New for SEAL\nsentence-transformers = \">=2.2.0\"\nrouge-score = \">=0.1.2\"\n```\n\n**Phase 2** (additional):\n```toml\npeft = \">=0.5.0\"  # LoRA\naccelerate = \">=0.21.0\"  # Multi-GPU\nwandb = \">=0.15.0\"  # Optional: experiment tracking\n```\n\n**Phase 3** (additional):\n```toml\ndatasets = \">=2.14.0\"  # HuggingFace datasets\nhuggingface-hub = \">=0.16.0\"  # Model/expert sharing\n```\n\n### Knowledge Prerequisites\n\n**Phase 1**:\n- [ ] Understanding of AI-OS codebase\n- [ ] Familiarity with PyTorch and Transformers\n- [ ] Experience with text generation\n- [ ] Understanding of prompt engineering\n\n**Phase 2**:\n- [ ] Understanding of reinforcement learning basics\n- [ ] Familiarity with policy gradient methods\n- [ ] Experience with multi-GPU training\n- [ ] Understanding of LoRA and PEFT\n\n**Phase 3**:\n- [ ] Advanced RL knowledge\n- [ ] Meta-learning concepts\n- [ ] Transfer learning experience\n- [ ] Research methodology\n\n---\n\n## Success Metrics\n\n### Phase 1 Metrics\n\n**Primary Metrics**:\n- [ ] **Expert Validation Loss**: 10-15% improvement over baseline\n- [ ] **Expert Perplexity**: Corresponding improvement\n- [ ] **Training Time**: <2x overhead for augmentation\n- [ ] **Generation Time**: <30s per passage per edit\n\n**Secondary Metrics**:\n- [ ] **Cache Hit Rate**: >50% on repeated datasets\n- [ ] **Quality Filter Rate**: <20% filtered outputs\n- [ ] **User Satisfaction**: Qualitative assessment\n- [ ] **Code Coverage**: >85%\n\n**Success Criteria**:\n- [ ] At least 10% improvement in expert quality\n- [ ] No regressions in existing features\n- [ ] Generation time acceptable (<30s/edit)\n- [ ] System stable and reliable\n\n### Phase 2 Metrics\n\n**Primary Metrics**:\n- [ ] **Expert Validation Loss**: 15-25% improvement over Phase 1\n- [ ] **Cumulative Improvement**: 25-35% over original baseline\n- [ ] **ReSTEM Time**: <40 GPU-hours per expert\n- [ ] **Reward Convergence**: Improvement within 2 iterations\n\n**Secondary Metrics**:\n- [ ] **Generator Quality**: Surpass GPT-4.1 synthetic data\n- [ ] **Cost per Expert**: <$100 on cloud\n- [ ] **Iteration Efficiency**: >20% rewards=1 after iteration 1\n- [ ] **Generalization**: Strategy transfers to new domains\n\n**Success Criteria**:\n- [ ] At least 15% improvement over Phase 1\n- [ ] Compute cost is reasonable\n- [ ] Generator quality measurably improves\n- [ ] Strategy generalizes across domains\n\n### Phase 3 Metrics\n\n**Research Metrics**:\n- [ ] **Novel Contributions**: At least 2 novel techniques\n- [ ] **Experimental Validation**: Positive results in 3+ experiments\n- [ ] **Transfer Learning**: >10% benefit from cross-domain transfer\n- [ ] **Adaptation Speed**: <5 steps for context adaptation\n\n**Success Criteria**:\n- [ ] At least one feature ready for production\n- [ ] Research findings documented and shareable\n- [ ] Clear roadmap for future work\n- [ ] Community interest and feedback\n\n---\n\n## Risk Management\n\n### Phase 1 Risks\n\n| Risk | Probability | Impact | Mitigation |\n|------|------------|--------|------------|\n| **Self-edits are low quality** | Medium | High | Use SEAL's proven prompts, add quality filtering, A/B test |\n| **Performance overhead too high** | Low | Medium | Optimize batching, use caching, make optional |\n| **Integration breaks existing features** | Low | High | Comprehensive testing, feature flags, gradual rollout |\n| **Limited improvement (<10%)** | Medium | Medium | Try multiple strategies, tune hyperparameters, document honestly |\n\n### Phase 2 Risks\n\n| Risk | Probability | Impact | Mitigation |\n|------|------------|--------|------------|\n| **RL doesn't converge** | Medium | High | Use SEAL hyperparameters, extensive tuning, fallback to Phase 1 |\n| **Compute cost too high** | Medium | Medium | Estimate upfront, optimize efficiency, set budget limits |\n| **Memory issues** | Medium | High | Test on smaller scales, implement gradient accumulation, CPU offload |\n| **No improvement over Phase 1** | Low | Medium | Validate Phase 1 first, careful experiment design |\n\n### Phase 3 Risks\n\n| Risk | Probability | Impact | Mitigation |\n|------|------------|--------|------------|\n| **Features too experimental** | High | Low | Clear research goals, document failures, learn from experiments |\n| **No clear production value** | Medium | Medium | Focus on specific use cases, validate with users |\n| **Resource drain** | Medium | Medium | Set time/budget limits, periodic review |\n\n---\n\n## Testing Strategy\n\n### Unit Testing\n**Target Coverage**: 85-90%\n\n**Phase 1**:\n- [ ] Test each prompt strategy\n- [ ] Test self-edit parsing\n- [ ] Test quality filtering\n- [ ] Test caching logic\n- [ ] Test error handling\n- [ ] Mock model generation\n\n**Phase 2**:\n- [ ] Test reward computation\n- [ ] Test TTT training\n- [ ] Test ReSTEM E-step\n- [ ] Test ReSTEM M-step\n- [ ] Test checkpoint management\n\n**Phase 3**:\n- [ ] Test adaptation logic\n- [ ] Test transfer learning\n- [ ] Test advanced features\n\n### Integration Testing\n\n**Phase 1**:\n- [ ] End-to-end generation pipeline\n- [ ] Training with augmented data\n- [ ] Cache persistence\n- [ ] CLI commands\n\n**Phase 2**:\n- [ ] Full ReSTEM optimization\n- [ ] Multi-GPU coordination\n- [ ] Checkpoint save/load\n- [ ] Resource cleanup\n\n**Phase 3**:\n- [ ] Cross-model transfer\n- [ ] Online adaptation\n- [ ] Complex workflows\n\n### Performance Testing\n\n**Benchmarks**:\n- [ ] Generation time per passage\n- [ ] Memory usage during generation\n- [ ] TTT time per candidate (Phase 2)\n- [ ] End-to-end training time\n- [ ] GPU utilization\n\n**Targets**:\n- [ ] Phase 1: <30s per edit\n- [ ] Phase 2: <40 GPU-hours per expert\n- [ ] Memory: <90% GPU utilization\n- [ ] No memory leaks over long runs\n\n### Manual Testing\n\n**Phase 1**:\n- [ ] Generate self-edits for sample data\n- [ ] Inspect output quality\n- [ ] Train sample expert\n- [ ] Verify improvement\n- [ ] Test all CLI commands\n\n**Phase 2**:\n- [ ] Run ReSTEM on small dataset\n- [ ] Monitor GPU usage\n- [ ] Verify improvement over Phase 1\n- [ ] Test checkpoint recovery\n\n**Phase 3**:\n- [ ] Test experimental features\n- [ ] Validate research hypotheses\n- [ ] User testing (if applicable)\n\n---\n\n## Documentation Requirements\n\n### User Documentation\n\n**Phase 1**:\n - [ ] `docs/user_guide/SEAL_OVERVIEW.md` - What is SEAL? (placeholder)\n - [ ] `docs/user_guide/SEAL_SELF_EDITS.md` - How to use self-edits (placeholder)\n - [ ] `docs/tutorials/SEAL_FIRST_EXPERT.md` - Tutorial (placeholder)\n- [ ] `config/default.yaml` - Inline comments for SEAL config\n- [ ] `README.md` - Feature highlight\n\n**Phase 2**:\n - [ ] `docs/user_guide/SEAL_RESTEM.md` - ReSTEM guide (placeholder)\n - [ ] `docs/tutorials/SEAL_RESTEM_TUTORIAL.md` - Step-by-step (placeholder)\n - [ ] `docs/user_guide/SEAL_COST_ANALYSIS.md` - Cost estimation (placeholder)\n- [ ] Update existing docs with Phase 2 info\n\n**Phase 3**:\n - [ ] `docs/research/SEAL_METALEARNING.md` - Research documentation (placeholder)\n - [ ] `docs/advanced/SEAL_ADVANCED_FEATURES.md` - Advanced usage (placeholder)\n- [ ] Case studies and examples\n\n### API Documentation\n\n**All Phases**:\n- [ ] Comprehensive docstrings (Google format)\n- [ ] Type annotations\n- [ ] Usage examples in docstrings\n- [ ] API reference (Sphinx/MkDocs)\n- [ ] Code examples in `docs/api/`\n\n### Developer Documentation\n\n**Phase 1**:\n - [ ] `docs/development/SEAL_ARCHITECTURE.md` - Architecture overview (placeholder)\n - [ ] `docs/development/SEAL_CONTRIBUTING.md` - How to contribute (placeholder)\n- [ ] Inline code comments for complex logic\n\n**Phase 2**:\n - [ ] `docs/development/SEAL_RESTEM_DESIGN.md` - ReSTEM design doc (placeholder)\n - [ ] `docs/development/SEAL_TESTING.md` - Testing guide (placeholder)\n- [ ] Performance optimization notes\n\n**Phase 3**:\n- [ ] Research methodology\n- [ ] Experiment protocols\n- [ ] Lessons learned\n\n---\n\n## Rollout Plan\n\n### Phase 1 Rollout\n\n**Stage 1: Internal Testing (Week 4)**\n- [ ] Merge to `develop` branch\n- [ ] Deploy to staging environment\n- [ ] Internal team testing\n- [ ] Bug fixes and refinements\n\n**Stage 2: Beta Release (Week 5)**\n- [ ] Release as beta feature\n- [ ] Feature flag: `seal.enabled = false` (opt-in)\n- [ ] Gather user feedback\n- [ ] Monitor performance and errors\n\n**Stage 3: General Availability (Week 6)**\n- [ ] Enable by default: `seal.enabled = true`\n- [ ] Announce in release notes\n- [ ] Promote in documentation\n- [ ] Monitor adoption and satisfaction\n\n### Phase 2 Rollout\n\n**Stage 1: Internal Validation (Week 13-14)**\n- [ ] Validate on internal datasets\n- [ ] Compare costs and benefits\n- [ ] Document best practices\n- [ ] Create cost calculator\n\n**Stage 2: Opt-In Release (Week 15)**\n- [ ] Release as advanced feature\n- [ ] Require explicit opt-in\n- [ ] Provide cost estimates\n- [ ] Gather feedback from power users\n\n**Stage 3: Production (Week 16+)**\n- [ ] Enable for users who need it\n- [ ] Provide clear documentation\n- [ ] Ongoing monitoring and optimization\n\n### Phase 3 Rollout\n\n**Research Preview**:\n- [ ] Release as experimental features\n- [ ] Clear \"research preview\" labeling\n- [ ] Gather feedback and data\n- [ ] Decide which features to productionize\n\n---\n\n## Ongoing Maintenance\n\n### Post-Release Tasks\n\n**Phase 1**:\n- [ ] Monitor cache usage and performance\n- [ ] Collect user feedback\n- [ ] Track success metrics\n- [ ] Fix bugs and issues\n- [ ] Add new prompt strategies based on feedback\n\n**Phase 2**:\n- [ ] Monitor ReSTEM usage and costs\n- [ ] Optimize performance based on usage patterns\n- [ ] Tune hyperparameters for common domains\n- [ ] Update documentation with best practices\n\n**Phase 3**:\n- [ ] Evaluate research features\n- [ ] Productionize successful experiments\n- [ ] Deprecate failed experiments\n- [ ] Plan future research directions\n\n### Continuous Improvement\n\n- [ ] Regular performance profiling\n- [ ] Dependency updates\n- [ ] Security patches\n- [ ] Documentation updates\n- [ ] Community engagement\n\n---\n\n## Appendices\n\n### A. Useful Commands\n\n```bash\n# Phase 1: Generate self-edits\naios seal generate --input data.txt --output augmented.txt --strategy implications\n\n# Phase 1: Train expert with SEAL\naios hrm-hf train-expert --expert-id abc123 --dataset data.txt\n\n# Phase 1: A/B test\npython scripts/seal_ab_test.py --dataset data.txt --output results.json\n\n# Phase 2: Run ReSTEM\naios seal restem optimize --dataset data.txt --iterations 2\n\n# Phase 2: Monitor progress\naios seal restem status\n\n# Cache management\naios seal cache stats\naios seal cache clear\naios seal cache prune --max-size 5GB\n```\n\n### B. Key Files\n\n**Phase 1**:\n- `src/aios/core/seal/self_edit_generator.py`\n- `src/aios/core/seal/prompts.py`\n- `src/aios/core/seal/strategies.py`\n- `src/aios/core/training/auto_training_orchestrator.py`\n- `config/default.yaml` (SEAL section)\n\n**Phase 2**:\n- `src/aios/core/seal/restem_optimizer.py`\n- `src/aios/core/seal/reward.py`\n- `src/aios/core/seal/test_time_training.py`\n\n**Phase 3**:\n- `src/aios/core/seal/test_time_adaptation.py`\n- `src/aios/core/seal/few_shot_learning.py`\n- `src/aios/core/seal/transfer_learning.py`\n\n### C. References\n\n- **SEAL Paper**: https://arxiv.org/html/2506.10943v2\n- **SEAL GitHub**: https://github.com/Continual-Intelligence/SEAL\n- **AI-OS Docs**: `docs/INDEX.md`\n- **HRM Paper**: https://arxiv.org/html/2506.21734v3\n- **Dynamic Subbrains**: `docs/DYNAMIC_SUBBRAINS_ARCHITECTURE.md` (placeholder; doc not yet created)\n\n---\n\n## Revision History\n\n| Version | Date | Author | Changes |\n|---------|------|--------|---------|\n| 1.0 | January 15, 2025 | AI-OS Team | Initial detailed implementation plan |\n\n---\n\n## Sign-Off\n\n**Phase 1 Ready to Start**: \u2610 Yes \u2610 No  \n**Phase 2 Approved**: \u2610 Yes \u2610 No \u2610 Pending Phase 1 Results  \n**Phase 3 Approved**: \u2610 Yes \u2610 No \u2610 Research Only  \n\n**Project Lead Signature**: _______________  \n**Date**: _______________\n\n---\n\n**END OF DOCUMENT**\n\nThis plan is a living document. Update as implementation progresses and new insights are gained.\n", "tags": ["datasets", "experts", "gui", "training"], "headings": [{"line": 0, "text": "SEAL Integration Implementation Plan"}, {"line": 14, "text": "\ud83d\udccb Table of Contents"}, {"line": 29, "text": "Executive Summary"}, {"line": 49, "text": "Phase 1: Self-Edit Data Generation (Weeks 1-4)"}, {"line": 60, "text": "Week 1: Foundation & Core Classes"}, {"line": 62, "text": "1.1 Project Setup"}, {"line": 86, "text": "SEAL Integration"}, {"line": 99, "text": "1.2 Prompts Module"}, {"line": 105, "text": "Implement these prompt strategies from SEAL paper:"}, {"line": 130, "text": "1.3 Strategies Module"}, {"line": 173, "text": "1.4 Core Generator Class"}, {"line": 190, "text": "Implementation"}, {"line": 227, "text": "1.5 Configuration Integration"}, {"line": 234, "text": "Self-Edit Generation"}, {"line": 237, "text": "Strategy Selection"}, {"line": 243, "text": "Quality Filtering"}, {"line": 250, "text": "Caching"}, {"line": 255, "text": "Performance"}, {"line": 259, "text": "Advanced (Phase 2)"}, {"line": 274, "text": "Week 2: Integration with AutoTrainingOrchestrator"}, {"line": 276, "text": "2.1 Refactor AutoTrainingOrchestrator"}, {"line": 284, "text": "... existing code ..."}, {"line": 324, "text": "2.2 Update Expert Training Pipeline"}, {"line": 337, "text": "Original dataset size"}, {"line": 338, "text": "Augmented dataset size"}, {"line": 339, "text": "Augmentation ratio"}, {"line": 340, "text": "Average self-edits per passage"}, {"line": 341, "text": "Total tokens (before/after)"}, {"line": 356, "text": "2.3 CLI Integration"}, {"line": 362, "text": "Generate self-edits for a dataset (standalone)"}, {"line": 365, "text": "Preview self-edits (no saving)"}, {"line": 368, "text": "Cache management"}, {"line": 376, "text": "Train expert with self-edits (default)"}, {"line": 379, "text": "Train expert without self-edits"}, {"line": 382, "text": "Train with specific strategy"}, {"line": 399, "text": "Week 3: Testing & Validation"}, {"line": 401, "text": "3.1 Unit Testing"}, {"line": 433, "text": "3.2 Integration Testing"}, {"line": 455, "text": "3.3 A/B Experiment Setup"}, {"line": 466, "text": "Train two experts:"}, {"line": 467, "text": "Control: Standard training"}, {"line": 468, "text": "Treatment: Training with SEAL self-edits"}, {"line": 470, "text": "Compare:"}, {"line": 471, "text": "- Validation loss"}, {"line": 472, "text": "- Perplexity"}, {"line": 473, "text": "- Training time"}, {"line": 474, "text": "- Memory usage"}, {"line": 491, "text": "Week 4: Documentation & Refinement"}, {"line": 493, "text": "4.1 User Documentation"}, {"line": 517, "text": "4.2 API Documentation"}, {"line": 529, "text": "Example 1: Basic usage"}, {"line": 530, "text": "Example 2: Custom strategy"}, {"line": 531, "text": "Example 3: Advanced configuration"}, {"line": 532, "text": "Example 4: Programmatic access"}, {"line": 535, "text": "4.3 Performance Optimization"}, {"line": 556, "text": "4.4 Final Validation"}, {"line": 587, "text": "Phase 1 Checklist Summary"}, {"line": 634, "text": "Phase 2: RL Optimization (Weeks 5-14)"}, {"line": 647, "text": "Week 5-6: ReSTEM Foundation"}, {"line": 649, "text": "5.1 Research & Planning"}, {"line": 673, "text": "5.2 Reward Function Implementation"}, {"line": 721, "text": "5.3 Test-Time Training (TTT) Module"}, {"line": 767, "text": "Week 7-8: ReSTEM Optimizer Core"}, {"line": 769, "text": "7.1 ReSTEM Optimizer Implementation"}, {"line": 785, "text": "Implementation"}, {"line": 823, "text": "7.2 Integration with Training Pipeline"}, {"line": 835, "text": "Split dataset"}, {"line": 836, "text": "Initialize ReSTEM optimizer"}, {"line": 837, "text": "Run optimization loop"}, {"line": 838, "text": "Train final expert with optimized generator"}, {"line": 839, "text": "Return metrics"}, {"line": 862, "text": "Week 9-10: Testing & Optimization"}, {"line": 864, "text": "9.1 Comprehensive Testing"}, {"line": 886, "text": "9.2 Hyperparameter Tuning"}, {"line": 903, "text": "9.3 Validation Experiment"}, {"line": 929, "text": "Week 11-12: CLI, GUI, and Usability"}, {"line": 931, "text": "11.1 CLI Commands"}, {"line": 936, "text": "Run ReSTEM optimization"}, {"line": 943, "text": "Use optimized generator for training"}, {"line": 952, "text": "View ReSTEM progress"}, {"line": 955, "text": "View reward statistics"}, {"line": 958, "text": "Compare generators"}, {"line": 962, "text": "11.2 GUI Integration (Optional)"}, {"line": 977, "text": "11.3 Documentation"}, {"line": 1002, "text": "Week 13-14: Production Readiness"}, {"line": 1004, "text": "13.1 Performance Optimization"}, {"line": 1025, "text": "13.2 Cost Analysis"}, {"line": 1040, "text": "13.3 Final Validation"}, {"line": 1066, "text": "Phase 2 Checklist Summary"}, {"line": 1111, "text": "Phase 3: Advanced Meta-Learning (Weeks 15+)"}, {"line": 1124, "text": "Week 15-16: Test-Time Adaptation"}, {"line": 1126, "text": "15.1 Context-Aware Fine-Tuning"}, {"line": 1166, "text": "15.2 Few-Shot Learning (ARC-Style)"}, {"line": 1194, "text": "Week 17-18: Cross-Model Transfer"}, {"line": 1196, "text": "17.1 Expert Portability"}, {"line": 1217, "text": "17.2 Generator Transfer"}, {"line": 1233, "text": "Week 19-20: Research Features"}, {"line": 1235, "text": "19.1 Advanced Routing"}, {"line": 1248, "text": "19.2 Continual ReSTEM"}, {"line": 1261, "text": "19.3 Documentation & Publication"}, {"line": 1278, "text": "Phase 3 Checklist Summary"}, {"line": 1310, "text": "Dependencies & Prerequisites"}, {"line": 1312, "text": "System Requirements"}, {"line": 1333, "text": "Software Dependencies"}, {"line": 1338, "text": "Existing"}, {"line": 1341, "text": "New for SEAL"}, {"line": 1359, "text": "Knowledge Prerequisites"}, {"line": 1381, "text": "Success Metrics"}, {"line": 1383, "text": "Phase 1 Metrics"}, {"line": 1403, "text": "Phase 2 Metrics"}, {"line": 1423, "text": "Phase 3 Metrics"}, {"line": 1439, "text": "Risk Management"}, {"line": 1441, "text": "Phase 1 Risks"}, {"line": 1450, "text": "Phase 2 Risks"}, {"line": 1459, "text": "Phase 3 Risks"}, {"line": 1469, "text": "Testing Strategy"}, {"line": 1471, "text": "Unit Testing"}, {"line": 1494, "text": "Integration Testing"}, {"line": 1513, "text": "Performance Testing"}, {"line": 1528, "text": "Manual Testing"}, {"line": 1550, "text": "Documentation Requirements"}, {"line": 1552, "text": "User Documentation"}, {"line": 1572, "text": "API Documentation"}, {"line": 1581, "text": "Developer Documentation"}, {"line": 1600, "text": "Rollout Plan"}, {"line": 1602, "text": "Phase 1 Rollout"}, {"line": 1622, "text": "Phase 2 Rollout"}, {"line": 1641, "text": "Phase 3 Rollout"}, {"line": 1651, "text": "Ongoing Maintenance"}, {"line": 1653, "text": "Post-Release Tasks"}, {"line": 1674, "text": "Continuous Improvement"}, {"line": 1684, "text": "Appendices"}, {"line": 1686, "text": "A. Useful Commands"}, {"line": 1689, "text": "Phase 1: Generate self-edits"}, {"line": 1692, "text": "Phase 1: Train expert with SEAL"}, {"line": 1695, "text": "Phase 1: A/B test"}, {"line": 1698, "text": "Phase 2: Run ReSTEM"}, {"line": 1701, "text": "Phase 2: Monitor progress"}, {"line": 1704, "text": "Cache management"}, {"line": 1710, "text": "B. Key Files"}, {"line": 1729, "text": "C. References"}, {"line": 1739, "text": "Revision History"}, {"line": 1747, "text": "Sign-Off"}]}, {"path": "planned_features/TOON_INTEGRATION.md", "content": "## PF-007: TOON Format Integration for Token-Efficient LLM Data Exchange\n\n### Summary\n\nIntegrate Token-Oriented Object Notation (TOON) as an optional serialization format for AI-OS to reduce token consumption when passing structured data to LLMs. TOON achieves 30-60% token savings over JSON for uniform tabular data while maintaining human readability and LLM-friendliness. This integration will support encoding/decoding in training pipelines, evaluation outputs, and LLM prompts.\n\n### Why this matters\n\n- **Token cost reduction**: Training and inference with large context windows can save 30-60% tokens on structured data, reducing API costs and improving throughput.\n- **LLM-friendly structure**: Explicit length markers `[N]` and field headers `{field1,field2}` help LLMs validate and generate structured output more reliably.\n- **Flexible serialization**: Drop-in replacement for JSON in data paths where uniform arrays dominate (logs, metrics, evaluation results, datasets).\n- **Python ecosystem alignment**: Multiple Python implementations available; official `toon_format` package in active development.\n\n---\n\n## What ships in PF-007\n\n- **Core utility module**: `src/aios/formats/toon_codec.py` (encoder/decoder with fallback to JSON)\n- **CLI integration**: \n  - `aios toon encode` - Convert JSON to TOON\n  - `aios toon decode` - Convert TOON to JSON\n  - Flags for existing commands: `--output-format toon|json` for metrics/evaluation outputs\n- **Training/Eval hooks**: Optional TOON encoding for JSONL metrics and evaluation results\n- **Config support**: YAML settings for TOON preferences (delimiter, indent, length markers)\n- **Documentation**: Examples, benchmarks, and best practices for when to use TOON vs JSON\n\n---\n\n## Architecture overview\n\n### Data paths affected\n\n1. **Metrics logging** (`artifacts/brains/actv1/metrics.jsonl`)\n   - Optional TOON encoding for training metrics with `--metrics-format toon`\n   - Particularly beneficial for batch eval results with uniform structure\n\n2. **Evaluation outputs** (`artifacts/evaluation/`)\n   - Encode evaluation results in TOON format for token-efficient LLM analysis\n   - Support both formats side-by-side for compatibility\n\n3. **Dataset exports** (`training_data/`)\n   - Convert curated datasets to TOON format for reduced storage and faster LLM ingestion\n   - Command: `aios datasets export --format toon`\n\n4. **LLM prompt payloads** (inline usage)\n   - Utility functions to encode context data in TOON before inserting into prompts\n   - Automatic format detection when decoding LLM outputs\n\n### Core components\n\n- **ToonCodec** (utility class): Wraps TOON encoder/decoder with graceful fallback\n  - `encode(data: Any, *, delimiter: str = ',', indent: int = 2, length_marker: bool = False) -> str`\n  - `decode(toon_str: str, *, strict: bool = True) -> Any`\n  - `is_toon_available() -> bool` (checks if TOON library installed)\n  \n- **Format negotiation**: Auto-detect format based on content or extension\n  - `.toon` extension for TOON files\n  - `.json` or `.jsonl` for JSON files\n  - Content sniffing: look for TOON patterns like `[N]{fields}:` headers\n\n- **Config schema** (`config/default.yaml`):\n  ```yaml\n  toon:\n    enabled: false  # Master switch\n    default_format: json  # json | toon\n    delimiter: ','  # ',' | '\\t' | '|'\n    indent: 2\n    length_marker: false  # Add # prefix to lengths\n    metrics: false  # Use TOON for metrics logging\n    evaluation: false  # Use TOON for evaluation outputs\n  ```\n\n---\n\n## Dependencies and setup\n\n### Python implementation options\n\n**Option 1: Official implementation (recommended)**\n```powershell\npip install toon-format  # When officially released\n```\n\n**Option 2: Community implementation (current)**\n```powershell\npip install python-toon  # https://github.com/xaviviro/python-toon\n```\n\n**Option 3: Custom lightweight implementation**\n- Implement minimal TOON encoder/decoder following [TOON spec v1.4](https://github.com/toon-format/spec)\n- Use conformance tests from spec repo to validate\n- Fallback option if no stable Python package exists\n\n### Installation strategy\n\nMake TOON optional dependency:\n```toml\n# pyproject.toml\n[project.optional-dependencies]\ntoon = [\n    \"python-toon>=0.1.0\",  # Or official package when available\n]\n```\n\nGraceful degradation: If TOON not installed, log info message and fall back to JSON.\n\n---\n\n## Implementation details\n\n### File: `src/aios/formats/toon_codec.py`\n\n```python\n\"\"\"TOON (Token-Oriented Object Notation) codec with fallback to JSON.\"\"\"\n\nfrom typing import Any, Literal, Optional\nimport json\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass ToonCodec:\n    \"\"\"Encode/decode TOON format with graceful JSON fallback.\"\"\"\n    \n    def __init__(self):\n        self._toon_available = self._check_toon_available()\n        if self._toon_available:\n            try:\n                from toon_format import encode as toon_encode\n                from toon_format import decode as toon_decode\n                self._encode_fn = toon_encode\n                self._decode_fn = toon_decode\n            except ImportError:\n                # Try alternative package name\n                from toon import encode as toon_encode\n                from toon import decode as toon_decode\n                self._encode_fn = toon_encode\n                self._decode_fn = toon_decode\n    \n    def _check_toon_available(self) -> bool:\n        \"\"\"Check if TOON library is installed.\"\"\"\n        try:\n            import toon_format\n            return True\n        except ImportError:\n            try:\n                import toon\n                return True\n            except ImportError:\n                return False\n    \n    def is_available(self) -> bool:\n        \"\"\"Return True if TOON encoding is available.\"\"\"\n        return self._toon_available\n    \n    def encode(\n        self,\n        data: Any,\n        *,\n        delimiter: Literal[',', '\\t', '|'] = ',',\n        indent: int = 2,\n        length_marker: bool = False,\n    ) -> str:\n        \"\"\"Encode data to TOON format, fallback to JSON.\"\"\"\n        if not self._toon_available:\n            logger.debug(\"TOON not available, falling back to JSON\")\n            return json.dumps(data, indent=indent)\n        \n        try:\n            options = {\n                'delimiter': delimiter,\n                'indent': indent,\n            }\n            if length_marker:\n                options['length_marker'] = '#'\n            \n            return self._encode_fn(data, **options)\n        except Exception as e:\n            logger.warning(f\"TOON encoding failed: {e}, falling back to JSON\")\n            return json.dumps(data, indent=indent)\n    \n    def decode(self, content: str, *, strict: bool = True) -> Any:\n        \"\"\"Decode TOON or JSON format (auto-detect).\"\"\"\n        # Try TOON first if available and content looks like TOON\n        if self._toon_available and self._looks_like_toon(content):\n            try:\n                return self._decode_fn(content, strict=strict)\n            except Exception as e:\n                logger.debug(f\"TOON decoding failed: {e}, trying JSON\")\n        \n        # Fallback to JSON\n        return json.loads(content)\n    \n    def _looks_like_toon(self, content: str) -> bool:\n        \"\"\"Heuristic check if content is TOON format.\"\"\"\n        # Look for TOON patterns: array headers like [N], [N]{fields}:\n        lines = content.strip().split('\\n')[:5]  # Check first few lines\n        for line in lines:\n            if '[' in line and ']:' in line:\n                return True\n            if '[' in line and ']{' in line and '}:' in line:\n                return True\n        return False\n\n# Global singleton\n_codec = ToonCodec()\n\ndef encode_toon(data: Any, **options) -> str:\n    \"\"\"Convenience function for encoding.\"\"\"\n    return _codec.encode(data, **options)\n\ndef decode_toon(content: str, **options) -> Any:\n    \"\"\"Convenience function for decoding.\"\"\"\n    return _codec.decode(content, **options)\n\ndef is_toon_available() -> bool:\n    \"\"\"Check if TOON encoding is available.\"\"\"\n    return _codec.is_available()\n```\n\n### File: `src/aios/cli/toon_cli.py`\n\n```python\n\"\"\"CLI commands for TOON format conversion.\"\"\"\n\nimport typer\nfrom pathlib import Path\nfrom typing import Optional, Literal\nimport json\n\nfrom aios.formats.toon_codec import ToonCodec, is_toon_available\n\napp = typer.Typer(help=\"Convert between JSON and TOON formats\")\n\n@app.command()\ndef encode(\n    input_path: Path = typer.Argument(..., help=\"Input JSON file or - for stdin\"),\n    output_path: Optional[Path] = typer.Option(None, \"-o\", \"--output\", help=\"Output TOON file (stdout if omitted)\"),\n    delimiter: Literal[',', '\\t', '|'] = typer.Option(',', \"--delimiter\", help=\"Array delimiter\"),\n    indent: int = typer.Option(2, \"--indent\", help=\"Indentation spaces\"),\n    length_marker: bool = typer.Option(False, \"--length-marker\", help=\"Add # prefix to array lengths\"),\n    stats: bool = typer.Option(False, \"--stats\", help=\"Show token count estimates\"),\n):\n    \"\"\"Convert JSON to TOON format.\"\"\"\n    if not is_toon_available():\n        typer.echo(\"Error: TOON library not installed. Install with: pip install python-toon\", err=True)\n        raise typer.Exit(1)\n    \n    # Read input\n    if str(input_path) == '-':\n        import sys\n        data = json.load(sys.stdin)\n    else:\n        with open(input_path) as f:\n            data = json.load(f)\n    \n    # Encode\n    codec = ToonCodec()\n    toon_output = codec.encode(\n        data,\n        delimiter=delimiter,\n        indent=indent,\n        length_marker=length_marker,\n    )\n    \n    # Show stats if requested\n    if stats:\n        json_output = json.dumps(data, indent=indent)\n        json_tokens = estimate_tokens(json_output)\n        toon_tokens = estimate_tokens(toon_output)\n        savings = (1 - toon_tokens / json_tokens) * 100\n        typer.echo(f\"\\n\ud83d\udcca Token Comparison:\", err=True)\n        typer.echo(f\"  JSON:    {json_tokens:,} tokens\", err=True)\n        typer.echo(f\"  TOON:    {toon_tokens:,} tokens\", err=True)\n        typer.echo(f\"  Savings: {savings:.1f}%\\n\", err=True)\n    \n    # Write output\n    if output_path:\n        output_path.write_text(toon_output)\n        typer.echo(f\"\u2713 Encoded to {output_path}\")\n    else:\n        typer.echo(toon_output)\n\n@app.command()\ndef decode(\n    input_path: Path = typer.Argument(..., help=\"Input TOON file or - for stdin\"),\n    output_path: Optional[Path] = typer.Option(None, \"-o\", \"--output\", help=\"Output JSON file (stdout if omitted)\"),\n    indent: int = typer.Option(2, \"--indent\", help=\"JSON indentation spaces\"),\n    strict: bool = typer.Option(True, \"--strict/--no-strict\", help=\"Enable strict validation\"),\n):\n    \"\"\"Convert TOON to JSON format.\"\"\"\n    if not is_toon_available():\n        typer.echo(\"Error: TOON library not installed. Install with: pip install python-toon\", err=True)\n        raise typer.Exit(1)\n    \n    # Read input\n    if str(input_path) == '-':\n        import sys\n        content = sys.stdin.read()\n    else:\n        content = input_path.read_text()\n    \n    # Decode\n    codec = ToonCodec()\n    data = codec.decode(content, strict=strict)\n    \n    # Write output\n    json_output = json.dumps(data, indent=indent)\n    if output_path:\n        output_path.write_text(json_output)\n        typer.echo(f\"\u2713 Decoded to {output_path}\")\n    else:\n        typer.echo(json_output)\n\ndef estimate_tokens(text: str) -> int:\n    \"\"\"Rough token estimate (4 chars = 1 token for English).\"\"\"\n    return len(text) // 4\n```\n\n### Integration with training pipeline\n\n**File: `src/aios/cli/hrm_hf/train_actv1.py`**\n\nAdd TOON support to metrics logging:\n\n```python\ndef _write_jsonl_helper(\n    log_file: Path,\n    payload: dict,\n    *,\n    format: Literal['json', 'toon'] = 'json',\n    toon_options: Optional[dict] = None,\n):\n    \"\"\"Write metrics in JSON or TOON format.\"\"\"\n    if format == 'toon':\n        from aios.formats.toon_codec import is_toon_available, encode_toon\n        if is_toon_available():\n            toon_options = toon_options or {}\n            line = encode_toon(payload, **toon_options)\n        else:\n            logger.warning(\"TOON not available, falling back to JSON\")\n            line = json.dumps(payload)\n    else:\n        line = json.dumps(payload)\n    \n    with open(log_file, 'a', encoding='utf-8') as f:\n        f.write(line + '\\n')\n```\n\n**CLI flags in `src/aios/cli/hrm_hf_cli.py`**:\n\n```python\n@app.command()\ndef train_actv1(\n    # ... existing params ...\n    metrics_format: Literal['json', 'toon'] = typer.Option(\n        'json',\n        '--metrics-format',\n        help='Output format for metrics (json or toon)',\n    ),\n    toon_delimiter: Literal[',', '\\t', '|'] = typer.Option(\n        ',',\n        '--toon-delimiter',\n        help='Delimiter for TOON format (comma, tab, or pipe)',\n    ),\n):\n    \"\"\"Train with optional TOON metrics output.\"\"\"\n    # ... pass through to training function\n```\n\n---\n\n## CLI design\n\n### New command group: `aios toon`\n\n```powershell\n# Encode JSON to TOON\naios toon encode input.json -o output.toon\n\n# Decode TOON to JSON\naios toon decode data.toon -o output.json\n\n# Pipe operations\ncat data.json | aios toon encode --stats\n\n# Tab-delimited for better compression\naios toon encode input.json --delimiter \"\\t\" --stats\n```\n\n### Integration with existing commands\n\n```powershell\n# Training with TOON metrics\naios hrm-hf train-actv1 \\\n    --model gpt2 \\\n    --dataset-file training_data/curated_datasets/test_sample.txt \\\n    --metrics-format toon \\\n    --toon-delimiter \"\\t\" \\\n    --log-file artifacts/brains/actv1/metrics.toon\n\n# Export evaluation in TOON format\naios evaluation export \\\n    --format toon \\\n    --output artifacts/evaluation/results.toon\n```\n\n---\n\n## Testing and acceptance criteria\n\n### Unit tests: `tests/test_toon_integration.py`\n\n```python\ndef test_toon_codec_encode_simple():\n    \"\"\"Test encoding simple objects.\"\"\"\n    data = {\"id\": 123, \"name\": \"Alice\", \"active\": True}\n    codec = ToonCodec()\n    output = codec.encode(data)\n    assert \"id:\" in output\n    assert \"Alice\" in output\n\ndef test_toon_codec_encode_tabular():\n    \"\"\"Test encoding arrays of objects (tabular format).\"\"\"\n    data = {\n        \"items\": [\n            {\"sku\": \"A1\", \"qty\": 2, \"price\": 9.99},\n            {\"sku\": \"B2\", \"qty\": 1, \"price\": 14.5},\n        ]\n    }\n    codec = ToonCodec()\n    output = codec.encode(data)\n    assert \"items[2]{sku,qty,price}:\" in output\n    assert \"A1,2,9.99\" in output\n\ndef test_toon_codec_roundtrip():\n    \"\"\"Test encode -> decode roundtrip.\"\"\"\n    original = {\n        \"users\": [\n            {\"id\": 1, \"name\": \"Alice\", \"role\": \"admin\"},\n            {\"id\": 2, \"name\": \"Bob\", \"role\": \"user\"},\n        ]\n    }\n    codec = ToonCodec()\n    encoded = codec.encode(original)\n    decoded = codec.decode(encoded)\n    assert decoded == original\n\ndef test_toon_fallback_when_unavailable():\n    \"\"\"Test graceful fallback to JSON when TOON not installed.\"\"\"\n    # Mock TOON unavailable\n    # Verify JSON output returned instead\n```\n\n### Integration tests\n\n```powershell\n# Test metrics logging with TOON\naios hrm-hf train-actv1 \\\n    --model gpt2 \\\n    --dataset-file training_data/curated_datasets/test_sample.txt \\\n    --steps 1 --batch-size 2 \\\n    --metrics-format toon \\\n    --log-file artifacts/test_toon_metrics.toon\n\n# Verify output is valid TOON\naios toon decode artifacts/test_toon_metrics.toon\n```\n\n### Acceptance criteria\n\n- \u2705 TOON encoding reduces token count by 30-60% for tabular metrics data\n- \u2705 CLI can convert between JSON and TOON formats bidirectionally\n- \u2705 Training pipeline can output metrics in TOON format with flag\n- \u2705 Graceful fallback to JSON when TOON not installed (no crashes)\n- \u2705 Documentation includes benchmarks and when to use TOON vs JSON\n\n---\n\n## Use cases and recommendations\n\n### When to use TOON\n\n\u2705 **Excellent candidates:**\n- Training metrics with uniform structure (loss, accuracy, step numbers)\n- Evaluation results with consistent fields across samples\n- Large tabular datasets for LLM ingestion\n- Configuration exports with repeated structures\n- Batch prediction outputs\n\n\u2705 **Benefits:**\n- 30-60% token savings on uniform tabular data\n- Better than JSON for arrays of objects\n- Comparable to CSV but with nested object support\n\n### When to stick with JSON\n\n\u274c **Keep using JSON for:**\n- Deeply nested configurations with varied structures\n- Non-uniform data with inconsistent fields\n- Legacy compatibility requirements\n- Data consumed by non-TOON-aware tools\n\n### Benchmark expectations\n\nBased on TOON project benchmarks:\n\n| Data Type | TOON vs JSON | TOON vs JSON (compact) |\n|-----------|--------------|------------------------|\n| Uniform employee records | -60.7% | -36.8% |\n| E-commerce orders (mixed) | -33.1% | +5.5% |\n| Event logs (semi-uniform) | -15.0% | +19.9% |\n| Deeply nested config | -31.3% | +11.9% |\n\n*Note: TOON typically uses more tokens than minified JSON for deeply nested data, but offers better structure for LLMs.*\n\n---\n\n## Risks and mitigations\n\n### Risk: Python package stability\n- **Mitigation**: Use optional dependency + graceful fallback; consider custom implementation\n- **Status**: Official `toon_format` package in development; community implementations available\n\n### Risk: Format adoption\n- **Mitigation**: Keep JSON as default; TOON opt-in via flags; support both formats side-by-side\n- **Status**: TOON gaining traction in LLM community; 11.3k GitHub stars\n\n### Risk: Compatibility with existing tools\n- **Mitigation**: Provide conversion utilities; document migration path\n- **Status**: CLI tools make conversion trivial\n\n### Risk: LLM understanding\n- **Mitigation**: Include format examples in prompts; test with multiple models\n- **Status**: Benchmarks show 68.7% retrieval accuracy (vs JSON 65.7%) with token savings\n\n---\n\n## Rollout plan\n\n### Phase 1: Foundation (2 days)\n- [ ] Implement `ToonCodec` utility with fallback\n- [ ] Add optional dependency to `pyproject.toml`\n- [ ] Create unit tests for encode/decode\n- [ ] Add config schema to `config/default.yaml`\n\n### Phase 2: CLI tools (1 day)\n- [ ] Implement `aios toon encode/decode` commands\n- [ ] Add `--stats` flag for token comparison\n- [ ] Test conversion workflows\n\n### Phase 3: Training integration (1 day)\n- [ ] Add `--metrics-format toon` flag to training CLI\n- [ ] Implement TOON output in `_write_jsonl_helper`\n- [ ] Test training runs with TOON metrics\n\n### Phase 4: Documentation & examples (1 day)\n- [ ] Write user guide with examples\n- [ ] Add benchmark results\n- [ ] Create migration guide from JSON to TOON\n- [ ] Document when to use TOON vs JSON\n\n### Phase 5: Optional enhancements (future)\n- [ ] TOON support in evaluation exports\n- [ ] Dataset conversion utilities\n- [ ] VS Code extension for TOON syntax highlighting\n- [ ] Custom TOON implementation if official package delays\n\n---\n\n## Documentation outline\n\n### User guide sections\n\n1. **What is TOON?**\n   - Overview and benefits\n   - Token efficiency benchmarks\n   - LLM-friendly structure\n\n2. **Installation**\n   ```powershell\n   pip install \"aios[toon]\"\n   ```\n\n3. **Quick start**\n   - Converting files\n   - Using in training\n   - Reading TOON output\n\n4. **When to use TOON**\n   - Decision matrix\n   - Use case examples\n   - Performance expectations\n\n5. **Format reference**\n   - Syntax examples\n   - Object encoding\n   - Array encoding (tabular)\n   - Delimiter options\n\n6. **Troubleshooting**\n   - Package not installed\n   - Format detection issues\n   - Encoding errors\n\n### Examples\n\n```powershell\n# Example 1: Compare token usage\naios toon encode training_metrics.json --stats\n\n# Example 2: Convert existing metrics to TOON\nGet-ChildItem artifacts/brains/actv1/*.jsonl | ForEach-Object {\n    $output = $_.FullName -replace '\\.jsonl$', '.toon'\n    aios toon encode $_.FullName -o $output --delimiter \"\\t\" --stats\n}\n\n# Example 3: Training with TOON output\naios hrm-hf train-actv1 \\\n    --model artifacts/hf_implant/gpt2 \\\n    --dataset-file training_data/curated_datasets/test_sample.txt \\\n    --steps 100 --batch-size 4 \\\n    --metrics-format toon \\\n    --toon-delimiter \"\\t\" \\\n    --log-file artifacts/brains/actv1/metrics.toon\n\n# Example 4: Analyze TOON metrics with LLM\n$metrics = Get-Content artifacts/brains/actv1/metrics.toon -Raw\n# Pass to LLM with prompt: \"Analyze these training metrics in TOON format...\"\n```\n\n---\n\n## References\n\n- TOON official spec: https://github.com/toon-format/spec (v1.4)\n- TOON TypeScript implementation: https://github.com/toon-format/toon\n- TOON website: https://toonformat.dev/\n- Community Python implementation: https://github.com/xaviviro/python-toon\n- Official Python package (in dev): https://github.com/toon-format/toon-python\n- Format playground: https://www.curiouslychase.com/playground/format-tokenization-exploration\n\n### Key insights from TOON project\n\n1. **Token efficiency**: 30-60% savings on uniform data\n2. **LLM accuracy**: 68.7% vs JSON's 65.7% on retrieval tasks\n3. **Best use case**: Arrays of objects with identical primitive fields\n4. **Delimiter impact**: Tab delimiter often more efficient than comma\n5. **Length markers**: `[#N]` notation helps LLMs validate structure\n\n---\n\n## Future enhancements\n\n### Post-MVP features\n\n1. **Streaming TOON parser**\n   - Process large TOON files line-by-line\n   - Memory-efficient for big datasets\n\n2. **TOON compression analysis**\n   - Automated tool to analyze JSON \u2192 TOON savings potential\n   - Recommend TOON for files above threshold\n\n3. **VS Code integration**\n   - Syntax highlighting for `.toon` files\n   - Hover tooltips with field info\n   - Convert commands in context menu\n\n4. **Custom TOON recognizers for AI-OS**\n   - Domain-specific patterns (model names, metrics)\n   - Optimized for AI-OS data structures\n\n5. **TOON-aware log viewer**\n   - GUI tool to browse TOON metrics\n   - Side-by-side comparison with JSON\n\n---\n\n## Developer checklist\n\n### Implementation\n- [ ] Create `src/aios/formats/toon_codec.py`\n- [ ] Create `src/aios/cli/toon_cli.py`\n- [ ] Add TOON config to `config/default.yaml`\n- [ ] Update `pyproject.toml` with optional dependency\n- [ ] Integrate into training pipeline\n- [ ] Add CLI flags to `hrm_hf_cli.py`\n\n### Testing\n- [ ] Unit tests for `ToonCodec`\n- [ ] CLI command tests\n- [ ] Integration test with training\n- [ ] Roundtrip tests (encode \u2192 decode)\n- [ ] Fallback behavior tests\n\n### Documentation\n- [ ] User guide in `docs/guide/toon_integration.md`\n- [ ] Update training docs with TOON flags\n- [ ] Add examples to `docs/examples/`\n- [ ] Update README with TOON mention\n\n### Validation\n- [ ] Benchmark token savings on actual AI-OS metrics\n- [ ] Test LLM comprehension with TOON metrics\n- [ ] Verify graceful degradation without TOON package\n- [ ] Performance testing on large datasets\n\n---\n\n## Operator checklist\n\n### Pre-deployment\n- [ ] Install TOON package: `pip install python-toon`\n- [ ] Test conversion: `aios toon encode <sample.json> --stats`\n- [ ] Verify token savings meet expectations (>20% for tabular data)\n\n### Migration\n- [ ] Convert existing metrics: `aios toon encode <metrics.jsonl> -o <metrics.toon>`\n- [ ] Update analysis scripts to handle TOON format\n- [ ] Train team on TOON syntax and when to use it\n\n### Production use\n- [ ] Add `--metrics-format toon` to training scripts for long runs\n- [ ] Use tab delimiter for maximum compression: `--toon-delimiter \"\\t\"`\n- [ ] Monitor file sizes and token usage\n- [ ] Keep JSON format for compatibility where needed\n\n---\n\n## Success metrics\n\n### Quantitative\n- \u2705 30-60% token reduction on uniform training metrics\n- \u2705 No performance regression in training throughput\n- \u2705 100% roundtrip accuracy (encode \u2192 decode)\n- \u2705 Graceful fallback when TOON unavailable\n\n### Qualitative\n- \u2705 Users can easily convert between formats\n- \u2705 LLM analysis of TOON metrics is as good or better than JSON\n- \u2705 Documentation is clear and includes decision matrix\n- \u2705 Integration feels natural (opt-in, not forced)\n\n---\n\n## Quickstart (Windows/PowerShell)\n\n```powershell\n# 1) Install TOON support\npip install \"aios[toon]\"\n\n# 2) Test conversion on existing file\naios toon encode artifacts/brains/actv1/metrics.jsonl --stats\n\n# 3) Run training with TOON output\naios hrm-hf train-actv1 `\n    --model gpt2 `\n    --dataset-file training_data/curated_datasets/test_sample.txt `\n    --steps 100 --batch-size 4 `\n    --metrics-format toon `\n    --toon-delimiter \"\\t\" `\n    --log-file artifacts/brains/actv1/metrics.toon\n\n# 4) Convert back to JSON for analysis\naios toon decode artifacts/brains/actv1/metrics.toon -o metrics_decoded.json\n\n# 5) Compare file sizes\n(Get-Item artifacts/brains/actv1/metrics.toon).Length\n(Get-Item metrics_decoded.json).Length\n```\n\n---\n\n## Summary\n\nTOON integration provides a token-efficient alternative to JSON for AI-OS data serialization, particularly beneficial for training metrics, evaluation outputs, and LLM prompt payloads. The implementation follows AI-OS patterns with optional dependencies, graceful fallbacks, and clear documentation. By adopting TOON for uniform tabular data, operators can reduce token costs by 30-60% while maintaining or improving LLM comprehension.\n", "tags": ["cli", "datasets", "evaluation", "training"], "headings": [{"line": 0, "text": "PF-007: TOON Format Integration for Token-Efficient LLM Data Exchange"}, {"line": 2, "text": "Summary"}, {"line": 6, "text": "Why this matters"}, {"line": 15, "text": "What ships in PF-007"}, {"line": 28, "text": "Architecture overview"}, {"line": 30, "text": "Data paths affected"}, {"line": 48, "text": "Core components"}, {"line": 74, "text": "Dependencies and setup"}, {"line": 76, "text": "Python implementation options"}, {"line": 93, "text": "Installation strategy"}, {"line": 97, "text": "pyproject.toml"}, {"line": 108, "text": "Implementation details"}, {"line": 110, "text": "File: `src/aios/formats/toon_codec.py`"}, {"line": 133, "text": "Try alternative package name"}, {"line": 183, "text": "Try TOON first if available and content looks like TOON"}, {"line": 190, "text": "Fallback to JSON"}, {"line": 195, "text": "Look for TOON patterns: array headers like [N], [N]{fields}:"}, {"line": 204, "text": "Global singleton"}, {"line": 220, "text": "File: `src/aios/cli/toon_cli.py`"}, {"line": 248, "text": "Read input"}, {"line": 256, "text": "Encode"}, {"line": 265, "text": "Show stats if requested"}, {"line": 276, "text": "Write output"}, {"line": 295, "text": "Read input"}, {"line": 302, "text": "Decode"}, {"line": 306, "text": "Write output"}, {"line": 319, "text": "Integration with training pipeline"}, {"line": 354, "text": "... existing params ..."}, {"line": 367, "text": "... pass through to training function"}, {"line": 372, "text": "CLI design"}, {"line": 374, "text": "New command group: `aios toon`"}, {"line": 377, "text": "Encode JSON to TOON"}, {"line": 380, "text": "Decode TOON to JSON"}, {"line": 383, "text": "Pipe operations"}, {"line": 386, "text": "Tab-delimited for better compression"}, {"line": 390, "text": "Integration with existing commands"}, {"line": 393, "text": "Training with TOON metrics"}, {"line": 401, "text": "Export evaluation in TOON format"}, {"line": 409, "text": "Testing and acceptance criteria"}, {"line": 411, "text": "Unit tests: `tests/test_toon_integration.py`"}, {"line": 450, "text": "Mock TOON unavailable"}, {"line": 451, "text": "Verify JSON output returned instead"}, {"line": 454, "text": "Integration tests"}, {"line": 457, "text": "Test metrics logging with TOON"}, {"line": 465, "text": "Verify output is valid TOON"}, {"line": 469, "text": "Acceptance criteria"}, {"line": 479, "text": "Use cases and recommendations"}, {"line": 481, "text": "When to use TOON"}, {"line": 495, "text": "When to stick with JSON"}, {"line": 503, "text": "Benchmark expectations"}, {"line": 518, "text": "Risks and mitigations"}, {"line": 520, "text": "Risk: Python package stability"}, {"line": 524, "text": "Risk: Format adoption"}, {"line": 528, "text": "Risk: Compatibility with existing tools"}, {"line": 532, "text": "Risk: LLM understanding"}, {"line": 538, "text": "Rollout plan"}, {"line": 540, "text": "Phase 1: Foundation (2 days)"}, {"line": 546, "text": "Phase 2: CLI tools (1 day)"}, {"line": 551, "text": "Phase 3: Training integration (1 day)"}, {"line": 556, "text": "Phase 4: Documentation & examples (1 day)"}, {"line": 562, "text": "Phase 5: Optional enhancements (future)"}, {"line": 570, "text": "Documentation outline"}, {"line": 572, "text": "User guide sections"}, {"line": 605, "text": "Examples"}, {"line": 608, "text": "Example 1: Compare token usage"}, {"line": 611, "text": "Example 2: Convert existing metrics to TOON"}, {"line": 617, "text": "Example 3: Training with TOON output"}, {"line": 626, "text": "Example 4: Analyze TOON metrics with LLM"}, {"line": 628, "text": "Pass to LLM with prompt: \"Analyze these training metrics in TOON format...\""}, {"line": 633, "text": "References"}, {"line": 642, "text": "Key insights from TOON project"}, {"line": 652, "text": "Future enhancements"}, {"line": 654, "text": "Post-MVP features"}, {"line": 679, "text": "Developer checklist"}, {"line": 681, "text": "Implementation"}, {"line": 689, "text": "Testing"}, {"line": 696, "text": "Documentation"}, {"line": 702, "text": "Validation"}, {"line": 710, "text": "Operator checklist"}, {"line": 712, "text": "Pre-deployment"}, {"line": 717, "text": "Migration"}, {"line": 722, "text": "Production use"}, {"line": 730, "text": "Success metrics"}, {"line": 732, "text": "Quantitative"}, {"line": 738, "text": "Qualitative"}, {"line": 746, "text": "Quickstart (Windows/PowerShell)"}, {"line": 749, "text": "1) Install TOON support"}, {"line": 752, "text": "2) Test conversion on existing file"}, {"line": 755, "text": "3) Run training with TOON output"}, {"line": 764, "text": "4) Convert back to JSON for analysis"}, {"line": 767, "text": "5) Compare file sizes"}, {"line": 774, "text": "Summary"}]}, {"path": "planned_features/TPU_CLOUD_SUPPORT.md", "content": "# TPU and Cloud Accelerator Support\n\n**Status:** \ud83d\udccb Planned  \n**Priority:** High (Option A: Google Cloud TPU first)  \n**Complexity:** High (multi-week)  \n**Target Version:** Future Release  \n**Created:** 2025-12-11  \n**Owner:** @AI-OS-Team\n\n## Overview\n\nAdd first-class support for TPUs (Google Cloud TPU VM) and cloud accelerators (AWS GPU/Trainium) across CLI, GUI, installers, and training/evaluation pipelines. Today AI-OS is CUDA-first; this plan introduces an accelerator abstraction that can target CUDA, XLA (TPU), and NeuronX (Trainium), and provides remote-execution hooks for cloud runs.\n\nOption A is to ship TPU support first (Google Cloud TPU v4/v5e via torch_xla), then add cloud GPU/Trainium (AWS EC2 P4d/P5 + Trn1/Trn2) as a follow-on.\n\n## Feasibility (no paid hardware access)\n\n- **Local validation path:** torch_xla provides a CPU backend (`PJRT_DEVICE=CPU`) that can execute tiny models; we can use this to sanity-check adapter code, launcher plumbing, and config validation without TPU hardware. Performance will be extremely slow but sufficient for unit/integration tests.\n- **Mocked device inventory:** Add mocks/fakes for device enumeration (XLA/Neuron) so CLI/GUI can be exercised in CI without accelerators present.\n- **Dry-run launchers:** Implement `--dry-run` for cloud launch scripts to validate command generation, env vars, and bucket paths without actually creating instances.\n- **Static analysis:** Dependency guards (optional extras), config schema validation, and type/unit tests around the accelerator abstraction give confidence before real hardware.\n- **Community/manual verification:** Once merged, seek community testers with TPU/Trainium to confirm real throughput; keep feature gated/experimental until validated.\n\n## Current State\n\n- `config/default.yaml` hardcodes `train_device: cuda` and `run_device: cuda`; VRAM detection assumes CUDA.\n- DeepSpeed configs exist for ZeRO stages (1\u20133) with CPU/NVMe offload but are CUDA-oriented.\n- Mixed-vendor GPU and AMD/Intel GPU support plans exist (`MIXED_GPU_VENDOR_SUPPORT.md`, `AMD_INTEL_GPU_SUPPORT_FIX.md`), but no TPU/Neuron abstractions.\n- Installers and GUI do not surface non-CUDA accelerators.\n\n## Goals\n\n### Must Have (TPU first)\n- XLA device backend option (`train_device: xla`, `run_device: xla`) with torch_xla 2.2+.\n- TPU VM local-mode training/eval support (single host, multi-core with `xmp.spawn`).\n- TPU Pod slice support via PJRT multi-host launch (gcloud multi-ssh).\n- Data/model I/O on GCS buckets with authenticated access.\n- CLI/GUI surface for selecting `xla` and showing TPU topology (cores, slice type).\n- Logging/metrics capture (torch_xla debug metrics) into existing diagnostics pipeline.\n\n### Should Have (Cloud GPU/Trainium)\n- AWS GPU presets (P4d/P5) with CUDA/NVLink awareness; AMI/DLC suggestions.\n- Trainium (Trn1/Trn2) backend via torch-neuronx / NeuronX Distributed (NxD) for training; neuron runtimes for inference.\n- Remote launcher that provisions/SSH\u2019s into cloud instances (optionally user-provided) and runs AI-OS jobs headless.\n- Artifact sync (checkpoints, logs) between cloud and local via S3/GCS.\n\n### Nice to Have\n- Cost/time estimator using known per-hour pricing and VRAM/throughput tables.\n- Auto-select best accelerator given context length, batch, and budget.\n- GUI wizards for cloud credential setup and instance selection.\n\n### Out of Scope (for this feature)\n- Azure TPU/NPUs; on-prem TPU pods; managed training services (Vertex AI, SageMaker) fully integrated (can be future work).\n- Heterogeneous TPU+GPU simultaneous training.\n\n## Design\n\n### Accelerator Abstraction\n- Introduce `AcceleratorType = ['cuda', 'xla', 'neuron', 'cpu']` with a thin adapter:\n  - Device discovery: CUDA (torch.cuda), XLA (torch_xla.xla_model devices), Neuron (torch_neuronx / neuronx_distributed), CPU fallback.\n  - Memory/properties API returning `{name, vendor, total_mem_mb, available_mem_mb, topology_info}`.\n  - AMP/autocast helpers per backend.\n- Reuse mixed-vendor detection patterns from `MIXED_GPU_VENDOR_SUPPORT.md`; extend to XLA/Neuron.\n\n**Backend specifics to handle:**\n- XLA: uses PJRT runtime; device strings `xla:0..N`; requires `xm.mark_step()` or `xm.optimizer_step`. Multi-core uses `xmp.spawn`; multi-host uses gcloud `--worker=all` wrapper. Some CUDA custom ops (FlashAttention, bitsandbytes) are unavailable\u2014must feature-flag.\n- Neuron (Trainium): uses `torch-neuronx` and `neuronx_distributed` (`import neuronx_distributed as nxd`); BF16 preferred, FP16 unsupported; ops coverage is narrower than CUDA\u2014need safe fallbacks and op list checks. Launchers typically use `torchrun --nproc_per_node=<n_neuron_cores> --nnodes=<hosts>` with `NEURON_RT_NUM_CORES`.\n\n### Configuration\n- `config/default.yaml`: allow `train_device`/`run_device` values `cuda|xla|neuron|cpu`; add `cloud_provider: none|gcp|aws` and TPU/Neuron specific knobs (zone, instance type, slice, bucket paths).\n- New TPU config section (e.g., `tpu:`): `{type: v5e-8|v4-8|v4-16, zone, project, service_account, pjrt_device: TPU, gcs_bucket}`.\n- New AWS section (e.g., `aws:`): `{region, instance_type: p4d.24xlarge|p5.48xlarge|trn1.32xlarge, ami_or_dlc, s3_bucket}`.\n- DeepSpeed: add TPU-friendly defaults (no CUDA custom ops; use bf16 where available); add NxD config templates for Trainium.\n\n**Installer/optional deps:**\n- Add optional extras: `xla` (torch_xla wheels pinned to torch version) and `neuron` (torch-neuronx, neuronx_distributed). Keep them out of default install to avoid heavy wheels.\n- Installers/scripts should detect platform: on TPU VM, `pip install torch==<pin> torch_xla==<same>` or use prebuilt images; on Trainium, install Neuron repo key and `pip install torch-neuronx neuronx_distributed`.\n\n### Training/Eval Pipeline Changes\n- CLI (`aios train/eval`) to route device creation through accelerator adapter; when `xla`, wrap entrypoint with `PJRT_DEVICE=TPU` and `xmp.spawn` for multi-core.\n- Data loaders: ensure `MpDeviceLoader` path for TPU; pin memory off by default for XLA.\n- Checkpoint/optimizer state: confirm torch_xla `xm.save` for TPU; S3/GCS sync hooks for remote runs.\n- Metrics: capture torch_xla metrics (`xm.get_metrics`) and include in diagnostics JSON.\n\n### Remote Execution (Cloud)\n- **GCP TPU VM**: Use gcloud CLI to create/start TPU VM (v5e-8, v4-8/16), install AI-OS deps, run training via SSH (`--worker=all` for pod slices). Provide scripts under `installers/scripts/cloud/`.\n- **AWS GPU**: Provide user guidance to launch P4d/P5 with NVIDIA drivers + Deep Learning AMI/Container; optional helper script to SSH and run AI-OS headless.\n- **AWS Trainium**: Optional pilot: install `torch-neuronx`, `neuronx_distributed`; use NxD launcher for multi-device; document limitations (FP16 unsupported, prefer bf16).\n- Artifact handling: sync checkpoints/logs to S3/GCS; resume locally by download.\n\n**Reference launch flows (sketch):**\n- GCP TPU v5e-8 single host: `gcloud compute tpus tpu-vm create NAME --zone=Z --accelerator-type=v5e-8 --version=tpu-vm-v5-lite`; SSH, install deps, `PJRT_DEVICE=TPU python3 train.py` or `python3 -m torch_xla.distributed.xla_spawn --num_devices=8 train.py`.\n- GCP TPU v4-16 pod slice: same create command with `--worker=all --command=\"PJRT_DEVICE=TPU python3 train.py\"` to run on all hosts.\n- AWS P4d/P5: recommend AWS Deep Learning AMI or DLC; run `torchrun --nproc_per_node=8 train.py` with NCCL/TCP tuned; ensure `NCCL_IB_HCA`, `NCCL_NET_GDR_LEVEL=PHB` set for good NVLink/ENA perf.\n- AWS Trainium: install Neuron (`pip install torch-neuronx neuronx_distributed`), set `NEURON_CC_FLAGS=\"--model-type transformer\"` as needed, launch with `torchrun --nproc_per_node=<cores> --nnodes=<hosts> --rdzv_backend=c10d --rdzv_endpoint=<host>:29400 train_neuron.py`.\n\n## Phased Plan\n\n### Phase 1 \u2014 TPU Local (TPU VM single host)\n- Add accelerator abstraction module (`src/aios/core/accelerators.py`), integrate with CLI/GUI device display and `torch_info_cmd`.\n- Add XLA backend support to training/eval launchers; ensure PJRT env + `xmp.spawn` path.\n- Update config defaults to accept `xla`; add validation.\n- Add doc/UX guidance for installing torch_xla on TPU VM (pip wheels or preinstalled images).\n\n**Feasibility gates:** CPU/XLA backend smoke tests pass; dry-run launchers generate correct commands; config schema validation covers `xla` path.\n\n### Phase 2 \u2014 TPU Pods & Cloud Ops (GCP)\n- Add gcloud-based helper scripts to provision TPU VM, upload data, and start training across hosts (using `--worker=all`).\n- Add bucket-based dataset/checkpoint paths; optional rsync/gsutil wrappers.\n- GUI/CLI status surfaces TPU topology and logs; collect torch_xla metrics.\n\n**Feasibility gates:** dry-run pod launch works; bucket sync tested locally; metrics collection path exercised with mock metrics.\n\n### Phase 3 \u2014 AWS Cloud (GPU + Trainium)\n- Add AWS config schema and guidance for P4d/P5 with DeepSpeed; confirm nvlink awareness in topology readout.\n- (Pilot) Trainium path with torch-neuronx/NxD templates; detect Neuron devices; add warnings for unsupported ops.\n- Add S3 sync helpers parallel to GCS flow.\n\n**Feasibility gates:** dry-run AWS launcher emits correct commands; Neuron device detection mocked in CI; S3 sync script dry-run passes.\n\n## Risks & Mitigations\n- **Dependency weight**: torch_xla / torch-neuronx are large; gate installs behind optional extras and runtime detection.\n- **Kernel incompatibilities**: Custom CUDA ops may fail on XLA/Neuron; provide backend-specific fallbacks and disable unsupported optimizations.\n- **Network/egress costs**: Large dataset sync to cloud; add dry-run size estimator and warning prompts.\n- **Security**: Manage cloud credentials via env/CLI; never store secrets in config; document least-privilege IAM roles.\n\n## References (external)\n- PyTorch/XLA TPU docs: https://docs.pytorch.org/xla/release/2.2/index.html\n- AWS Neuron (Trainium/Inferentia) docs: https://awsdocs-neuron.readthedocs-hosted.com/en/latest/\n- AWS Deep Learning Containers/AMIs for PyTorch GPUs: https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-ec2-tutorials-training.html\n", "tags": ["cli", "evaluation", "gui", "training"], "headings": [{"line": 0, "text": "TPU and Cloud Accelerator Support"}, {"line": 9, "text": "Overview"}, {"line": 15, "text": "Feasibility (no paid hardware access)"}, {"line": 23, "text": "Current State"}, {"line": 30, "text": "Goals"}, {"line": 32, "text": "Must Have (TPU first)"}, {"line": 40, "text": "Should Have (Cloud GPU/Trainium)"}, {"line": 46, "text": "Nice to Have"}, {"line": 51, "text": "Out of Scope (for this feature)"}, {"line": 55, "text": "Design"}, {"line": 57, "text": "Accelerator Abstraction"}, {"line": 68, "text": "Configuration"}, {"line": 78, "text": "Training/Eval Pipeline Changes"}, {"line": 84, "text": "Remote Execution (Cloud)"}, {"line": 96, "text": "Phased Plan"}, {"line": 98, "text": "Phase 1 \u2014 TPU Local (TPU VM single host)"}, {"line": 106, "text": "Phase 2 \u2014 TPU Pods & Cloud Ops (GCP)"}, {"line": 113, "text": "Phase 3 \u2014 AWS Cloud (GPU + Trainium)"}, {"line": 120, "text": "Risks & Mitigations"}, {"line": 126, "text": "References (external)"}]}, {"path": "planned_features/UNLIMIFORMER_INTEGRATION.md", "content": "## Unlimiformer integration for unlimited-length inputs\n\nThis planned feature proposes integrating Unlimiformer (NeurIPS 2023) into AI\u2011OS to enable effectively unlimited-length inputs for supported Hugging Face models during evaluation/inference and optionally during training.\n\nReferences\n- Paper: \u201cUnlimiformer: Long-Range Transformers with Unlimited Length Input\u201d (Bertsch et al., 2023)\n- Code: https://github.com/abertsch72/unlimiformer (MIT License)\n\n\n### Why this matters\n- Handle very long prompts (books, entire project repos, long transcripts) without truncation.\n- Keep base attention unchanged; add retrieval-based attention above a chosen layer (\u201clayer_begin\u201d).\n- Works with existing pretrained models (e.g., BART/T5 encoder\u2013decoder; decoder-only models such as Llama\u20112 according to the repo README) and can be used purely at evaluation time or for specialized training regimes.\n\n\n## Scope and success criteria\n\nIn scope (Phase 1)\n- Evaluation/inference integration for HF-backed \u201cbrains\u201d (aios.core.hrm_models.hf_adapter) with decoder-only models (LLaMA family) using Unlimiformer\u2019s retrieval at generation time.\n- Early-stopping evaluation using Unlimiformer on long validation inputs.\n- Configuration flags in AI\u2011OS to toggle Unlimiformer without breaking existing flows.\n\nIn scope (Phase 2)\n- Training-time modes: \u201crandom-encoded\u201d and \u201cretrieval\u201d training; alternating schedule as in the paper.\n- Encoder\u2013decoder support (e.g., BART/T5) on summarization-like datasets.\n\nOut of scope (initially)\n- Direct integration into the custom ACTV1 HRM model (non-HF architecture). This would require substantial porting of the attention hook path and is a potential Phase 3 exploration.\n\nSuccess criteria\n- P0: For a HF LLaMA\u2011family model configured in AI\u2011OS, users can enable Unlimiformer and successfully generate from inputs > base context (e.g., \u2265 100k tokens) with stable memory via FAISS datastore; outputs match expectations on long-doc summarization prompts.\n- P1: Automated test(s) verify that enabling Unlimiformer leaves standard generation unchanged on short inputs (parity within tolerance) and does not regress normal inference when disabled.\n\n\n## High-level design\n\nConcept\n- Unlimiformer augments a HF model with a retrieval path over the full input. Layers below `layer_begin` attend as usual over the last `context_window_size` tokens; from `layer_begin` upwards, cross/retrieval attention uses K\u2011NN over a datastore of hidden states (optionally via FAISS) to bring in relevant tokens from the entire (long) input.\n\nIntegration strategy in AI\u2011OS\n1) HF integration layer\n   - Extend or wrap `HFCausalLM_HRMAdapter` to optionally enable Unlimiformer for decoder-only models.\n   - Provide a thin compatibility shim that mirrors the Unlimiformer repo\u2019s expected arguments (e.g., `test_unlimiformer`, `layer_begin`, `use_datastore`, `gpu_index`, `gpu_datastore`, `index_devices`, `datastore_device`, `knn`).\n   - Keep Unlimiformer disabled by default.\n\n2) Vendor minimal Unlimiformer components\n   - Add `aios.integrations.unlimiformer` module containing the minimal files required from the upstream `src/` (per MIT license), focusing on inference paths: model wrapper, indexing/datastore, and argument schema.\n   - Avoid forking the training script initially; rely on our Typer CLI and config system.\n\n3) Configuration plumbing\n   - Add optional config keys under `brains.trainer_overrides` (HF mode) to toggle Unlimiformer and pass-through parameters.\n   - Example keys (all optional):\n     - `unlimiformer.enabled: false`\n     - `unlimiformer.layer_begin: 16`\n     - `unlimiformer.knn: true`\n     - `unlimiformer.use_datastore: true`\n     - `unlimiformer.gpu_datastore: true`\n     - `unlimiformer.gpu_index: true`\n     - `unlimiformer.index_devices: [1]`\n     - `unlimiformer.datastore_device: 1`\n     - `unlimiformer.context_window_size: 4096` (fallback sliding window for lower layers)\n     - `unlimiformer.eval_max_source_length: 999999`\n   - Defaults keep feature off, requiring no extra dependencies.\n\n4) Dependency management\n   - Add optional extra `[unlimiformer]` in `pyproject.toml` including: `faiss-cpu` (Windows-friendly), `faiss-gpu` (Linux/CUDA environments, optional), `transformers>=4.33`, and numpy/scipy as needed.\n   - Detect platform; default to FAISS\u2011CPU on Windows; allow GPU index/datastore where supported.\n\n5) Memory/runtimes\n   - For very long inputs, use FAISS datastore (CPU by default) to keep GPU memory stable.\n   - Provide flags to move index/datastore to GPU when memory allows.\n\n6) UX\n   - CLI switches added to relevant commands (e.g., `aios hrm-hf chat` or evaluation flows) to toggle Unlimiformer.\n   - GUI toggle in HRM/HF panels: \u201cUnlimited context (Unlimiformer)\u201d with tooltips and safe defaults.\n\n\n## Detailed design and APIs\n\n### Module layout\n- `src/aios/integrations/unlimiformer/`\n  - `__init__.py`: feature gate and version checks\n  - `adapter.py`: small wrapper class that:\n    - Accepts a HF model and tokenizer\n    - Builds/attaches Unlimiformer components\n    - Exposes a `.generate(...)` and `.prepare_inputs_for_long_context(prefix, prompt, suffix)` helper\n  - `datastore.py`: thin wrapper over FAISS index/datastore setup (CPU/GPU), device routing, and persistence\n  - `args.py`: shared dataclass/TypedDict mapping AI\u2011OS config to Unlimiformer expected settings\n  - `compat.py`: utilities to resolve `layer_begin` based on model depth and provide recommended defaults\n\n\n### HF adapter extension points\nLocation: `src/aios/core/hrm_models/hf_adapter.py`\n\nAdd optional Unlimiformer activation path:\n- New optional constructor parameter `unlimiformer: Optional[UnlimiformerConfigLike] = None`.\n- When provided and `unlimiformer.enabled` is true, wrap the underlying `self.model` with the Unlimiformer augmentation via `aios.integrations.unlimiformer.adapter.enable(self.model, tokenizer, cfg)`.\n- Ensure `forward(...)` and `generate(...)` continue to route through HF model seamlessly; Unlimiformer alters attention inside the model graph post-`layer_begin`.\n\nDatastore/index devices\n- Respect `datastore_device` and `index_devices` (GPU ids) when available; otherwise default to CPU FAISS.\n- Expose graceful fallbacks on Windows to CPU FAISS.\n\n\n### Configuration surface (proposed)\nUnder `brains.trainer_overrides` when `kind: hf`:\n\n```\nunlimiformer:\n  enabled: false\n  layer_begin: null          # null \u2192 auto (> 1/2 of total layers)\n  context_window_size: 4096  # sliding window for lower layers\n  knn: true\n  use_datastore: true\n  gpu_datastore: false       # default false on Windows\n  gpu_index: false\n  datastore_device: 1        # optional\n  index_devices: [1]         # optional list\n  eval_max_source_length: 999999\n```\n\nCLI flags (Typer) mirror the above and default to disabled.\n\n\n### Training integration (Phase 2)\n- Add a new command for HF fine\u2011tuning with long inputs using Unlimiformer:\n  - `aios hrm-hf train-hf-unlimiformer` (or integrate into `training_cli.py`)\n  - Modes: `--unlimiformer-training`, `--random-unlimiformer-training`, `--alternating` as in the repo\n- Datasets: begin with summarization datasets (GovReport, SummScreen, BookSum) to mirror paper experiments.\n- Early stopping: Evaluate with `test_unlimiformer` enabled to select checkpoints.\n\n\n## Dependencies and compatibility\n- Unlimiformer repo is MIT; vendoring minimal files is permissible with attribution.\n- FAISS:\n  - `faiss-cpu` works on Windows; `faiss-gpu` is Linux/CUDA\u2011oriented. Provide runtime detection and degrade to CPU index/datastore on unsupported platforms.\n- Transformers version pinning: test with the current repo baseline; maintain a gated extra if newer transformers are required by Unlimiformer.\n\n\n## Risks, constraints, and mitigations\n- Windows GPU FAISS availability: default to CPU FAISS; add clear logs; allow GPU usage on Linux/CUDA.\n- Model coverage: initial focus on LLaMA\u2011family decoder\u2011only; expand to encoder\u2013decoder later.\n- Performance variance: `layer_begin` is critical; provide auto\u2011heuristic (> half of total layers) and expose as a user setting.\n- Memory pressure: ensure `use_datastore` defaults to true for very long inputs; include telemetry in logs.\n- Maintenance: vendor minimal code only; isolate under `aios.integrations.unlimiformer` to avoid invasive changes.\n\n\n## Test plan\nAutomated\n- Unit test: enabling Unlimiformer on a short prompt yields outputs close to baseline generation (within tolerance on logits/perplexity or token parity for deterministic settings).\n- E2E smoke: long input (> 64k tokens) generates without OOM, with FAISS CPU datastore, and token streaming stays responsive.\n\nManual\n- LLaMA\u20112 chat model: summarize a 100k\u2011token text with `unlimiformer.enabled=true`; compare quality and latency with and without GPU index/datastore.\n\n\n## Milestones and timeline\n- P0 (1\u20132 days)\n  - Vendor minimal Unlimiformer inference code and feature\u2011gate in HF adapter; add config flags; CPU FAISS only; LLaMA\u2011family support.\n  - Add docs and CLI flags; write basic unit/E2E smoke tests.\n- P1 (3\u20135 days)\n  - GPU datastore/index support on Linux/CUDA; GUI toggle; early\u2011stopping evaluation integration.\n- P2 (1\u20132 weeks)\n  - Training modes (random\u2011encoded, retrieval, alternating) for encoder\u2013decoder tasks; sample recipes.\n- P3 (exploratory)\n  - Investigate feasibility of HRM (ACTV1) architecture adaptation.\n\n\n## Rollout and observability\n- Feature flag: off by default.\n- Structured logs: write Unlimiformer settings and device placements; record datastore/index memory footprint and retrieval latency.\n- Add a troubleshooting section in docs (common FAISS issues, Windows notes).\n\n\n## Next actions (Phase 1)\n1) Add optional dependency group `[unlimiformer]` to `pyproject.toml` (faiss-cpu; platform\u2011specific notes for faiss\u2011gpu).\n2) Create `aios.integrations.unlimiformer` module and vendor minimal code.\n3) Extend `HFCausalLM_HRMAdapter` to accept an optional `unlimiformer` config and wrap the model.\n4) Add CLI and config flags; wire through `brains.trainer_overrides`.\n5) Add unit and smoke tests; document usage and caveats.\n\n\n---\nMaintainers: Please comment on feasibility, preferred model targets for Phase 1, and any CI/platform constraints (especially Windows FAISS).\n\n\n## Phase breakdown and checklists\n\nBelow are execution-ready checklists per phase with acceptance criteria and exit gates.\n\n### Phase 0 \u2014 Repo readiness and scaffolding (0.5\u20131 day)\n\nChecklist\n- [ ] Add optional dependency extra `[unlimiformer]` (faiss-cpu; pin transformers range if needed)\n- [ ] Create `src/aios/integrations/unlimiformer/` with MIT attribution NOTICE\n- [ ] Define `UnlimiformerConfig` TypedDict and feature-gate helpers\n- [ ] Add config block (disabled) under `brains.trainer_overrides.unlimiformer`\n- [ ] Wire no-op read/validation in HF adapter (no behavior change yet)\n\nAcceptance criteria\n- [ ] App starts unchanged with default config\n- [ ] CI/lint/tests remain green\n\nExit gate\n- [ ] Feature is fully hidden behind `enabled: false`\n\n\n### Phase 1 \u2014 Inference PoC (decoder-only, Windows-friendly) (1\u20132 days)\n\nChecklist\n- [ ] Vendor minimal Unlimiformer inference components (index/datastore + model hook)\n- [ ] Implement `enable_on_model(model, tokenizer, cfg)` to attach retrieval attention from `layer_begin`\n- [ ] Default to FAISS-CPU datastore/index on Windows; detect and log device placement\n- [ ] Add CLI flags to evaluation/generation path to toggle Unlimiformer\n- [ ] Provide an example: summarize a long text (\u2265100k tokens) with LLaMA\u2011family model\n- [ ] Unit test: enabling Unlimiformer on short inputs \u2248 baseline outputs\n- [ ] E2E smoke: long input completes without OOM using FAISS-CPU; stream tokens\n\nAcceptance criteria\n- [ ] Long-input generation succeeds on Windows with FAISS-CPU\n- [ ] Short-input parity within tolerance when Unlimiformer is enabled\n- [ ] Clear logs for index/datastore size and latency\n\nExit gate\n- [ ] Docs updated with Windows usage and limitations\n\n\n### Phase 1.1 \u2014 Linux/CUDA GPU index/datastore (1\u20132 days)\n\nChecklist\n- [ ] Detect CUDA and allow `gpu_index`/`gpu_datastore`\n- [ ] Validate FAISS-GPU install path with helpful errors/fallbacks\n- [ ] Benchmark basic throughput vs CPU on the same prompt\n\nAcceptance criteria\n- [ ] GPU index/datastore operates with visible latency reduction vs CPU on eligible hardware\n- [ ] Graceful fallback to CPU with warning logs when unavailable\n\nExit gate\n- [ ] Docs include CUDA/GPU prerequisites and troubleshooting\n\n\n### Phase 1.2 \u2014 GUI toggle + early-stopping evaluation (1\u20132 days)\n\nChecklist\n- [ ] Add GUI switch \u201cUnlimited context (Unlimiformer)\u201d under HF trainer settings\n- [ ] Wire values: enabled, layer_begin, datastore/index devices\n- [ ] Integrate Unlimiformer during evaluation used for early stopping (if configured)\n\nAcceptance criteria\n- [ ] GUI toggle persists to config and is honored by evaluation\n- [ ] Early-stopping can leverage long-context evaluation without regressions\n\nExit gate\n- [ ] UX doc and tooltip coverage for each setting\n\n\n### Phase 2 \u2014 Training modes and encoder\u2013decoder support (1\u20132 weeks)\n\nChecklist\n- [ ] Add CLI to enable training modes: `--unlimiformer-training`, `--random-unlimiformer-training`, `--alternating`\n- [ ] Implement training-time hooks (sampling from full input, random-encoded mode)\n- [ ] Add dataset recipes for GovReport/SummScreen/BookSum (encoder\u2013decoder models)\n- [ ] Early-stopping/eval configured with `test_unlimiformer`\n- [ ] Add metrics logging for retrieval hits/latency during training\n\nAcceptance criteria\n- [ ] Reproduce representative improvements similar to paper on at least one dataset (relative, not exact)\n- [ ] Training is stable with recommended defaults; resource use documented\n\nExit gate\n- [ ] Docs include full training command examples and cost guidance\n\n\n### Phase 3 \u2014 ACTV1 HRM exploration (exploratory, timeboxed)\n\nChecklist\n- [ ] Spike: map HRM attention blocks to identify hook points analogous to Unlimiformer\n- [ ] Prototype minimal retrieval layer that augments HRM without changing base attention math\n- [ ] Measure memory/perf impact on long sequences\n\nAcceptance criteria\n- [ ] Decision doc: feasible/not feasible with outline of required changes and risks\n\nExit gate\n- [ ] If feasible, produce a separate follow-on feature spec; otherwise close spike with rationale\n\n\n### Phase 4 \u2014 Hardening, docs, and demos (2\u20134 days)\n\nChecklist\n- [ ] Consolidate structured logs and counters: index size, retrieval QPS/latency, device placement\n- [ ] Expand troubleshooting guide (FAISS install, GPU issues, Windows notes)\n- [ ] Create repeatable demos (scripts + sample prompts) for long\u2011doc summarization and QA\n- [ ] Finalize API stability and mark feature as beta (still default off)\n\nAcceptance criteria\n- [ ] Demo scripts run on Windows (CPU FAISS) and Linux/CUDA (GPU FAISS)\n- [ ] Users can follow docs to reproduce results without support\n\nExit gate\n- [ ] Sign-off from maintainers to publish feature docs\n\n\n## Testing matrix (summary)\n\n- Platforms\n  - [ ] Windows 11 + CUDA GPU present \u2192 default CPU FAISS works; GPU path explicitly disabled by default\n  - [ ] Linux + CUDA 12.x \u2192 CPU and GPU FAISS paths\n  - [ ] macOS (CPU only) \u2192 CPU FAISS path\n- Models\n  - [ ] LLaMA\u2011family (decoder\u2011only) \u2014 Phase 1\n  - [ ] BART/T5 (encoder\u2013decoder) \u2014 Phase 2\n- Inputs\n  - [ ] Short (\u2264 2k tokens) parity tests\n  - [ ] Long (\u2265 100k tokens) smoke + latency/throughput sampling\n- Modes\n  - [ ] Inference only\n  - [ ] Training: random\u2011encoded, retrieval, alternating (Phase 2)\n\n", "tags": ["datasets", "evaluation", "hrm", "training"], "headings": [{"line": 0, "text": "Unlimiformer integration for unlimited-length inputs"}, {"line": 9, "text": "Why this matters"}, {"line": 15, "text": "Scope and success criteria"}, {"line": 34, "text": "High-level design"}, {"line": 77, "text": "Detailed design and APIs"}, {"line": 79, "text": "Module layout"}, {"line": 91, "text": "HF adapter extension points"}, {"line": 104, "text": "Configuration surface (proposed)"}, {"line": 124, "text": "Training integration (Phase 2)"}, {"line": 132, "text": "Dependencies and compatibility"}, {"line": 139, "text": "Risks, constraints, and mitigations"}, {"line": 147, "text": "Test plan"}, {"line": 156, "text": "Milestones and timeline"}, {"line": 168, "text": "Rollout and observability"}, {"line": 174, "text": "Next actions (Phase 1)"}, {"line": 186, "text": "Phase breakdown and checklists"}, {"line": 190, "text": "Phase 0 \u2014 Repo readiness and scaffolding (0.5\u20131 day)"}, {"line": 207, "text": "Phase 1 \u2014 Inference PoC (decoder-only, Windows-friendly) (1\u20132 days)"}, {"line": 227, "text": "Phase 1.1 \u2014 Linux/CUDA GPU index/datastore (1\u20132 days)"}, {"line": 242, "text": "Phase 1.2 \u2014 GUI toggle + early-stopping evaluation (1\u20132 days)"}, {"line": 257, "text": "Phase 2 \u2014 Training modes and encoder\u2013decoder support (1\u20132 weeks)"}, {"line": 274, "text": "Phase 3 \u2014 ACTV1 HRM exploration (exploratory, timeboxed)"}, {"line": 288, "text": "Phase 4 \u2014 Hardening, docs, and demos (2\u20134 days)"}, {"line": 304, "text": "Testing matrix (summary)"}]}, {"path": "planned_features/WEB_GUI_PARITY.md", "content": "# PF-019: Web GUI Parity Control\n\n**Status**: \ud83d\udfe1 Planning Phase  \n**Date**: November 16, 2025  \n**Priority**: High  \n**Complexity**: Very High\n\n---\n\n## Summary\n\nDeliver a browser-based control surface that mirrors the desktop GUI pixel-for-pixel in layout, flows, and behaviours. The WebGUI must expose every control, panel, status indicator, and workflow currently available through the Tk-based app, while remaining in sync with the existing state management, background workers, and analytics pipelines. Users will launch it locally via `aios web`, which should start a web server, open (or log) a URL, and run alongside the existing core services without requiring the desktop GUI to be present.\n\n---\n\n## Success Criteria\n\n- **Functional parity**: All panels (`Chat`, `Brains`, `Datasets`, `Training`, `Evaluation`, `Resources`, `Debug`, `Settings`, `MCP Manager`, `Help`, `Output`) behave identically, surface the same live data, and update in real time.\n- **Single source of truth**: State mutations performed through the browser immediately propagate to `gui_state.json` and the in-memory structures that the Tk GUI uses; no divergence when both UIs run simultaneously.\n- **Cross-platform**: `aios web` works on Windows and Linux with Python-only dependencies plus a Node toolchain for front-end builds; optional Mac support documented.\n- **Security baseline**: Local-only access by default with optional authentication and TLS knobs for remote deployment.\n- **Observability**: Structured logs and metrics that distinguish WebGUI events from Tk GUI events without breaking the existing analytics pipeline.\n- **Testing**: Automated parity regression test suite proving behaviour equivalence for critical workflows, and Playwright smoke tests for the WebGUI.\n\n---\n\n## Constraints & Considerations\n\n- **UI fidelity**: Visual design must match colours, typography, spacing, and component behaviour of the Tk app. Introduce a design token map sourced from the existing theme configuration so that any future theme tweaks propagate to both UIs.\n- **Shared runtime**: Reuse existing services in `src/aios/gui/app` and `src/aios/gui/components` rather than duplicating business logic. Where Tk widgets hold state, extract presenter/view-model layers so they can feed both Tk and web surfaces.\n- **Async-first**: All new server components must use `asyncio`, integrate with the existing `setup_resources` loop, and avoid blocking the worker pools that power agents and training operations.\n- **No breaking change**: Tk GUI must continue to run unchanged; users can run either UI independently or both concurrently.\n- **Asset bundling**: Front-end assets must be buildable offline and distributable with the Python package; avoid heavyweight runtime dependencies.\n\n---\n\n## Proposed Architecture\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 aios web (CLI entrypoint)   \u2502        \u2502 Front-end (React/Vite)     \u2502\n\u2502  \u2022 loads headless app core  \u2502        \u2502  \u2022 UI clone of Tk panels   \u2502\n\u2502  \u2022 boots FastAPI+Socket.IO  \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  \u2022 State synced via WS/REST\u2502\n\u2502  \u2022 serves static assets     \u2502        \u2502  \u2022 Theming via design tokens\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502                                    \u2502\n               \u25bc                                    \u2502\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u2502\n      \u2502 Shared GUI Core    \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2502  \u2022 Panel presenters\u2502  Web view-models reuse\u2502\n      \u2502  \u2022 State adapters  \u2502  Tk services & storage\u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n       Existing services, threads,\n       async loop, analytics, etc.\n```\n\n### Backend Service (Python)\n\n- Framework: FastAPI (for REST) + `uvicorn` runner with ASGI lifespan hooks tied into `setup_resources`.\n- Reuse `setup_resources`, `setup_event_handlers`, and panel initialization to build a headless `AiosTkApp`-derived context without instantiating Tk widgets.\n- Expose REST endpoints per panel (e.g., `/api/chat/send`, `/api/brains/list`) and WebSocket channels for event-driven updates (log stream, training progress, background tasks).\n- Implement a state sync layer that watches `schedule_state_save` and repository-specific caches, emitting change diffs to clients.\n\n### Front-End (TypeScript)\n\n- Stack: React 18 + Vite + React Query/Zustand for state management; Tailwind or CSS modules constrained by design tokens extracted from Tk theme definitions.\n- Component library: Build custom components that match Tk layouts; consider using Blueprint.js or Ant Design only if they can be themed precisely; otherwise, bespoke components ensure parity.\n- Navigation: Mirror Tk notebook tabs as horizontal navigation; subpanels map 1:1.\n- Real-time data: WebSocket subscriptions for logs, progress bars, chat streaming; REST for CRUD operations.\n\n### CLI Entry (`aios web`)\n\n- Add subcommand in `src/aios/cli/aios.py` wiring to `aios.cli.web_cli:run`.\n- Responsibilities: ensure venv dependencies, start FastAPI service, build (or verify) front-end assets, optionally open the browser (respect `--no-open` flag), log host/port.\n- Flags: `--host`, `--port`, `--theme`, `--open/--no-open`, `--auth-token`, `--certfile`, `--keyfile`, `--allow-remote` (with warnings).\n\n---\n\n## Implementation Phases\n\n### Phase 0 \u2013 Discovery & Parity Mapping\n\n- Catalogue every panel, widget, and command in the Tk UI; produce a JSON inventory describing fields, events, validation rules, and dependencies.\n- Document where state lives (e.g., `state_management`, panel-specific caches) and identify Tk-only code paths that must be abstracted.\n- Identify blocking dependencies (e.g., direct Tk widget calls inside services) and plan refactors to extract logic into reusable classes.\n\n### Phase 1 \u2013 Core Refactor for Shared State\n\n- Introduce a `src/aios/gui/core` (or expand existing `core`) module that exposes headless presenters/controllers for each panel.\n- Update Tk panels to consume the new presenters via dependency injection so they remain functional.\n- Ensure presenters expose async-safe methods and emit typed events (e.g., via `asyncio.Queue` or RxPy) that the Web backend can subscribe to.\n- Extend state persistence to broadcast diffs whenever `schedule_state_save` runs, enabling live reload in the browser.\n\n### Phase 2 \u2013 Web Backend Skeleton\n\n- Scaffold FastAPI app under `src/aios/webui/server.py` integrating shared presenters.\n- Implement auth middleware (token header) and optional HTTPS.\n- Provide endpoints per panel in read/write parity (e.g., GET `/api/settings`, POST `/api/settings/theme`).\n- Set up Socket.IO or Starlette WebSocket endpoints for:\n  - Log/event stream (`/ws/logs`)\n  - Status updates (resources, training progress)\n  - Chat streaming (token-by-token)\n- Add background tasks that bridge presenter events to WebSocket broadcasts.\n\n### Phase 3 \u2013 Front-End Application\n\n- Create `webui/` directory (ignored by lint when not built) containing Vite project with React + TypeScript + ESLint/Prettier.\n- Define design tokens JSON generated from Tk theme definitions; load at runtime to guarantee parity.\n- Build tab components mimicking Tk layout; incorporate virtualization where Tk uses scrollable lists.\n- Implement data bindings with React Query for REST (CRUD) and custom hooks for WebSocket streams.\n- Provide fallback skeleton states replicating Tk loading overlays.\n\n### Phase 4 \u2013 Packaging & CLI Integration\n\n- Integrate Vite build into Python packaging via `pyproject.toml` optional dependencies (`webui`).\n- Update `setup.cfg` / entry points to include `aios web` command; ensure `aios` console script imports minimal dependencies until the command runs.\n- Provide `scripts/build_webui.sh` and `.ps1` for dev ergonomics; integrate into CI using npm caches.\n- Ensure built assets land under `src/aios/webui/static/` and are included with `pkgutil.get_data` for offline use.\n\n### Phase 5 \u2013 Verification & Docs\n\n- Implement automated parity tests:\n  - Presenter unit tests covering CRUD flows.\n  - Snapshot/state diff tests comparing Tk + Web responses given identical fixtures.\n  - Playwright smoke suite covering critical workflows (chat generation, brain selection, dataset upload, training start/stop, settings save).\n- Update `docs/guide/` with WebGUI usage, security warnings, remote deployment guide.\n- Record demo walkthrough once stable.\n\n---\n\n## Testing Strategy\n\n- **Unit tests**: For presenters and API contracts using pytest + httpx AsyncClient.\n- **Integration tests**: Launch FastAPI app in-process, run Playwright headless tests to validate UI flows; reuse fixtures to ensure deterministic data.\n- **Concurrency tests**: Simulate simultaneous Tk + Web interactions to verify lock-free state updates and absence of race conditions (use `pytest-asyncio` with `asyncio.Event` barriers).\n- **Performance tests**: Benchmark WebSocket throughput and latency for chat streaming; ensure under-load behaviour matches Tk event loop responsiveness.\n\n---\n\n## Security & Deployment Notes\n\n- Default bind: `127.0.0.1:8123`. Require explicit `--allow-remote` to bind to `0.0.0.0` and show warning banner in UI.\n- Token-based auth stored in `~/.config/ai-os/webgui_token`; optional `AIOS_WEB_TOKEN` env override.\n- Optional TLS via user-provided `--certfile/--keyfile`.\n- Document reverse-proxy guidance (Caddy/Nginx) for secure remote access.\n\n---\n\n## Open Questions / Risks\n\n- **Tk dependency**: Some panels might rely on direct widget references; refactor effort may be substantial.\n- **Theme fidelity**: Tk theming primitives differ from CSS; will need a reliable mapping process.\n- **Front-end build size**: Need to ensure packaged assets keep overall wheel size acceptable; may need optional extra.\n- **Simultaneous sessions**: Determine how to handle multiple browsers; probably allow read-only by default, configurable to accept multi-writer with event conflict resolution.\n- **Accessibility**: Evaluate if Tk shortcuts (e.g., keyboard accelerators) must be mapped to the Web UI for parity.\n\n---\n\n## Deliverables Checklist\n\n- Shared presenter layer with parity coverage for each panel.\n- FastAPI WebGUI server with REST + WebSocket endpoints.\n- React/Vite front-end that mirrors the Tk UI.\n- CLI command `aios web` with documented options.\n- Parity test suite and Playwright automation in CI.\n- Updated documentation in `docs/guide/` and release notes.\n- Security review artefacts (threat model, auth configuration).\n\n---\n\n## Next Steps\n\n1. Approve architecture direction (shared presenters + FastAPI + React).\n2. Spin up Phase 0 discovery ticket to inventory Tk components.\n3. Allocate resources for refactor + front-end build, including Node environment setup in CI.\n", "tags": ["cli", "datasets", "evaluation", "gui", "mcp", "training"], "headings": [{"line": 0, "text": "PF-019: Web GUI Parity Control"}, {"line": 9, "text": "Summary"}, {"line": 15, "text": "Success Criteria"}, {"line": 26, "text": "Constraints & Considerations"}, {"line": 36, "text": "Proposed Architecture"}, {"line": 58, "text": "Backend Service (Python)"}, {"line": 65, "text": "Front-End (TypeScript)"}, {"line": 72, "text": "CLI Entry (`aios web`)"}, {"line": 80, "text": "Implementation Phases"}, {"line": 82, "text": "Phase 0 \u2013 Discovery & Parity Mapping"}, {"line": 88, "text": "Phase 1 \u2013 Core Refactor for Shared State"}, {"line": 95, "text": "Phase 2 \u2013 Web Backend Skeleton"}, {"line": 106, "text": "Phase 3 \u2013 Front-End Application"}, {"line": 114, "text": "Phase 4 \u2013 Packaging & CLI Integration"}, {"line": 121, "text": "Phase 5 \u2013 Verification & Docs"}, {"line": 132, "text": "Testing Strategy"}, {"line": 141, "text": "Security & Deployment Notes"}, {"line": 150, "text": "Open Questions / Risks"}, {"line": 160, "text": "Deliverables Checklist"}, {"line": 172, "text": "Next Steps"}]}, {"path": "planned_features/ZERO3_TENSOR_PIPELINE_INTEGRATION.md", "content": "# ZeRO-3 Hybrid Parallel Inference Integration Plan\n\n## Executive Summary\n- Deliver a unified inference stack that combines DeepSpeed ZeRO stage 3 inference, tensor parallelism, and pipeline parallelism so the Chat and Evaluation panels can serve and benchmark 70B\u20131T parameter models on heterogeneous clusters.\n- Stream model weights from CPU/NVMe while keeping GPU memory focused on activations and KV cache to sustain high-throughput batching without saturating a single device.[1][2]\n- Introduce topology-aware orchestration so tensor sharding stays inside fast NVLink/PCIe domains, while pipeline stages stretch across nodes when memory pressure demands it.[7][8][10]\n- Expose presets in the GUI that hide backend complexity but provide expert overrides for advanced operators.\n\n## Goals & Success Criteria\n- **Chat panel** can execute default prompts on >70B models with configurable ZeRO-3 + TP + PP and maintain <2.0\u00d7 latency regression versus today at batch=1.\n- **Evaluation panel** can launch lm-eval-harness jobs against GPTR-like workloads using the same distributed runtime with reproducible score parity (\u00b10.1) versus reference single-node baselines.\n- Provide battle-tested DeepSpeed config templates plus hardware compatibility checks for both Linux and Windows (native + WSL2) deployments.\n- Ship dashboards that surface prefill/decode latency, NVMe queue depth, NCCL collective timing, and pipeline bubble percentage.\n\n## Current State Baseline\n### Chat panel\n- Implements a Tkinter front end that delegates prompt handling to a synchronous `on_send` callback per session (`src/aios/gui/components/chat_panel.py`).\n- Relies on a single threaded worker path; stopping a response toggles a thread flag but there is no notion of distributed shards or multi-rank coordination.\n- Model lifecycle hooks (`on_load_brain`, `on_unload_model`) assume one device and no fine-grained partition control.\n\n### Evaluation panel\n- Uses `HarnessWrapper` and CLI orchestration to spawn lm-evaluation-harness runs (`src/aios/gui/components/evaluation_panel/panel_main.py`).\n- Currently expects each evaluation job to run on a single process or coarse multi-GPU launch; there is no support for composing tensor/pipeline sub-topologies.\n- Progress tracking is line-based and unaware of distributed stages, so pipeline stalls or NCCL retries are invisible to the UI.\n\n## Technical Background\n### ZeRO-3 inference\n- Streams layers from CPU or NVMe via configurable buffers while freeing most GPU memory for activations and large batch sizes.[1][2][6]\n- Prefetch buckets overlap storage transfers with compute; pinned memory improves PCIe bandwidth but is RAM intensive.[2]\n- Optional KV-cache offload keeps decode latency manageable for long chats.[3]\n\n### Tensor parallelism\n- Splits weight matrices across GPUs, requiring collective ops (`all_gather`/`all_reduce`) each layer; best kept within intra-node high-bandwidth fabrics.[7][8][16]\n- Communicator initialization must align with hardware topology; Megatron exposes APIs to bind ranks, priority, and NCCL tuning knobs.[10][13][15]\n\n### Pipeline parallelism\n- Breaks the network into sequential stages with micro-batch scheduling to reduce idle time.[7][9]\n- Demands careful load balancing; DeepSpeed `PipelineModule` and Megatron virtual pipeline sizes mitigate uneven decoder blocks.[9][10]\n\n### Hybrid (3D) deployment realities\n- Production systems coordinate DP + TP + PP; ZeRO-3 provides the memory savings that make TP/PP practical at inference time.[1][7][12]\n- DeepSpeed\u2019s inference engine currently errors if pipeline checkpoints are loaded, so extending or bypassing that limit is a prerequisite.[4]\n- FastGen\u2019s SplitFuse scheduler improves mixed prompt/generation throughput and remains compatible with ZeRO partitioning.[11][12]\n\n## Target Architecture\n### High-level flow\n```\nGUI (Chat/Eval) \u2192 Inference Service Broker \u2192 Runtime Orchestrator\n    \u2192 Cluster Topology Resolver \u2192 (ZeRO-3 Shard Manager + TP Group Manager + PP Scheduler)\n        \u2192 DeepSpeed/Megatron runtime (per rank) \u2192 Transport metrics back to GUI\n```\n\n### Runtime orchestration\n- Introduce a `HybridInferenceOrchestrator` core module that:\n  - Creates process groups for data, tensor, and pipeline dimensions using Megatron parallel APIs when available.[10]\n  - Builds DeepSpeed ZeRO-3 configs on the fly (CPU vs NVMe offload, buffer sizes, pinned memory) aligned with detected hardware.[2][6]\n  - Wraps pipeline stages with either DeepSpeed PipelineModule (training-compatible) or, when unsupported, a Megatron/TensorRT-LLM compatible executor.\n- Maintain topology definitions (JSON/YAML) mapping racks/nodes \u2192 NVLink islands; ensure TP ranks stay intra-node, PP extends inter-node as advised.[7][17]\n\n### Chat panel integration\n- Replace the single-threaded `on_send` path with a thin client that forwards requests to the orchestrator over an async RPC (gRPC or ZeroMQ) supporting streaming tokens.\n- Add UI toggles for preset modes (`Balanced`, `Latency`, `Throughput`) that map to ZeRO buffer sizes, TP degree, and PP depth.\n- Surface live telemetry (prefill vs decode latency, cache residency, throughput) fed from orchestrator metrics endpoints.\n\n### Evaluation panel integration\n- Extend benchmark runner to emit distributed launch descriptors (world size, TP, PP, ZeRO settings) and persist them in evaluation history for reproducibility.\n- Provide batch planners that chunk benchmark suites into jobs fitting available GPU slots while reusing warmed caches when possible.\n- Enhance progress tracker to consume JSON events reporting micro-batch advancement per pipeline stage, collective timings, and failure diagnostics.\n\n### Configuration artifacts\n- Ship curated DeepSpeed config templates (`config/deepspeed_zero3_tp{N}_pp{M}.json`) covering CPU and NVMe offload permutations.[2][6]\n- Add GUI-level presets that resolve to those configs or auto-tune them based on detected GPU memory, NVMe bandwidth, and Windows vs Linux paths.\n- Ensure Windows compatibility by supporting UNC-style NVMe paths, enumerating `cudaDeviceProp::integrated` flags via `nvidia-smi`, and providing WSL2-specific guidance.\n\n## Implementation Roadmap\n1. **Discovery & prerequisites (2 weeks)**\n   - Audit current inference backends, identify assumption points about single GPU, and draft interface contracts for the orchestrator.\n   - Extend hardware detection to report NVLink domains, PCIe bandwidth, NVMe IOPS, and OS-specific path nuances.\n   - Prototype ZeRO-3 inference using DeepSpeedExamples baseline to validate offload throughput and pinned memory limits on Linux + Windows.[2][3]\n\n2. **ZeRO-3 backend foundation (3 weeks)**\n   - Refactor inference loader to produce dynamic ZeRO-3 configs (prefetch buckets, buffer counts, kv offload toggles).[1][2][6]\n   - Add orchestrator service exposing REST/gRPC endpoints for session creation, token streaming, and job submission.\n   - Integrate KV-cache residency controls and pinned memory safeties (auto downgrade when RAM low).[3]\n\n3. **Tensor parallel integration (3 weeks)**\n   - Embed Megatron/TensorRT communicator initialization to align TP groups with intra-node topology; expose TP degree selection in GUI expert drawer.[7][8][10][16]\n   - Instrument NCCL timings and provide watchdogs for all-gather/all-reduce hotspots; bubble up alerts to the UI.\n   - Update lm-eval harness wrapper to accept TP-aware checkpoints and environment vars (e.g., `TP_SIZE`, `CUDA_DEVICE_MAX_CONNECTIONS`).\n\n4. **Pipeline parallel enablement (4 weeks)**\n   - Fork or extend DeepSpeed inference engine to lift pipeline checkpoint restriction, or integrate Megatron pipeline executor when stage counts >1.[4][10]\n   - Implement stage balancing heuristics using layer profiles; allow manual overrides for uneven decoder segments.[9]\n   - Add pipeline-aware batching (micro-batch scheduling, SplitFuse integration) to keep stages saturated during chat streaming.[11][12]\n\n5. **GUI wiring & UX (2 weeks)**\n   - Wire chat panel to orchestrator via async client, add preset selectors, and display real-time metrics.\n   - Update evaluation panel to configure distributed runs, visualize pipeline stage progress, and archive topology metadata with results.\n   - Provide warning banners when operator-selected modes exceed detected hardware capabilities.\n\n6. **Validation & hardening (3 weeks)**\n   - Build automated smoke suites exercising different TP/PP/ZeRO combinations on Linux and Windows runners (local + WSL2).\n   - Stress test with representative chat transcripts (long-context, multi-turn) and lm-eval benchmark sets to ensure score parity.\n   - Document rollback paths and guardrails (e.g., fallback to single GPU when distributed bring-up fails).\n\n## Observability & Tooling\n- Collect metrics: NVMe queue depth, PCIe throughput, NCCL op duration, pipeline stage utilization, KV-cache hit rate.\n- Publish results via Prometheus exporters and simple GUI graphs; alert when thresholds breached.\n- Capture orchestrator logs with rank IDs; integrate with existing analytics event stream (`logs/diagnostics/analytics_events.jsonl`).\n\n## Testing Strategy\n- Unit tests for config generation (ZeRO buffers, TP topology mapping) across Windows/Linux path conventions.\n- Integration tests launching mini GPT models (e.g., 7B, 13B, 70B) in various TP/PP configurations; assert token accuracy vs baseline outputs.\n- Regression suite for evaluation panel to confirm harness results remain within tolerances and metadata persisted.\n\n## Risks & Mitigations\n- **DeepSpeed pipeline gap**: Current inference engine rejects PP checkpoints.[4] \u2192 Mitigate by upstream contribution or wrapping Megatron pipeline executor.\n- **Bandwidth constraints**: NVMe and PCIe throughput may bottleneck ZeRO streaming.[1] \u2192 Autoscale prefetch buckets, provide operator warnings, document hardware minima.\n- **Windows heterogeneity**: Driver and filesystem differences can destabilize NVMe offload. \u2192 Default to CPU offload on Windows, recommend WSL2 for NVMe, run CI coverage on both OS families.\n- **Operational complexity**: Exposing too many knobs risks misconfiguration. \u2192 Offer curated presets and guardrails, hide advanced controls behind expert toggles.\n\n## Open Questions\n- Should we adopt DeepSpeed FastGen end-to-end or reuse existing batching logic with minimal changes?[11]\n- Do we standardize on Megatron runtimes for both TP and PP, or keep DeepSpeed TP when PP depth is 1?\n- What is the minimum hardware spec we officially support (GPU memory, NVMe latency, network link speed)?\n\n## References\n[1] https://www.deepspeed.ai/2022/09/09/zero-inference.html  \n[2] https://raw.githubusercontent.com/microsoft/DeepSpeedExamples/master/inference/huggingface/zero_inference/README.md  \n[3] https://raw.githubusercontent.com/microsoft/DeepSpeedExamples/master/inference/huggingface/zero_inference/run_model.py  \n[4] https://raw.githubusercontent.com/microsoft/DeepSpeed/master/deepspeed/inference/engine.py  \n[5] https://raw.githubusercontent.com/microsoft/DeepSpeed/master/deepspeed/inference/config.py  \n[6] https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training  \n[7] https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/legacy/performance/performance-tuning-guide/deciding-model-sharding-strategy.md#how-to-set-tensor-parallelism-and-pipeline-parallelism  \n[8] https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/features/parallel-strategy.md#parallelism-in-tensorrt-llm  \n[9] https://www.deepspeed.ai/tutorials/pipeline/#load-balancing-pipeline-modules  \n[10] https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/parallel_state.py#L640-L948  \n[11] https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-fastgen/README.md#3-dynamic-splitfuse-a-novel-prompt-and-generation-composition-strategy  \n[12] https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-fastgen/README.md#2-existing-llm-serving-techniques-in-literature  \n[13] https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/parallel_state.py#L181-L260  \n[14] https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/parallel_state.py#L948-L1105  \n[15] https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/nccl_allocator.py#L1-L206  \n[16] https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/layers/tensor_parallel.py#L1-L214  \n[17] https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/commands/serve.py#L269-L299  \n[18] https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/parallel_state.py#L300-L520  \n[19] https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/parallel_state.py#L900-L1050  \n[20] https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/parallel_state.py#L98-L164\n", "tags": ["cli", "evaluation", "experts", "gui"], "headings": [{"line": 0, "text": "ZeRO-3 Hybrid Parallel Inference Integration Plan"}, {"line": 2, "text": "Executive Summary"}, {"line": 8, "text": "Goals & Success Criteria"}, {"line": 14, "text": "Current State Baseline"}, {"line": 15, "text": "Chat panel"}, {"line": 20, "text": "Evaluation panel"}, {"line": 25, "text": "Technical Background"}, {"line": 26, "text": "ZeRO-3 inference"}, {"line": 31, "text": "Tensor parallelism"}, {"line": 35, "text": "Pipeline parallelism"}, {"line": 39, "text": "Hybrid (3D) deployment realities"}, {"line": 44, "text": "Target Architecture"}, {"line": 45, "text": "High-level flow"}, {"line": 52, "text": "Runtime orchestration"}, {"line": 59, "text": "Chat panel integration"}, {"line": 64, "text": "Evaluation panel integration"}, {"line": 69, "text": "Configuration artifacts"}, {"line": 74, "text": "Implementation Roadmap"}, {"line": 105, "text": "Observability & Tooling"}, {"line": 110, "text": "Testing Strategy"}, {"line": 115, "text": "Risks & Mitigations"}, {"line": 121, "text": "Open Questions"}, {"line": 126, "text": "References"}]}]