[{"path": "README.md", "content": "# AI-OS Documentation\n\n[![Link Check](https://github.com/Wulfic/AI-OS/actions/workflows/link-check.yml/badge.svg)](https://github.com/Wulfic/AI-OS/actions/workflows/link-check.yml)\n\nWelcome to the AI-OS documentation! \n\nAI-OS is a Hierarchical Reasoning Model (HRM) training framework designed for Windows, featuring an intuitive GUI and powerful CLI for training custom language models with extreme context lengths and advanced optimization features.\n\n## \ud83c\udfaf Quick Navigation\n\nStart here for key references:\n- \u2705 **[Complete Feature Index](guide/features/COMPLETE_FEATURE_INDEX.md)** \u2014 Find every feature doc\n- \u2705 **[Feature Combination Matrix](guide/features/FEATURE_COMBINATION_MATRIX.md)** \u2014 Compatibility and combos\n\n---\n\n## Quick Links\n\n- Start at: [Guide Index](guide/INDEX.MD)\n- Training API Quick Reference: [guide/api/TRAINING_API_QUICK_REFERENCE.md](guide/api/TRAINING_API_QUICK_REFERENCE.md)\n\n## What is AI-OS?\n\nAI-OS implements a Hierarchical Reasoning Model (HRM) training toolkit with Mixture\u2011of\u2011Experts (MoE) support and extreme\u2011context optimizations, providing:\n\n- **\ud83e\udde0 HRM Training** - Train hierarchical reasoning models from scratch or fine-tune existing models\n- **\ud83d\udcbb Intuitive GUI** - Full-featured training interface with real-time monitoring\n- **\u26a1 Long Context** - Strategies for extended context lengths; see research notes\n- **\ud83c\udfaf Dynamic Subbrains** - Goal-driven, self-organizing expert networks\n- **\ud83d\udcca Rich Chat Interface** - Interactive chat with syntax highlighting and markdown support\n- **\ud83d\udd27 Advanced Optimization** - 8-bit optimizers, gradient checkpointing, mixed precision training\n\n## Installation\n\n### Quick Start (Windows)\n\n```powershell\n# Clone the repository\ngit clone https://github.com/Wulfic/AI-OS.git\ncd AI-OS\n\n# Create virtual environment\npython -m venv .venv\n.\\.venv\\Scripts\\Activate.ps1\n\n# Install dependencies\npip install -e .\n\n# Launch GUI\naios gui\n```\n\nFor installation, see scripts and installers documentation:\n\n- Windows: `scripts/install_aios_on_windows.ps1`\n- Ubuntu: `scripts/install_aios_on_ubuntu.sh`\n- Windows with Ubuntu via Docker: `scripts/install_aios_ubuntu_via_docker_on_windows.ps1`\n\n## Basic Usage\n\n### GUI Mode\n```powershell\naios gui\n```\n\nThe GUI provides:\n- **Training Tab** - Configure and monitor training runs\n- **Brains Tab** - Manage trained models\n- **Chat Tab** - Interactive chat with your models\n- **Datasets Tab** - Manage and download datasets\n- **Experts Tab** - Dynamic Subbrains expert management\n\n### CLI Mode\n\nTrain a model:\n```powershell\naios hrm-hf train-actv1 \\\n  --model \"gpt2\" \\\n  --dataset-file \"path/to/dataset.txt\" \\\n  --brain-name \"MyModel\" \\\n  --steps 1000\n```\n\nChat with a trained model:\n```powershell\naios chat --brain-path \"artifacts/brains/actv1/MyModel\"\n```\n\n## Key Features\n\n### \ud83d\ude80 Extreme Context Length\nTrain models with longer contexts using available strategies. See:\n- [FLASH_ATTENTION_VS_CHUNKING.md](guide/features/FLASH_ATTENTION_VS_CHUNKING.md) (discussion)\n- [EXTREME_CONTEXT_LENGTH_TRAINING.md](research/context_length/EXTREME_CONTEXT_LENGTH_TRAINING.md) (status/notes)\n\n### \ud83c\udfaf Dynamic Subbrains\nSelf-organizing expert networks that specialize dynamically based on goals. Subbrains manager panel is WIP; see GUI for status.\n\n### \ud83d\udcbe 8-Bit Optimization\nReduce memory usage with 8-bit optimizers. See: [2025-10-19_8bit_optimizer_verification.md](maintenance/2025-10-19_8bit_optimizer_verification.md)\n\n### \ud83d\udcca Rich Monitoring\nReal-time training metrics, GPU monitoring, and interactive charts.\n\n## Documentation Structure\n\n```\ndocs/\n\u251c\u2500\u2500 INDEX.md             # Main documentation index\n\u251c\u2500\u2500 README.md            # Overview and quick links\n\u251c\u2500\u2500 guide/               # Guides and API quick refs (see guide/api)\n\u2502   \u2514\u2500\u2500 api/             # CLI and tooltips quick references\n\u251c\u2500\u2500 maintenance/         # Fix logs, validation notes, production checks\n\u251c\u2500\u2500 planned_features/    # Plans and design docs for future work\n\u2514\u2500\u2500 research/            # Research notes and optimization deep dives\n  \u251c\u2500\u2500 context_length/\n  \u2514\u2500\u2500 optimization/\n```\n\n## Common Training Recipes\n\n### High-Performance Training\n```powershell\naios hrm-hf train-actv1 \\\n  --model \"gpt2\" \\\n  --dataset-file \"dataset.txt\" \\\n  --max-seq-len 10000 \\\n  --gradient-checkpointing \\\n  --amp \\\n  --batch-size 2\n```\n\n### Memory-Efficient Training\n```powershell\naios hrm-hf train-actv1 \\\n  --model \"gpt2\" \\\n  --dataset-file \"dataset.txt\" \\\n  --gradient-checkpointing \\\n  --amp \\\n  --batch-size 1 \\\n  --max-seq-len 3000\n```\n\n### Multi-GPU Training\n```powershell\naios hrm-hf train-actv1 \\\n  --model \"MyModel\" \\\n  --dataset-file \"dataset.txt\" \\\n  --ddp \\\n  --cuda-ids 0,1 \\\n  --batch-size 1\n```\n\n## Getting Help\n\n- Training API Reference: [guide/api/TRAINING_API_QUICK_REFERENCE.md](guide/api/TRAINING_API_QUICK_REFERENCE.md)\n- GitHub Issues: https://github.com/Wulfic/AI-OS/issues\n\n## Contributing\n\nWe welcome contributions! For development notes, see the repository README and issues:\n- Code organization and architecture\n- Refactoring progress and plans  \n- Implementation status and roadmaps\n\n## Link checking (docs)\n\nWe use lychee to catch broken links in documentation. From the repo root:\n\n```powershell\n# Check links in docs only\nlychee --config lychee.toml docs\n```\n\nIf you don\u2019t have lychee installed, see https://github.com/lycheeverse/lychee for installation instructions.\n\n### Local check without installing lychee\nYou can also run a lightweight local checker (no install required):\n\n```powershell\n.venv\\Scripts\\python.exe scripts\\check_docs_links.py\n```\n\nThis validates relative links within `docs/` and ignores code blocks to avoid false positives.\n\n## Project Status\n\n**Last Updated**: October 12, 2025\n\n- \u2705 Core HRM training implementation\n- \u2705 GUI with rich monitoring and chat\n-- \u26a0\ufe0f Long-context strategies: experimental; see research status\n- \u2705 Dynamic Subbrains Phase 1 & 2\n- \u2705 8-bit optimizer integration\n- \u2705 Rich Chat system with markdown support\n- \ud83d\udd04 Ongoing: Dynamic Subbrains Phase 3\n\n## License\n\nSee [LICENSE](../LICENSE) for details.\n\n## References\n\n- Project Repository: https://github.com/Wulfic/AI-OS\n- Documentation Index: [Guide Index](guide/INDEX.MD)\n\n---\n\nStart at the documentation index: [Guide Index](guide/INDEX.MD)\n", "tags": ["cli", "datasets", "experts", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "AI-OS Documentation"}, {"line": 8, "text": "\ud83c\udfaf Quick Navigation"}, {"line": 16, "text": "Quick Links"}, {"line": 21, "text": "What is AI-OS?"}, {"line": 32, "text": "Installation"}, {"line": 34, "text": "Quick Start (Windows)"}, {"line": 37, "text": "Clone the repository"}, {"line": 41, "text": "Create virtual environment"}, {"line": 45, "text": "Install dependencies"}, {"line": 48, "text": "Launch GUI"}, {"line": 58, "text": "Basic Usage"}, {"line": 60, "text": "GUI Mode"}, {"line": 72, "text": "CLI Mode"}, {"line": 88, "text": "Key Features"}, {"line": 90, "text": "\ud83d\ude80 Extreme Context Length"}, {"line": 95, "text": "\ud83c\udfaf Dynamic Subbrains"}, {"line": 98, "text": "\ud83d\udcbe 8-Bit Optimization"}, {"line": 101, "text": "\ud83d\udcca Rich Monitoring"}, {"line": 104, "text": "Documentation Structure"}, {"line": 119, "text": "Common Training Recipes"}, {"line": 121, "text": "High-Performance Training"}, {"line": 132, "text": "Memory-Efficient Training"}, {"line": 143, "text": "Multi-GPU Training"}, {"line": 153, "text": "Getting Help"}, {"line": 158, "text": "Contributing"}, {"line": 165, "text": "Link checking (docs)"}, {"line": 170, "text": "Check links in docs only"}, {"line": 176, "text": "Local check without installing lychee"}, {"line": 185, "text": "Project Status"}, {"line": 197, "text": "License"}, {"line": 201, "text": "References"}]}, {"path": "guide/DATASET_PREPROCESSING.md", "content": "# Dataset Preprocessing Guide\n\n## Overview\n\nThe dataset preprocessing utility converts downloaded datasets into an optimized block-based structure for efficient training with accurate progress tracking.\n\n## Why Preprocess?\n\n**Without Preprocessing:**\n- \u274c Slow or failed dataset size detection (especially on network drives)\n- \u274c No block/chunk progress tracking\n- \u274c Unpredictable performance on large datasets\n- \u274c Shows \"0/???\" for chunks and blocks\n\n**With Preprocessing:**\n- \u2705 Instant dataset size detection (reads metadata file)\n- \u2705 Accurate block and chunk progress tracking\n- \u2705 Consistent performance regardless of storage location\n- \u2705 Shows \"15/25\" for chunks, \"2/10\" for blocks\n- \u2705 Optimal for network drives and large datasets\n\n## When to Preprocess\n\nPreprocess datasets in these scenarios:\n- Downloaded to network drives (Z:, mapped drives, NAS)\n- Large datasets (>1GB, millions of samples)\n- Datasets with many small files\n- When training shows \"epoch tracking disabled\"\n\n## Usage\n\n### Command Line\n\n```bash\n# Basic preprocessing (100k samples per block)\naios hrm-hf preprocess-dataset Z:\\training_datasets\\tinystories\n\n# Custom block size\naios hrm-hf preprocess-dataset ~/datasets/my_corpus --block-size 50000\n\n# ASCII-only filtering\naios hrm-hf preprocess-dataset /data/multilingual --ascii-only\n\n# Overwrite existing preprocessed structure\naios hrm-hf preprocess-dataset ./datasets/corpus --overwrite\n```\n\n### Python API\n\n```python\nfrom aios.cli.datasets.preprocess_dataset import preprocess_dataset\n\n# Preprocess dataset\ntotal_samples, samples_per_block, total_blocks = preprocess_dataset(\n    dataset_path=\"Z:/training_datasets/tinystories\",\n    samples_per_block=100000,  # 100k samples per block\n    ascii_only=False,\n    overwrite=False\n)\n\nprint(f\"Preprocessed: {total_samples:,} samples in {total_blocks} blocks\")\n```\n\n## Structure Created\n\n```\ndataset_name/\n\u251c\u2500\u2500 dataset_info.json     # Metadata (instant size detection)\n\u251c\u2500\u2500 raw/                  # Original files (preserved)\n\u2502   \u251c\u2500\u2500 file1.txt\n\u2502   \u251c\u2500\u2500 file2.txt\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 block_0/              # First 100k samples\n\u2502   \u2514\u2500\u2500 samples.txt       # One sample per line\n\u251c\u2500\u2500 block_1/              # Next 100k samples\n\u2502   \u2514\u2500\u2500 samples.txt\n\u251c\u2500\u2500 block_2/\n\u2502   \u2514\u2500\u2500 samples.txt\n\u2514\u2500\u2500 ...\n```\n\n### Metadata File (dataset_info.json)\n\n```json\n{\n  \"dataset_name\": \"tinystories\",\n  \"total_samples\": 2456789,\n  \"samples_per_block\": 100000,\n  \"total_blocks\": 25,\n  \"ascii_only\": false,\n  \"preprocessed_by\": \"AI-OS dataset preprocessor\",\n  \"structure\": \"block_N/samples.txt format\"\n}\n```\n\n## Supported Input Formats\n\nThe preprocessor automatically detects and handles:\n\n### 1. HuggingFace Datasets\n- Saved with `dataset.save_to_disk()`\n- Contains `dataset_info.json`, `.arrow` files, or `data/` directory\n- Extracts text from columns: text, content, sentence, article, etc.\n\n### 2. Plain Text Files\n- `.txt`, `.csv`, `.json`, `.jsonl` files\n- Recursively scans subdirectories\n- One sample per line\n\n### 3. Mixed Directories\n- Combination of text files and HF dataset files\n- Automatically chooses best extraction method\n\n## Training with Preprocessed Datasets\n\nOnce preprocessed, training automatically detects the structure:\n\n```bash\n# Just point to the preprocessed directory\naios hrm-hf train-actv1 --dataset-file Z:\\training_datasets\\tinystories --steps 1000\n\n# Training output will show:\n# \u2713 Epoch tracking initialized\n# \u2713 Dataset: tinystories\n# \u2713 Total: 2,456,789 samples in 25 blocks\n# \u2713 Chunk: 15/25   Block: 2/25   Epoch: 0\n```\n\n## Parameters\n\n### --block-size (default: 100000)\nNumber of samples per block. Larger blocks = fewer files but more memory per block load.\n\n**Guidelines:**\n- **Small datasets (<100k samples)**: Use 10000-50000\n- **Medium datasets (100k-1M)**: Use 100000 (default)\n- **Large datasets (>1M)**: Use 100000-200000\n\n### --ascii-only\nFilter to ASCII-only text, removing non-ASCII characters and samples.\n\n**Use when:**\n- Training English-only models\n- Avoiding encoding issues\n- Reducing dataset size\n\n### --overwrite\nRebuild the preprocessed structure from scratch.\n\n**Use when:**\n- Updating after adding/removing raw files\n- Changing block size\n- Fixing corrupted structure\n\n## Performance\n\n### Before Preprocessing\n```\nDataset: Z:\\training_datasets\\tinystories (network drive)\nDetection: 45-120 seconds (or fails with timeout)\nProgress: \"Chunk: 0/???  Block: 0/???\"\n```\n\n### After Preprocessing\n```\nDataset: Z:\\training_datasets\\tinystories\nDetection: <1 second (reads metadata file)\nProgress: \"Chunk: 15/25  Block: 2/25  Epoch: 0\"\n```\n\n## Checking Status\n\n```python\nfrom aios.cli.datasets.preprocess_dataset import is_preprocessed, get_preprocessed_info\n\n# Check if dataset is preprocessed\nif is_preprocessed(\"Z:/training_datasets/tinystories\"):\n    print(\"Dataset is preprocessed!\")\n    \n    # Get metadata\n    info = get_preprocessed_info(\"Z:/training_datasets/tinystories\")\n    print(f\"Samples: {info['total_samples']:,}\")\n    print(f\"Blocks: {info['total_blocks']}\")\n```\n\n## Troubleshooting\n\n### \"No text samples found\"\n- Check that raw files contain readable text\n- Verify file extensions (.txt, .csv, .json, .jsonl)\n- Try without `--ascii-only` flag\n\n### \"Preprocessed structure exists\"\n- Use `--overwrite` to rebuild\n- Or delete `dataset_info.json` and `block_*` directories manually\n\n### \"Permission denied\"\n- Ensure write access to dataset directory\n- Try running with elevated privileges\n- Check network drive permissions\n\n### Slow preprocessing\n- Normal for large datasets (millions of samples)\n- Progress shown every 100 files\n- Consider preprocessing on local drive first, then moving\n\n## Best Practices\n\n1. **Preprocess once, train many times**\n   - Preprocessing is one-time cost\n   - Subsequent training runs are fast\n\n2. **Keep raw files**\n   - Original files moved to `raw/` subdirectory\n   - Can rebuild anytime with `--overwrite`\n\n3. **Use standard block size**\n   - 100k samples per block works well for most datasets\n   - Only adjust if specific memory constraints\n\n4. **Preprocess before long training runs**\n   - Ensures accurate progress tracking\n   - Prevents \"epoch tracking disabled\" issues\n\n5. **Version control metadata only**\n   - Add `block_*/` to .gitignore\n   - Keep `dataset_info.json` for reference\n   - Raw files can be re-downloaded\n\n## Integration with GUI\n\nThe GUI automatically detects preprocessed datasets:\n- Shows accurate total blocks in \"Training Progress\"\n- Displays chunk progress within current block\n- Updates blocks as \"X/Y\" instead of \"X\"\n\nNo GUI changes needed - just preprocess the dataset and start training!\n", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Dataset Preprocessing Guide"}, {"line": 2, "text": "Overview"}, {"line": 6, "text": "Why Preprocess?"}, {"line": 21, "text": "When to Preprocess"}, {"line": 29, "text": "Usage"}, {"line": 31, "text": "Command Line"}, {"line": 34, "text": "Basic preprocessing (100k samples per block)"}, {"line": 37, "text": "Custom block size"}, {"line": 40, "text": "ASCII-only filtering"}, {"line": 43, "text": "Overwrite existing preprocessed structure"}, {"line": 47, "text": "Python API"}, {"line": 52, "text": "Preprocess dataset"}, {"line": 63, "text": "Structure Created"}, {"line": 81, "text": "Metadata File (dataset_info.json)"}, {"line": 95, "text": "Supported Input Formats"}, {"line": 99, "text": "1. HuggingFace Datasets"}, {"line": 104, "text": "2. Plain Text Files"}, {"line": 109, "text": "3. Mixed Directories"}, {"line": 113, "text": "Training with Preprocessed Datasets"}, {"line": 118, "text": "Just point to the preprocessed directory"}, {"line": 121, "text": "Training output will show:"}, {"line": 122, "text": "\u2713 Epoch tracking initialized"}, {"line": 123, "text": "\u2713 Dataset: tinystories"}, {"line": 124, "text": "\u2713 Total: 2,456,789 samples in 25 blocks"}, {"line": 125, "text": "\u2713 Chunk: 15/25   Block: 2/25   Epoch: 0"}, {"line": 128, "text": "Parameters"}, {"line": 130, "text": "--block-size (default: 100000)"}, {"line": 138, "text": "--ascii-only"}, {"line": 146, "text": "--overwrite"}, {"line": 154, "text": "Performance"}, {"line": 156, "text": "Before Preprocessing"}, {"line": 163, "text": "After Preprocessing"}, {"line": 170, "text": "Checking Status"}, {"line": 175, "text": "Check if dataset is preprocessed"}, {"line": 179, "text": "Get metadata"}, {"line": 185, "text": "Troubleshooting"}, {"line": 187, "text": "\"No text samples found\""}, {"line": 192, "text": "\"Preprocessed structure exists\""}, {"line": 196, "text": "\"Permission denied\""}, {"line": 201, "text": "Slow preprocessing"}, {"line": 206, "text": "Best Practices"}, {"line": 229, "text": "Integration with GUI"}]}, {"path": "guide/dataset_size_detection_quick_reference.md", "content": "# Quick Reference: HuggingFace Dataset Size Detection\n\n## For Developers\n\n### Import and Use\n\n```python\nfrom src.aios.gui.components.dataset_download_panel.hf_size_detection import (\n    get_hf_dataset_metadata,\n    enrich_dataset_with_size,\n    format_size_display\n)\n\n# Get metadata for any HuggingFace dataset\ninfo = get_hf_dataset_metadata(\"stanfordnlp/imdb\")\nprint(f\"Rows: {info['num_rows']:,}\")\nprint(f\"Size: {info['size_gb']:.2f} GB\")\nprint(f\"Blocks: {info['total_blocks']}\")\n\n# Enrich existing dataset dictionary\ndataset = {\"path\": \"stanfordnlp/imdb\", \"name\": \"IMDB\"}\nenriched = enrich_dataset_with_size(dataset)\n# Now has: size_gb, num_rows, total_blocks, etc.\n\n# Format for display\nsize_str, rows_str, blocks_str = format_size_display(enriched)\nprint(f\"{size_str} | {rows_str} | {blocks_str}\")\n```\n\n## CLI Testing\n\n```bash\n# Test a specific dataset\npython src/aios/gui/components/dataset_download_panel/hf_size_detection.py stanfordnlp/imdb\n\n# Test with config and split\npython src/aios/gui/components/dataset_download_panel/hf_size_detection.py wikipedia 20220301.en train\n\n# Run built-in tests\npython src/aios/gui/components/dataset_download_panel/hf_size_detection.py\n```\n\n## Understanding the Output\n\n### Dataset Metadata Fields\n\nWhen you call `get_hf_dataset_metadata()`, you get:\n\n```python\n{\n    \"num_rows\": 25000,              # Total rows in dataset\n    \"num_rows_estimated\": False,     # True if estimated from file size\n    \"num_bytes\": 20971520,          # Size in bytes\n    \"size_mb\": 20.0,                # Size in megabytes\n    \"size_gb\": 0.02,                # Size in gigabytes\n    \"total_blocks\": 1,              # Number of 100k-sample blocks\n    \"samples_per_block\": 100000,    # Block size constant\n    \"is_partial\": False,            # True if data incomplete\n    \"source\": \"dataset_viewer_api\"  # Which API provided data\n}\n```\n\n### Block Calculation\n\n```\nBlocks = ceil(total_samples / 100,000)\n\nExamples:\n- 25,000 rows = 1 block\n- 150,000 rows = 2 blocks\n- 6,458,670 rows = 65 blocks\n```\n\n## Integration Examples\n\n### In Search Results\n\nThe Dataset Download Panel automatically enriches search results:\n\n```python\n# This happens automatically in search_operations.py\nfor ds in results:\n    enrich_dataset_with_size(ds)  # Adds size info\n```\n\n### In Custom Code\n\n```python\n# Get size for a specific dataset\nfrom src.aios.gui.components.dataset_download_panel.hf_size_detection import get_hf_dataset_metadata\n\ndataset_name = \"stanfordnlp/imdb\"\nmetadata = get_hf_dataset_metadata(dataset_name)\n\nif metadata:\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"  Size: {metadata['size_gb']:.2f} GB\")\n    print(f\"  Rows: {metadata['num_rows']:,}\")\n    print(f\"  Blocks: {metadata['total_blocks']}\")\n    print(f\"  Source: {metadata['source']}\")\n    \n    if metadata['num_rows_estimated']:\n        print(\"  Note: Row count is estimated\")\nelse:\n    print(f\"Could not get size info for {dataset_name}\")\n```\n\n## API Details\n\n### Dataset Viewer API (Primary)\n\n**Endpoint**: `https://datasets-server.huggingface.co/size?dataset={name}`\n\n**Pros**:\n- Fast (100-500ms)\n- Exact row counts\n- No auth needed\n- No library required\n\n**Cons**:\n- Not all datasets supported\n- Some return partial data\n\n### Hub API (Fallback)\n\n**Method**: `HfApi().dataset_info(repo_id=name, files_metadata=True)`\n\n**Pros**:\n- Works for any dataset\n- Reliable\n- Comprehensive file info\n\n**Cons**:\n- Slower (200-800ms)\n- Only file sizes (must estimate rows)\n- Requires huggingface_hub library\n\n## Troubleshooting\n\n### \"Unknown\" size displayed\n\n**Cause**: Both APIs failed to get size info\n\n**Solutions**:\n1. Check dataset name is correct\n2. Verify network connection\n3. Check if dataset was renamed on HuggingFace\n4. Try accessing the dataset directly on HuggingFace Hub\n\n### Row count marked \"(est.)\"\n\n**Cause**: Dataset Viewer API unavailable, using Hub API with estimation\n\n**Info**: \n- Estimated as: `total_bytes / 500` (assumes 500 bytes per row)\n- Generally accurate within 20-30%\n- More accurate for text datasets\n\n### Slow size detection\n\n**Cause**: Network latency or API slowdown\n\n**Solutions**:\n1. Results are cached with search results\n2. Size detection runs in background thread\n3. Individual failures don't block UI\n\n## Constants\n\n```python\nSAMPLES_PER_BLOCK = 100000       # Standard block size\nDEFAULT_BYTES_PER_ROW = 500      # For estimation\nAPI_TIMEOUT = 10                 # Seconds\n```\n\n## Return Values\n\n### Success\n\n```python\nmetadata = {\n    \"num_rows\": int,\n    \"size_gb\": float,\n    \"total_blocks\": int,\n    # ... other fields\n}\n```\n\n### Failure\n\n```python\nmetadata = None  # All methods failed\n```\n\nWhen integrated into datasets:\n```python\ndataset = {\n    \"size_gb\": 0.0,        # Default if detection fails\n    \"num_rows\": 0,         # Default\n    \"total_blocks\": 0,     # Default\n    # ... other fields\n}\n```\n", "tags": ["cli", "datasets", "gui", "training"], "headings": [{"line": 0, "text": "Quick Reference: HuggingFace Dataset Size Detection"}, {"line": 2, "text": "For Developers"}, {"line": 4, "text": "Import and Use"}, {"line": 13, "text": "Get metadata for any HuggingFace dataset"}, {"line": 19, "text": "Enrich existing dataset dictionary"}, {"line": 22, "text": "Now has: size_gb, num_rows, total_blocks, etc."}, {"line": 24, "text": "Format for display"}, {"line": 29, "text": "CLI Testing"}, {"line": 32, "text": "Test a specific dataset"}, {"line": 35, "text": "Test with config and split"}, {"line": 38, "text": "Run built-in tests"}, {"line": 42, "text": "Understanding the Output"}, {"line": 44, "text": "Dataset Metadata Fields"}, {"line": 62, "text": "Block Calculation"}, {"line": 73, "text": "Integration Examples"}, {"line": 75, "text": "In Search Results"}, {"line": 80, "text": "This happens automatically in search_operations.py"}, {"line": 85, "text": "In Custom Code"}, {"line": 88, "text": "Get size for a specific dataset"}, {"line": 107, "text": "API Details"}, {"line": 109, "text": "Dataset Viewer API (Primary)"}, {"line": 123, "text": "Hub API (Fallback)"}, {"line": 137, "text": "Troubleshooting"}, {"line": 139, "text": "\"Unknown\" size displayed"}, {"line": 149, "text": "Row count marked \"(est.)\""}, {"line": 158, "text": "Slow size detection"}, {"line": 167, "text": "Constants"}, {"line": 175, "text": "Return Values"}, {"line": 177, "text": "Success"}, {"line": 184, "text": "... other fields"}, {"line": 188, "text": "Failure"}, {"line": 200, "text": "... other fields"}]}, {"path": "guide/EVALUATION_CUSTOM_MODELS.md", "content": "# Evaluating AI-OS Native Brains\n\nThis guide explains how AI-OS native brains (actv1, etc.) can be evaluated using standard benchmarks through the lm-evaluation-harness framework.\n\n## Overview\n\nAI-OS includes a custom adapter that allows native brains to be evaluated using the same benchmarks as HuggingFace models. The adapter wraps AI-OS brains and provides the interface expected by lm_eval.\n\n## Supported Brain Types\n\nCurrently supported:\n- **actv1**: Custom hierarchical reasoning model with H-layers and L-layers\n\nFuture support planned for additional brain architectures.\n\n## How It Works\n\n### 1. Custom Model Adapter\n\nThe `AIOSBrainModel` class in `aios.core.evaluation.aios_lm_eval_adapter` implements the lm_eval API:\n\n- **Model Loading**: Loads AI-OS brains using the standard brain loading mechanism\n- **Tokenization**: Uses the brain's configured tokenizer\n- **Inference**: Delegates to the brain's `run()` method for generation tasks\n- **Log Likelihood**: Computes probabilities for multiple-choice tasks using the model's forward pass\n\n### 2. Automatic Registration\n\nWhen you select a brain in the Evaluation tab, the system:\n\n1. Detects the brain type by reading `brain.json`\n2. Registers the custom adapter with lm_eval\n3. Configures the evaluation to use `--model aios --model_args brain_path=/path/to/brain`\n\n### 3. Benchmark Compatibility\n\nThe adapter supports all standard lm_eval benchmark types:\n\n- **Multiple Choice** (MMLU, ARC, etc.): Uses `loglikelihood()` to score options\n- **Generation** (HumanEval, GSM8K, etc.): Uses `generate_until()` for text generation\n- **Language Modeling** (WikiText, etc.): Uses `loglikelihood_rolling()` for perplexity\n\n## Using the GUI\n\n### Evaluating a Brain\n\n1. Open the **Evaluation** tab\n2. Under **Model Selection**, select **AI-OS Brain** as the source\n3. Choose your brain from the dropdown (e.g., \"English-v1\")\n4. Select benchmarks to run\n5. Click **Start Evaluation**\n\nThe system will automatically:\n- Detect it's a native AI-OS brain\n- Load the custom adapter\n- Run the evaluation\n- Display results\n\n### Supported Benchmarks\n\nAll standard benchmarks work with native brains:\n\n**Quick & General**:\n- hellaswag (common sense reasoning)\n- arc_easy, arc_challenge (science questions)\n- winogrande (pronoun resolution)\n- boolq (yes/no questions)\n\n**Academic**:\n- mmlu (57 academic subjects)\n- truthfulqa_mc1, truthfulqa_mc2 (truthfulness)\n\n**Coding & Math**:\n- humaneval (code generation)\n- mbpp (Python programming)\n- gsm8k (grade school math)\n\n**Language Modeling**:\n- wikitext (perplexity on Wikipedia)\n- lambada (next-word prediction)\n\n## Using the CLI\n\nYou can also evaluate brains from the command line:\n\n```bash\n# Evaluate using the custom adapter\nlm_eval \\\n  --model aios \\\n  --model_args brain_path=artifacts/brains/actv1/English-v1 \\\n  --tasks hellaswag,arc_easy \\\n  --device cuda:0 \\\n  --batch_size 1\n```\n\nOr use the AI-OS CLI wrapper:\n\n```bash\n# This automatically detects brain type and uses the adapter\naios eval run artifacts/brains/actv1/English-v1 \\\n  --tasks hellaswag,arc_easy \\\n  --device cuda:0\n```\n\n## Implementation Details\n\n### Model Interface\n\nThe adapter implements these key methods:\n\n```python\nclass AIOSBrainModel(LM):\n    def loglikelihood(self, requests):\n        \"\"\"Score multiple choice options\"\"\"\n        # Uses model.forward() to get logits\n        # Computes log probabilities for each token\n        \n    def generate_until(self, requests):\n        \"\"\"Generate text with stopping criteria\"\"\"\n        # Uses brain.run() for generation\n        # Handles stop sequences\n        \n    def loglikelihood_rolling(self, requests):\n        \"\"\"Compute perplexity\"\"\"\n        # Scores each token given context\n```\n\n### Brain Loading\n\nBrains are loaded through the standard AI-OS mechanism:\n\n1. Read `brain.json` to get configuration\n2. Load checkpoint (`actv1_student.safetensors`)\n3. Load tokenizer (configured in brain.json)\n4. Move to specified device (GPU/CPU)\n\n### Batch Processing\n\nCurrently, the adapter uses batch_size=1 for simplicity. This means:\n- Each sample is processed individually\n- More reliable for custom architectures\n- Slightly slower than batched processing\n\nFuture versions may support batching for better performance.\n\n## Limitations & Considerations\n\n### Current Limitations\n\n1. **Batch Size**: Fixed at 1 for now\n2. **Speed**: Native brains may be slower than optimized HF models\n3. **Memory**: Full model loaded in memory during evaluation\n4. **Accuracy**: Some benchmarks may not align perfectly with brain's training\n\n### Performance Tips\n\n**For Faster Evaluation**:\n- Use `--limit 100` to test on subset first\n- Start with smaller benchmarks (hellaswag, arc_easy)\n- Use GPU for significant speedup\n\n**For Memory Constraints**:\n- Evaluate one benchmark at a time\n- Use CPU if GPU memory is limited\n- Close other applications\n\n### Result Interpretation\n\n**Comparing Scores**:\n- Native brains may score differently than HF models\n- Architecture differences affect benchmark performance\n- Focus on trends across multiple benchmarks\n\n**Benchmark Selection**:\n- Choose benchmarks aligned with brain's training\n- Text-heavy brains: language modeling, QA\n- Code-trained brains: HumanEval, MBPP\n- Math-trained brains: GSM8K, MATH\n\n## Troubleshooting\n\n### \"Model Not Found\" Error\n\n```\nOSError: English-v1 is not a local folder\n```\n\n**Cause**: Trying to use brain name as HF model  \n**Solution**: Make sure \"AI-OS Brain\" is selected as model source\n\n### \"Brain Type Not Supported\"\n\n```\nValueError: Unsupported brain type: xyz\n```\n\n**Cause**: Brain type not yet supported by adapter  \n**Solution**: Currently only actv1 brains are supported. Check brain.json type field.\n\n### Poor Performance\n\nIf brain scores unexpectedly low:\n- Check if brain has been trained\n- Verify tokenizer matches training\n- Try benchmarks aligned with training data\n- Compare with baseline model of similar size\n\n### Memory Errors\n\n```\nCUDA out of memory\n```\n\n**Solutions**:\n- Use `--device cpu` to run on CPU\n- Close other GPU applications\n- Reduce `--limit` to evaluate fewer samples\n- Evaluate benchmarks one at a time\n\n## Examples\n\n### Quick Test\n\n```bash\n# Fast test on 100 samples\naios gui\n# Select brain, choose \"Quick Test\" preset, set limit to 100\n```\n\n### Full MMLU Suite\n\n```bash\n# Comprehensive academic benchmark\nlm_eval \\\n  --model aios \\\n  --model_args brain_path=artifacts/brains/actv1/English-v1 \\\n  --tasks mmlu \\\n  --device cuda:0 \\\n  --output_path artifacts/evaluation/mmlu_full\n```\n\n### Compare Multiple Brains\n\n```bash\n# Evaluate multiple brains on same benchmark\nfor brain in English-v1 Math-v1 Code-v1; do\n  aios eval run artifacts/brains/actv1/$brain \\\n    --tasks hellaswag,arc_easy,gsm8k \\\n    --output artifacts/evaluation/$brain\ndone\n\n# Compare results\naios eval compare --ids 1,2,3\n```\n\n## Advanced Usage\n\n### Custom Benchmarks\n\nYou can create custom tasks for lm_eval:\n\n```python\n# my_custom_task.py\nfrom lm_eval.api.task import Task\n\nclass MyTask(Task):\n    def __init__(self):\n        super().__init__()\n        # Define your task\n```\n\nThen use it:\n\n```bash\nlm_eval \\\n  --model aios \\\n  --model_args brain_path=artifacts/brains/actv1/English-v1 \\\n  --tasks my_custom_task \\\n  --include_path ./\n```\n\n### Programmatic Evaluation\n\n```python\nfrom aios.core.evaluation import HarnessWrapper, register_aios_model\n\n# Register adapter\nregister_aios_model()\n\n# Create wrapper\nharness = HarnessWrapper()\n\n# Run evaluation\nresult = harness.run_evaluation(\n    model_name=\"brain_path=artifacts/brains/actv1/English-v1\",\n    tasks=[\"hellaswag\", \"arc_easy\"],\n    model_type=\"aios\",\n    device=\"cuda:0\",\n)\n\nprint(f\"Overall Score: {result.overall_score:.2%}\")\n```\n\n## Future Enhancements\n\nPlanned improvements:\n\n1. **Batch Support**: Enable batch_size > 1 for faster evaluation\n2. **More Brain Types**: Support for additional architectures\n3. **Custom Metrics**: Brain-specific evaluation metrics\n4. **Fine-grained Control**: Per-layer analysis, attention visualization\n5. **Comparative Analysis**: Built-in brain-to-brain comparison\n\n## Related Documentation\n\n- [Model Evaluation Guide](EVALUATION.md) - General evaluation guide\n- [Benchmark Selection](BENCHMARK_SELECTION.md) - Choosing appropriate benchmarks\n- [ACTv1 Architecture](../research/ACTV1_ARCHITECTURE.md) - Brain architecture details\n\n---\n\n**Last Updated**: November 8, 2025\n", "tags": ["cli", "evaluation", "gui"], "headings": [{"line": 0, "text": "Evaluating AI-OS Native Brains"}, {"line": 4, "text": "Overview"}, {"line": 8, "text": "Supported Brain Types"}, {"line": 15, "text": "How It Works"}, {"line": 17, "text": "1. Custom Model Adapter"}, {"line": 26, "text": "2. Automatic Registration"}, {"line": 34, "text": "3. Benchmark Compatibility"}, {"line": 42, "text": "Using the GUI"}, {"line": 44, "text": "Evaluating a Brain"}, {"line": 58, "text": "Supported Benchmarks"}, {"line": 81, "text": "Using the CLI"}, {"line": 86, "text": "Evaluate using the custom adapter"}, {"line": 98, "text": "This automatically detects brain type and uses the adapter"}, {"line": 104, "text": "Implementation Details"}, {"line": 106, "text": "Model Interface"}, {"line": 114, "text": "Uses model.forward() to get logits"}, {"line": 115, "text": "Computes log probabilities for each token"}, {"line": 119, "text": "Uses brain.run() for generation"}, {"line": 120, "text": "Handles stop sequences"}, {"line": 124, "text": "Scores each token given context"}, {"line": 127, "text": "Brain Loading"}, {"line": 136, "text": "Batch Processing"}, {"line": 145, "text": "Limitations & Considerations"}, {"line": 147, "text": "Current Limitations"}, {"line": 154, "text": "Performance Tips"}, {"line": 166, "text": "Result Interpretation"}, {"line": 179, "text": "Troubleshooting"}, {"line": 181, "text": "\"Model Not Found\" Error"}, {"line": 190, "text": "\"Brain Type Not Supported\""}, {"line": 199, "text": "Poor Performance"}, {"line": 207, "text": "Memory Errors"}, {"line": 219, "text": "Examples"}, {"line": 221, "text": "Quick Test"}, {"line": 224, "text": "Fast test on 100 samples"}, {"line": 226, "text": "Select brain, choose \"Quick Test\" preset, set limit to 100"}, {"line": 229, "text": "Full MMLU Suite"}, {"line": 232, "text": "Comprehensive academic benchmark"}, {"line": 241, "text": "Compare Multiple Brains"}, {"line": 244, "text": "Evaluate multiple brains on same benchmark"}, {"line": 251, "text": "Compare results"}, {"line": 255, "text": "Advanced Usage"}, {"line": 257, "text": "Custom Benchmarks"}, {"line": 262, "text": "my_custom_task.py"}, {"line": 268, "text": "Define your task"}, {"line": 281, "text": "Programmatic Evaluation"}, {"line": 286, "text": "Register adapter"}, {"line": 289, "text": "Create wrapper"}, {"line": 292, "text": "Run evaluation"}, {"line": 303, "text": "Future Enhancements"}, {"line": 313, "text": "Related Documentation"}]}, {"path": "guide/gui_async_patterns.md", "content": "# GUI Async Patterns & Worker Pool Usage Guide\n\n**Author**: AI-OS Development Team  \n**Last Updated**: November 2025  \n**Applies to**: AI-OS GUI v2.0+\n\n## Table of Contents\n\n1. [Overview](#overview)\n2. [Architecture](#architecture)\n3. [Worker Pool Patterns](#worker-pool-patterns)\n4. [Common Patterns](#common-patterns)\n5. [Anti-Patterns to Avoid](#anti-patterns-to-avoid)\n6. [Best Practices](#best-practices)\n7. [Examples](#examples)\n8. [Troubleshooting](#troubleshooting)\n\n---\n\n## Overview\n\nThe AI-OS GUI uses an async worker pool architecture to keep the interface responsive while performing blocking operations like subprocess calls, file I/O, and network requests.\n\n### Key Components\n\n- **AsyncWorkerPool**: Thread pool for background operations\n- **AsyncEventLoop**: Dedicated asyncio loop for async/await patterns\n- **TimerManager**: Debounced timer management\n- **ProcessReaper**: Subprocess cleanup\n\n### Performance Characteristics\n\n- **Worker Pool Size**: `(cpu_count * 4) + 1`, minimum 12 workers\n- **CLI Cache TTL**: 2 seconds for read-only operations\n- **Refresh Throttling**: 5 seconds for panel refreshes\n- **Configurable**: Set `AIOS_WORKER_THREADS` environment variable\n\n---\n\n## Architecture\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Tkinter Main Thread                  \u2502\n\u2502  - Event Loop                                           \u2502\n\u2502  - UI Updates                                           \u2502\n\u2502  - Event Handlers                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n                   \u2502 Submit work via worker_pool.submit()\n                   \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              AsyncWorkerPool (12-50 threads)            \u2502\n\u2502  - CLI subprocess calls                                 \u2502\n\u2502  - File I/O operations                                  \u2502\n\u2502  - Network requests                                     \u2502\n\u2502  - Heavy computations                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n                   \u2502 Schedule UI updates via .after(0, callback)\n                   \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Tkinter Main Thread (UI Update)            \u2502\n\u2502  - Update widgets                                       \u2502\n\u2502  - Refresh displays                                     \u2502\n\u2502  - Show results                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## Worker Pool Patterns\n\n### Pattern 1: Simple Background Operation\n\n**Use Case**: Single async operation with UI update\n\n```python\ndef refresh(self):\n    \"\"\"Refresh panel data asynchronously.\"\"\"\n    \n    def _do_work():\n        # Background work (blocking I/O, subprocess, etc.)\n        data = self._fetch_data()  # Blocking call OK here\n        \n        # Schedule UI update on main thread\n        def _update_ui():\n            self._display_data(data)\n        \n        try:\n            self.after(0, _update_ui)\n        except Exception:\n            pass  # Widget destroyed\n    \n    # Submit to worker pool\n    if self._worker_pool:\n        self._worker_pool.submit(_do_work)\n    else:\n        # Fallback if worker pool unavailable\n        import threading\n        threading.Thread(target=_do_work, daemon=True).start()\n```\n\n### Pattern 2: Operation with Loading Indicator\n\n**Use Case**: Long operation with user feedback\n\n```python\ndef refresh(self):\n    \"\"\"Refresh with loading indicator.\"\"\"\n    \n    # Prevent duplicate operations\n    if hasattr(self, '_loading') and self._loading:\n        return\n    \n    self._loading = True\n    self.status_var.set(\"Loading...\")\n    \n    def _do_work():\n        try:\n            data = self._fetch_data()\n            \n            def _update_ui():\n                self._display_data(data)\n                self.status_var.set(\"\")\n                self._loading = False\n            \n            self.after(0, _update_ui)\n        except Exception as e:\n            def _handle_error():\n                self.status_var.set(f\"Error: {e}\")\n                self._loading = False\n            \n            self.after(0, _handle_error)\n    \n    self._worker_pool.submit(_do_work)\n```\n\n### Pattern 3: Throttled Operation\n\n**Use Case**: Prevent excessive calls (e.g., rapid refresh)\n\n```python\ndef refresh(self, force: bool = False):\n    \"\"\"Refresh with throttling.\"\"\"\n    \n    # Throttle unless forced\n    if not force:\n        if not hasattr(self, '_last_refresh'):\n            self._last_refresh = 0.0\n        \n        import time\n        now = time.time()\n        if now - self._last_refresh < 5.0:  # 5-second throttle\n            return  # Too soon, skip\n        \n        self._last_refresh = now\n    \n    # Proceed with refresh (use Pattern 2)\n    # ...\n```\n\n### Pattern 4: Multiple Concurrent Operations\n\n**Use Case**: Parallel data fetching\n\n```python\ndef refresh_all(self):\n    \"\"\"Refresh multiple data sources concurrently.\"\"\"\n    \n    results = {'brains': None, 'datasets': None, 'models': None}\n    completed = {'count': 0}\n    total = len(results)\n    \n    def _fetch_and_update(key, fetch_fn):\n        try:\n            data = fetch_fn()\n            results[key] = data\n        except Exception as e:\n            results[key] = None\n        \n        # Update UI when all complete\n        completed['count'] += 1\n        if completed['count'] == total:\n            def _update_all():\n                self._display_brains(results['brains'])\n                self._display_datasets(results['datasets'])\n                self._display_models(results['models'])\n            \n            self.after(0, _update_all)\n    \n    # Submit all tasks concurrently\n    self._worker_pool.submit(_fetch_and_update, 'brains', self._fetch_brains)\n    self._worker_pool.submit(_fetch_and_update, 'datasets', self._fetch_datasets)\n    self._worker_pool.submit(_fetch_and_update, 'models', self._fetch_models)\n```\n\n---\n\n## Common Patterns\n\n### CLI Subprocess Calls\n\n```python\n# \u274c WRONG - Blocks GUI thread\ndef load_brain(self, name):\n    result = self._run_cli(['brains', 'load', name])\n    self.display_result(result)\n\n# \u2705 CORRECT - Async with worker pool\ndef load_brain(self, name):\n    def _load():\n        result = self._run_cli(['brains', 'load', name])\n        \n        def _show_result():\n            self.display_result(result)\n        \n        self.after(0, _show_result)\n    \n    self._worker_pool.submit(_load)\n```\n\n### File I/O Operations\n\n```python\n# \u274c WRONG - Blocks GUI\ndef save_data(self, data):\n    with open('data.json', 'w') as f:\n        json.dump(data, f)\n    self.status_var.set(\"Saved\")\n\n# \u2705 CORRECT - Async file I/O\ndef save_data(self, data):\n    def _save():\n        with open('data.json', 'w') as f:\n            json.dump(data, f)\n        \n        def _update_status():\n            self.status_var.set(\"Saved\")\n        \n        self.after(0, _update_status)\n    \n    self._worker_pool.submit(_save)\n```\n\n### Network Requests\n\n```python\n# \u274c WRONG - Blocks during download\ndef download_dataset(self, url):\n    response = requests.get(url)\n    self.save_dataset(response.content)\n\n# \u2705 CORRECT - Async with progress\ndef download_dataset(self, url):\n    def _download():\n        response = requests.get(url, stream=True)\n        total = int(response.headers.get('content-length', 0))\n        downloaded = 0\n        \n        for chunk in response.iter_content(chunk_size=8192):\n            downloaded += len(chunk)\n            progress = (downloaded / total) * 100\n            \n            # Update progress bar\n            def _update_progress(p=progress):\n                self.progress_var.set(p)\n            \n            self.after(0, _update_progress)\n        \n        # Final update\n        def _complete():\n            self.status_var.set(\"Download complete\")\n        \n        self.after(0, _complete)\n    \n    self._worker_pool.submit(_download)\n```\n\n---\n\n## Anti-Patterns to Avoid\n\n### \u274c Anti-Pattern 1: Direct UI Updates from Worker Thread\n\n```python\n# WRONG - Tkinter is not thread-safe!\ndef _worker_function():\n    data = fetch_data()\n    self.label.config(text=data)  # CRASHES or CORRUPTS UI\n```\n\n**Problem**: Tkinter widgets can only be modified from the main thread.\n\n**Solution**: Always use `.after(0, callback)` to schedule UI updates.\n\n### \u274c Anti-Pattern 2: Blocking in Main Thread\n\n```python\n# WRONG - Freezes GUI\ndef on_button_click(self):\n    time.sleep(5)  # Blocks event loop\n    self.label.config(text=\"Done\")\n```\n\n**Problem**: Main thread is blocked, GUI becomes unresponsive.\n\n**Solution**: Move blocking work to worker pool.\n\n### \u274c Anti-Pattern 3: Creating Threads Directly\n\n```python\n# WRONG - Resource leak, no management\ndef do_work(self):\n    threading.Thread(target=self._work).start()\n```\n\n**Problem**: Threads not managed, can accumulate and leak.\n\n**Solution**: Use worker pool for automatic lifecycle management.\n\n### \u274c Anti-Pattern 4: Missing Error Handling\n\n```python\n# WRONG - Crashes silently\ndef _worker():\n    data = risky_operation()  # May raise exception\n    self.after(0, lambda: self.display(data))\n```\n\n**Problem**: Exceptions in worker threads are silent.\n\n**Solution**: Always wrap in try/except with error UI updates.\n\n### \u274c Anti-Pattern 5: No Progress Feedback\n\n```python\n# WRONG - User thinks app is frozen\ndef long_operation(self):\n    self._worker_pool.submit(self._do_long_work)\n```\n\n**Problem**: No visual feedback during long operations.\n\n**Solution**: Add loading indicators and status updates.\n\n---\n\n## Best Practices\n\n### 1. Always Update UI from Main Thread\n\n```python\n# Use .after(0, callback) for all widget updates\ndef _worker():\n    result = process_data()\n    self.after(0, lambda: self.label.config(text=result))\n```\n\n### 2. Provide User Feedback\n\n```python\n# Show loading state\nself.status_var.set(\"Processing...\")\nself._worker_pool.submit(work)\n\n# Update on completion\ndef _complete():\n    self.status_var.set(\"Complete\")\n```\n\n### 3. Handle Errors Gracefully\n\n```python\ndef _worker():\n    try:\n        result = risky_operation()\n        self.after(0, lambda: self._show_result(result))\n    except Exception as e:\n        self.after(0, lambda: self._show_error(str(e)))\n```\n\n### 4. Prevent Duplicate Operations\n\n```python\nif self._operation_in_progress:\n    return  # Already running\n\nself._operation_in_progress = True\n# ... submit work ...\n# Set to False in completion callback\n```\n\n### 5. Use Throttling for Frequent Operations\n\n```python\n# Only refresh if >5 seconds since last refresh\nif time.time() - self._last_refresh < 5.0:\n    return\n```\n\n### 6. Clean Up Resources\n\n```python\ndef cleanup(self):\n    \"\"\"Called on widget destruction.\"\"\"\n    self._worker_pool.shutdown(wait=False, timeout=2.0)\n```\n\n### 7. Leverage CLI Caching\n\n```python\n# Caching is automatic for read-only CLI operations\nresult = self._run_cli(['brains', 'stats'])  # Cached for 2 seconds\n```\n\n---\n\n## Examples\n\n### Example 1: Complete Panel Refresh\n\n```python\nclass MyPanel(ttk.LabelFrame):\n    def __init__(self, parent, *, worker_pool=None):\n        super().__init__(parent, text=\"My Panel\")\n        self._worker_pool = worker_pool\n        self._loading = False\n        self._last_refresh = 0.0\n        \n        # Build UI\n        self.status_var = tk.StringVar(value=\"\")\n        self.status_label = ttk.Label(self, textvariable=self.status_var)\n        self.status_label.pack()\n        \n        self.data_tree = ttk.Treeview(self)\n        self.data_tree.pack()\n        \n        ttk.Button(self, text=\"Refresh\", command=self.refresh).pack()\n    \n    def refresh(self, force: bool = False):\n        \"\"\"Refresh panel data with throttling and loading indicator.\"\"\"\n        \n        # Throttle\n        if not force:\n            import time\n            now = time.time()\n            if now - self._last_refresh < 5.0:\n                return\n            self._last_refresh = now\n        \n        # Prevent duplicates\n        if self._loading:\n            return\n        \n        self._loading = True\n        self.status_var.set(\"Loading...\")\n        \n        def _fetch_data():\n            \"\"\"Background data fetching.\"\"\"\n            try:\n                # Blocking I/O operations\n                data = self._run_cli(['my-data', 'list'])\n                parsed = self._parse_data(data)\n                \n                # Schedule UI update\n                def _update_ui():\n                    self._populate_tree(parsed)\n                    self.status_var.set(\"\")\n                    self._loading = False\n                \n                self.after(0, _update_ui)\n            except Exception as e:\n                # Schedule error handling\n                def _show_error():\n                    self.status_var.set(f\"Error: {e}\")\n                    self._loading = False\n                \n                self.after(0, _show_error)\n        \n        # Submit to worker pool\n        if self._worker_pool:\n            self._worker_pool.submit(_fetch_data)\n        else:\n            import threading\n            threading.Thread(target=_fetch_data, daemon=True).start()\n    \n    def _populate_tree(self, data):\n        \"\"\"Populate treeview with data (main thread only).\"\"\"\n        # Clear existing\n        for item in self.data_tree.get_children():\n            self.data_tree.delete(item)\n        \n        # Add new items\n        for item in data:\n            self.data_tree.insert('', 'end', values=(item['name'], item['value']))\n```\n\n### Example 2: User Action with Confirmation\n\n```python\ndef delete_item(self, item_name):\n    \"\"\"Delete item with confirmation dialog.\"\"\"\n    from tkinter import messagebox\n    \n    # Confirmation dialog (main thread, blocking is OK)\n    if not messagebox.askyesno(\"Confirm\", f\"Delete {item_name}?\"):\n        return\n    \n    # Show progress\n    self.status_var.set(f\"Deleting {item_name}...\")\n    \n    def _do_delete():\n        \"\"\"Background deletion.\"\"\"\n        try:\n            result = self._run_cli(['delete', item_name])\n            success = 'error' not in result.lower()\n            \n            def _update_ui():\n                if success:\n                    self.status_var.set(\"Deleted successfully\")\n                    self.refresh(force=True)  # Refresh list\n                else:\n                    self.status_var.set(\"Delete failed\")\n                    messagebox.showerror(\"Error\", result)\n            \n            self.after(0, _update_ui)\n        except Exception as e:\n            def _show_error():\n                self.status_var.set(\"Error\")\n                messagebox.showerror(\"Error\", str(e))\n            \n            self.after(0, _show_error)\n    \n    self._worker_pool.submit(_do_delete)\n```\n\n---\n\n## Troubleshooting\n\n### Problem: GUI Freezes\n\n**Symptoms**: Window becomes unresponsive, \"Not Responding\" in title\n\n**Causes**:\n- Blocking operation in main thread\n- Synchronous subprocess call\n- Heavy computation without worker pool\n\n**Solutions**:\n```python\n# Check for blocking calls:\ngrep -r \"subprocess.run\\|time.sleep\\|\\.wait()\" src/aios/gui/\n\n# Move to worker pool:\nself._worker_pool.submit(blocking_operation)\n```\n\n### Problem: UI Updates Don't Appear\n\n**Symptoms**: Changes happen but UI doesn't refresh\n\n**Causes**:\n- Direct widget updates from worker thread\n- Missing `.after(0, callback)`\n\n**Solutions**:\n```python\n# Always schedule UI updates:\ndef _worker():\n    data = fetch()\n    self.after(0, lambda: self.widget.config(text=data))\n```\n\n### Problem: \"RuntimeError: main thread is not in main loop\"\n\n**Symptoms**: Crash when updating UI from worker\n\n**Cause**: Direct UI modification from worker thread\n\n**Solution**:\n```python\n# Wrap all UI updates in .after():\nself.after(0, self._update_widgets)\n```\n\n### Problem: Worker Pool Exhaustion\n\n**Symptoms**: Operations queue up, long delays\n\n**Causes**:\n- Too many concurrent operations\n- Workers blocked on long-running tasks\n- Insufficient worker count\n\n**Solutions**:\n```python\n# Increase worker pool size:\nexport AIOS_WORKER_THREADS=32\n\n# Check pool size:\nprint(f\"Workers: {app._worker_pool._max_workers}\")\n\n# Optimize long-running tasks:\n# - Break into chunks\n# - Add timeouts\n# - Use async/await for I/O\n```\n\n### Problem: Memory Leaks\n\n**Symptoms**: Memory usage grows over time\n\n**Causes**:\n- Widget references in closures\n- Unclosed file handles\n- Accumulated cache\n\n**Solutions**:\n```python\n# Use weak references:\nimport weakref\nwidget_ref = weakref.ref(widget)\n\n# Clear cache periodically:\nif len(self._cli_cache) > 50:\n    oldest = min(self._cli_cache.keys(), \n                 key=lambda k: self._cli_cache[k][1])\n    del self._cli_cache[oldest]\n\n# Cleanup on destroy:\ndef cleanup(self):\n    self._worker_pool.shutdown(wait=False)\n```\n\n---\n\n## Performance Tuning\n\n### Worker Pool Sizing\n\n```bash\n# For CPU-bound tasks:\nexport AIOS_WORKER_THREADS=$(python -c \"import os; print(os.cpu_count())\")\n\n# For I/O-bound tasks (default, recommended):\nexport AIOS_WORKER_THREADS=$(python -c \"import os; print((os.cpu_count() or 4) * 4 + 1)\")\n\n# For high-concurrency scenarios:\nexport AIOS_WORKER_THREADS=64\n```\n\n### Cache Configuration\n\n```python\n# Adjust cache TTL in CliBridgeMixin.__init__():\nself._cache_ttl = 5.0  # Cache for 5 seconds instead of 2\n```\n\n### Throttling Intervals\n\n```python\n# Adjust per-panel in refresh() method:\nif time.time() - self._last_refresh < 10.0:  # 10-second throttle\n    return\n```\n\n---\n\n## Testing Async Code\n\n### Manual Testing Checklist\n\n- [ ] Click buttons rapidly - UI stays responsive\n- [ ] Switch tabs quickly - no freezing\n- [ ] Trigger multiple refreshes - no crashes\n- [ ] Close window during operations - clean shutdown\n- [ ] Network down - graceful error handling\n- [ ] Large datasets - progress feedback shown\n\n### Automated Testing\n\nUse the profiling script:\n\n```bash\npython scripts/profile_gui_responsiveness.py\n```\n\nThis will:\n- Measure startup time\n- Test tab switching speed\n- Monitor UI thread blocking\n- Check refresh operation latency\n- Generate performance report\n\n---\n\n## References\n\n- **AsyncWorkerPool**: `src/aios/gui/utils/resource_management/async_pool.py`\n- **CliBridgeMixin**: `src/aios/gui/mixins/cli_bridge.py`\n- **Panel Setup**: `src/aios/gui/app/panel_setup.py`\n- **Example Panel**: `src/aios/gui/components/brains_panel/panel_main.py`\n\n---\n\n## Changelog\n\n### November 2025\n- Initial version\n- Added CLI caching (2s TTL)\n- Increased worker pool size (4x CPU cores)\n- Added loading indicators to brains panel\n- Implemented async panel initialization\n\n---\n\n*For questions or improvements to this guide, please open an issue or PR.*\n", "tags": ["cli", "gui"], "headings": [{"line": 0, "text": "GUI Async Patterns & Worker Pool Usage Guide"}, {"line": 6, "text": "Table of Contents"}, {"line": 19, "text": "Overview"}, {"line": 23, "text": "Key Components"}, {"line": 30, "text": "Performance Characteristics"}, {"line": 39, "text": "Architecture"}, {"line": 71, "text": "Worker Pool Patterns"}, {"line": 73, "text": "Pattern 1: Simple Background Operation"}, {"line": 82, "text": "Background work (blocking I/O, subprocess, etc.)"}, {"line": 85, "text": "Schedule UI update on main thread"}, {"line": 94, "text": "Submit to worker pool"}, {"line": 98, "text": "Fallback if worker pool unavailable"}, {"line": 103, "text": "Pattern 2: Operation with Loading Indicator"}, {"line": 111, "text": "Prevent duplicate operations"}, {"line": 138, "text": "Pattern 3: Throttled Operation"}, {"line": 146, "text": "Throttle unless forced"}, {"line": 158, "text": "Proceed with refresh (use Pattern 2)"}, {"line": 159, "text": "..."}, {"line": 162, "text": "Pattern 4: Multiple Concurrent Operations"}, {"line": 181, "text": "Update UI when all complete"}, {"line": 191, "text": "Submit all tasks concurrently"}, {"line": 199, "text": "Common Patterns"}, {"line": 201, "text": "CLI Subprocess Calls"}, {"line": 204, "text": "\u274c WRONG - Blocks GUI thread"}, {"line": 209, "text": "\u2705 CORRECT - Async with worker pool"}, {"line": 222, "text": "File I/O Operations"}, {"line": 225, "text": "\u274c WRONG - Blocks GUI"}, {"line": 231, "text": "\u2705 CORRECT - Async file I/O"}, {"line": 245, "text": "Network Requests"}, {"line": 248, "text": "\u274c WRONG - Blocks during download"}, {"line": 253, "text": "\u2705 CORRECT - Async with progress"}, {"line": 264, "text": "Update progress bar"}, {"line": 270, "text": "Final update"}, {"line": 281, "text": "Anti-Patterns to Avoid"}, {"line": 283, "text": "\u274c Anti-Pattern 1: Direct UI Updates from Worker Thread"}, {"line": 286, "text": "WRONG - Tkinter is not thread-safe!"}, {"line": 296, "text": "\u274c Anti-Pattern 2: Blocking in Main Thread"}, {"line": 299, "text": "WRONG - Freezes GUI"}, {"line": 309, "text": "\u274c Anti-Pattern 3: Creating Threads Directly"}, {"line": 312, "text": "WRONG - Resource leak, no management"}, {"line": 321, "text": "\u274c Anti-Pattern 4: Missing Error Handling"}, {"line": 324, "text": "WRONG - Crashes silently"}, {"line": 334, "text": "\u274c Anti-Pattern 5: No Progress Feedback"}, {"line": 337, "text": "WRONG - User thinks app is frozen"}, {"line": 348, "text": "Best Practices"}, {"line": 350, "text": "1. Always Update UI from Main Thread"}, {"line": 353, "text": "Use .after(0, callback) for all widget updates"}, {"line": 359, "text": "2. Provide User Feedback"}, {"line": 362, "text": "Show loading state"}, {"line": 366, "text": "Update on completion"}, {"line": 371, "text": "3. Handle Errors Gracefully"}, {"line": 382, "text": "4. Prevent Duplicate Operations"}, {"line": 389, "text": "... submit work ..."}, {"line": 390, "text": "Set to False in completion callback"}, {"line": 393, "text": "5. Use Throttling for Frequent Operations"}, {"line": 396, "text": "Only refresh if >5 seconds since last refresh"}, {"line": 401, "text": "6. Clean Up Resources"}, {"line": 409, "text": "7. Leverage CLI Caching"}, {"line": 412, "text": "Caching is automatic for read-only CLI operations"}, {"line": 418, "text": "Examples"}, {"line": 420, "text": "Example 1: Complete Panel Refresh"}, {"line": 430, "text": "Build UI"}, {"line": 443, "text": "Throttle"}, {"line": 451, "text": "Prevent duplicates"}, {"line": 461, "text": "Blocking I/O operations"}, {"line": 465, "text": "Schedule UI update"}, {"line": 473, "text": "Schedule error handling"}, {"line": 480, "text": "Submit to worker pool"}, {"line": 489, "text": "Clear existing"}, {"line": 493, "text": "Add new items"}, {"line": 498, "text": "Example 2: User Action with Confirmation"}, {"line": 505, "text": "Confirmation dialog (main thread, blocking is OK)"}, {"line": 509, "text": "Show progress"}, {"line": 539, "text": "Troubleshooting"}, {"line": 541, "text": "Problem: GUI Freezes"}, {"line": 552, "text": "Check for blocking calls:"}, {"line": 555, "text": "Move to worker pool:"}, {"line": 559, "text": "Problem: UI Updates Don't Appear"}, {"line": 569, "text": "Always schedule UI updates:"}, {"line": 575, "text": "Problem: \"RuntimeError: main thread is not in main loop\""}, {"line": 583, "text": "Wrap all UI updates in .after():"}, {"line": 587, "text": "Problem: Worker Pool Exhaustion"}, {"line": 598, "text": "Increase worker pool size:"}, {"line": 601, "text": "Check pool size:"}, {"line": 604, "text": "Optimize long-running tasks:"}, {"line": 605, "text": "- Break into chunks"}, {"line": 606, "text": "- Add timeouts"}, {"line": 607, "text": "- Use async/await for I/O"}, {"line": 610, "text": "Problem: Memory Leaks"}, {"line": 621, "text": "Use weak references:"}, {"line": 625, "text": "Clear cache periodically:"}, {"line": 631, "text": "Cleanup on destroy:"}, {"line": 638, "text": "Performance Tuning"}, {"line": 640, "text": "Worker Pool Sizing"}, {"line": 643, "text": "For CPU-bound tasks:"}, {"line": 646, "text": "For I/O-bound tasks (default, recommended):"}, {"line": 649, "text": "For high-concurrency scenarios:"}, {"line": 653, "text": "Cache Configuration"}, {"line": 656, "text": "Adjust cache TTL in CliBridgeMixin.__init__():"}, {"line": 660, "text": "Throttling Intervals"}, {"line": 663, "text": "Adjust per-panel in refresh() method:"}, {"line": 670, "text": "Testing Async Code"}, {"line": 672, "text": "Manual Testing Checklist"}, {"line": 681, "text": "Automated Testing"}, {"line": 698, "text": "References"}, {"line": 707, "text": "Changelog"}, {"line": 709, "text": "November 2025"}]}, {"path": "guide/INDEX.MD", "content": "# AI-OS User Guide - Index\n**Last Updated**: October 18, 2025  \n**Purpose**: Primary navigation hub for all AI-OS documentation  \n**Audience**: Users, developers, and contributors\n\n---\n\n## \ud83d\udcd6 Welcome to the AI-OS Guide\n\nThis guide provides comprehensive documentation for AI-OS, including feature documentation, testing resources, bug tracking, and API references. Start here to navigate all available documentation.\n\nNote on scope: This guide indexes and verifies ONLY files within the `docs/guide/` folder (and its `api/` subfolder). References to other docs are provided for convenience but are out of scope for this guide\u2019s consistency checks.\n\n---\n\n## \ud83d\ude80 Quick Start\n\n### New Users\n1. **Start here**: [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md) - Learn what AI-OS can do\n2. **Then review**: [Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md) - Compatibility and caveats\n3. **API quick reference**: [Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md)\n\n### Developers\n1. **Feature planning**: [Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md)\n2. **API reference**: [Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md)\n3. **Advanced features**: [Advanced Training Features](features/ADVANCED_FEATURES.md)\n\n### Contributors\n1. **Run diagnostics (optional)**: See `scripts/README.md`\n2. **Open issues**: https://github.com/Wulfic/AI-OS/issues\n3. **Verify features**: [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)\n\n---\n\n## \ud83d\udcda Core Documentation\n\n### Navigation Hubs\nUse these to explore features, combinations, and references:\n- [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)\n- [Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md)\n- [Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md)\n\n---\n\n### [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)\n**Feature Documentation** | **1,402 lines** | **100+ features**\n\nComprehensive mapping of EVERY feature in AI-OS with implementation status.\n\n**Contents**:\n- \u2705 CLI Commands (15+)\n- \u2705 Core Training Features (10+)\n- \u2705 Memory Optimization (6 features)\n- \u2705 Model Architecture (6 components)\n- \u2705 Dataset System (5 features)\n- \u2705 Tokenizers (15 types)\n- \u2705 Dynamic Subbrains / MoE (7 components)\n- \u2705 GUI Features (8 panels)\n- \u26a0\ufe0f Multi-GPU & Distributed (needs verification)\n- \u2705 Tools & Integrations (10+)\n\n**Use this when**: You want to know what features exist and where they're implemented\n\n---\n\n### Issues and Bug Tracking\nUse GitHub Issues to report and track problems:\n- https://github.com/Wulfic/AI-OS/issues\n\n---\n\n### [Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md)\n**Compatibility Guide** | **719 lines** | **30+ combinations**\n\nDocuments which features work together and known incompatibilities.\n\n**Verified Combinations**:\n- \u2705 Gradient Checkpointing + AMP (~60-70% memory reduction)\n- \u2705 Gradient Checkpointing + AMP + 8-bit (~70-80% memory reduction)\n- \u2705 Streaming Dataset + Shuffling\n\n**Needs Testing**:\n- \u2753 DDP + anything (unverified)\n- \u2753 DeepSpeed + anything (unverified)\n- \u2753 Chunking + anything (unclear if implemented)\n\n**Use this when**: Planning training configurations or debugging feature interactions\n\n---\n\n### Advanced Features and Notes\nSee advanced flags, platform notes, and troubleshooting:\n- [Advanced Training Features](features/ADVANCED_FEATURES.md)\n\n---\n\n## \ud83d\udd0c API References\n\n### [Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md)\n**CLI Reference** | **363 lines**\n\nQuick reference for CLI training API changes and usage.\n\n**Contents**:\n- What works vs what's removed\n- CLI parameter reference\n- Python API examples\n- Migration guides\n\n**Use this when**: Writing training scripts or using the CLI\n\n---\n\n### [GUI Tooltips Quick Reference](api/TOOLTIPS_QUICK_REFERENCE.md)\n**GUI Documentation** | **174 lines**\n\nComplete reference for all GUI tooltips and interface elements.\n\n**Contents**:\n- Preset configurations\n- Brain naming conventions\n- Custom architecture fields\n- Resource settings\n- Training controls\n\n**Use this when**: Using the GUI or debugging UI issues\n\n---\n\n## \u2705 Verification checklist (maintainers)\n\nUse this quick checklist when adding or updating docs in this folder:\n- Each file begins with: Title, and \u201cLast Updated: <Month DD, YYYY>\u201d, \u201cPurpose\u201d, optional \u201cStatus\u201d\n- Links within `docs/guide/` use relative paths without `docs/` prefix\n- Links to parent docs (outside guide) use `../` correctly\n- Each core doc ends with a \u201cBack to Guide Index\u201d link\n- Dates reflect the latest meaningful change\n- Terminology: use consistent emoji status markers (\u2705 \u26a0\ufe0f \u274c \u2753) and section headers\n- If new API docs are added, update `docs/guide/api/README.md`\n\n### Link and anchor conventions (contributors)\n- Use kebab-case fragment IDs for custom anchors: `### Example Section {#example-section}`\n- Prefer relative links within this folder (no leading `docs/`), e.g., `[Core Training](features/CORE_TRAINING.md)`\n- When linking to feature docs from feature docs, use `./Other_Feature.md` for siblings and `../INDEX.MD` for guide root\n- End each feature doc with: \u201cBack to Feature Index\u201d and \u201cBack to Guide Index\u201d links\n- Keep file names as-is; normalize link text and ensure accurate casing in paths on case-sensitive systems\n\n## \ud83e\uddea Testing & Diagnostics\n\n### Diagnostics\nSee `scripts/README.md` for available diagnostic scripts and how to run them.\n\n---\n\n## \ud83d\udcca Project Health Metrics\n\n### Implementation Status\n- \u2705 **Fully Implemented**: 58% of features\n- \u26a0\ufe0f **Partially Implemented**: 23% of features\n- \u274c **Not Implemented**: 19% of features\n\n### Test Coverage\n- \u2705 **System Tests Passing**: 77.6% (52/67 tests)\n- \u26a0\ufe0f **Unit Tests**: Minimal coverage (needs expansion)\n- \u274c **Integration Tests**: Not implemented\n\n### Bug Status\n- \ud83d\udd34 **Critical Bugs**: 2 open\n- \ud83d\udfe0 **High Priority**: 9 open\n- \ud83d\udfe1 **Medium Priority**: 9 open\n- \ud83d\udfe2 **Low Priority**: 3 open\n- **Total**: 26 tracked bugs\n\n### Documentation Quality\n- \u2705 **Feature Documentation**: 100+ features documented\n- \u26a0\ufe0f **Accuracy**: Some false claims identified\n- \u2705 **Bug Tracking**: Comprehensive\n- \u2705 **Testing**: 67 automated tests\n\n---\n\n## \ud83c\udfaf Common Workflows\n\n### Testing Changes\n1. Run diagnostics: `python scripts/comprehensive_diagnostics.py`\n2. Review failures in `artifacts/diagnostics/diagnostics_latest.txt`\n3. Fix issues identified\n4. Re-run tests to verify\n5. Update documentation if behavior changed\n\n### Reporting Bugs\n1. Check if bug already tracked in GitHub Issues\n2. If new, add to appropriate severity section\n3. Include: Description, Impact, Reproduction, Workaround, Fix Required\n4. Update bug statistics table\n5. Add to action plan if high priority\n\n### Verifying Features\n1. Find feature in [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)\n2. Check implementation status and file location\n3. Look for related tests in diagnostic script\n4. Test manually if automated tests don't exist\n5. Update status (\u2705/\u26a0\ufe0f/\u274c) based on results\n6. Document findings\n\n### Planning Training\n1. Review available features in [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)\n2. Check compatibility in [Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md)\n3. Check for known issues in GitHub Issues\n4. Refer to [Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md)\n5. Start training with verified feature combinations\n\n---\n\n## \ud83d\udd0d Known Critical Issues\n\n### 1. FlashAttention availability and verification\n**Severity**: \ud83d\udd34 CRITICAL  \n**Details**: See [Advanced Training Features](features/ADVANCED_FEATURES.md) for platform notes\n\n- Documentation previously overstated FlashAttention as \"fully integrated\"\n- Current code path attempts FA2 under a user-controlled flag (`use_flash_attn`) and falls back to PyTorch SDPA if unavailable\n- Availability depends on GPU and whether `flash_attn` is installed\n**Impact**: Environment-dependent behavior; performance claims must be qualified\n\n**Action**: Verify on supported hardware, document behavior and fallback clearly\n\n**Action**: Update documentation to remove false claims\n\n---\n\n### 2. Many Features Unverified\n**Severity**: \ud83d\udfe0 HIGH  \n**Details**: Refer to feature docs and open issues for verification notes\n\n**Unverified Features**:\n- DDP (Multi-GPU) - config exists but not tested\n- DeepSpeed ZeRO - config exists but not tested\n- Chunked training - unclear if implemented\n- Most tokenizers - only GPT-2 verified\n- Feature combinations - most untested\n\n**Impact**: Unknown if features work as claimed\n\n**Action**: Systematic testing of all features (2-3 week effort)\n\n---\n\n### 3. Limited Test Coverage\n**Severity**: \ud83d\udfe0 HIGH  \n**Details**: See repository README and CI for coverage and health\n\n- Only 3 test files found in entire project\n- 67 system-level diagnostic tests (77.6% passing)\n- No unit tests for core functionality\n- No integration tests for feature combinations\n- No performance benchmarks\n\n**Impact**: Can't verify changes don't break things\n\n**Action**: Add comprehensive test suite (3-4 week effort)\n\n---\n\n## \ud83d\udee0\ufe0f Contributing\n\n### Before Making Changes\n1. \u2705 Run comprehensive diagnostics\n2. \u2705 Review related features in Complete Feature Index\n3. \u2705 Check for related bugs in Bug Tracker\n4. \u2705 Verify feature combinations won't break\n\n### After Making Changes\n1. \u2705 Run comprehensive diagnostics again\n2. \u2705 Update feature status if behavior changed\n3. \u2705 Add/update bug tracker entries if issues found\n4. \u2705 Update feature combination matrix if compatibility changed\n5. \u2705 Update this guide if new features added\n\n---\n\n## \ud83d\udcd6 Additional Documentation\n\n### In Other Locations\nThese documents are NOT in the guide folder but may be referenced:\n\n- **Main README**: `../../README.md` (project root)\n- **Quick Start**: `../QUICK_START.md`\n- **Docs Index**: `../INDEX.md`\n- **Tests**: `../../tests/` folder\n\n---\n\n## \ud83c\udd98 Getting Help\n\n### For Users\n- Check [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md) for feature documentation\n- Refer to [GUI Tooltips Quick Reference](api/TOOLTIPS_QUICK_REFERENCE.md) for UI help\n\n### For Developers\n- Use [Advanced Training Features](features/ADVANCED_FEATURES.md) for platform notes\n- Reference [Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md) for API usage\n\n### For Contributors\n- See diagnostics in `scripts/README.md`\n- Review feature status: [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)\n- Verify combinations: [Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md)\n\n---\n\n## \ud83d\udcdd Document Status\n\n| Document | Purpose | Last Updated |\n|----------|---------|--------------|\n| INDEX.MD (this file) | Guide navigation hub | Oct 18, 2025 |\n| features/COMPLETE_FEATURE_INDEX.md | Feature docs index | Oct 18, 2025 |\n| features/FEATURE_COMBINATION_MATRIX.md | Compatibility guide | Oct 18, 2025 |\n| features/ADVANCED_FEATURES.md | Advanced flags and notes | Oct 18, 2025 |\n| api/TRAINING_API_QUICK_REFERENCE.md | CLI quick reference | Oct 18, 2025 |\n| api/TOOLTIPS_QUICK_REFERENCE.md | GUI tooltips reference | Oct 18, 2025 |\n\n---\n\n## \ud83c\udf93 Learning Path\n\n### Beginner: Just Starting with AI-OS\n1. Review [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md) basics\n2. Check [GUI Tooltips Quick Reference](api/TOOLTIPS_QUICK_REFERENCE.md) for UI\n\n### Intermediate: Using AI-OS for Training\n1. Start with [Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md)\n2. Plan configurations using [Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md)\n3. Reference [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md) for advanced features\n\n### Advanced: Contributing to AI-OS\n1. See diagnostics in `scripts/README.md`\n2. Reference feature docs and API as needed\n3. Keep documentation and links accurate\n\n---\n\n## \ud83d\udcde Support\n\nFor questions, issues, or contributions:\n- **Issues**: https://github.com/Wulfic/AI-OS/issues\n- **Features**: Review [Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)\n- **Documentation**: Use this Guide Index and feature docs\n\n---\n\n**Last Updated**: January 31, 2025  \n**Next Review**: February 28, 2025  \n**Maintainer**: AI-OS Documentation Team\n", "tags": ["cli", "datasets", "gui", "training"], "headings": [{"line": 0, "text": "AI-OS User Guide - Index"}, {"line": 7, "text": "\ud83d\udcd6 Welcome to the AI-OS Guide"}, {"line": 15, "text": "\ud83d\ude80 Quick Start"}, {"line": 17, "text": "New Users"}, {"line": 22, "text": "Developers"}, {"line": 27, "text": "Contributors"}, {"line": 34, "text": "\ud83d\udcda Core Documentation"}, {"line": 36, "text": "Navigation Hubs"}, {"line": 44, "text": "[Complete Feature Index](features/COMPLETE_FEATURE_INDEX.md)"}, {"line": 65, "text": "Issues and Bug Tracking"}, {"line": 71, "text": "[Feature Combination Matrix](features/FEATURE_COMBINATION_MATRIX.md)"}, {"line": 90, "text": "Advanced Features and Notes"}, {"line": 96, "text": "\ud83d\udd0c API References"}, {"line": 98, "text": "[Training API Quick Reference](api/TRAINING_API_QUICK_REFERENCE.md)"}, {"line": 113, "text": "[GUI Tooltips Quick Reference](api/TOOLTIPS_QUICK_REFERENCE.md)"}, {"line": 129, "text": "\u2705 Verification checklist (maintainers)"}, {"line": 140, "text": "Link and anchor conventions (contributors)"}, {"line": 147, "text": "\ud83e\uddea Testing & Diagnostics"}, {"line": 149, "text": "Diagnostics"}, {"line": 154, "text": "\ud83d\udcca Project Health Metrics"}, {"line": 156, "text": "Implementation Status"}, {"line": 161, "text": "Test Coverage"}, {"line": 166, "text": "Bug Status"}, {"line": 173, "text": "Documentation Quality"}, {"line": 181, "text": "\ud83c\udfaf Common Workflows"}, {"line": 183, "text": "Testing Changes"}, {"line": 190, "text": "Reporting Bugs"}, {"line": 197, "text": "Verifying Features"}, {"line": 205, "text": "Planning Training"}, {"line": 214, "text": "\ud83d\udd0d Known Critical Issues"}, {"line": 216, "text": "1. FlashAttention availability and verification"}, {"line": 231, "text": "2. Many Features Unverified"}, {"line": 248, "text": "3. Limited Test Coverage"}, {"line": 264, "text": "\ud83d\udee0\ufe0f Contributing"}, {"line": 266, "text": "Before Making Changes"}, {"line": 272, "text": "After Making Changes"}, {"line": 281, "text": "\ud83d\udcd6 Additional Documentation"}, {"line": 283, "text": "In Other Locations"}, {"line": 293, "text": "\ud83c\udd98 Getting Help"}, {"line": 295, "text": "For Users"}, {"line": 299, "text": "For Developers"}, {"line": 303, "text": "For Contributors"}, {"line": 310, "text": "\ud83d\udcdd Document Status"}, {"line": 323, "text": "\ud83c\udf93 Learning Path"}, {"line": 325, "text": "Beginner: Just Starting with AI-OS"}, {"line": 329, "text": "Intermediate: Using AI-OS for Training"}, {"line": 334, "text": "Advanced: Contributing to AI-OS"}, {"line": 341, "text": "\ud83d\udcde Support"}]}, {"path": "guide/QUICK_START.md", "content": "# Quick Start\n\nThis single page gets you from zero to the GUI.\n\n## Windows (PowerShell)\n\n```powershell\n# From repo root\n./scripts/install_aios_on_windows.ps1 -Action install -Yes\naios gui\n```\n\nDeveloper setup:\n\n```powershell\npython -m venv .venv\n. .\\.venv\\Scripts\\Activate.ps1\npip install -e .\naios gui\n```\n\n## Ubuntu\n\n```bash\n./scripts/install_aios_on_ubuntu.sh install --yes\naios gui\n```\n\n## CLI training (example)\n\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --steps 1000\n```\n\nTrouble? See docs/README.md for guides and maintenance notes.\n", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Quick Start"}, {"line": 4, "text": "Windows (PowerShell)"}, {"line": 7, "text": "From repo root"}, {"line": 21, "text": "Ubuntu"}, {"line": 28, "text": "CLI training (example)"}]}, {"path": "guide/api/README.md", "content": "# API References - Guide\n**Last Updated**: October 18, 2025  \n**Purpose**: Index of API reference documents within `docs/guide/api/`\n\n---\n\n## Available References\n\n- [TRAINING_API_QUICK_REFERENCE.md](TRAINING_API_QUICK_REFERENCE.md)\n  - CLI training usage, parameters, examples, and migration notes.\n\n- [TOOLTIPS_QUICK_REFERENCE.md](TOOLTIPS_QUICK_REFERENCE.md)\n  - GUI tooltips and control descriptions for the training interface.\n\n---\n\nBack to [Guide Index](../INDEX.MD)\n", "tags": ["cli", "gui", "training"], "headings": [{"line": 0, "text": "API References - Guide"}, {"line": 6, "text": "Available References"}]}, {"path": "guide/api/TOOLTIPS_QUICK_REFERENCE.md", "content": "# Quick Reference: All Tooltip Texts\n\n## Preset Section\n\n### Preset Header\n```\nQuick architecture presets with pre-configured parameters.\nSelect Custom for full control over all parameters.\n```\n\n### 1M Preset\n```\n~1M parameters: Tiny model (hidden=256, 2+2 layers)\nFast training, minimal VRAM (~0.5 GB)\n```\n\n### 5M Preset\n```\n~5M parameters: Small model (hidden=512, 2+2 layers)\nGood for testing and quick experiments (~1.5 GB)\n```\n\n### 10M Preset\n```\n~10M parameters: Medium model (hidden=768, 2+2 layers)\nBalanced size/performance (~2.5 GB)\n```\n\n### 20M Preset\n```\n~20M parameters: Large model (hidden=1024, 2+2 layers)\nGood quality, moderate VRAM (~4 GB)\n```\n\n### 50M Preset\n```\n~50M parameters: Very large (hidden=1536, 2+2 layers)\nHigh quality, needs more VRAM (~7 GB)\n```\n\n### Custom Preset\n```\nCustom architecture: Configure all parameters manually.\nReveals advanced options for hidden size, layers, heads, etc.\n```\n\n## Brain Name Field\n```\nUnique name for this brain/model.\nWill be saved to: artifacts/brains/actv1/{name}/\nUse descriptive names like: large_context_v1, fast_inference, etc.\n```\n\n## Custom Architecture Fields\n\n### Hidden Size\n```\nModel width / embedding dimension.\nLarger = more expressive but more VRAM.\nMust be divisible by num_heads.\nExamples: 256, 512, 768, 1024, 1536, 2048\n```\n\n### H Layers\n```\nNumber of Hierarchical reasoning layers.\nHigher-level abstract processing.\nMore layers = deeper reasoning but slower.\nTypical: 2-8 layers\n```\n\n### L Layers\n```\nNumber of Local processing layers.\nLower-level detail processing.\nMore layers = better detail but slower.\nTypical: 2-8 layers\n```\n\n### Num Heads\n```\nNumber of attention heads per layer.\nMore heads = more parallel attention patterns.\nMust evenly divide hidden_size.\nExamples: 4, 8, 12, 16, 24, 32\n```\n\n### Expansion\n```\nFeed-forward network expansion factor.\nFFN size = hidden_size \u00d7 expansion.\nHigher = more capacity but more VRAM.\nTypical: 2.0-4.0\n```\n\n### H Cycles\n```\nNumber of processing cycles per H layer.\nMore cycles = more refinement per layer.\nTypical: 1-3 cycles\n```\n\n### L Cycles\n```\nNumber of processing cycles per L layer.\nMore cycles = more refinement per layer.\nTypical: 1-3 cycles\n```\n\n### Position Encoding\n```\nPosition encoding method:\n\n\u2022 rope (Rotary): Best for long contexts,\n  relative positions, no learned params.\n  RECOMMENDED for most use cases.\n\n\u2022 learned: Absolute positions,\n  trained embeddings, fixed max length.\n\n\u2022 sincos: Classic Transformer approach,\n  no learned params, absolute positions.\n```\n\n## Visual Map\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        Create New HRM Student                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                      \u2502\n\u2502  Choose architecture preset: \u2190 \"Quick presets...\"   \u2502\n\u2502  \u25cb 1M        \u2190 \"~1M params: Tiny model...\"          \u2502\n\u2502  \u25cb 5M        \u2190 \"~5M params: Small model...\"         \u2502\n\u2502  \u25cb 10M       \u2190 \"~10M params: Medium model...\"       \u2502\n\u2502  \u25cb 20M       \u2190 \"~20M params: Large model...\"        \u2502\n\u2502  \u25cb 50M       \u2190 \"~50M params: Very large...\"         \u2502\n\u2502  \u25cf Custom    \u2190 \"Custom architecture: Configure...\"   \u2502\n\u2502                                                      \u2502\n\u2502  Brain name: [new_brain] \u2190 \"Unique name...\"         \u2502\n\u2502                                                      \u2502\n\u2502  \u250c\u2500 Custom Architecture \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  Hidden size:  [512] \u2190 \"Model width...\"       \u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  H layers:     [2]   \u2190 \"Hierarchical...\"      \u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  L layers:     [2]   \u2190 \"Local processing...\"  \u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  Num heads:    [8]   \u2190 \"Attention heads...\"   \u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  Expansion:    [2.0] \u2190 \"FFN expansion...\"     \u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  H cycles:     [2]   \u2190 \"Processing cycles...\" \u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  L cycles:     [2]   \u2190 \"Processing cycles...\" \u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  Pos encoding: [rope\u25bc] \u2190 \"rope/learned/sincos\"\u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502  Note: DeepSpeed ZeRO can be selected in      \u2502  \u2502\n\u2502  \u2502        the main training panel                 \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                      \u2502\n\u2502  [Create]  [Cancel]                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Hover Behavior\n- Tooltips appear after **0.5 second** hover delay\n- Tooltips stay visible while hovering\n- Tooltips disappear when mouse moves away\n- Multi-line tooltips are properly formatted\n- All interactive elements have tooltips\n", "tags": ["gui", "hrm", "training"], "headings": [{"line": 0, "text": "Quick Reference: All Tooltip Texts"}, {"line": 2, "text": "Preset Section"}, {"line": 4, "text": "Preset Header"}, {"line": 10, "text": "1M Preset"}, {"line": 16, "text": "5M Preset"}, {"line": 22, "text": "10M Preset"}, {"line": 28, "text": "20M Preset"}, {"line": 34, "text": "50M Preset"}, {"line": 40, "text": "Custom Preset"}, {"line": 46, "text": "Brain Name Field"}, {"line": 53, "text": "Custom Architecture Fields"}, {"line": 55, "text": "Hidden Size"}, {"line": 63, "text": "H Layers"}, {"line": 71, "text": "L Layers"}, {"line": 79, "text": "Num Heads"}, {"line": 87, "text": "Expansion"}, {"line": 95, "text": "H Cycles"}, {"line": 102, "text": "L Cycles"}, {"line": 109, "text": "Position Encoding"}, {"line": 124, "text": "Visual Map"}, {"line": 167, "text": "Hover Behavior"}]}, {"path": "guide/api/TRAINING_API_QUICK_REFERENCE.md", "content": "# Quick Reference: New Training API\n\n## For CLI Users\n\n### \u2705 What Still Works (No Changes)\n\nAll standard training parameters work exactly as before:\n\n```bash\n# Basic training\naios hrm-hf train-actv1 \\\n    --model artifacts/hf_implant/base_model \\\n    --dataset-file training_data/my_data.txt \\\n    --max-seq-len 128 \\\n    --batch-size 8 \\\n    --steps 200\n\n# With optimizations\naios hrm-hf train-actv1 \\\n    --model base_model \\\n    --dataset-file data.txt \\\n    --optimize \\\n    --gradient-checkpointing \\\n    --amp \\\n    --zero-stage zero2\n\n# Multi-GPU DDP\naios hrm-hf train-actv1 \\\n    --model base_model \\\n    --dataset-file data.txt \\\n    --ddp \\\n    --cuda-ids 0,1 \\\n    --world-size 2\n```\n\n\n---\n\n## For Python API Users\n\n```python\nfrom aios.core.hrm_training import TrainingConfig\nfrom aios.cli.hrm_hf.train_actv1_impl import train_actv1_impl\n\n# Create config object\nconfig = TrainingConfig(\n    model=\"artifacts/hf_implant/base_model\",\n    dataset_file=\"training_data/my_data.txt\",\n    max_seq_len=128,\n    batch_size=8,\n    steps=200,\n    lr=2e-4,\n    device=\"auto\",\n    # ... other parameters ...\n)\n\n# Validate (optional but recommended)\nconfig.validate()\n\n# Train\ntrain_actv1_impl(config=config)\n```\n\n**Error you'll see**:\n```\nTypeError: train_actv1_impl() missing 1 required positional argument: 'config'\n```\n\n---\n\n## Common Migration Examples\n\n### Example 1: Basic Training Script\n\n**Before**:\n```python\nfrom aios.cli.hrm_hf.train_actv1_impl import train_actv1_impl\n\ntrain_actv1_impl(\n    model=\"base_model\",\n    dataset_file=\"data.txt\",\n    max_seq_len=256,\n    batch_size=16,\n    steps=500,\n)\n```\n\n**After**:\n```python\nfrom aios.core.hrm_training import TrainingConfig\nfrom aios.cli.hrm_hf.train_actv1_impl import train_actv1_impl\n\nconfig = TrainingConfig(\n    model=\"base_model\",\n    dataset_file=\"data.txt\",\n    max_seq_len=256,\n    batch_size=16,\n    steps=500,\n)\ntrain_actv1_impl(config=config)\n```\n\n### Example 2: Programmatic Configuration\n\n**Before**:\n```python\n# Build kwargs dynamically\nkwargs = {\n    \"model\": \"base_model\",\n    \"dataset_file\": \"data.txt\",\n}\n\nif use_gpu:\n    kwargs[\"device\"] = \"cuda\"\n    kwargs[\"ddp\"] = True\n\ntrain_actv1_impl(**kwargs)\n```\n\n**After**:\n```python\n# Build config dynamically (better type safety!)\nconfig = TrainingConfig(\n    model=\"base_model\",\n    dataset_file=\"data.txt\",\n)\n\nif use_gpu:\n    config.device = \"cuda\"\n    config.ddp = True\n\ntrain_actv1_impl(config=config)\n```\n\n### Example 3: Config Persistence\n\n**New feature**: Save and load configurations!\n\n```python\nfrom aios.core.hrm_training import TrainingConfig\n\n# Create config\nconfig = TrainingConfig(\n    model=\"base_model\",\n    dataset_file=\"data.txt\",\n    max_seq_len=1024,\n    batch_size=4,\n    steps=1000,\n    optimize=True,\n    zero_stage=\"zero2\",\n)\n\n# Save to JSON\nimport json\nwith open(\"my_training_config.json\", \"w\") as f:\n    json.dump(config.to_dict(), f, indent=2)\n\n# Load later\nwith open(\"my_training_config.json\", \"r\") as f:\n    config_dict = json.load(f)\n\nconfig = TrainingConfig.from_dict(config_dict)\ntrain_actv1_impl(config=config)\n```\n\n### Example 4: Config Validation\n\n```python\nfrom aios.core.hrm_training import TrainingConfig\n\n# Create invalid config\nconfig = TrainingConfig(\n    model=\"base_model\",\n    dataset_file=None,  # \u274c Required!\n    max_seq_len=128,\n)\n\ntry:\n    config.validate()\nexcept ValueError as e:\n    print(f\"Config error: {e}\")\n    # Output: \"Config error: dataset_file is required for training\"\n```\n\n---\n\n## TrainingConfig Parameters\n\nAll parameters available in TrainingConfig:\n\n### Core Training:\n- `model` (str) - HF model name or path\n- `dataset_file` (str) - Training data file\n- `max_seq_len` (int) - Sequence length, default: 128\n- `batch_size` (int) - Batch size, default: 8\n- `steps` (int) - Training steps, default: 200\n- `lr` (float) - Learning rate, default: 2e-4\n- `device` (str) - Device: auto|cpu|cuda|xpu|mps|dml, default: \"auto\"\n- `halt_max_steps` (int) - Max ACT segments, default: 2\n- `save_dir` (str) - Output directory, default: \"training_data/actv1\"\n\n### Data Processing:\n- `ascii_only` (bool) - Filter to ASCII-only lines, default: False\n- `eval_file` (str|None) - Held-out eval file, default: None\n- `eval_batches` (int) - Eval batches, default: 10\n- `sys_mem_cap_pct` (int|None) - Memory cap %, default: None\n\n### Training Control:\n- `stop_file` (str|None) - Stop signal file, default: None\n- `log_file` (str|None) - Metrics log file, default: None\n- `student_init` (str|None) - Resume from checkpoint, default: None\n- `iterate` (bool) - Loop training indefinitely, default: False\n\n### Output Bundle:\n- `brain_name` (str|None) - Brain bundle name, default: None\n- `bundle_dir` (str) - Bundle directory, default: \"artifacts/brains/actv1\"\n\n### Model Architecture:\n- `h_layers` (int) - High-level layers, default: 2\n- `l_layers` (int) - Low-level layers, default: 2\n- `hidden_size` (int) - Hidden dimension, default: 512\n- `expansion` (float) - FFN expansion, default: 2.0\n- `num_heads` (int) - Attention heads, default: 8\n- `h_cycles` (int) - High-level cycles, default: 2\n- `l_cycles` (int) - Low-level cycles, default: 2\n- `pos_encodings` (str) - Position encoding: rope|alibi|none, default: \"rope\"\n\n### Optimization:\n- `optimize` (bool) - Auto-optimize for VRAM, default: False\n- `gradient_checkpointing` (bool) - Enable grad checkpointing, default: True\n- `use_amp` (bool) - Use mixed precision, default: True\n- `use_cpu_offload` (bool) - Offload to CPU, default: False\n- `zero_stage` (str) - DeepSpeed ZeRO: none|zero1|zero2|zero3, default: \"none\"\n\n### Multi-GPU:\n- `cuda_ids` (str|None) - CUDA device IDs (e.g., \"0,1\"), default: None\n- `ddp` (bool) - Enable DDP, default: False\n- `world_size` (int|None) - Number of GPUs, default: None\n- `strict` (bool) - No device fallbacks, default: False\n\n### Deprecated (Still in Config for Compatibility):\n- `teacher` (str|None) - Not used, default: None\n- `teacher_device` (str) - Not used, default: \"cuda\"\n- `kl` (float) - Not used, default: 0.0\n- `kl_temp` (float) - Not used, default: 1.0\n\n---\n\n## Need Help?\n\n### Check Configuration:\n```python\nfrom aios.core.hrm_training import TrainingConfig\n\nconfig = TrainingConfig(model=\"test\", dataset_file=\"data.txt\")\nprint(config)  # Shows formatted summary\n```\n\n### Validate Before Training:\n```python\ntry:\n    config.validate()\n    print(\"\u2713 Config is valid!\")\nexcept ValueError as e:\n    print(f\"\u2717 Config error: {e}\")\n```\n\n### Get CLI Args:\n```python\ncli_args = config.to_cli_args()\nprint(\" \".join(cli_args))  # See what CLI command this config represents\n```\n\n---\n\n**Questions?** Check the full docs:\n- `docs/REFACTORING_PHASE2_COMPLETE.md` - Phase 2 details\n- `docs/DEPRECATION_CLEANUP_COMPLETE.md` - What was removed\n- `src/aios/core/hrm_training/training_config.py` - Source code\n", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Quick Reference: New Training API"}, {"line": 2, "text": "For CLI Users"}, {"line": 4, "text": "\u2705 What Still Works (No Changes)"}, {"line": 9, "text": "Basic training"}, {"line": 17, "text": "With optimizations"}, {"line": 26, "text": "Multi-GPU DDP"}, {"line": 38, "text": "For Python API Users"}, {"line": 44, "text": "Create config object"}, {"line": 53, "text": "... other parameters ..."}, {"line": 56, "text": "Validate (optional but recommended)"}, {"line": 59, "text": "Train"}, {"line": 70, "text": "Common Migration Examples"}, {"line": 72, "text": "Example 1: Basic Training Script"}, {"line": 102, "text": "Example 2: Programmatic Configuration"}, {"line": 106, "text": "Build kwargs dynamically"}, {"line": 121, "text": "Build config dynamically (better type safety!)"}, {"line": 134, "text": "Example 3: Config Persistence"}, {"line": 141, "text": "Create config"}, {"line": 152, "text": "Save to JSON"}, {"line": 157, "text": "Load later"}, {"line": 165, "text": "Example 4: Config Validation"}, {"line": 170, "text": "Create invalid config"}, {"line": 181, "text": "Output: \"Config error: dataset_file is required for training\""}, {"line": 186, "text": "TrainingConfig Parameters"}, {"line": 190, "text": "Core Training:"}, {"line": 201, "text": "Data Processing:"}, {"line": 207, "text": "Training Control:"}, {"line": 213, "text": "Output Bundle:"}, {"line": 217, "text": "Model Architecture:"}, {"line": 227, "text": "Optimization:"}, {"line": 234, "text": "Multi-GPU:"}, {"line": 240, "text": "Deprecated (Still in Config for Compatibility):"}, {"line": 248, "text": "Need Help?"}, {"line": 250, "text": "Check Configuration:"}, {"line": 258, "text": "Validate Before Training:"}, {"line": 267, "text": "Get CLI Args:"}]}, {"path": "guide/features/ADVANCED_FEATURES.md", "content": "# Advanced Training Features \u2014 AI-OS\nLast Updated: October 20, 2025\nPurpose: Practical guide to advanced HRM-HF training flags, best\u2011known\u2011good combinations, platform notes, and troubleshooting.\nStatus: Implemented\n\n## Overview\nThis guide consolidates all advanced knobs for the HRM-HF trainer (`aios hrm-hf train-actv1`) into one place. It groups options by function (attention/positional, memory & performance, precision/quantization, dataset streaming & chunking, distributed, MoE, PEFT, hot\u2011reload inference) and calls out compatibility constraints on Windows vs Linux.\n\nSource of truth (CLI): `src/aios/cli/hrm_hf_cli.py` \u2192 command `train-actv1`\nConfiguration model: `src/aios/core/hrm_training/training_config.py`\n\nSee also:\n- Memory & VRAM: [MEMORY_OPTIMIZATION.md](./MEMORY_OPTIMIZATION.md)\n- FlashAttention: [FLASH_ATTENTION.md](./FLASH_ATTENTION.md) and [FLASH_ATTENTION_VS_CHUNKING.md](./FLASH_ATTENTION_VS_CHUNKING.md)\n- Multi\u2011GPU & streaming: [MULTI_GPU_DISTRIBUTED.md](./MULTI_GPU_DISTRIBUTED.md), [PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md](./PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md)\n- MoE: [DYNAMIC_SUBBRAINS_MOE.md](./DYNAMIC_SUBBRAINS_MOE.md)\n- PEFT/LoRA: [LORA_PEFT.md](./LORA_PEFT.md)\n- Core training entry: [CORE_TRAINING.md](./CORE_TRAINING.md)\n\n## Prerequisites\n- Windows PowerShell (pwsh), repo venv activated\n- GPU and drivers installed. CUDA recommended on NVIDIA. DirectML supported for inference; training support varies.\n- For 8\u2011bit optimizer and INT8/INT4 quantization: `bitsandbytes` with CUDA GPU. Windows support is limited\u2014prefer Linux for 8/4\u2011bit.\n- For FlashAttention 2: Ampere+ GPU; dedicated FA2 build required on most setups. Not typically available on Windows\u2014falls back to SDPA.\n- For DeepSpeed ZeRO: DeepSpeed installation (Linux recommended). ZeRO often not supported or unstable on Windows.\n\n## Commands (CLI syntax)\n\n### a) Direct CLI\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 200 --batch-size 8\n```\n\n### b) Module invocation (exact venv)\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 200 --batch-size 8\n```\n\n## Key option groups (with notes)\n\n### Attention & positional\n- `--use-flash-attn/--no-flash-attn`: Enable FlashAttention 2. Requires Ampere+ and proper install. On Windows, usually unavailable; will fall back to SDPA.\n- `--window-size <int|None>`: Sliding\u2011window attention. Use 256\u2013512 for extreme contexts (50\u2013100k tokens) to reduce memory. Works with or without FlashAttention.\n- `--pos-encodings rope|alibi|none`: Choose positional encoding. Default: rope.\n\n### Memory & performance\n- `--gradient-checkpointing/--no-gradient-checkpointing`: \u2193VRAM by ~30\u201350% at ~20% speed cost. Default: enabled.\n- `--amp/--no-amp`: Mixed precision activations (FP16/BF16). Big savings with minimal quality impact. Default: enabled.\n- `--cpu-offload/--no-cpu-offload`: Offload carry states to CPU between chunks for ultra\u2011long contexts (>500k). Slower; requires sufficient system RAM.\n- `--use-8bit-optimizer`: Use bitsandbytes 8\u2011bit optimizer (~75% optimizer memory reduction). Requires CUDA + bitsandbytes.\n- `--dataset-chunk-size <int>`: Samples per training cycle in iterate mode. Smaller uses less memory, larger is faster.\n\n### Precision & quantization\n- `--model-dtype fp32|fp16|bf16`: Weight precision when loading full\u2011precision models. Separate from AMP.\n- `--load-in-8bit`: INT8 weight loading (75% memory reduction). Requires bitsandbytes + CUDA.\n- `--load-in-4bit`: INT4 (QLoRA\u2011style) loading (\u224887.5% memory reduction). Strongly pair with PEFT.\nNotes:\n- When `--load-in-8bit` or `--load-in-4bit` is set, the base weights load quantized; AMP still controls activation precision. On Windows, bitsandbytes support is limited\u2014prefer Linux.\n\n### Dataset streaming & chunked training\n- `--use-chunked-training`: Split sequences into chunks to fit memory for long contexts.\n- `--chunk-size <tokens>`: 1024\u20134096 typical. Smaller = less VRAM, slower.\n- `--linear-dataset/--no-linear-dataset`: Linear order (default) enables progress tracking and resume.\n- `--dataset-start-offset <int>`: Resume index for linear mode.\n- `--iterate`: Repeat generate\u2192train cycles until stopped.\n\n### Distributed & multi\u2011GPU\n- `--ddp`: Enable torch.distributed (CUDA only). Best on Linux.\n- `--world-size <int>`: Number of processes/GPUs for DDP.\n- `--cuda-ids \"0,1\"`: Pin devices explicitly.\n- `--parallel-independent`: Windows\u2011friendly multi\u2011GPU alternative. Trains separate data blocks on different GPUs sequentially, then merges checkpoints. Bypasses DDP.\n- `--zero-stage none|zero1|zero2|zero3`: DeepSpeed ZeRO. Requires DeepSpeed; Linux recommended.\n- `--strict`: Disallow device fallbacks; fail fast on mismatches.\n\n### MoE (Mixture of Experts)\n- `--use-moe/--no-moe` (default: enabled)\n- `--num-experts <int>`: Total experts (capacity vs VRAM trade\u2011off).\n- `--num-experts-per-tok <int>`: Top\u2011k experts per token; lower = faster/less memory.\n- `--moe-capacity-factor <float>`: Load\u2011balancing headroom.\n- `--auto-adjust-moe-lr/--no-auto-adjust-moe-lr`: Auto reduce LR for MoE stability.\nTips: Start with 8 experts, top\u2011k=2, capacity 1.25; raise gradually.\n\n### PEFT (parameter\u2011efficient fine\u2011tuning)\n- `--use-peft/--no-peft` and `--peft-method lora|adalora|ia3|loha|lokr`\n- `--lora-r`, `--lora-alpha`, `--lora-dropout`, `--lora-target-modules \"q_proj,v_proj\"`\nBest practice: With `--load-in-4bit`, enable PEFT and tune adapters only to keep memory low.\n\n### Inference hot\u2011reload during training\n- `--inference-device cuda:N`: Use a dedicated GPU for inference while training on another.\n- `--hot-reload-steps <int>`: Frequency to reload inference model from checkpoints.\n\n### Auto optimization\n- `--optimize`: Auto\u2011find a stable combination for context length (up to ~100k) and batch size based on VRAM. May override `--max-seq-len` and `--batch-size`.\n\n## Try it: minimal safe examples\n\n### 1) Quick dry\u2011run (single GPU)\nRuns 1 step with tiny batch to validate pipeline and logging.\n```powershell\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t--steps 1 --batch-size 2 `\n\t--halt-max-steps 1 `\n\t--eval-batches 1 `\n\t--log-file artifacts/brains/actv1/metrics.jsonl\n```\nVS Code Task: Run \u201cRun brief HRM CLI dry-run\u201d.\n\nExpected outputs:\n- Metrics log appended at `artifacts/brains/actv1/metrics.jsonl`\n- Checkpoints under `training_data/actv1` (default `--save-dir`)\n\n### 2) Windows multi\u2011GPU without DDP\nUse parallel\u2011independent with chunked training to reduce VRAM pressure.\n```powershell\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t--parallel-independent `\n\t--use-chunked-training --chunk-size 2048 `\n\t--amp --gradient-checkpointing `\n\t--steps 200 --batch-size 4\n```\n\n### 3) QLoRA\u2011style PEFT on a single GPU (very low VRAM)\n```powershell\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t--load-in-4bit --use-peft --peft-method lora `\n\t--lora-r 16 --lora-alpha 32 --lora-dropout 0.05 `\n\t--lora-target-modules \"q_proj,v_proj\" `\n\t--amp --gradient-checkpointing `\n\t--steps 200 --batch-size 8\n```\nNote: Requires bitsandbytes + CUDA; prefer Linux.\n\n### 4) MoE enabled with conservative routing\n```powershell\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t--use-moe --num-experts 8 --num-experts-per-tok 2 `\n\t--moe-capacity-factor 1.25 --auto-adjust-moe-lr `\n\t--amp --gradient-checkpointing `\n\t--steps 200 --batch-size 8\n```\n\n## Compatibility, constraints, and tips\n\n- FlashAttention 2:\n\t- Works best on Linux with an Ampere+ GPU and proper FA2 install.\n\t- On Windows, expect fallback to SDPA; performance gain may be limited.\n- DeepSpeed ZeRO:\n\t- Requires DeepSpeed; Linux recommended. ZeRO not typically supported on Windows.\n- DDP:\n\t- Best on Linux. On Windows, prefer `--parallel-independent`.\n- Quantization:\n\t- `--load-in-4bit` pairs best with `--use-peft`. Keep base LM frozen; train adapters.\n\t- If bitsandbytes isn\u2019t available, omit `--load-in-8bit/--load-in-4bit` and consider `--use-8bit-optimizer` only.\n- Heads and shapes:\n\t- Ensure `--num-heads` divides `--hidden-size`. The validator will error otherwise.\n- Chunked training:\n\t- For very long contexts, combine `--use-chunked-training`, `--chunk-size 1024\u20132048`, `--gradient-checkpointing`, and optionally `--cpu-offload`.\n- Logging & resume:\n\t- Use `--log-file` for JSONL metrics; pair with `--linear-dataset` and `--dataset-start-offset` to resume deterministically.\n\n## Troubleshooting\n\n- \u201cConfiguration error \u2026\u201d on launch:\n\t- Check incompatible shapes (e.g., `num_heads` must divide `hidden_size`).\n\t- Remove `--use-flash-attn` if FA2 isn\u2019t installed; it will fall back, but explicit removal helps isolate issues.\n\t- If using ZeRO on Windows, remove `--zero-stage`.\n- \u201cbitsandbytes not found\u201d or CUDA errors:\n\t- Remove `--load-in-8bit/--load-in-4bit` and/or `--use-8bit-optimizer`, or switch to Linux with CUDA.\n- DDP hang on Windows:\n\t- Switch to `--parallel-independent`. Verify `--cuda-ids` and drivers.\n- OOM during long\u2011context training:\n\t- Lower `--chunk-size`, enable `--gradient-checkpointing`, ensure `--amp`, and reduce `--batch-size`.\n\n## References\n- CLI entry: `src/aios/cli/hrm_hf_cli.py` (train\u2011actv1)\n- Config: `src/aios/core/hrm_training/training_config.py`\n- Related docs:\n\t- [FLASH_ATTENTION.md](./FLASH_ATTENTION.md)\n\t- [FLASH_ATTENTION_VS_CHUNKING.md](./FLASH_ATTENTION_VS_CHUNKING.md)\n\t- [PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md](./PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md)\n\t- [MULTI_GPU_DISTRIBUTED.md](./MULTI_GPU_DISTRIBUTED.md)\n\t- [LORA_PEFT.md](./LORA_PEFT.md)\n\t- [DYNAMIC_SUBBRAINS_MOE.md](./DYNAMIC_SUBBRAINS_MOE.md)\n\t- [CORE_TRAINING.md](./CORE_TRAINING.md)\n\t- [CLI_COMMANDS.md](./CLI_COMMANDS.md)\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](./COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Advanced Training Features \u2014 AI-OS"}, {"line": 5, "text": "Overview"}, {"line": 19, "text": "Prerequisites"}, {"line": 26, "text": "Commands (CLI syntax)"}, {"line": 28, "text": "a) Direct CLI"}, {"line": 33, "text": "b) Module invocation (exact venv)"}, {"line": 38, "text": "Key option groups (with notes)"}, {"line": 40, "text": "Attention & positional"}, {"line": 45, "text": "Memory & performance"}, {"line": 52, "text": "Precision & quantization"}, {"line": 59, "text": "Dataset streaming & chunked training"}, {"line": 66, "text": "Distributed & multi\u2011GPU"}, {"line": 74, "text": "MoE (Mixture of Experts)"}, {"line": 82, "text": "PEFT (parameter\u2011efficient fine\u2011tuning)"}, {"line": 87, "text": "Inference hot\u2011reload during training"}, {"line": 91, "text": "Auto optimization"}, {"line": 94, "text": "Try it: minimal safe examples"}, {"line": 96, "text": "1) Quick dry\u2011run (single GPU)"}, {"line": 112, "text": "2) Windows multi\u2011GPU without DDP"}, {"line": 123, "text": "3) QLoRA\u2011style PEFT on a single GPU (very low VRAM)"}, {"line": 135, "text": "4) MoE enabled with conservative routing"}, {"line": 145, "text": "Compatibility, constraints, and tips"}, {"line": 164, "text": "Troubleshooting"}, {"line": 177, "text": "References"}]}, {"path": "guide/features/CLI_COMMANDS.md", "content": "# CLI Commands - AI-OS\nGenerated: October 20, 2025\nPurpose: Reference for the `aios` CLI and subcommands\nStatus: Implemented\n\n## Overview\n\n- Main entry point: `aios`\n- File: `src/aios/cli/aios.py`\n\nSub-commands overview:\n- hrm-hf \u2013 HuggingFace-based HRM training (see Core Training)\n- brains \u2013 Brain management\n- gui \u2013 Launch GUI (see GUI Features)\n- status \u2013 System status\n- datasets \u2013 Dataset management (see Datasets)\n- cache \u2013 Cache management\n- goals \u2013 Goals management\n- eval \u2013 Evaluation utilities (see Advanced Features \u2192 Evaluation)\n- artifacts \u2013 Artifacts management\n- cleanup \u2013 Cleanup utilities\n- crawl \u2013 Web crawling (see Tools & Integrations)\n- optimization \u2013 Optimization utilities (see Memory Optimization)\n- modelcard \u2013 Model card generation\n- agent \u2013 Agent commands\n- budgets \u2013 Budget management (see Advanced Features \u2192 Budgets)\n- core \u2013 Core commands\n- hf-cache \u2013 HuggingFace cache management\n- dml \u2013 DirectML utilities\n\n## HRM-HF Training\n\n- Command: `aios hrm-hf`\n- File: `src/aios/cli/hrm_hf_cli.py`\n- Subcommand: `train-actv1` \u2013 Train HRM models with ACT v1\n- File: `src/aios/cli/hrm_hf/train_actv1.py`\n- Deep dive: See Core Training and Memory Optimization docs\n\nKey parameters (selection):\n- Model: `--model <name_or_path>`\n- Brain naming: `--brain-name`, `--bundle-dir`\n- Training control: `--steps`, `--batch-size`, `--lr`, `--max-seq-len`, `--iterate`, `--stop-file`, `--resume`, `--stop-after-epoch`\n- Architecture: `--h-layers`, `--l-layers`, `--hidden-size`, `--expansion`, `--num-heads`, `--h-cycles`, `--l-cycles`, `--halt-max-steps`, `--window-size`, `--pos-encodings`\n- Memory optimization: `--gradient-checkpointing|--no-gradient-checkpointing`, `--amp|--no-amp`, `--use-8bit-optimizer`, `--use-chunked-training`, `--chunk-size`, `--cpu-offload`\n- Dataset: `--dataset-file`, `--ascii-only`, `--linear-dataset`, `--dataset-start-offset`, `--dataset-chunk-size`\n- Evaluation: `--eval-file`, `--eval-batches`, `--log-file`\n- Multi-GPU: `--ddp`, `--cuda-ids`, `--world-size`, `--parallel-independent`, `--strict`\n- DeepSpeed: `--zero-stage <none|zero1|zero2|zero3>`\n- MoE: `--use-moe`, `--num-experts`, `--num-experts-per-tok`, `--moe-capacity-factor`, `--auto-adjust-moe-lr`\n- PEFT: `--use-peft`, `--peft-method`, `--lora-r`, `--lora-alpha`, `--lora-dropout`, `--lora-target-modules`\n- Precision/Quant: `--model-dtype fp32|fp16|bf16`, `--load-in-8bit`, `--load-in-4bit`\n- Inference hot\u2011reload: `--inference-device`, `--hot-reload-steps`\n\n## Brains Management\n\n- Command: `aios brains`\n- File: `src/aios/cli/brains.py`\n- Subcommands: list, load, info, delete, export, import\n- Related: Core Training \u2192 Brain Bundle System, GUI Features \u2192 Brains Panel\n\n## Datasets Management\n\n- Command: `aios datasets`\n- File: `src/aios/cli/datasets_cli.py`\n- Features: list/download, scan, metadata, verification\n- Related: Datasets doc\n\n## Goals Management\n\n- Command: `aios goals`\n- File: `src/aios/cli/goals_cli.py`\n- Create/list/activate goals, link to experts, goal-driven training.\n- Related: Dynamic Subbrains/MoE and Advanced Features \u2192 Orchestrator\n\n## Cache and HF Cache\n\n- `aios cache` \u2192 Clear/show stats\n- `aios hf-cache` \u2192 Location, move, clear, size reporting\n\n## Evaluation\n\n- Command: `aios eval`\n- File: `src/aios/cli/eval_cli.py`\n- Run evaluations, generate reports, compare models.\n- Related: Advanced Features \u2192 Evaluation\n\n## Crawling\n\n- Command: `aios crawl`\n- File: `src/aios/cli/crawl_cli.py`\n- Web crawling, dataset generation from web.\n- Related: Tools & Integrations\n\n## Optimization\n\n- Command: `aios optimization`\n- File: `src/aios/cli/optimization_cli.py`\n- Memory/VRAM estimation and parameter optimization.\n- Related: Memory Optimization\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "evaluation", "experts", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "CLI Commands - AI-OS"}, {"line": 5, "text": "Overview"}, {"line": 30, "text": "HRM-HF Training"}, {"line": 53, "text": "Brains Management"}, {"line": 60, "text": "Datasets Management"}, {"line": 67, "text": "Goals Management"}, {"line": 74, "text": "Cache and HF Cache"}, {"line": 79, "text": "Evaluation"}, {"line": 86, "text": "Crawling"}, {"line": 93, "text": "Optimization"}]}, {"path": "guide/features/COMPLETE_FEATURE_INDEX.md", "content": "# Complete Feature Index - AI-OS\nGenerated: October 20, 2025\nPurpose: Ultra-lean index linking to feature docs and deep dives\nStatus: Implemented\n\n---\n\n## Quick Links\n\n- Core docs\n    1. [CLI Commands](CLI_COMMANDS.md)\n    2. [Core Training](CORE_TRAINING.md)\n    3. [Memory Optimization](MEMORY_OPTIMIZATION.md)\n    4. [Model Architecture](MODEL_ARCHITECTURE.md)\n    5. [Datasets](DATASETS.md)\n    6. [Tokenizers](TOKENIZERS.md)\n    7. [Dynamic Subbrains / MoE](DYNAMIC_SUBBRAINS_MOE.md)\n    8. [GUI Features](GUI_FEATURES.md)\n    9. [Multi-GPU & Distributed](MULTI_GPU_DISTRIBUTED.md)\n    10. [Tools & Integrations](TOOLS_INTEGRATIONS.md)\n    11. [Advanced Features](ADVANCED_FEATURES.md)\n\n- Deep dives & matrices\n    - [Feature Combination Matrix](FEATURE_COMBINATION_MATRIX.md)\n    - [Flash Attention](FLASH_ATTENTION.md)\n    - [Flash Attention vs Chunking](FLASH_ATTENTION_VS_CHUNKING.md)\n    - [Parallel Training Block Chunk System](PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md)\n    - [Configurable Dataset Chunk Size](CONFIGURABLE_DATASET_CHUNK_SIZE.md)\n    - [LoRA / PEFT](LORA_PEFT.md)\n\n---\n\nBack to Guide Index: [../INDEX.MD](../INDEX.MD)\n\n", "tags": ["cli", "datasets", "gui", "training"], "headings": [{"line": 0, "text": "Complete Feature Index - AI-OS"}, {"line": 7, "text": "Quick Links"}]}, {"path": "guide/features/CONFIGURABLE_DATASET_CHUNK_SIZE.md", "content": "# Configurable Dataset Chunk Size (Sub-topic)\nLast Updated: October 20, 2025\n\nStatus: Implemented and used by CLI/GUI\n\nCanonical feature doc: `PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md`\n\n## Overview\n\nThe dataset chunk size controls how many samples are processed per training cycle when using iterate-style loading. Smaller chunks reduce peak memory usage; larger chunks improve throughput at the cost of higher memory.\n\n## How it works\n\n- CLI flag: --dataset-chunk-size <int> (default: 4000)\n- TrainingConfig field: dataset_chunk_size: int\n- GUI: \u201cChunk size\u201d field under Steps; the \u201cAuto\u201d button aligns steps to the chunk size\n\n## Recommended values\n\n- 8 GB VRAM: 2000\u20133000\n- 12\u201316 GB VRAM: 4000 (default)\n- 24 GB+ VRAM: 8000+\n\n## CLI examples (Windows PowerShell)\n\n- Low VRAM (8 GB)\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --dataset-chunk-size 2000 --steps 100\n```\n\n- Balanced (12 GB)\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --dataset-chunk-size 4000 --steps 100\n```\n\n- High VRAM (24 GB+)\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --dataset-chunk-size 8000 --steps 100\n```\n\n## Where it\u2019s wired in\n\n- src/aios/cli/hrm_hf_cli.py\n  --dataset-chunk-size option passed into config\n- src/aios/core/hrm_training/training_config/base_fields.py\n  dataset_chunk_size: int = 4000\n- src/aios/cli/hrm_hf/data.py, encoding.py, block_manager.py, train_actv1.py\n  use dataset_chunk_size to bound lines read and chunk within blocks\n- GUI\n  - Variable: panel.dataset_chunk_size_var (default \"4000\")\n  - Auto button in helpers.py sets steps to match chunk size\n\n## Try it\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --dataset-chunk-size 4000 --steps 1 --batch-size 2 --halt-max-steps 1 --eval-batches 1 --log-file artifacts/brains/actv1/metrics.jsonl\n```\nExpected: training parses and logs metrics; dataset loader will cap lines per cycle at 4000.\n\n## Notes\n\n- This setting affects data loading cadence and memory pressure, not model architecture.\n- If you also use gradient accumulation, consider total tokens per optimizer step when tuning throughput vs memory.\n", "tags": ["cli", "datasets", "evaluation", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Configurable Dataset Chunk Size (Sub-topic)"}, {"line": 7, "text": "Overview"}, {"line": 11, "text": "How it works"}, {"line": 17, "text": "Recommended values"}, {"line": 23, "text": "CLI examples (Windows PowerShell)"}, {"line": 40, "text": "Where it\u2019s wired in"}, {"line": 52, "text": "Try it"}, {"line": 58, "text": "Notes"}]}, {"path": "guide/features/CORE_TRAINING.md", "content": "# Core Training - AI-OS\nGenerated: October 20, 2025\nPurpose: Architecture and training engine for HRM\nStatus: Implemented\n\n## Key Files\n- `src/aios/core/hrm_training/training_config.py` \u2013 TrainingConfig (874+ lines)\n- `src/aios/cli/hrm_hf/train_actv1.py` \u2013 Training loop (~2000 lines)\n- `src/aios/core/hrm_engine.py` \u2013 Engine utilities\n\n## Overview\nThe core training flow is exposed via the `aios hrm-hf train-actv1` command. It loads a base model and tokenizer, applies HRM training logic, logs metrics, writes checkpoints, and maintains a brain bundle directory under `artifacts/brains/actv1/`.\n\n## Training Configuration\n- Single source of truth for parameters\n- Validation, type checking, CLI arg conversion, defaults, serialization\n\n## Training Loop Features\n- Gradient accumulation, loss/optimizer/scheduler\n- Checkpoints, metrics logging, OOM handling, graceful stop file\n\n## Brain Bundle System\nDirectory structure:\n```\nartifacts/brains/actv1/<brain-name>/\n\u251c\u2500 config.json\n\u251c\u2500 model.safetensors\n\u251c\u2500 tokenizer.json\n\u251c\u2500 metadata.json\n\u251c\u2500 training_args.json\n\u2514\u2500 checkpoints/\n```\nFeatures: auto-create, save model/tokenizer/config, resume\n\n## Commands (CLI syntax)\n\nYou can run training either via the CLI entry point or directly through Python's module interface. On Windows, prefer PowerShell examples below.\n\n### a) Direct CLI (named flags)\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1000 --batch-size 2 --halt-max-steps 1 --eval-batches 2 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n### b) Module invocation (positional model)\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1000 --batch-size 2 --halt-max-steps 1 --eval-batches 2 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n### Key parameters (selection)\n- Model selection: `--model <name_or_path>` or positional `gpt2`\n- Dataset: `--dataset-file <path>` (txt/jsonl); optional `--ascii-only`\n- Steps and batching: `--steps <int>`, `--batch-size <int>`\n- Halting: `--halt-max-steps <int>` (controls ACT halting behavior)\n- Evaluation: `--eval-file <path>`, `--eval-batches <int>`\n- Logging: `--log-file <path>` (JSONL)\n- Iteration control: `--iterate`, `--stop-file <path>`\n- Brain bundle: `--brain-name <str>`, `--bundle-dir <path>`\n- Architecture knobs: `--h-layers`, `--l-layers`, `--hidden-size`, `--expansion`, `--num-heads`, `--h-cycles`, `--l-cycles`, `--window-size`, `--pos-encodings`\n- Memory: `--gradient-checkpointing|--no-gradient-checkpointing`, `--amp|--no-amp`, `--8bit-optimizer`\n- Multi-GPU: `--ddp`, `--cuda-ids <list>`, `--world-size <int>`\n- DeepSpeed: `--zero-stage <none|zero1|zero2|zero3>` (uses configs in `config/`)\n- Experts: `--expert-id <id>` (train/freeze expert-specific components)\n\nNotes:\n- Paths are relative to repo root unless absolute. PowerShell accepts forward slashes (`/`) in Python paths.\n- For Windows shells, escape backslashes or quote paths with spaces.\n\n## Iterate Mode\n- `--iterate`: restart after completion with new shuffle; supports stop file\n\n## Evaluation During Training\n- `--eval-file`, `--eval-batches`\n- Periodic eval, validation loss, perplexity, history\n\n## Expert Training Mode\n- `--expert-id <id>`: train individual expert, freeze base, save under `artifacts/experts/<id>/`\n- Related: Dynamic Subbrains/MoE\n\n## Inputs\n- Dataset file(s): `training_data/curated_datasets/*.txt` (example set provided)\n- Optional eval file: `training_data/eval_test_dataset.txt`\n- Base model: HuggingFace hub id or local path (e.g., `gpt2` or `artifacts/hf_implant/base_model`)\n- Tokenizer: auto-resolved from model or `artifacts/hf_implant/tokenizers`\n\n## Outputs\n- Brain bundle under `artifacts/brains/actv1/<brain-name>/`\n- Metrics log (JSONL): default/explicit `artifacts/brains/actv1/metrics.jsonl`\n- Checkpoints under the bundle `checkpoints/`\n- Optional evaluation summaries in metrics/logs\n\n## Try it: quick dry-run examples\n\nThese mirror VS Code tasks configured in this repo and are safe to run. Ensure your venv is active.\n\n### Option 1: Direct CLI dry-run\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1 --batch-size 2 --halt-max-steps 1 --eval-batches 1 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n### Option 2: Module (positional model)\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1 --batch-size 2 --halt-max-steps 1 --eval-batches 1 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n### Option 3: Use VS Code Task\n- Run: Tasks \u2192 \"Run brief HRM CLI dry-run\" or \"Run brief HRM CLI dry-run (positional)\" or \"Run HRM dry-run (module)\"\n- Expected outputs:\n\t- Metrics JSONL at `artifacts/brains/actv1/metrics.jsonl`\n\t- Brain bundle directories under `artifacts/brains/actv1/`\n\t- Console logs including training/eval step counts\n\n## Usage Notes\n- Use AMP and gradient checkpointing for memory savings\n- Use 8-bit optimizer for larger models when bitsandbytes is available\n\nRelated: Memory Optimization, Model Architecture, Datasets, Tokenizers\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)\n\n## Troubleshooting\n\n- OOM (out of memory): lower `--batch-size`, `--max-seq-len`, or `--dataset-chunk-size`; enable `--gradient-checkpointing` and `--amp`; consider `--use-8bit-optimizer` if bitsandbytes is installed.\n- FlashAttention: ensure `--use-flash-attn` and that your GPU supports it; otherwise it will fall back to SDPA.\n- Multi-GPU on Windows: prefer `--parallel-independent` with `--cuda-ids`; DDP often fails on Windows. If you need DDP, set `$env:AIOS_DDP_SPAWN = \"1\"` before running with `--ddp`.\n- Resume: when using parallel mode, `chunk_tracker_state.json` in the brain bundle enables resume; delete it if you want a fresh start.\n\nSee also:\n- Parallel Training Block/Chunk System\n- Multi-GPU & Distributed", "tags": ["cli", "datasets", "evaluation", "experts", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Core Training - AI-OS"}, {"line": 5, "text": "Key Files"}, {"line": 10, "text": "Overview"}, {"line": 13, "text": "Training Configuration"}, {"line": 17, "text": "Training Loop Features"}, {"line": 21, "text": "Brain Bundle System"}, {"line": 34, "text": "Commands (CLI syntax)"}, {"line": 38, "text": "a) Direct CLI (named flags)"}, {"line": 43, "text": "b) Module invocation (positional model)"}, {"line": 48, "text": "Key parameters (selection)"}, {"line": 67, "text": "Iterate Mode"}, {"line": 70, "text": "Evaluation During Training"}, {"line": 74, "text": "Expert Training Mode"}, {"line": 78, "text": "Inputs"}, {"line": 84, "text": "Outputs"}, {"line": 90, "text": "Try it: quick dry-run examples"}, {"line": 94, "text": "Option 1: Direct CLI dry-run"}, {"line": 99, "text": "Option 2: Module (positional model)"}, {"line": 104, "text": "Option 3: Use VS Code Task"}, {"line": 111, "text": "Usage Notes"}, {"line": 119, "text": "Troubleshooting"}]}, {"path": "guide/features/DATASETS.md", "content": "# Datasets - AI-OS\nGenerated: October 20, 2025\nPurpose: Dataset system, readers, registry, and streaming\nStatus: Implemented\n\n## Files\n- Readers: `src/aios/data/datasets/*.py`\n- Registry: `src/aios/core/datasets/registry.py`\n- Catalog: `src/aios/core/datasets/catalog.py`\n- Streaming: `src/aios/data/streaming_cache.py`, `src/aios/data/stream_manager.py`\n\n## Supported Formats\n- Plain text, CSV, archives (tar/zip), directories, JSON, JSONL\n\n## Dataset Registry\n- Rich metadata (20+ fields), search, recommendations\n- Expert-dataset usage tracking, JSON persistence, local scanning\n\n## Streaming Dataset\n- Memory-efficient loading, infinite streaming, shuffling, caching\n\n## CLI\n\nThe datasets functionality is exposed as discrete commands under the main `aios` CLI:\n\n### Discovery and capacity\n- Show storage usage and cap:\n\t```powershell\n\taios datasets-stats\n\t```\n\tOutput example: `{ \"usage_gb\": 0.125, \"cap_gb\": 15.0 }`\n\n- Set storage capacity cap (persisted to ~/.config/aios/datasets.json):\n\t```powershell\n\taios datasets-set-cap 20\n\t```\n\tOutput example: `{ \"ok\": true, \"cap_gb\": 20.0 }`\n\n- List known datasets within size limit:\n\t```powershell\n\taios datasets-list-known --max-size-gb 10\n\t```\n\tOutput: JSON array of known items `{ name, url, approx_size_gb, notes }`\n\n### Building datasets (web-assisted)\nThese commands create datasets under the resolved base directory from `datasets_base_dir()`:\n`training_data/curated_datasets/<type>/<dataset_name>/`\n\n- Build text dataset by extracting main readable text from top sites:\n\t```powershell\n\taios datasets-build-text \"boats\" --max-docs 50 --per-site 10 --search-results 10 --min-chars 400 --progress\n\t```\n\tOutputs: `manifest.jsonl` with `{ path, label, url, title, chars, excerpt }` and text files.\n\tOptions: `--allow-ext txt,pdf,doc,docx,rtf,md,html,htm` to restrict downloads; `--file-prefix` to prefix filenames; `--store-dataset` to set dataset folder name; `--overwrite` to replace existing.\n\n- Build websites snapshot dataset (HTML pages):\n\t```powershell\n\taios datasets-build-websites \"boats\" --max-pages 30 --per-site 10 --search-results 10 --min-bytes 2000 --progress\n\t```\n\tOutputs: `pages/*.html` and `manifest.jsonl` with `{ path, url, title, bytes, links }`.\n\n- Build images dataset (perceptual dedup optional):\n\t```powershell\n\taios datasets-build-images \"boats\" --max-images 100 --per-site 20 --pages-per-site 5 --near-duplicate-threshold 8 --allow-ext jpg,png,webp --progress\n\t```\n\tOutputs: image files and `manifest.jsonl` with `{ path, label, source_url, page_url, title, alt }`.\n\n- Build generic raw files dataset (by extension allowlist):\n\t```powershell\n\taios datasets-build-raw \"boats\" --max-files 50 --per-site 10 --allow-ext pdf,csv,json,txt,zip --progress\n\t```\n\tOutputs: `files/*` and `manifest.jsonl` with `{ path, label, source_url, page_url, bytes }`.\n\n- Build videos dataset:\n\t```powershell\n\taios datasets-build-videos \"boats\" --max-videos 20 --per-site 5 --allow-ext mp4,webm --progress\n\t```\n\tOutputs: video files and `manifest.jsonl` with `{ path, label, source_url, page_url, bytes }`.\n\nNotes:\n- These commands respect a storage capacity cap and will stop early if the cap would be exceeded.\n- Networking is best-effort and may skip pages or files if unavailable or too small.\n- Use `--overwrite` to rebuild a dataset folder.\n\n### Base directory resolution\nDataset base directory is chosen in this order:\n1) `AIOS_DATASETS_DIR` environment variable\n2) Project root detection \u2192 `training_data/curated_datasets`\n3) Fallback: `~/.local/share/aios/datasets`\n\nUse this to find your outputs. Example (project root):\n`training_data/curated_datasets/text/<dataset_name>/manifest.jsonl`\n\n## Inputs\n- Web-sourced datasets via CLI builders as shown above\n- Local files in supported formats (txt/csv/json/jsonl/archives/directories)\n\n## Outputs\n- Organized dataset directories under the base dir\n- Manifest files (`manifest.jsonl`) describing items for each dataset type\n- Storage cap config at `~/.config/aios/datasets.json`\n\n## Try it: quick local example\nUse an existing small text file to validate training pipeline compatibility:\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1 --batch-size 2 --halt-max-steps 1 --eval-batches 1 --log-file artifacts/brains/actv1/metrics.jsonl\n```\nExpected: metrics JSONL created and a brain bundle directory under `artifacts/brains/actv1/`.\n\n## Troubleshooting\n- \"cap_exceeded\": Increase cap via `aios datasets-set-cap <GB>` or delete old datasets\n- Permission issues on Windows: run VS Code as a user with write access to dataset dir and HF cache dir\n- Empty manifests: Increase `--max-docs`/`--max-pages` or relax `--min-chars`/`--min-bytes`\n\nRelated: Tokenizers, Core Training, Dynamic Subbrains/MoE\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "experts", "gui", "training"], "headings": [{"line": 0, "text": "Datasets - AI-OS"}, {"line": 5, "text": "Files"}, {"line": 11, "text": "Supported Formats"}, {"line": 14, "text": "Dataset Registry"}, {"line": 18, "text": "Streaming Dataset"}, {"line": 21, "text": "CLI"}, {"line": 25, "text": "Discovery and capacity"}, {"line": 44, "text": "Building datasets (web-assisted)"}, {"line": 84, "text": "Base directory resolution"}, {"line": 93, "text": "Inputs"}, {"line": 97, "text": "Outputs"}, {"line": 102, "text": "Try it: quick local example"}, {"line": 109, "text": "Troubleshooting"}]}, {"path": "guide/features/DYNAMIC_SUBBRAINS_MOE.md", "content": "# Dynamic Subbrains (Mixture of Experts)\n\nPurpose: Sparse expert routing for efficiency and specialization. Includes expert metadata/registry, expert-only training, and goal-aware routing hooks.\n\nStatus: Implemented core MoE in ACTv1 + expert registry and expert-only training. Goal-aware biasing and full GUI management are WIP.\n\nKey files:\n- MoE layer and routing stats: `src/aios/core/hrm_models/moe_layer.py`\n- Goal-aware router (biasing by goals): `src/aios/core/hrm_models/goal_aware_router.py`\n- Expert metadata and registry: `src/aios/core/hrm_models/expert_metadata.py`\n- ACTv1 model uses MoE by default: `src/aios/core/hrm_models/impl/hrm_act_v1.py` and `src/aios/core/brains/hf_brain.py`\n- Training CLI flags (MoE and experts): `src/aios/cli/hrm_hf_cli.py`\n- Expert-only training implementation: `src/aios/cli/hrm_hf/expert_training.py`\n- Metrics logging (load balancing + expert usage): `src/aios/cli/hrm_hf/training_logic/train_epoch.py`\n- GUI Subbrains Manager (WIP): `src/aios/gui/components/subbrains_manager_panel/`\n\nSee also:\n- Core training: `CORE_TRAINING.md`\n- Memory optimization (8-bit optimizer, AMP): `MEMORY_OPTIMIZATION.md`\n- Multi-GPU/Windows-friendly parallel training: `MULTI_GPU_DISTRIBUTED.md`, `PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md`\n- Goals CLI (link goals to experts): `CLI_COMMANDS.md` (Goals section)\n\n## What you get\n- Sparse MoE with top-k expert routing per token for ~75% compute reduction while increasing capacity.\n- Automatic auxiliary load-balancing loss to prevent collapse.\n- Periodic expert-usage metrics in your log file (routing probabilities, token counts).\n- Expert-only training mode that produces standalone expert checkpoints and updates a persistent registry.\n- Goal-aware router module (WIP hookup) to bias expert selection by active goals.\n\n## Commands (PowerShell, Windows-first)\n\n1) Train ACTv1 with MoE (default enabled)\n- Flags come from `aios hrm-hf train-actv1`. MoE-related flags:\n\t- `--use-moe/--no-moe` (default: `--use-moe`)\n\t- `--num-experts <int>` (default: 8)\n\t- `--num-experts-per-tok <int>` (top-k, default: 2)\n\t- `--moe-capacity-factor <float>` (default: 1.25)\n\t- `--auto-adjust-moe-lr/--no-auto-adjust-moe-lr` (default: on; reduces LR for MoE stability)\n\nExample (small dry-run, logs expert usage):\n\n\t\taios hrm-hf train-actv1 `\n\t\t\t--model artifacts/hf_implant/base_model `\n\t\t\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t\t\t--steps 20 --batch-size 8 `\n\t\t\t--use-moe --num-experts 8 --num-experts-per-tok 2 --moe-capacity-factor 1.25 `\n\t\t\t--log-file artifacts/brains/actv1/metrics.jsonl\n\nDisable MoE (train dense FFN instead):\n\n\t\taios hrm-hf train-actv1 `\n\t\t\t--model artifacts/hf_implant/base_model `\n\t\t\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t\t\t--steps 20 --batch-size 8 `\n\t\t\t--no-moe `\n\t\t\t--log-file artifacts/brains/actv1/metrics.jsonl\n\nTips:\n- Lower `--num-experts-per-tok` to 1 to reduce active compute/memory on very constrained GPUs.\n- Keep `--auto-adjust-moe-lr` enabled unless you know what you\u2019re doing; MoE routers can be unstable at higher LR.\n\n2) Train a standalone expert only (writes artifacts/experts and updates registry)\n- Trigger by passing `--expert-id <string>` to `train-actv1`.\n- Uses a lightweight FeedForward expert, saves as `.safetensors`, and writes/updates `artifacts/experts/registry.json`.\n\nExample (quick expert build):\n\n\t\taios hrm-hf train-actv1 `\n\t\t\t--model artifacts/hf_implant/base_model `\n\t\t\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t\t\t--steps 3 --batch-size 2 `\n\t\t\t--expert-id test-expert-004 `\n\t\t\t--default-goal \"Improve summarization quality\" `\n\t\t\t--log-file artifacts/experts/test-expert-004/metrics.jsonl\n\nOutputs:\n- `artifacts/experts/test-expert-004/expert.safetensors`\n- `artifacts/experts/registry.json` (created or updated with metadata including `expert_id`, `name`, `goals`, `checkpoint_path`, `is_active/is_frozen`, hierarchy fields)\n\n3) Link goals to experts (biasing signal for router)\n- Goals live in the directives DB and can be associated with an expert.\n- Commands are under `aios goals-*`.\n\nExamples:\n\n\t\t# Add a goal and link to an expert immediately\n\t\taios goals-add \"Improve summarization quality\" --expert-id test-expert-004\n\n\t\t# Link an existing goal to an expert\n\t\taios goals-link-expert 42 test-expert-004\n\n\t\t# List active goals\n\t\taios goals-list\n\n\t\t# List goals for an expert\n\t\taios goals-list-for-expert test-expert-004\n\nNotes:\n- The `GoalAwareRouter` module supports biasing toward experts linked to active goals. Integration into the default training/inference loop is in progress; track `src/aios/core/hrm_models/goal_aware_router.py`.\n\n## Inputs and Outputs\n\nInputs (training flags relevant to MoE/experts):\n- `--use-moe`, `--num-experts`, `--num-experts-per-tok`, `--moe-capacity-factor`, `--auto-adjust-moe-lr`\n- Standard training knobs: `--max-seq-len`, `--batch-size`, `--steps`, `--lr`, `--amp`, `--gradient-checkpointing`, etc.\n- Expert-only mode: `--expert-id <id>` plus optional `--default-goal` to seed goal linkage.\n\nOutputs (files and metrics):\n- Brain training logs: your `--log-file` JSONL includes, when MoE is enabled:\n\t- `lb_loss`: load-balancing loss value (applied internally; coef ~0.05)\n\t- Periodic `expert_usage` events with:\n\t\t- `avg_routing_prob`: average probability per expert\n\t\t- `token_counts`: tokens routed to each expert\n\t\t- `total_tokens`: total tokens seen when sampled\n- Expert-only training:\n\t- `artifacts/experts/<expert-id>/expert.safetensors`\n\t- `artifacts/experts/registry.json` with entries like:\n\t\t- `expert_id`, `name`, `description`, `category`, `goals`, timestamps\n\t\t- `is_active`, `is_frozen`, `parent_expert_id`, `child_expert_ids`\n\t\t- `checkpoint_path`: e.g., `artifacts\\\\experts\\\\<expert-id>\\\\expert.safetensors`\n\t\t- `training_config`: hidden/intermediate sizes, steps, batch size, etc.\n\n## How routing works (high level)\n- Each MoE layer computes router logits over N experts and activates the top-k experts per token (`--num-experts-per-tok`).\n- An auxiliary load-balancing loss is added to spread traffic across experts and avoid collapse. Metrics include `lb_loss` and `moe_layers` count.\n- `expert_usage` entries in logs let you validate router health and specialization during training.\n\n## GUI: Subbrains Manager (WIP)\n- The panel shows the expert registry with counts and hierarchy and can refresh from disk:\n\t- Code: `src/aios/gui/components/subbrains_manager_panel/`\n\t- Data loader: `data_manager.py` reads `artifacts/experts/registry.json`\n- Actions like create/delete/freeze are currently placeholders that print \u201cCLI command needed\u201d. Use CLI for expert training and goals linking.\n- As features land, the panel will manage expert lifecycle and goal associations directly.\n\n## Troubleshooting\n- Training is unstable (NaNs/Inf) with MoE:\n\t- Keep `--auto-adjust-moe-lr` enabled (default). It reduces LR for MoE automatically.\n\t- Lower base `--lr` and/or `--num-experts-per-tok`.\n\t- Ensure AMP/precision settings are stable (`--amp` by default; try `--model-dtype bf16` on supported GPUs).\n- VRAM pressure with many experts:\n\t- Reduce `--num-experts` and/or set `--num-experts-per-tok 1`.\n\t- Use `--amp`, `--gradient-checkpointing`, and `--use-8bit-optimizer` (requires bitsandbytes).\n- No expert usage metrics in log:\n\t- Ensure `--use-moe` is on.\n\t- `expert_usage` logs are periodic (every ~100 steps) and sampled from early layers; short runs may not emit them.\n- Can\u2019t find the expert registry:\n\t- Path: `artifacts/experts/registry.json`. It\u2019s created on first expert-only training.\n\n## Try it quickly\n- Minimal MoE run with metrics:\n\n\t\taios hrm-hf train-actv1 `\n\t\t\t--model artifacts/hf_implant/base_model `\n\t\t\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t\t\t--steps 30 --batch-size 4 `\n\t\t\t--use-moe --num-experts 8 --num-experts-per-tok 2 `\n\t\t\t--log-file artifacts/brains/actv1/metrics.jsonl\n\n- Train one tiny expert and link a goal:\n\n\t\taios hrm-hf train-actv1 `\n\t\t\t--model artifacts/hf_implant/base_model `\n\t\t\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t\t\t--steps 3 --batch-size 2 `\n\t\t\t--expert-id demo-expert-001 `\n\t\t\t--default-goal \"Focus on troubleshooting clarity\" `\n\t\t\t--log-file artifacts/experts/demo-expert-001/metrics.jsonl\n\n\t\taios goals-list-for-expert demo-expert-001\n\n## Notes and next steps\n- Goal-aware router module exists and exposes bias controls; full wiring to training/inference loops and GUI controls is in progress.\n- The GUI Subbrains Manager will gain create/delete/freeze operations backed by CLI endpoints.\n- We\u2019ll expose advanced router knobs (e.g., load-balance loss coef) once stabilized.\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "experts", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Dynamic Subbrains (Mixture of Experts)"}, {"line": 22, "text": "What you get"}, {"line": 29, "text": "Commands (PowerShell, Windows-first)"}, {"line": 85, "text": "\t\t# Add a goal and link to an expert immediately"}, {"line": 88, "text": "\t\t# Link an existing goal to an expert"}, {"line": 91, "text": "\t\t# List active goals"}, {"line": 94, "text": "\t\t# List goals for an expert"}, {"line": 100, "text": "Inputs and Outputs"}, {"line": 122, "text": "How routing works (high level)"}, {"line": 127, "text": "GUI: Subbrains Manager (WIP)"}, {"line": 134, "text": "Troubleshooting"}, {"line": 148, "text": "Try it quickly"}, {"line": 170, "text": "Notes and next steps"}]}, {"path": "guide/features/FEATURE_COMBINATION_MATRIX.md", "content": "# Feature Combination Matrix - AI-OS\nLast Updated: November 7, 2025\nPurpose: Feature compatibility reference - which combinations are verified and which are experimental\n\n> **Note for v1.0.0:** This matrix documents the current testing status of feature combinations. \n> Items marked as \"EXPERIMENTAL\" or with TODO notes represent experimental combinations \n> that may work but haven't been comprehensively tested. Use with appropriate caution.\n\n---\n\n## \ud83d\udcca Status Legend\n\n| Status | Meaning |\n|--------|---------|\n| \u2705 **VERIFIED** | Tested and confirmed working |\n| \u26a0\ufe0f **EXPERIMENTAL** | Should work but not comprehensively tested |\n| \u274c **INCOMPATIBLE** | Known to be incompatible |\n| \u2753 **UNTESTED** | Status unclear, use with caution |\n| \ud83d\udea7 **PARTIAL** | Partially works with known limitations |\n\n---\n\n## \ud83d\udd2c Memory Optimization Combinations\n\n### Gradient Checkpointing + AMP\n**Status**: \u2705 **VERIFIED WORKING**  \n**Benefit**: ~60-70% memory reduction  \n**Speed Impact**: ~20% slower  \n**Recommended**: Yes, for most training\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --gradient-checkpointing `\n  --amp `\n  --steps 100\n```\n\n**Test Results**:\n- \u2705 Trains successfully\n- \u2705 Memory reduction confirmed\n- \u2705 No quality loss observed\n- \u2705 Works on single GPU\n- \u26a0\ufe0f Multi-GPU not tested\n\n---\n\n### Gradient Checkpointing + AMP + 8-bit Optimizer\n**Status**: \u2705 **VERIFIED WORKING**  \n**Benefit**: ~70-80% memory reduction  \n**Speed Impact**: ~25% slower  \n**Recommended**: Yes, for large models (>100M params)\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --gradient-checkpointing `\n  --amp `\n  --use-8bit-optimizer `\n  --steps 100\n```\n\n**Test Results**:\n- \u2705 Trains successfully\n- \u2705 Massive memory reduction\n- \u2705 Quality maintained\n- \u2705 Works with bitsandbytes 0.48.1\n- \u26a0\ufe0f Multi-GPU not tested\n\nRequirements:\n- bitsandbytes installed\n- CUDA-capable GPU (Linux preferred)\n\n---\n\n### Gradient Checkpointing + Long Context\n**Status**: \u26a0\ufe0f **EXPERIMENTAL**  \n**Expected**: Should work  \n**Use Case**: Train with longer sequences on limited VRAM\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --gradient-checkpointing `\n  --max-seq-len 2048 `\n  --batch-size 1 `\n  --steps 100\n```\n\n**Expected Behavior**:\n- \u2705 Should enable 2K-4K context on 11GB GPU\n- \u26a0\ufe0f Will be slower due to checkpointing\n- \u26a0\ufe0f Batch size must be very small\n\n**Note**: Not extensively tested with contexts above 2048 tokens. Start with smaller contexts and increase gradually.\n\n---\n\n### All Memory Optimizations Combined\n**Status**: \u26a0\ufe0f **PARTIAL**  \n**Features**: Gradient Checkpointing + AMP + 8-bit + Chunking  \n**Expected**: Maximum memory efficiency  \n**Use Case**: Train very large models or very long contexts\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --gradient-checkpointing `\n  --amp `\n  --use-8bit-optimizer `\n  --use-chunked-training --chunk-size 1024 `\n  --max-seq-len 8192 `\n  --batch-size 1 `\n  --steps 100\n```\n\nNotes:\n- \u2705 Chunked training is implemented (`--use-chunked-training`, `--chunk-size`)\n- \u26a0\ufe0f Expect slower throughput at very small chunk sizes\n\n**TODO**: \n1. Verify chunked training is implemented\n2. Test with various chunk sizes\n3. Measure actual memory usage\n\n---\n\n## \ud83d\ude80 Multi-GPU Combinations\n\n### DDP + Gradient Checkpointing\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Fast distributed training with memory efficiency\n\nExample (Linux recommended):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --ddp `\n  --cuda-ids \"0,1\" `\n  --world-size 2 `\n  --gradient-checkpointing `\n  --steps 100\n```\n\n**Issues**:\n- \u2753 DDP implementation not verified\n- \u2753 Does `_maybe_spawn` function exist?\n- \u2753 Gradient sync working?\n\nWindows tip: Prefer `--parallel-independent` instead of DDP.\n\n---\n\n### DDP + AMP\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Fast training with mixed precision across GPUs\n\nExample (Linux recommended):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --ddp `\n  --cuda-ids \"0,1\" `\n  --world-size 2 `\n  --amp `\n  --steps 100\n```\n\n**Note**: Not extensively tested with if AMP works correctly with DDP\n\n---\n\n### DDP + All Memory Optimizations\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Maximum efficiency across multiple GPUs\n\nExample (Linux recommended):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --ddp `\n  --cuda-ids \"0,1\" `\n  --world-size 2 `\n  --gradient-checkpointing `\n  --amp `\n  --use-8bit-optimizer `\n  --steps 100\n```\n\n---\n\nBack to [Guide Index](../INDEX.MD)\n\n**Questions**:\n- Does 8-bit optimizer work with DDP?\n- Are optimizer states synchronized?\n- Is there communication overhead?\n\n**TODO**: Comprehensive multi-GPU testing\n\n---\n\n## \ud83e\udde0 DeepSpeed Combinations\n\n### DeepSpeed ZeRO-1 + Gradient Checkpointing\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Optimizer state partitioning + activation checkpointing\n\nExample (Linux + DeepSpeed):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --zero-stage zero1 `\n  --gradient-checkpointing `\n  --cuda-ids \"0,1\" `\n  --steps 100\n```\n\n**TODO**:\n1. Verify DeepSpeed is actually initialized\n2. Test ZeRO-1 stage\n3. Measure memory reduction\n\n---\n\n### DeepSpeed ZeRO-2 + AMP\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Gradient partitioning + mixed precision\n\nExample (Linux + DeepSpeed):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --zero-stage zero2 `\n  --amp `\n  --cuda-ids \"0,1\" `\n  --steps 100\n```\n\n**Note**: Not extensively tested with and measure\n\n---\n\n### DeepSpeed ZeRO-3 (Maximum Memory Reduction)\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Parameter partitioning for massive models\n\nExample (Linux + DeepSpeed):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --zero-stage zero3 `\n  --gradient-checkpointing `\n  --amp `\n  --cuda-ids \"0,1\" `\n  --steps 100\n```\n\n**Note**: Not extensively tested with ZeRO-3 stage\n\n---\n\n### DeepSpeed + 8-bit Optimizer\n**Status**: \u2753 **COMPATIBILITY UNKNOWN**  \n**Question**: Can DeepSpeed work with bitsandbytes?\n\nExample (Compat unknown):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --zero-stage zero2 `\n  --use-8bit-optimizer `\n  --cuda-ids \"0,1\" `\n  --steps 100\n```\n\n**Potential Issue**: DeepSpeed has its own optimizer management - may conflict with bitsandbytes\n\n**Note**: Not extensively tested with compatibility\n\n---\n\n## \ud83e\udde9 MoE / Dynamic Subbrains Combinations\n\n### MoE + Gradient Checkpointing\n**Status**: \u26a0\ufe0f **UNTESTED**  \n**Expected**: Should work  \n**Use Case**: Train models with experts efficiently\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --use-moe `\n  --num-experts 4 `\n  --gradient-checkpointing `\n  --steps 100\n```\n\n**Note**: Not extensively tested with MoE with checkpointing\n\n---\n\n### MoE + AMP + 8-bit\n**Status**: \u26a0\ufe0f **UNTESTED**  \n**Expected**: Memory-efficient expert training\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --use-moe `\n  --num-experts 8 `\n  --gradient-checkpointing `\n  --amp `\n  --use-8bit-optimizer `\n  --steps 100\n```\n\n**Note**: Not extensively tested with expert training with optimizations\n\n---\n\n### Expert Training + Memory Optimizations\n**Status**: \u26a0\ufe0f **UNTESTED**  \n**Expected**: Efficient single expert training\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/base_model `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --expert-id \"python_expert\" `\n  --gradient-checkpointing `\n  --amp `\n  --use-8bit-optimizer `\n  --steps 100\n```\n\n**Note**: Not extensively tested with expert-only training mode\n\n---\n\n### MoE + Multi-GPU\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Expert parallelism across GPUs\n\nExample (Linux recommended):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --use-moe `\n  --num-experts 8 `\n  --ddp `\n  --cuda-ids \"0,1\" `\n  --world-size 2 `\n  --steps 100\n```\n\n**Questions**:\n- How are experts distributed across GPUs?\n- Is expert selection synchronized?\n- What's the communication pattern?\n\n**Note**: Not extensively tested with and document expert parallelism\n\n---\n\n### MoE + DeepSpeed\n**Status**: \u2753 **EXPERIMENTAL**  \n**Expected**: Expert partitioning with ZeRO\n\nExample (Linux + DeepSpeed):\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --use-moe `\n  --num-experts 16 `\n  --zero-stage zero3 `\n  --cuda-ids \"0,1\" `\n  --steps 100\n```\n\n**Note**: Not extensively tested with DeepSpeed with MoE\n\n---\n\n## \ud83d\udcda Context Length Combinations\n\n### Long Context + Chunking\n**Status**: \u2705 **SUPPORTED**  \n**Expected**: Enable 10K+ contexts by chunking\n\nExample:\n```powershell\naios hrm-hf train-actv1 `\n  --model gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --max-seq-len 10000 `\n  --use-chunked-training `\n  --chunk-size 1024 `\n  --gradient-checkpointing `\n  --amp `\n  --steps 100\n```\n\n**Questions**:\n- Is chunking actually implemented?\n- How does it split sequences?\n- What's the memory impact?\n\n**TODO**: \n1. Verify chunking implementation\n2. Test with various context lengths: 8K, 16K, 32K\n3. Measure actual memory usage\n\n---\n\n### Long Context + Multi-GPU\n**Status**: \u26a0\ufe0f **UNTESTED**  \n**Expected**: Distribute long sequences across GPUs\n\n**Command**:\n```bash\naios hrm-hf train-actv1 \\\n  --model gpt2 \\\n  --dataset-file data.txt \\\n  --max-seq-len 8192 \\\n  --ddp \\\n  --cuda-ids \"0,1\" \\\n  --world-size 2 \\\n  --gradient-checkpointing \\\n  --batch-size 1 \\\n  --steps 1000\n```\n\n**Note**: Not extensively tested with long context with DDP\n\n---\n\n### FlashAttention + Memory/Chunking\n**Status**: \u26a0\ufe0f **PLATFORM-DEPENDENT**  \nNotes:\n- `--use-flash-attn` is supported by the CLI and will enable FA2 when installed and compatible (Ampere+).\n- On Windows, FA2 is commonly unavailable; training falls back to PyTorch SDPA.\n- Combine with `--window-size` for extreme contexts when FA2 is not available.\n\n---\n\n## \ud83d\udd24 Tokenizer Combinations\n\n### Custom Tokenizer + Training\n**Status**: \u26a0\ufe0f **UNTESTED** (except GPT-2)  \n**Expected**: Should work with any HuggingFace tokenizer\n\n**Verified**:\n- \u2705 GPT-2 tokenizer\n\n**Needs Testing**:\n- \u26a0\ufe0f Qwen 2.5\n- \u26a0\ufe0f Mistral\n- \u26a0\ufe0f Code Llama\n- \u26a0\ufe0f DeepSeek-Coder V2\n- \u26a0\ufe0f StarCoder2\n- \u26a0\ufe0f Phi-3\n- \u26a0\ufe0f Llama 3 (requires HF auth)\n\n**Note**: Not extensively tested with each tokenizer with basic training\n\n---\n\n### Large Vocabulary + Memory Optimizations\n**Status**: \u26a0\ufe0f **UNTESTED**  \n**Use Case**: Tokenizers with 100K+ tokens (DeepSeek, Qwen, Llama 3)\n\n**Command**:\n```bash\naios hrm-hf train-actv1 \\\n  --model \"deepseek-ai/deepseek-coder-v2-base\" \\\n  --dataset-file data.txt \\\n  --gradient-checkpointing \\\n  --amp \\\n  --8bit-optimizer \\\n  --steps 1000\n```\n\n**Considerations**:\n- Large vocabulary = larger embedding layer\n- More memory needed for embeddings\n- May need aggressive optimizations\n\n**Note**: Not extensively tested with large-vocab tokenizers\n\n---\n\n## \ud83d\udcca Dataset Format Combinations\n\n### Streaming Dataset + Linear Mode\n**Status**: \u2705 **SUPPORTED**  \nFeatures:\n- Linear progression with resume via `--dataset-start-offset`\n- Iterate mode for long\u2011running cycles via `--iterate`\n\n**Features**:\n- \u2705 Infinite streaming\n- \u2705 Shuffle support\n- \u2705 Caching\n- \u2705 Memory-efficient\n\n---\n\n### Large Dataset + Multi-GPU\n**Status**: \u26a0\ufe0f **UNTESTED**  \n**Expected**: Distributed dataset loading\n\n**Command**:\n```bash\naios hrm-hf train-actv1 \\\n  --model gpt2 \\\n  --dataset-file large_dataset.txt \\\n  --ddp \\\n  --cuda-ids \"0,1\" \\\n  --world-size 2 \\\n  --steps 10000\n```\n\n**Questions**:\n- Is dataset split across workers?\n- Is shuffling consistent?\n- What's the I/O pattern?\n\n**Note**: Not extensively tested with with multi-GB datasets\n\n---\n\n### Archive Dataset + Training\n**Status**: \u26a0\ufe0f **PARTIALLY TESTED**  \n**Supported Formats**: .tar, .tar.gz, .tar.bz2, .zip\n\n**Known Issues**:\n- \u26a0\ufe0f Large archives may hang (BUG-002)\n- \u26a0\ufe0f Many small files may be slow\n\n**Note**: Not extensively tested with archive loading performance\n\n---\n\n## \ud83c\udfae GUI Feature Combinations\n\n### GUI + Background Training\n**Status**: \u26a0\ufe0f **UNTESTED**  \n**Expected**: GUI should remain responsive during training\n\n**Note**: Not extensively tested with GUI responsiveness during training\n\n---\n\n### GUI + Multi-GPU\n**Status**: \u2753 **EXPERIMENTAL**  \n**Question**: Does GUI support multi-GPU configuration?\n\n**Note**: Not extensively tested with GUI multi-GPU controls\n\n---\n\n### GUI + Long Training\nStatus varies by machine. For multi\u2011day runs, prefer CLI logging to `--log-file` and view metrics separately.\n\n---\n\n## \ud83e\uddea Testing Recommendations\n\n### High Priority Tests:\n\n1. **DDP Verification** (3 tests)\n   - DDP + basic training\n   - DDP + memory optimizations\n   - DDP + MoE\n\n2. **DeepSpeed Verification** (3 tests)\n   - ZeRO-1 basic\n   - ZeRO-2 with AMP\n   - ZeRO-3 maximum reduction\n\n3. **Chunking Verification** (3 tests)\n   - Verify implementation exists\n   - Test 8K context\n   - Test 16K context\n\n4. **Tokenizer Testing** (7 tests)\n   - Test each \"supported\" tokenizer\n\n5. **MoE Combinations** (3 tests)\n   - MoE + memory opts\n   - MoE + multi-GPU\n   - MoE + long context\n\n### Medium Priority Tests:\n\n1. **Long Context** (3 tests)\n   - 2K, 4K, 8K without chunking\n   - Measure actual limits\n\n2. **Dataset Formats** (3 tests)\n   - Large CSV\n   - Large archive\n   - Many small files\n\n3. **Feature Interactions** (5 tests)\n   - All memory opts combined\n   - Multi-GPU + all opts\n   - MoE + all opts\n\n### Low Priority Tests:\n\n1. **GUI** (3 tests)\n   - Long training responsiveness\n   - Multi-GPU controls\n   - All panels working\n\n2. **Edge Cases** (5 tests)\n   - Very small models\n   - Very large models\n   - Very long contexts\n   - Very large batches\n   - Very small batches\n\n---\n\n## \ud83d\udccb Compatibility Matrix\n\n### Quick Reference Table\n\n| Feature 1 | Feature 2 | Status | Notes |\n|-----------|-----------|--------|-------|\n| Gradient Checkpointing | AMP | \u2705 Verified | ~60\u201370% memory reduction |\n| Gradient Checkpointing | 8\u2011bit Optimizer | \u2705 Supported | Requires bitsandbytes + CUDA |\n| AMP | 8\u2011bit Optimizer | \u2705 Supported | Common combo |\n| All Memory Opts | Combined | \u26a0\ufe0f Partial | Chunking + AMP + Checkpointing + 8\u2011bit supported; tune chunk size |\n| DDP (Linux) | Gradient Checkpointing | \u2705 Supported | Use `--ddp` + `--world-size` |\n| DDP (Linux) | AMP | \u2705 Supported | |\n| DDP (Linux) | 8\u2011bit Optimizer | \u2753 Unknown | May conflict with BnB; test on your setup |\n| Parallel\u2011Independent (Windows) | Chunking | \u2705 Supported | Windows\u2011friendly multi\u2011GPU |\n| DeepSpeed (Linux) | Gradient Checkpointing | \u2705 Supported | Requires DeepSpeed install |\n| DeepSpeed (Linux) | AMP | \u2705 Supported | |\n| DeepSpeed (Linux) | 8\u2011bit Optimizer | \u2753 Unknown | DeepSpeed optimizer mgmt may conflict |\n| MoE | Memory Opts | \u2705 Supported | Start conservative: k=2, capacity 1.25 |\n| MoE | DDP/DeepSpeed | \u2753 Needs Verify | Routing/load\u2011balance interactions |\n| Chunking | Long Context | \u2705 Supported | Use 1024\u20132048 chunk sizes |\n| FlashAttention (Linux) | AMP | \u2705 Supported | When FA2 installed; falls back to SDPA otherwise |\n| FlashAttention (Windows) | Any | \u26a0\ufe0f Platform | Often unavailable; rely on SDPA + window\u2011size |\n\n---\n\n## \ud83c\udfaf Action Items\n\n### Immediate (Week 1):\n1. \u2705 Document all known combinations\n2. \u23f3 Verify DDP implementation\n3. \u23f3 Verify DeepSpeed implementation\n4. \u23f3 Verify chunking implementation\n\n### Short-term (Week 2-3):\n1. Test all memory optimization combinations\n2. Test DDP with various configurations\n3. Test DeepSpeed stages\n4. Test tokenizers\n\n### Medium-term (Week 4-6):\n1. Test MoE combinations\n2. Test long context scenarios\n3. Test dataset formats\n4. Create automated combination tests\n\n### Long-term (Month 2+):\n1. Create CI/CD for combination testing\n2. Add performance benchmarks\n3. Document optimal combinations for different use cases\n4. Create combination recommendation tool\n\n---\n\n## \ud83d\udcda Related Documents\n\n- [COMPLETE_FEATURE_INDEX.md](./COMPLETE_FEATURE_INDEX.md) \u2013 Complete feature list\n- [FLASH_ATTENTION.md](./FLASH_ATTENTION.md) \u2022 [FLASH_ATTENTION_VS_CHUNKING.md](./FLASH_ATTENTION_VS_CHUNKING.md)\n- [PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md](./PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md)\n- [MULTI_GPU_DISTRIBUTED.md](./MULTI_GPU_DISTRIBUTED.md)\n- [LORA_PEFT.md](./LORA_PEFT.md)\n- [DYNAMIC_SUBBRAINS_MOE.md](./DYNAMIC_SUBBRAINS_MOE.md)\n\n---\n\n**Matrix Version**: 1.0  \n**Last Updated**: October 18, 2025  \n**Maintained By**: Testing Team\n\n**Status**: \ud83d\udd04 In Progress - Many combinations need verification\n", "tags": ["datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Feature Combination Matrix - AI-OS"}, {"line": 10, "text": "\ud83d\udcca Status Legend"}, {"line": 22, "text": "\ud83d\udd2c Memory Optimization Combinations"}, {"line": 24, "text": "Gradient Checkpointing + AMP"}, {"line": 49, "text": "Gradient Checkpointing + AMP + 8-bit Optimizer"}, {"line": 79, "text": "Gradient Checkpointing + Long Context"}, {"line": 104, "text": "All Memory Optimizations Combined"}, {"line": 135, "text": "\ud83d\ude80 Multi-GPU Combinations"}, {"line": 137, "text": "DDP + Gradient Checkpointing"}, {"line": 162, "text": "DDP + AMP"}, {"line": 182, "text": "DDP + All Memory Optimizations"}, {"line": 213, "text": "\ud83e\udde0 DeepSpeed Combinations"}, {"line": 215, "text": "DeepSpeed ZeRO-1 + Gradient Checkpointing"}, {"line": 237, "text": "DeepSpeed ZeRO-2 + AMP"}, {"line": 256, "text": "DeepSpeed ZeRO-3 (Maximum Memory Reduction)"}, {"line": 276, "text": "DeepSpeed + 8-bit Optimizer"}, {"line": 297, "text": "\ud83e\udde9 MoE / Dynamic Subbrains Combinations"}, {"line": 299, "text": "MoE + Gradient Checkpointing"}, {"line": 319, "text": "MoE + AMP + 8-bit"}, {"line": 340, "text": "Expert Training + Memory Optimizations"}, {"line": 360, "text": "MoE + Multi-GPU"}, {"line": 386, "text": "MoE + DeepSpeed"}, {"line": 406, "text": "\ud83d\udcda Context Length Combinations"}, {"line": 408, "text": "Long Context + Chunking"}, {"line": 437, "text": "Long Context + Multi-GPU"}, {"line": 459, "text": "FlashAttention + Memory/Chunking"}, {"line": 468, "text": "\ud83d\udd24 Tokenizer Combinations"}, {"line": 470, "text": "Custom Tokenizer + Training"}, {"line": 490, "text": "Large Vocabulary + Memory Optimizations"}, {"line": 514, "text": "\ud83d\udcca Dataset Format Combinations"}, {"line": 516, "text": "Streaming Dataset + Linear Mode"}, {"line": 530, "text": "Large Dataset + Multi-GPU"}, {"line": 554, "text": "Archive Dataset + Training"}, {"line": 566, "text": "\ud83c\udfae GUI Feature Combinations"}, {"line": 568, "text": "GUI + Background Training"}, {"line": 576, "text": "GUI + Multi-GPU"}, {"line": 584, "text": "GUI + Long Training"}, {"line": 589, "text": "\ud83e\uddea Testing Recommendations"}, {"line": 591, "text": "High Priority Tests:"}, {"line": 616, "text": "Medium Priority Tests:"}, {"line": 632, "text": "Low Priority Tests:"}, {"line": 648, "text": "\ud83d\udccb Compatibility Matrix"}, {"line": 650, "text": "Quick Reference Table"}, {"line": 673, "text": "\ud83c\udfaf Action Items"}, {"line": 675, "text": "Immediate (Week 1):"}, {"line": 681, "text": "Short-term (Week 2-3):"}, {"line": 687, "text": "Medium-term (Week 4-6):"}, {"line": 693, "text": "Long-term (Month 2+):"}, {"line": 701, "text": "\ud83d\udcda Related Documents"}]}, {"path": "guide/features/FLASH_ATTENTION.md", "content": "# Flash Attention 2: Window Size Guide\n\nThis feature can be toggled via the training CLI or GUI:\n- CLI: enable optimized kernels (if available) and optionally set a sliding window:\n    ```powershell\n    aios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 10 --batch-size 1 --amp --gradient-checkpointing --window-size 2048 --log-file artifacts/brains/actv1/metrics.jsonl\n    ```\n    Note: FA2 usage is environment-dependent; when unavailable, PyTorch SDPA is used as a fallback. Windowing works with FA2 or SDPA.\n- GUI: \u201cFlashAttn-2\u201d checkbox and \u201cWindow Size\u201d field (see GUI Features \u2192 Training panel optimizations)\n\nThis page complements the canonical attention-optimization feature doc:\n- Canonical: [FLASH_ATTENTION_VS_CHUNKING.md](FLASH_ATTENTION_VS_CHUNKING.md)\n\n## What is Window Size?\n\n**Window size** is NOT about enabling Flash Attention - it's about **limiting attention range** using a sliding window.\n\n### Sliding Window Attention\n\nInstead of each token attending to ALL previous tokens (full attention), it only attends to the N most recent tokens.\n\n```\nFull Attention (window_size = None or 0):\nToken 1000 can attend to: Token 1, 2, 3, ..., 999, 1000 (all 1000 tokens)\n\nSliding Window (window_size = 512):\nToken 1000 can attend to: Token 488, 489, ..., 999, 1000 (only 512 tokens)\n```\n\n## Why Use Sliding Window?\n\n### Benefits\n\u2705 **Reduced memory** - Less attention computation\n\u2705 **Faster training** - Fewer attention scores to compute\n\u2705 **Enables longer contexts** - Can fit more tokens in VRAM\n\u2705 **Local coherence** - Most relevant context is usually recent\n\n### Trade-offs\n\u274c **Limited long-range attention** - Can't see tokens outside window\n\u274c **May lose important context** - Earlier information might be needed\n\u274c **Not suitable for all tasks** - Some tasks need full context\n\n## Choosing the Right Window Size\n\n### Decision Matrix\n\n| Context Length | Recommended Window | Reasoning |\n|----------------|-------------------|-----------|\n| **< 2K tokens** | `None` (full) | No need for windowing, fits easily |\n| **2K-8K tokens** | `None` or `2048` | Full attention works fine |\n| **8K-16K tokens** | `2048-4096` | Balance memory and context |\n| **16K-32K tokens** | `1024-2048` | Need windowing for efficiency |\n| **32K-64K tokens** | `512-1024` | Aggressive windowing needed |\n| **64K-100K tokens** | `256-512` | Very aggressive windowing |\n| **100K+ tokens** | `256` | Maximum memory savings |\n\n### Rule of Thumb\n\n```python\nif context_length < 8192:\n    window_size = None  # Full attention\nelif context_length < 32768:\n    window_size = 2048  # Moderate window\nelse:\n    window_size = 512   # Aggressive window\n```\n\n## Window Size vs Context Length\n\n**IMPORTANT**: Window size is NOT the same as max sequence length!\n\n```\nmax_seq_len = 50000    # How many tokens to train on\nwindow_size = 512      # How far each token can \"see\" back\n\nExample with 50K tokens:\n\u251c\u2500 Token 1:    Sees tokens 1 (only itself)\n\u251c\u2500 Token 100:  Sees tokens 1-100 (all previous, window not limiting yet)\n\u251c\u2500 Token 1000: Sees tokens 488-1000 (512 token window)\n\u2514\u2500 Token 50000: Sees tokens 49488-50000 (512 token window)\n```\n\n## Practical Examples\n\n### Example 1: Short Story (4K tokens)\n```yaml\nmax_seq_len: 4096\nwindow_size: None  # Full attention - story is short enough\nuse_flash_attn: True  # Enable Flash Attention for speed\n```\n**Result**: Each word can see the ENTIRE story\n\n### Example 2: Long Document (32K tokens)\n```yaml\nmax_seq_len: 32768\nwindow_size: 2048  # Sliding window - see ~2K tokens back\nuse_flash_attn: True  # Enable Flash Attention for efficiency\n```\n**Result**: Each word sees ~2K tokens of recent context\n\n### Example 3: Extreme Context (100K tokens)\n```yaml\nmax_seq_len: 100000\nwindow_size: 512   # Very limited window - memory constrained\nuse_flash_attn: True  # Enable Flash Attention\nuse_chunked_training: True  # Also enable chunking\nchunk_size: 2048   # Process in 2048-token chunks\n```\n**Result**: Each word sees only ~500 tokens back, processed in chunks\n\n## Window Size for Different Tasks\n\n### Full Attention (window_size = None)\n**Best for**:\n- Short contexts (< 8K tokens)\n- Tasks requiring global understanding\n- Document classification\n- Sentiment analysis\n\n### Medium Window (1024-4096)\n**Best for**:\n- Long documents (8K-32K tokens)\n- Story writing\n- Technical documentation\n- Most training scenarios\n\n### Small Window (256-512)\n**Best for**:\n- Extreme contexts (50K+ tokens)\n- Memory-constrained scenarios\n- Stream-of-consciousness text\n- Chat logs\n\n## How Flash Attention Uses Window Size\n\n### With Flash Attention 2 Enabled\n\n```python\nif window_size is not None:\n    # Flash Attention uses efficient sliding window\n    window = (window_size - 1, 0)  # Look back window_size-1 tokens\n    output = flash_attn_func(q, k, v, causal=True, window_size=window)\nelse:\n    # Flash Attention with full attention\n    output = flash_attn_func(q, k, v, causal=True)\n```\n\n### Without Flash Attention (Fallback)\n\n```python\nif window_size is not None:\n    # PyTorch SDPA with manual mask (less efficient)\n    mask = create_sliding_window_mask(window_size)\n    output = scaled_dot_product_attention(q, k, v, attn_mask=mask)\nelse:\n    # PyTorch SDPA with full attention\n    output = scaled_dot_product_attention(q, k, v, is_causal=True)\n```\n\n**Flash Attention is MORE EFFICIENT at sliding windows** - another reason to use it!\n\n## Common Misconceptions\n\n### \u274c WRONG: \"Window size is how many tokens I can train on\"\n\u2705 CORRECT: Window size is how far back each token can attend. You can train on 100K tokens with a 512 window.\n\n### \u274c WRONG: \"Larger window always better\"\n\u2705 CORRECT: Larger window uses more memory. Choose based on what your task needs and memory allows.\n\n### \u274c WRONG: \"Window size enables Flash Attention\"\n\u2705 CORRECT: Window size is a parameter TO Flash Attention. The checkbox enables it, window size configures it.\n\n### \u274c WRONG: \"I need window_size = max_seq_len\"\n\u2705 CORRECT: That's just full attention. Use `window_size = None` instead.\n\n## Testing Window Sizes\n\n### Start Conservative\n1. Begin with **no window** (full attention) for short contexts\n2. If OOM, enable window at **max_seq_len / 4**\n3. Gradually reduce window if still OOM\n4. Monitor training quality - smaller windows may reduce accuracy\n\n### Monitor Impact\n```python\n# Log attention range\neffective_context = min(window_size or max_seq_len, max_seq_len)\nprint(f\"Each token attends to {effective_context} previous tokens\")\n```\n\n## GUI Settings\n\n### Flash Attention Checkbox\n- **Checked**: Use Flash Attention 2 (if available)\n- **Unchecked**: Use PyTorch SDPA fallback\n\n### Window Size Entry\n- **Empty or 0**: Full attention (no window)\n- **256-8192**: Sliding window size in tokens\n- **Default: 512**: Good balance for long contexts\n\n### Recommended Combinations\n\n```\nShort context (< 8K):\n\u2611 FlashAttn-2  Window: [    ] (empty/full attention)\n\nMedium context (8K-32K):\n\u2611 FlashAttn-2  Window: [2048]\n\nLong context (32K-64K):\n\u2611 FlashAttn-2  Window: [1024]\n\u2611 Context Chunking  Chunk Size: [4096]\n\nExtreme context (100K+):\n\u2611 FlashAttn-2  Window: [512]\n\u2611 Context Chunking  Chunk Size: [2048]\n```\n\n## Performance Impact\n\n### Memory Usage (50K token sequence)\n\n| Configuration | VRAM Usage | Speed |\n|--------------|------------|-------|\n| Full Attn, No Flash | ~20GB \u274c | Baseline |\n| Full Attn + Flash | ~4GB \u2705 | +30% faster |\n| Window 2048 + Flash | ~2GB \u2705 | +50% faster |\n| Window 512 + Flash | ~1GB \u2705 | +80% faster |\n\n### Accuracy Impact\n\n```\nWindow Size vs Task Performance:\n- Full attention: 100% baseline accuracy\n- Window 4096: 99-100% (minimal impact)\n- Window 2048: 95-99% (slight impact on long-range tasks)\n- Window 512: 90-95% (noticeable for tasks needing full context)\n- Window 256: 85-90% (significant for most tasks)\n```\n\n## Summary\n\n| Parameter | Purpose | Values | Default |\n|-----------|---------|--------|---------|\n| **use_flash_attn** | Enable Flash Attention | True/False | Should be True (GUI checkbox) |\n| **window_size** | Sliding window size | None or 256-8192 | 512 |\n| **max_seq_len** | Total sequence length | Any | 2048 |\n\n**Key Insight**: Window size is about **local vs global attention**, not about enabling Flash Attention. The checkbox enables Flash Attention, the window size configures how it attends.\n\n**Recommendation**: \n- Enable Flash Attention checkbox (for speed)\n- Set window_size based on your context length and memory\n- Use \"Optimize Settings\" button to find optimal values\n", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Flash Attention 2: Window Size Guide"}, {"line": 13, "text": "What is Window Size?"}, {"line": 17, "text": "Sliding Window Attention"}, {"line": 29, "text": "Why Use Sliding Window?"}, {"line": 31, "text": "Benefits"}, {"line": 37, "text": "Trade-offs"}, {"line": 42, "text": "Choosing the Right Window Size"}, {"line": 44, "text": "Decision Matrix"}, {"line": 56, "text": "Rule of Thumb"}, {"line": 67, "text": "Window Size vs Context Length"}, {"line": 82, "text": "Practical Examples"}, {"line": 84, "text": "Example 1: Short Story (4K tokens)"}, {"line": 92, "text": "Example 2: Long Document (32K tokens)"}, {"line": 100, "text": "Example 3: Extreme Context (100K tokens)"}, {"line": 110, "text": "Window Size for Different Tasks"}, {"line": 112, "text": "Full Attention (window_size = None)"}, {"line": 119, "text": "Medium Window (1024-4096)"}, {"line": 126, "text": "Small Window (256-512)"}, {"line": 133, "text": "How Flash Attention Uses Window Size"}, {"line": 135, "text": "With Flash Attention 2 Enabled"}, {"line": 139, "text": "Flash Attention uses efficient sliding window"}, {"line": 143, "text": "Flash Attention with full attention"}, {"line": 147, "text": "Without Flash Attention (Fallback)"}, {"line": 151, "text": "PyTorch SDPA with manual mask (less efficient)"}, {"line": 155, "text": "PyTorch SDPA with full attention"}, {"line": 161, "text": "Common Misconceptions"}, {"line": 163, "text": "\u274c WRONG: \"Window size is how many tokens I can train on\""}, {"line": 166, "text": "\u274c WRONG: \"Larger window always better\""}, {"line": 169, "text": "\u274c WRONG: \"Window size enables Flash Attention\""}, {"line": 172, "text": "\u274c WRONG: \"I need window_size = max_seq_len\""}, {"line": 175, "text": "Testing Window Sizes"}, {"line": 177, "text": "Start Conservative"}, {"line": 183, "text": "Monitor Impact"}, {"line": 185, "text": "Log attention range"}, {"line": 190, "text": "GUI Settings"}, {"line": 192, "text": "Flash Attention Checkbox"}, {"line": 196, "text": "Window Size Entry"}, {"line": 201, "text": "Recommended Combinations"}, {"line": 219, "text": "Performance Impact"}, {"line": 221, "text": "Memory Usage (50K token sequence)"}, {"line": 230, "text": "Accuracy Impact"}, {"line": 241, "text": "Summary"}]}, {"path": "guide/features/FLASH_ATTENTION_VS_CHUNKING.md", "content": "# Flash Attention 2 vs Context Chunking: Technical Deep Dive\n\nNote: Canonical source of truth for attention optimization in AI-OS. Window sizing guidance is a sub-topic; see `FLASH_ATTENTION.md`.\n\n## Executive Summary\n\n**Flash Attention 2** and **Context Chunking** solve different problems in the memory hierarchy:\n- **Flash Attention 2**: Optimizes the *attention computation algorithm itself* (compute-level optimization)\n- **Context Chunking**: Manages *sequence length* when even optimized attention can't fit in VRAM (data-level optimization)\n\nThey're complementary because Flash Attention 2 makes each chunk more efficient, and chunking makes Flash Attention 2 viable for extreme contexts.\n\n---\n\n## The Memory Problem: Understanding the Layers\n\n### Standard Attention Memory Usage\n\nFor a sequence of length `N` with `d` dimensions:\n\n```\nStandard Attention Memory = O(N\u00b2)\n```\n\n**Example: 50K token sequence**\n- Attention matrix: 50,000 \u00d7 50,000 = 2.5 billion entries\n- At fp16 (2 bytes): **5GB just for attention scores**\n- Plus gradients, activations, KV cache: **~20GB total**\n\nThis is why long contexts OOM (Out Of Memory).\n\n---\n\n## Solution 1: Flash Attention 2 (Algorithm Optimization)\n\n### What It Does\n\nFlash Attention 2 **changes how attention is computed** to avoid materializing the full N\u00d7N matrix in VRAM.\n\n### Key Innovation: Tiling + Recomputation\n\nInstead of computing the full attention matrix:\n1. **Tiles attention into blocks** (e.g., 128\u00d7128 chunks)\n2. **Streams through HBM** (High Bandwidth Memory) efficiently\n3. **Recomputes values** instead of storing intermediate results\n4. **Fuses operations** to minimize memory reads/writes\n\n```python\n# Standard Attention (simplified)\nQ, K, V = input.chunk(3)\nscores = Q @ K.T  # N\u00d7N matrix materialized in VRAM \u274c\nattn = softmax(scores)\noutput = attn @ V\n\n# Flash Attention 2 (simplified concept)\noutput = flash_attn_func(Q, K, V)  # Never materializes N\u00d7N \u2705\n# Internally uses tiling: processes 128\u00d7128 blocks at a time\n```\n\n### Memory Reduction\n\n```\nFlash Attention 2 Memory = O(N)\n```\n\n**Same 50K example:**\n- No full attention matrix stored\n- Memory: **~2-4GB** instead of 20GB\n- Can handle **2-3x longer contexts** with same VRAM\n\n### What It DOESN'T Solve\n\nFlash Attention 2 still needs to:\n- Store full input sequence (50K tokens \u00d7 embedding dimension)\n- Store full gradients during backprop\n- Store model activations for each token\n\n**Limit: ~16K-32K tokens on consumer GPUs (24GB VRAM)**\n\n---\n\n## Solution 2: Context Chunking (Sequence Management)\n\n### What It Does\n\nContext Chunking **splits the sequence into smaller pieces** and processes them separately.\n\n### How It Works\n\n```python\n# Without Chunking (standard)\ninput_tokens = [1, 2, 3, 4, ..., 50000]  # All at once\noutput = model(input_tokens)  # OOM! \u274c\n\n# With Chunking\nchunk_1 = [1, 2, ..., 2048]      # Process first 2048 tokens\noutput_1, hidden_state_1 = model(chunk_1)\n\nchunk_2 = [2049, 2050, ..., 4096]  # Process next 2048 tokens\noutput_2, hidden_state_2 = model(chunk_2, carry=hidden_state_1)\n\n# Continue for all chunks...\n```\n\n### Key Features\n\n1. **Recurrent State Passing**: Hidden states carry context between chunks\n2. **Gradient Accumulation**: Gradients accumulated across chunks\n3. **CPU Offloading**: Can move carry states to CPU for extreme contexts (100K+)\n\n### Memory Reduction\n\n```\nChunked Training Memory = O(chunk_size)\n```\n\n**Same 50K example with 2048 chunk size:**\n- Only 2048 tokens in VRAM at once\n- Rest stored in RAM or on disk\n- Memory: **~1-2GB** (very conservative)\n\n### What It DOESN'T Solve\n\n- **Slower training**: Processing chunks sequentially has overhead (~10-20%)\n- **Reduced parallelism**: Can't process all tokens in parallel\n- **Potential context loss**: Chunks may have limited view of full context\n\n---\n\n## Why They're Complementary\n\n### Scenario 1: Medium Context (8K-16K tokens)\n\n**Flash Attention 2 ONLY:**\n```\nSequence: 16K tokens\nFlash Attention: 16K fits comfortably in VRAM \u2705\nChunking: NOT NEEDED\nResult: Fast, efficient training\n```\n\n### Scenario 2: Large Context (32K-64K tokens)\n\n**Flash Attention 2 + Chunking:**\n```\nSequence: 64K tokens\nFlash Attention: Each chunk processed efficiently\nChunking: 64K \u00f7 2048 = 32 chunks\nResult: Feasible on 24GB GPU\n```\n\nWithout Flash Attention:\n- Standard attention on 2048-token chunks would still use more memory\n- Training would be even slower\n\n### Scenario 3: Extreme Context (100K+ tokens)\n\n**Flash Attention 2 + Aggressive Chunking + CPU Offload:**\n```\nSequence: 100K tokens\nFlash Attention: Optimizes each 512-token chunk\nChunking: 100K \u00f7 512 = 195 chunks\nCPU Offload: Carry states stored in RAM\nResult: Possible on consumer hardware (slow but works)\n```\n\n---\n\n## Visual Comparison\n\n### Memory Usage Hierarchy\n\n```\nSame 50K Token Sequence:\n\nStandard Attention (NO CHUNKING)\n\u251c\u2500 Attention Matrix: 5GB\n\u251c\u2500 Activations: 8GB\n\u251c\u2500 Gradients: 7GB\n\u2514\u2500 Total: ~20GB \u274c OOM on 24GB GPU\n\nFlash Attention 2 (NO CHUNKING)\n\u251c\u2500 No Attention Matrix: 0GB (tiled)\n\u251c\u2500 Activations: 2GB (optimized)\n\u251c\u2500 Gradients: 2GB (optimized)\n\u2514\u2500 Total: ~4GB \u2705 Fits, but limited headroom\n\nStandard Attention + CHUNKING (2048 chunks)\n\u251c\u2500 Attention Matrix: 200MB (per chunk)\n\u251c\u2500 Activations: 400MB (per chunk)\n\u251c\u2500 Gradients: 300MB (per chunk)\n\u2514\u2500 Total: ~1GB \u2705 Fits, but SLOW\n\nFlash Attention 2 + CHUNKING (2048 chunks)\n\u251c\u2500 No Attention Matrix: 0GB\n\u251c\u2500 Activations: 80MB (per chunk, optimized)\n\u251c\u2500 Gradients: 60MB (per chunk, optimized)\n\u2514\u2500 Total: ~200MB \u2705 Fits, FASTER than standard chunking\n```\n\n---\n\n## Practical Decision Matrix\n\n| Context Length | GPU VRAM | Recommendation | Why |\n|----------------|----------|----------------|-----|\n| **< 4K tokens** | Any | Flash Attention 2 | No chunking needed, maximum speed |\n| **4K-16K tokens** | 24GB+ | Flash Attention 2 | Fits comfortably without chunking |\n| **4K-16K tokens** | 12GB | Flash Attention 2 + Optional Chunking | Test first; chunk if needed |\n| **16K-32K tokens** | 24GB+ | Flash Attention 2 + Light Chunking (4096) | Balance speed and memory |\n| **32K-64K tokens** | 24GB | Flash Attention 2 + Chunking (2048) | Flash Attention makes chunks efficient |\n| **64K-100K tokens** | 24GB | Flash Attention 2 + Aggressive Chunking (512-1024) | Extreme context requires both |\n| **100K+ tokens** | 24GB | Flash Attention 2 + Ultra-Aggressive Chunking (256-512) + CPU Offload | Maximum memory savings |\n\n---\n\n## Code Example: How They Work Together\n\n```python\n# In aios/core/hrm_models/impl/layers.py\nclass HRMAttention:\n    def forward(self, hidden_states):\n        # Flash Attention 2 optimizes THIS computation\n        try:\n            from flash_attn import flash_attn_func\n            # Even with chunking, each chunk uses Flash Attention\n            attn_output = flash_attn_func(q, k, v, causal=True)\n        except:\n            # Fallback to standard attention (slower)\n            attn_output = F.scaled_dot_product_attention(q, k, v)\n        \n        return attn_output\n\n# In aios/core/hrm_models/chunked_training.py\ndef chunked_segment_rollout(model, batch, chunk_size=2048):\n    full_sequence = batch['input_ids']  # e.g., 50K tokens\n    \n    # Split into chunks\n    for chunk_start in range(0, len(full_sequence), chunk_size):\n        chunk = full_sequence[chunk_start:chunk_start + chunk_size]\n        \n        # Each chunk STILL uses Flash Attention inside the model!\n        output, carry_state = model(chunk, carry_state=prev_carry)\n        \n        # Flash Attention makes THIS step 2-3x faster\n        loss.backward()  # Gradient computation\n        \n    return total_loss\n```\n\n---\n\n## Key Insight: Different Problems, Different Solutions\n\n### Flash Attention 2 Solves:\n\u274c **Problem**: Attention computation is memory-inefficient (O(N\u00b2))  \n\u2705 **Solution**: Smarter algorithm that avoids materializing full matrix (O(N))  \n\ud83d\udcca **Impact**: 2-3x longer contexts with same VRAM  \n\u26a1 **Speed**: Actually FASTER than standard attention  \n\n### Context Chunking Solves:\n\u274c **Problem**: Even optimized attention can't fit 100K tokens in VRAM  \n\u2705 **Solution**: Process sequence in smaller pieces  \n\ud83d\udcca **Impact**: Unlimited context length (constrained by time, not memory)  \n\u26a1 **Speed**: 10-20% slower due to sequential processing  \n\n---\n\n## Analogy: Moving a Mountain\n\n**Problem**: Move 100 tons of rocks from point A to B\n\n**Flash Attention 2 = Better Truck**\n- Upgraded from 1-ton truck to 3-ton truck\n- Each trip carries 3x more rocks\n- Same number of trips needed, but faster per trip\n- **Benefit**: Can move 3x more rocks in same time\n\n**Context Chunking = Multiple Trips**\n- Split 100 tons into 20 trips of 5 tons each\n- Make multiple trips back and forth\n- **Benefit**: Can move ANY amount (not limited by truck size)\n\n**Both Together = Best Solution**\n- Use the 3-ton truck (Flash Attention)\n- Make fewer trips (Chunking with larger chunks)\n- Result: Move 100 tons efficiently\n\n---\n\n## When to Use What\n\n### \u2705 Use Flash Attention 2 Alone\n- Context \u2264 16K tokens on 24GB GPU\n- You want maximum training speed\n- You have compatible CUDA GPU\n\n### \u2705 Use Chunking Alone (Rare)\n- Very old GPU without Flash Attention support\n- CPU-only training (Flash Attention requires CUDA)\n- Debugging/testing with small contexts\n\n### \u2705 Use Both Together (Common for Large Contexts)\n- Context > 16K tokens\n- Training on consumer GPUs (8-24GB VRAM)\n- Want balance of memory efficiency and speed\n- Extreme contexts (50K-100K+ tokens)\n\n### \u274c Use Neither (Default for Short Contexts)\n- Context \u2264 2K tokens\n- Plenty of VRAM available\n- Standard attention works fine\n\n---\n\n## Summary Table\n\n| Feature | Flash Attention 2 | Context Chunking |\n|---------|------------------|------------------|\n| **Optimization Level** | Algorithm/Compute | Data/Sequence |\n| **Memory Complexity** | O(N) from O(N\u00b2) | O(chunk_size) |\n| **Speed Impact** | Faster (+20-30%) | Slower (-10-20%) |\n| **Max Context Gain** | 2-3x | Unlimited |\n| **Hardware Requirement** | CUDA GPU | Any |\n| **When Needed** | Always beneficial | Only for very long contexts |\n| **Typical Use** | 4K-32K contexts | 32K-100K+ contexts |\n\n---\n\n## Bottom Line\n\n**Flash Attention 2** makes attention computation efficient.  \n**Context Chunking** makes extremely long sequences feasible.\n\nTogether, they enable training with **50K-100K token contexts on consumer GPUs** that would otherwise require data center hardware.\n\n**Your system now gives users full control** - they can enable chunking when needed, and Flash Attention 2 automatically optimizes whatever they choose to do. The \"Optimize Settings\" button helps find the sweet spot between memory and speed.\n\n---\n\n## How to switch between approaches (CLI)\n\n- Full attention (no window, no explicit chunking flag):\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 50 --batch-size 2 --amp --gradient-checkpointing --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n- Sliding window attention (works with FA2 or SDPA):\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 50 --batch-size 2 --amp --gradient-checkpointing --window-size 2048 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n- Long-context with dataset chunk cadence:\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 50 --batch-size 1 --amp --gradient-checkpointing --dataset-chunk-size 4000 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nNotes:\n- Windowed attention reduces attention range; dataset chunk size controls data loading/encoding cadence and memory pressure.\n- The training loop may also use internal chunking for long sequences; see Configurable Dataset Chunk Size and Parallel Training Block/Chunk System.\n\n## Measuring impact (Optimization CLI)\n\nUse the optimization CLI to gather throughput/VRAM metrics and write results under `artifacts/optimization/`.\n\n```powershell\naios optimize --model artifacts/hf_implant/base_model --batch-sizes \"1,2,4,8\" --test-duration 10 --output-dir artifacts/optimization\n```\n\nOutputs include JSON like `artifacts/optimization/results_<session>.json` and GPU metrics JSONL; compare runs with/without windowing or different dataset chunk sizes.\n", "tags": ["gui"], "headings": [{"line": 0, "text": "Flash Attention 2 vs Context Chunking: Technical Deep Dive"}, {"line": 4, "text": "Executive Summary"}, {"line": 14, "text": "The Memory Problem: Understanding the Layers"}, {"line": 16, "text": "Standard Attention Memory Usage"}, {"line": 33, "text": "Solution 1: Flash Attention 2 (Algorithm Optimization)"}, {"line": 35, "text": "What It Does"}, {"line": 39, "text": "Key Innovation: Tiling + Recomputation"}, {"line": 48, "text": "Standard Attention (simplified)"}, {"line": 54, "text": "Flash Attention 2 (simplified concept)"}, {"line": 56, "text": "Internally uses tiling: processes 128\u00d7128 blocks at a time"}, {"line": 59, "text": "Memory Reduction"}, {"line": 70, "text": "What It DOESN'T Solve"}, {"line": 81, "text": "Solution 2: Context Chunking (Sequence Management)"}, {"line": 83, "text": "What It Does"}, {"line": 87, "text": "How It Works"}, {"line": 90, "text": "Without Chunking (standard)"}, {"line": 94, "text": "With Chunking"}, {"line": 101, "text": "Continue for all chunks..."}, {"line": 104, "text": "Key Features"}, {"line": 110, "text": "Memory Reduction"}, {"line": 121, "text": "What It DOESN'T Solve"}, {"line": 129, "text": "Why They're Complementary"}, {"line": 131, "text": "Scenario 1: Medium Context (8K-16K tokens)"}, {"line": 141, "text": "Scenario 2: Large Context (32K-64K tokens)"}, {"line": 155, "text": "Scenario 3: Extreme Context (100K+ tokens)"}, {"line": 168, "text": "Visual Comparison"}, {"line": 170, "text": "Memory Usage Hierarchy"}, {"line": 202, "text": "Practical Decision Matrix"}, {"line": 216, "text": "Code Example: How They Work Together"}, {"line": 219, "text": "In aios/core/hrm_models/impl/layers.py"}, {"line": 222, "text": "Flash Attention 2 optimizes THIS computation"}, {"line": 225, "text": "Even with chunking, each chunk uses Flash Attention"}, {"line": 228, "text": "Fallback to standard attention (slower)"}, {"line": 233, "text": "In aios/core/hrm_models/chunked_training.py"}, {"line": 237, "text": "Split into chunks"}, {"line": 241, "text": "Each chunk STILL uses Flash Attention inside the model!"}, {"line": 244, "text": "Flash Attention makes THIS step 2-3x faster"}, {"line": 252, "text": "Key Insight: Different Problems, Different Solutions"}, {"line": 254, "text": "Flash Attention 2 Solves:"}, {"line": 260, "text": "Context Chunking Solves:"}, {"line": 268, "text": "Analogy: Moving a Mountain"}, {"line": 290, "text": "When to Use What"}, {"line": 292, "text": "\u2705 Use Flash Attention 2 Alone"}, {"line": 297, "text": "\u2705 Use Chunking Alone (Rare)"}, {"line": 302, "text": "\u2705 Use Both Together (Common for Large Contexts)"}, {"line": 308, "text": "\u274c Use Neither (Default for Short Contexts)"}, {"line": 315, "text": "Summary Table"}, {"line": 329, "text": "Bottom Line"}, {"line": 340, "text": "How to switch between approaches (CLI)"}, {"line": 361, "text": "Measuring impact (Optimization CLI)"}]}, {"path": "guide/features/GUI_FEATURES.md", "content": "# GUI Features (Desktop)\n\nPurpose: Visual management for training, brains, chat, datasets, resources, and early Subbrains (experts) administration.\n\nStatus: Implemented core panels. Subbrains Manager is WIP (view and refresh registry, actions are placeholders).\n\nKey modules:\n- App bootstrap and panel wiring: `src/aios/gui/app/app_main.py`, `src/aios/gui/app/panel_setup.py`, `src/aios/gui/app/logging_setup.py`\n- Services: `src/aios/gui/services/__init__.py`, `router.py`, `log_router.py`\n- Panels:\n\t- HRM Training: `src/aios/gui/components/hrm_training_panel/`\n\t- Brains: `src/aios/gui/components/brains_panel/`\n\t- Rich Chat: `src/aios/gui/components/rich_chat_panel/`\n\t- Datasets: `src/aios/gui/components/datasets_panel/`\n\t- Subbrains Manager (WIP): `src/aios/gui/components/subbrains_manager_panel/`\n\t- Resources: `src/aios/gui/components/resources_panel/`\n\t- Settings/Status: `src/aios/gui/components/*`\n\nSee also:\n- Core training: `CORE_TRAINING.md`\n- Dynamic Subbrains (MoE): `DYNAMIC_SUBBRAINS_MOE.md`\n- Memory optimization: `MEMORY_OPTIMIZATION.md`\n- Multi-GPU and parallel chunk system: `MULTI_GPU_DISTRIBUTED.md`, `PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md`\n\n## Panels and flows\n\n### HRM Training panel\n- Create/train ACTv1 brains with presets, VRAM estimator, logs, and stop/resume.\n- MoE awareness in estimator (shows total vs active params).\n- Where: `src/aios/gui/components/hrm_training_panel/`\n- Screenshot: `../../Screenshots/training_tab.png`\n\nTips (Windows):\n- Prefer \u201cParallel Independent\u201d mode for multi-GPU on Windows (DDP is CUDA-only and more finicky). See `MULTI_GPU_DISTRIBUTED.md`.\n- Use Optimize to search max context/batch fitting VRAM.\n\n### Brains panel\n- List brains (bundles), load/unload, view details, export/import.\n- Scans `artifacts/brains` and temporary \u201crouter-generated\u201d brains.\n- Where: `src/aios/gui/components/brains_panel/`\n- Screenshot: `../../Screenshots/brains_tab.png`\n\n### Rich Chat panel\n- Chat with loaded brain, Markdown and code highlighting.\n- Routing output formatter is shared service (`router.py`).\n- Where: `src/aios/gui/components/rich_chat_panel/`\n- Screenshot: `../../Screenshots/chat_tab.png`\n\n### Datasets panel\n- Discover/download datasets, verify metadata, manage cache.\n- Where: `src/aios/gui/components/datasets_panel/`\n- Screenshot: `../../Screenshots/datasets_tab.png`\n\n### Resources panel\n- CPU/GPU utilization targets, throttle modes, real-time stats for training.\n- Where: `src/aios/gui/components/resources_panel/`\n- Screenshot: `../../Screenshots/resources_tab.png`\n\n### Subbrains Manager (WIP)\n- View expert registry with counts (total/active/frozen), hierarchy, average routing weight, activations.\n- Refresh reads `artifacts/experts/registry.json`. Actions like create/delete/freeze and goal linking currently print \u201cCLI command needed\u201d. Use CLI instead (see `DYNAMIC_SUBBRAINS_MOE.md`).\n- Where: `src/aios/gui/components/subbrains_manager_panel/`\n- Screenshots:\n\t- (WIP) No screenshot yet; see Brains/Resources tabs for related flows\n\n### Debug panel\n- Centralized debug output with categorized logs via LogRouter; helpful during training/inference.\n- Where: `src/aios/gui/app/logging_setup.py` registers the log router; debug UI integrated in app.\n- Screenshot: `../../Screenshots/debug_tab.png`\n\n### Evaluation panel\n- Run evaluation suites and view results; compare runs.\n- Where: `src/aios/gui/components/evaluation_panel/` (if separated) or integrated evaluation view.\n- Screenshot: `../../Screenshots/evaluation_tab.png`\n\n### Tools & MCP panel\n- Manage Tools and MCP servers; configure and observe status.\n- Where: `src/aios/gui/components/tools_panel/`, `src/aios/gui/components/mcp_panel/` (module names may vary by integration status)\n- Screenshots:\n\t- Tools: `../../Screenshots/mcp&tools_tools_tab.png`\n\t- MCP: `../../Screenshots/mcp&tools_mcp_tab.png`\n\n### Settings and Themes\n- Configure themes, fonts, logging level, cache, and other app settings.\n- Where: `src/aios/gui/components/settings_panel/`\n- Screenshots:\n\t- Settings: `../../Screenshots/settings_tab.png`\n\t- Themes: `../../Screenshots/darkmode_theme.png`, `../../Screenshots/halloween_theme.png`, `../../Screenshots/barbie_theme.png`, `../../Screenshots/matrix_theme.png`\n\n## Running the GUI\n\nFrom a configured environment, start the app:\n\n```powershell\naios gui\n```\n\nIf you need to pin a specific Python interpreter, ensure `.venv` is activated first.\n\n## Logging and routing\n- Unified log router shows categorized logs across panels (chat, training, dataset, error).\n- Training logs are written to JSONL files you select in the training panel; many features (MoE, PEFT, memory) surface metrics there.\n\n## Notes\n- Subbrains actions are WIP; rely on CLI for expert training and goals linking.\n- GPU stats charts may update in bursts; for stable readings, let training run for a few dozen steps.\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "experts", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "GUI Features (Desktop)"}, {"line": 24, "text": "Panels and flows"}, {"line": 26, "text": "HRM Training panel"}, {"line": 36, "text": "Brains panel"}, {"line": 42, "text": "Rich Chat panel"}, {"line": 48, "text": "Datasets panel"}, {"line": 53, "text": "Resources panel"}, {"line": 58, "text": "Subbrains Manager (WIP)"}, {"line": 65, "text": "Debug panel"}, {"line": 70, "text": "Evaluation panel"}, {"line": 75, "text": "Tools & MCP panel"}, {"line": 82, "text": "Settings and Themes"}, {"line": 89, "text": "Running the GUI"}, {"line": 99, "text": "Logging and routing"}, {"line": 103, "text": "Notes"}]}, {"path": "guide/features/LORA_PEFT.md", "content": "# LoRA/PEFT Comprehensive Analysis for AI-OS\n\nNote: Canonical source of truth for LoRA/PEFT in AI-OS. Other LoRA/PEFT docs in this folder have been consolidated into this page.\n\nQuick links:\n- Quick start presets: see Configuration Presets\n- Parameter impact overview: see PEFT Methods Comparison and Target Modules Explained\n- Troubleshooting and validation: see Testing & Validation\n\n**Date:** October 19, 2025  \n**System:** AI-OS HRM ACTv1 Training  \n**PEFT Library:** Hugging Face PEFT (v0.11.1+)\n\n---\n\n## Table of Contents\n1. [Overview](#overview)\n2. [What is PEFT?](#what-is-peft)\n3. [Implementation Details](#implementation-details)\n4. [Parameter Breakdown](#parameter-breakdown)\n5. [PEFT Methods Comparison](#peft-methods-comparison)\n6. [Target Modules Explained](#target-modules-explained)\n7. [Configuration Presets](#configuration-presets)\n8. [Memory & Performance Impact](#memory--performance-impact)\n9. [Best Practices](#best-practices)\n10. [Testing & Validation](#testing--validation)\n11. [Commands (CLI)](#commands-cli)\n12. [Inputs & Outputs](#inputs--outputs)\n13. [Try it (PowerShell)](#try-it-powershell)\n\n---\n\n## Overview\n\nParameter-Efficient Fine-Tuning (PEFT) in AI-OS allows training with **95-99% fewer trainable parameters** by using adapter techniques like LoRA. Instead of updating all 87M+ model parameters, PEFT adds small adapter layers (~500K-8M params) that achieve comparable or better results.\n\n**Key Benefits:**\n- \u2705 **Memory Reduction:** 40-60% less VRAM usage\n- \u2705 **Speed:** Faster training and convergence\n- \u2705 **Quality:** Comparable or better results than full fine-tuning\n- \u2705 **Flexibility:** Easy to merge adapters or switch between them\n- \u2705 **Compatibility:** Works with all other optimizations (gradient checkpointing, AMP, etc.)\n\n---\n\n## What is PEFT?\n\nPEFT techniques modify only a small subset of model parameters while keeping the base model frozen. This is achieved through:\n\n1. **Adapter Layers:** Small neural network modules inserted into the model\n2. **Low-Rank Decomposition:** Decomposing weight updates into smaller matrices\n3. **Selective Training:** Only training specific components (e.g., attention layers)\n\n### Why Use PEFT?\n\n| Scenario | Full Fine-Tuning | PEFT (LoRA) |\n|----------|------------------|-------------|\n| Parameters to train | 87M (100%) | 500K-8M (1-5%) |\n| VRAM Required (GPT-2 size) | 12-16 GB | 6-10 GB |\n| Training Speed | Baseline | 1.5-2\u00d7 faster |\n| Convergence | Requires more data | Often better with less data |\n| Risk of Catastrophic Forgetting | High | Low |\n| Storage per fine-tune | Full model (~350 MB) | Adapter only (~10-30 MB) |\n\n---\n\n## Implementation Details\n\n### Code Location\n- **Main Implementation:** `src/aios/cli/hrm_hf/model_precision.py` (`apply_peft()` function)\n- **Configuration:** `src/aios/core/hrm_training/training_config/advanced_fields.py`\n- **GUI Controls:** `src/aios/gui/components/hrm_training_panel/`\n\n### How It Works\n\n```python\n# From model_precision.py\ndef apply_peft(model, config, log_fn):\n    if not config.use_peft:\n        return model\n    \n    # 1. Parse target modules\n    target_modules_list = [m.strip() for m in config.lora_target_modules.split(',')]\n    \n    # 2. Create PEFT config\n    if config.peft_method == \"lora\":\n        peft_config = LoraConfig(\n            r=config.lora_r,                    # Rank\n            lora_alpha=config.lora_alpha,      # Scaling\n            lora_dropout=config.lora_dropout,  # Regularization\n            target_modules=target_modules_list,\n            task_type=TaskType.CAUSAL_LM,\n        )\n    # ... (adalora, ia3 methods also supported)\n    \n    # 3. Wrap model with PEFT\n    model = get_peft_model(model, peft_config)\n    \n    return model\n```\n\n### Integration Points\n1. **Training Pipeline:** Called in `train_actv1_impl()` after model creation\n2. **Memory Estimation:** Integrated into VRAM calculator in GUI\n3. **Checkpoint Saving:** PEFT adapters saved separately or merged\n4. **Inference:** Can load adapters dynamically\n\n---\n\n## Parameter Breakdown\n\n### 1. `use_peft` (Boolean)\n**Default:** `false`\n\n**Description:** Master switch to enable/disable PEFT.\n\n**When to Enable:**\n- \u2705 Limited VRAM (< 12 GB available)\n- \u2705 Want faster training iteration\n- \u2705 Fine-tuning for specific tasks\n- \u2705 Need to maintain multiple model variants\n\n**When to Disable:**\n- \u274c Full model capacity needed\n- \u274c Training from scratch (not fine-tuning)\n- \u274c Abundant VRAM available (24+ GB)\n\n---\n\n### 2. `peft_method` (String)\n**Default:** `\"lora\"`  \n**Options:** `lora`, `adalora`, `ia3`\n\n#### **LoRA (Low-Rank Adaptation)** \ud83c\udf1f *Recommended*\n- **Best for:** General purpose, most stable\n- **Params:** Configurable via `lora_r`\n- **Quality:** Excellent\n- **Speed:** Fast\n\n**How it works:** Adds low-rank matrices A and B to weight updates\n```\n\u0394W = B \u00d7 A (where B is d\u00d7r and A is r\u00d7k, r << d,k)\n```\n\n#### **AdaLoRA (Adaptive LoRA)**\n- **Best for:** Dynamic rank allocation\n- **Params:** Similar to LoRA\n- **Quality:** Potentially better than LoRA\n- **Speed:** Slightly slower (adaptive overhead)\n\n**How it works:** Dynamically adjusts rank across layers based on importance\n\n#### **IA3 (Infused Adapter)**\n- **Best for:** Minimal parameters (~100K)\n- **Params:** Fewest parameters\n- **Quality:** Good for specific tasks\n- **Speed:** Fastest\n\n**How it works:** Learns scaling vectors instead of full matrices\n\n---\n\n### 3. `lora_r` (Integer - Rank)\n**Default:** `16`  \n**Range:** `1-256` (practical: `4-64`)\n\n**Description:** The rank of the low-rank decomposition. Controls adapter capacity.\n\n**Impact on Model:**\n- **Higher rank** = More capacity, more parameters, more VRAM\n- **Lower rank** = Less capacity, fewer parameters, less VRAM\n\n**Parameter Count Formula:**\n```\nparams_per_layer = 2 \u00d7 rank \u00d7 layer_dimension\nFor GPT-2 (d=768), q_proj with r=16:\n  params = 2 \u00d7 16 \u00d7 768 = 24,576 params per layer\n```\n\n**Recommendations:**\n\n| Rank | Parameters | VRAM Impact | Use Case |\n|------|-----------|-------------|----------|\n| `r=4` | ~250K | +1 GB | Very simple fine-tuning |\n| `r=8` | ~500K | +1.5 GB | Minimal configuration |\n| `r=16` | ~2M | +2-3 GB | **Recommended default** |\n| `r=32` | ~8M | +4-5 GB | Complex tasks, high quality |\n| `r=64` | ~32M | +8-10 GB | Very complex, rarely needed |\n\n**Rule of Thumb:**\n- Start with `r=16`\n- Increase if model underfits\n- Decrease if VRAM limited or overfitting occurs\n\n---\n\n### 4. `lora_alpha` (Integer - Scaling)\n**Default:** `32`  \n**Range:** `1-1024` (practical: `8-128`)\n\n**Description:** Scaling parameter for LoRA adapter outputs.\n\n**Mathematical Impact:**\n```\neffective_adapter_contribution = (lora_alpha / lora_r) \u00d7 adapter_output\n```\n\n**Effective Learning Rate:**\n- Higher `alpha` relative to `r` = Stronger adapter influence\n- Lower `alpha` relative to `r` = More conservative adaptation\n\n**Recommendations:**\n\n| Configuration | Ratio | Use Case |\n|---------------|-------|----------|\n| `r=8, \u03b1=8` | 1:1 | Conservative, minimal changes |\n| `r=16, \u03b1=32` | 2:1 | **Standard (recommended)** |\n| `r=16, \u03b1=16` | 1:1 | More conservative |\n| `r=16, \u03b1=64` | 4:1 | Aggressive adaptation |\n| `r=32, \u03b1=64` | 2:1 | High capacity, standard scaling |\n\n**Best Practice:**\n- Use `lora_alpha = 2 \u00d7 lora_r` as starting point\n- Increase alpha if adapters aren't learning enough\n- Decrease alpha if training is unstable\n\n---\n\n### 5. `lora_dropout` (Float)\n**Default:** `0.05`  \n**Range:** `0.0-0.5` (practical: `0.0-0.2`)\n\n**Description:** Dropout probability applied to LoRA adapter layers for regularization.\n\n**Purpose:**\n- Prevent overfitting\n- Improve generalization\n- Add noise during training\n\n**Recommendations:**\n\n| Dropout | Regularization | Use Case |\n|---------|----------------|----------|\n| `0.0` | None | Large datasets (>100K samples) |\n| `0.05` | **Light (recommended)** | General purpose |\n| `0.1` | Medium | Medium datasets (10K-100K) |\n| `0.2-0.3` | High | Small datasets (<10K samples) |\n\n**When to Adjust:**\n- **Increase** if model overfits to training data\n- **Decrease** if model underfits or dataset is very large\n- **Set to 0** for maximum adapter capacity (stable datasets)\n\n---\n\n### 6. `lora_target_modules` (String - Comma-separated)\n**Default:** `\"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"`\n\n**Description:** Specifies which model layers should have LoRA adapters applied.\n\n**Available Modules in HRM ACTv1:**\n\n#### **Attention Modules** (Recommended)\n- `q_proj` - Query projection (attention keys)\n- `k_proj` - Key projection (attention values)\n- `v_proj` - Value projection (what gets attended to)\n- `o_proj` - Output projection (attention combination)\n\n#### **MLP/Feed-Forward Modules**\n- `gate_proj` - Gating mechanism\n- `up_proj` - Upward projection (expand)\n- `down_proj` - Downward projection (compress)\n\n#### **Always Trainable (Cannot be frozen)**\n- `lm_head` - Language model output head\n- `q_head` - HRM halting/pondering head\n\n---\n\n## Target Modules Explained\n\n### Preset Configurations\n\n#### **Minimal** (Recommended for VRAM < 8 GB)\n```\n\"q_proj,v_proj\"\n```\n- **Parameters:** ~500K-1M\n- **VRAM:** +1.5-2 GB\n- **Quality:** Good for most tasks\n- **Speed:** Fastest training\n- **Best for:** Limited hardware, quick iterations\n\n#### **Balanced** (Recommended Default)\n```\n\"q_proj,k_proj,v_proj,o_proj\"\n```\n- **Parameters:** ~2M-4M\n- **VRAM:** +2.5-4 GB\n- **Quality:** Very good\n- **Speed:** Fast\n- **Best for:** General fine-tuning, balanced quality/speed\n\n#### **Full** (Maximum Quality)\n```\n\"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n```\n- **Parameters:** ~6M-12M\n- **VRAM:** +4-6 GB\n- **Quality:** Best possible with PEFT\n- **Speed:** Moderate\n- **Best for:** Complex tasks, maximum quality needed\n\n### Module Impact Analysis\n\n| Module | Function | Impact on Performance | Training Cost |\n|--------|----------|----------------------|---------------|\n| `q_proj` | **Query generation** | \u2b50\u2b50\u2b50 High - Critical for attention | Low |\n| `k_proj` | **Key generation** | \u2b50\u2b50 Medium - Important for attention | Low |\n| `v_proj` | **Value generation** | \u2b50\u2b50\u2b50 High - What gets attended to | Low |\n| `o_proj` | **Attention output** | \u2b50\u2b50 Medium - Combines attention | Low |\n| `gate_proj` | **MLP gating** | \u2b50 Low-Medium - Controls information flow | Medium |\n| `up_proj` | **MLP expansion** | \u2b50 Low-Medium - Increases dimensionality | Medium |\n| `down_proj` | **MLP compression** | \u2b50 Low-Medium - Reduces dimensionality | Medium |\n\n**Key Insight:**\n- Attention modules (`q,k,v,o`) are most impactful per parameter\n- MLP modules add capacity but with diminishing returns\n- Always include `q_proj` and `v_proj` at minimum\n\n---\n\n## PEFT Methods Comparison\n\n### Detailed Comparison Table\n\n| Feature | LoRA | AdaLoRA | IA3 |\n|---------|------|---------|-----|\n| **Trainable Params** | 0.5M-8M | 0.5M-8M | 50K-500K |\n| **Memory Overhead** | +2-4 GB | +2.5-5 GB | +1-2 GB |\n| **Training Speed** | Fast | Medium | Fastest |\n| **Quality** | Excellent | Excellent+ | Good |\n| **Stability** | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50 |\n| **Complexity** | Low | Medium | Low |\n| **Recommended For** | **General use** | Research, optimal quality | Extreme efficiency |\n| **Hyperparameters** | `r`, `alpha`, `dropout` | `r`, `alpha`, `dropout` | None (module-specific) |\n\n### When to Use Each Method\n\n#### Use **LoRA** when:\n- \u2705 General fine-tuning (recommended default)\n- \u2705 Want predictable, stable results\n- \u2705 Well-documented hyperparameters\n- \u2705 Good community support\n\n#### Use **AdaLoRA** when:\n- \u2705 Want slightly better quality\n- \u2705 Have heterogeneous layers (some need more capacity)\n- \u2705 Willing to trade speed for quality\n- \u2705 Experimenting with optimal configurations\n\n#### Use **IA3** when:\n- \u2705 Extremely limited VRAM\n- \u2705 Need fastest possible training\n- \u2705 Task is relatively simple\n- \u2705 Every MB of memory counts\n\n---\n\n## Configuration Presets\n\n### Preset 1: **Budget** (< 8 GB VRAM)\n```yaml\nuse_peft: true\npeft_method: \"ia3\"\nlora_target_modules: \"q_proj,v_proj\"\n```\n- **Trainable params:** ~100K-200K\n- **VRAM overhead:** +1-1.5 GB\n- **Quality:** Good\n- **Use case:** Lightweight fine-tuning, minimal resources\n\n---\n\n### Preset 2: **Efficient** (8-12 GB VRAM) \ud83c\udf1f *Recommended*\n```yaml\nuse_peft: true\npeft_method: \"lora\"\nlora_r: 16\nlora_alpha: 32\nlora_dropout: 0.05\nlora_target_modules: \"q_proj,v_proj\"\n```\n- **Trainable params:** ~500K-1M\n- **VRAM overhead:** +2-2.5 GB\n- **Quality:** Very Good\n- **Use case:** Most common scenarios, balanced efficiency\n\n---\n\n### Preset 3: **Balanced** (12-16 GB VRAM)\n```yaml\nuse_peft: true\npeft_method: \"lora\"\nlora_r: 16\nlora_alpha: 32\nlora_dropout: 0.05\nlora_target_modules: \"q_proj,k_proj,v_proj,o_proj\"\n```\n- **Trainable params:** ~2M-3M\n- **VRAM overhead:** +3-4 GB\n- **Quality:** Excellent\n- **Use case:** Standard fine-tuning with ample resources\n\n---\n\n### Preset 4: **High Quality** (16-24 GB VRAM)\n```yaml\nuse_peft: true\npeft_method: \"lora\"\nlora_r: 32\nlora_alpha: 64\nlora_dropout: 0.05\nlora_target_modules: \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n```\n- **Trainable params:** ~8M-12M\n- **VRAM overhead:** +5-6 GB\n- **Quality:** Maximum (with PEFT)\n- **Use case:** Complex tasks, maximum quality needed\n\n---\n\n### Preset 5: **Adaptive** (Research/Optimization)\n```yaml\nuse_peft: true\npeft_method: \"adalora\"\nlora_r: 16\nlora_alpha: 32\nlora_dropout: 0.1\nlora_target_modules: \"q_proj,k_proj,v_proj,o_proj\"\n```\n- **Trainable params:** ~2M-3M (dynamic)\n- **VRAM overhead:** +3.5-4.5 GB\n- **Quality:** Excellent+\n- **Use case:** Research, finding optimal configurations\n\n---\n\n## Memory & Performance Impact\n\n### Memory Breakdown (GPT-2 124M Model Example)\n\n#### **Full Fine-Tuning** (no PEFT)\n```\nBase model:          ~500 MB\nGradients:          ~500 MB\nOptimizer states:   ~2000 MB (Adam)\nActivations:        ~8000 MB (batch=8, seq=1024)\n-----------------------------------\nTOTAL:              ~11 GB\n```\n\n#### **PEFT (LoRA r=16, Balanced)**\n```\nBase model:          ~500 MB (frozen, can use 8-bit)\nLoRA adapters:       ~20 MB\nLoRA gradients:      ~20 MB\nLoRA optimizer:      ~80 MB\nActivations:         ~8000 MB (same)\n-----------------------------------\nTOTAL:              ~8.6 GB  (23% reduction)\n```\n\n#### **PEFT + All Optimizations**\n```\nBase model (8-bit):  ~125 MB\nLoRA adapters:       ~20 MB\nLoRA optimizer:      ~80 MB\nActivations (gc):    ~2000 MB (gradient checkpointing)\n-----------------------------------\nTOTAL:              ~2.2 GB  (80% reduction!)\n```\n\n### Performance Benchmarks\n\n| Configuration | Trainable Params | VRAM | Training Speed | Quality |\n|--------------|-----------------|------|----------------|---------|\n| Full Fine-Tuning | 124M (100%) | 11 GB | 1.0\u00d7 (baseline) | 100% |\n| LoRA r=4 Minimal | 250K (0.2%) | 9 GB | 1.3\u00d7 | 85% |\n| LoRA r=8 Minimal | 500K (0.4%) | 9.5 GB | 1.25\u00d7 | 92% |\n| LoRA r=16 Minimal | 1M (0.8%) | 10 GB | 1.2\u00d7 | 97% |\n| LoRA r=16 Balanced | 2M (1.6%) | 10.5 GB | 1.15\u00d7 | 99% |\n| LoRA r=32 Full | 8M (6.5%) | 11 GB | 1.1\u00d7 | 99.5% |\n\n**Key Findings:**\n- LoRA r=16 with balanced modules achieves 99% quality at 1.6% parameters\n- Speed improvements come from fewer gradients to compute\n- VRAM savings enable larger batch sizes (\u2192 better quality)\n\n---\n\n## Best Practices\n\n### 1. **Start with Recommended Defaults**\n```yaml\nuse_peft: true\npeft_method: \"lora\"\nlora_r: 16\nlora_alpha: 32\nlora_dropout: 0.05\nlora_target_modules: \"q_proj,v_proj\"  # or \"q_proj,k_proj,v_proj,o_proj\"\n```\n\n### 2. **Tune Rank Based on Task Complexity**\n- **Simple tasks** (sentiment, classification): `r=4-8`\n- **Medium tasks** (summarization, QA): `r=8-16`\n- **Complex tasks** (creative writing, reasoning): `r=16-32`\n- **Very complex** (code generation, math): `r=32-64`\n\n### 3. **Adjust Alpha with Rank**\n- Maintain `alpha = 2 \u00d7 r` ratio\n- Increase alpha if adapters learn too slowly\n- Decrease alpha if training becomes unstable\n\n### 4. **Use Dropout for Small Datasets**\n- `dataset < 1K samples`: `dropout = 0.2-0.3`\n- `dataset 1K-10K`: `dropout = 0.1`\n- `dataset 10K-100K`: `dropout = 0.05`\n- `dataset > 100K`: `dropout = 0.0-0.05`\n\n### 5. **Target Modules Strategy**\n- **Always start with:** `q_proj,v_proj`\n- **If underfitting, add:** `k_proj,o_proj`\n- **If still underfitting, add:** `gate_proj,up_proj,down_proj`\n- **Never remove:** `q_proj,v_proj` (most impactful)\n\n### 6. **Combine with Other Optimizations**\nPEFT works great with:\n- \u2705 Gradient checkpointing (memory)\n- \u2705 AMP/mixed precision (speed + memory)\n- \u2705 8-bit optimizers (memory)\n- \u2705 CPU offloading (extreme memory savings)\n- \u2705 Flash Attention (speed)\n\n### 7. **Monitor Training Metrics**\n- **Trainable params** should be < 5% of total\n- **Loss convergence** should be similar to full fine-tuning\n- **VRAM usage** should be 20-50% lower\n- **Training speed** should be 1.1-1.5\u00d7 faster\n\n### 8. **Save and Merge Adapters**\n```python\n# Save adapter only (small file ~10-30 MB)\nmodel.save_pretrained(\"path/to/lora_adapter\")\n\n# Merge adapter into base model (optional)\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(\"path/to/merged_model\")\n```\n\n---\n\n## Testing & Validation\n\n### Validation Checklist\n\n#### \u2705 **Configuration Validation**\n- [ ] `use_peft` correctly enables/disables PEFT\n- [ ] All three methods (lora, adalora, ia3) work\n- [ ] Target modules parse correctly\n- [ ] Invalid configurations raise helpful errors\n\n#### \u2705 **Training Validation**\n- [ ] Model trains successfully with PEFT\n- [ ] Loss decreases over training\n- [ ] Gradients flow only to adapter parameters\n- [ ] Checkpoints save correctly\n\n#### \u2705 **Memory Validation**\n- [ ] VRAM usage is lower than full fine-tuning\n- [ ] Larger batch sizes fit in memory\n- [ ] Gradient checkpointing + PEFT works\n\n#### \u2705 **Quality Validation**\n- [ ] Eval metrics comparable to full fine-tuning\n- [ ] Model output quality is good\n- [ ] No catastrophic forgetting\n- [ ] Adapters load correctly for inference\n\n### Common Issues & Solutions\n\n#### **Issue: \"No trainable parameters\"**\n**Cause:** Target modules don't match model architecture  \n**Solution:** Use `q_proj,v_proj` for HRM models\n\n#### **Issue: \"PEFT library not available\"**\n**Cause:** `peft` package not installed  \n**Solution:** `pip install peft>=0.11.1`\n\n#### **Issue: \"Training loss doesn't decrease\"**\n**Cause:** `lora_alpha` too low or rank too small  \n**Solution:** Increase `lora_alpha` or `lora_r`\n\n#### **Issue: \"Out of memory with PEFT enabled\"**\n**Cause:** Other factors (batch size, sequence length)  \n**Solution:** Reduce batch size or enable gradient checkpointing\n\n#### **Issue: \"Training is unstable\"**\n**Cause:** `lora_alpha` too high  \n**Solution:** Reduce `lora_alpha` or add more dropout\n\n---\n\n## Commands (CLI)\n\nPowerShell examples for enabling PEFT with `aios hrm-hf train-actv1`:\n\nMinimal (q,v only \u2014 best VRAM efficiency):\n\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 `\n    --model gpt2 `\n    --dataset-file training_data/curated_datasets/test_sample.txt `\n    --steps 200 `\n    --batch-size 4 `\n    --halt-max-steps 1 `\n    --use-peft `\n    --peft-method lora `\n    --lora-r 16 `\n    --lora-alpha 32 `\n    --lora-dropout 0.05 `\n    --lora-target-modules \"q_proj,v_proj\" `\n    --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nBalanced (q,k,v,o):\n\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 `\n    --model gpt2 `\n    --dataset-file training_data/curated_datasets/test_sample.txt `\n    --steps 200 `\n    --batch-size 4 `\n    --halt-max-steps 1 `\n    --use-peft `\n    --peft-method lora `\n    --lora-r 16 `\n    --lora-alpha 32 `\n    --lora-dropout 0.05 `\n    --lora-target-modules \"q_proj,k_proj,v_proj,o_proj\" `\n    --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nAdaLoRA variant:\n\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 `\n    --model gpt2 `\n    --dataset-file training_data/curated_datasets/test_sample.txt `\n    --steps 200 `\n    --batch-size 4 `\n    --halt-max-steps 1 `\n    --use-peft `\n    --peft-method adalora `\n    --lora-r 16 `\n    --lora-alpha 32 `\n    --lora-dropout 0.1 `\n    --lora-target-modules \"q_proj,k_proj,v_proj,o_proj\" `\n    --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nNotes:\n- Flags are wired in `src/aios/cli/hrm_hf_cli.py` and applied in `src/aios/cli/hrm_hf/model_precision.py`.\n- Use `--amp` and `--gradient-checkpointing` with PEFT for best VRAM efficiency.\n\n## Inputs & Outputs\n\nInputs:\n- Base model: `--model <hf-id-or-local-path>`\n- Dataset: `--dataset-file <path or hf://\u2026>`\n- PEFT toggles: `--use-peft`, `--peft-method`, `--lora-r`, `--lora-alpha`, `--lora-dropout`, `--lora-target-modules`\n\nOutputs:\n- Brain bundle under `artifacts/brains/actv1/<brain-name>/`\n- Metrics JSONL at `artifacts/brains/actv1/metrics.jsonl`\n- Optional PEFT adapter save/merge (see code snippet below)\n\n## Try it (PowerShell)\n\nQuick dry-run to verify PEFT wiring:\n\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 `\n    --model gpt2 `\n    --dataset-file training_data/curated_datasets/test_sample.txt `\n    --steps 1 `\n    --batch-size 2 `\n    --halt-max-steps 1 `\n    --use-peft `\n    --peft-method lora `\n    --lora-r 8 `\n    --lora-alpha 16 `\n    --lora-target-modules \"q_proj,v_proj\" `\n    --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nExpected log lines include a `{\"peft\": \"enabled\", ...}` entry with trainable parameter percentages < 5%.\n\n---\n\n## Conclusion\n\nLoRA/PEFT in AI-OS provides a powerful, efficient way to fine-tune models with:\n- **95-99% fewer trainable parameters**\n- **40-60% VRAM savings**\n- **Faster training speeds**\n- **Comparable or better quality**\n\n### Recommended Starting Point\n```yaml\nuse_peft: true\npeft_method: \"lora\"\nlora_r: 16\nlora_alpha: 32\nlora_dropout: 0.05\nlora_target_modules: \"q_proj,v_proj\"\n```\n\n**Then adjust based on:**\n- VRAM availability \u2192 increase `r` or target modules\n- Task complexity \u2192 increase `r` and `alpha`\n- Dataset size \u2192 adjust `dropout`\n- Quality needs \u2192 add more target modules\n\n### Further Reading\n- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n- [PEFT Documentation](https://huggingface.co/docs/peft)\n- [AdaLoRA Paper](https://arxiv.org/abs/2303.10512)\n- [IA3 Paper](https://arxiv.org/abs/2205.05638)\n\n---\n\n**Last Updated:** October 19, 2025  \n**Version:** 1.0  \n**AI-OS Version:** Compatible with all ACTv1 models\n\nSee also: Memory Optimization \u2022 Core Training \u2022 GUI Features\n", "tags": ["cli", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "LoRA/PEFT Comprehensive Analysis for AI-OS"}, {"line": 15, "text": "Table of Contents"}, {"line": 32, "text": "Overview"}, {"line": 45, "text": "What is PEFT?"}, {"line": 53, "text": "Why Use PEFT?"}, {"line": 66, "text": "Implementation Details"}, {"line": 68, "text": "Code Location"}, {"line": 73, "text": "How It Works"}, {"line": 76, "text": "From model_precision.py"}, {"line": 81, "text": "1. Parse target modules"}, {"line": 84, "text": "2. Create PEFT config"}, {"line": 93, "text": "... (adalora, ia3 methods also supported)"}, {"line": 95, "text": "3. Wrap model with PEFT"}, {"line": 101, "text": "Integration Points"}, {"line": 109, "text": "Parameter Breakdown"}, {"line": 111, "text": "1. `use_peft` (Boolean)"}, {"line": 129, "text": "2. `peft_method` (String)"}, {"line": 133, "text": "**LoRA (Low-Rank Adaptation)** \ud83c\udf1f *Recommended*"}, {"line": 144, "text": "**AdaLoRA (Adaptive LoRA)**"}, {"line": 152, "text": "**IA3 (Infused Adapter)**"}, {"line": 162, "text": "3. `lora_r` (Integer - Rank)"}, {"line": 196, "text": "4. `lora_alpha` (Integer - Scaling)"}, {"line": 228, "text": "5. `lora_dropout` (Float)"}, {"line": 255, "text": "6. `lora_target_modules` (String - Comma-separated)"}, {"line": 262, "text": "**Attention Modules** (Recommended)"}, {"line": 268, "text": "**MLP/Feed-Forward Modules**"}, {"line": 273, "text": "**Always Trainable (Cannot be frozen)**"}, {"line": 279, "text": "Target Modules Explained"}, {"line": 281, "text": "Preset Configurations"}, {"line": 283, "text": "**Minimal** (Recommended for VRAM < 8 GB)"}, {"line": 293, "text": "**Balanced** (Recommended Default)"}, {"line": 303, "text": "**Full** (Maximum Quality)"}, {"line": 313, "text": "Module Impact Analysis"}, {"line": 332, "text": "PEFT Methods Comparison"}, {"line": 334, "text": "Detailed Comparison Table"}, {"line": 347, "text": "When to Use Each Method"}, {"line": 349, "text": "Use **LoRA** when:"}, {"line": 355, "text": "Use **AdaLoRA** when:"}, {"line": 361, "text": "Use **IA3** when:"}, {"line": 369, "text": "Configuration Presets"}, {"line": 371, "text": "Preset 1: **Budget** (< 8 GB VRAM)"}, {"line": 384, "text": "Preset 2: **Efficient** (8-12 GB VRAM) \ud83c\udf1f *Recommended*"}, {"line": 400, "text": "Preset 3: **Balanced** (12-16 GB VRAM)"}, {"line": 416, "text": "Preset 4: **High Quality** (16-24 GB VRAM)"}, {"line": 432, "text": "Preset 5: **Adaptive** (Research/Optimization)"}, {"line": 448, "text": "Memory & Performance Impact"}, {"line": 450, "text": "Memory Breakdown (GPT-2 124M Model Example)"}, {"line": 452, "text": "**Full Fine-Tuning** (no PEFT)"}, {"line": 462, "text": "**PEFT (LoRA r=16, Balanced)**"}, {"line": 473, "text": "**PEFT + All Optimizations**"}, {"line": 483, "text": "Performance Benchmarks"}, {"line": 501, "text": "Best Practices"}, {"line": 503, "text": "1. **Start with Recommended Defaults**"}, {"line": 513, "text": "2. **Tune Rank Based on Task Complexity**"}, {"line": 519, "text": "3. **Adjust Alpha with Rank**"}, {"line": 524, "text": "4. **Use Dropout for Small Datasets**"}, {"line": 530, "text": "5. **Target Modules Strategy**"}, {"line": 536, "text": "6. **Combine with Other Optimizations**"}, {"line": 544, "text": "7. **Monitor Training Metrics**"}, {"line": 550, "text": "8. **Save and Merge Adapters**"}, {"line": 552, "text": "Save adapter only (small file ~10-30 MB)"}, {"line": 555, "text": "Merge adapter into base model (optional)"}, {"line": 562, "text": "Testing & Validation"}, {"line": 564, "text": "Validation Checklist"}, {"line": 566, "text": "\u2705 **Configuration Validation**"}, {"line": 572, "text": "\u2705 **Training Validation**"}, {"line": 578, "text": "\u2705 **Memory Validation**"}, {"line": 583, "text": "\u2705 **Quality Validation**"}, {"line": 589, "text": "Common Issues & Solutions"}, {"line": 591, "text": "**Issue: \"No trainable parameters\"**"}, {"line": 595, "text": "**Issue: \"PEFT library not available\"**"}, {"line": 599, "text": "**Issue: \"Training loss doesn't decrease\"**"}, {"line": 603, "text": "**Issue: \"Out of memory with PEFT enabled\"**"}, {"line": 607, "text": "**Issue: \"Training is unstable\"**"}, {"line": 613, "text": "Commands (CLI)"}, {"line": 675, "text": "Inputs & Outputs"}, {"line": 687, "text": "Try it (PowerShell)"}, {"line": 710, "text": "Conclusion"}, {"line": 718, "text": "Recommended Starting Point"}, {"line": 734, "text": "Further Reading"}]}, {"path": "guide/features/MCP_INTEGRATION_COMPLETE.md", "content": "# MCP Integration Complete - Setup Guide\n\n## \u2705 What's Been Done\n\nI've fully integrated MCP (Model Context Protocol) tools into your AI-OS chat system. Here's what was added:\n\n### 1. Configuration Files Created\n\n**`config/mcp_servers.json`** - MCP server configuration\n- Filesystem server (file operations)\n- Memory server (knowledge graph)\n- DuckDuckGo server (web search)\n\n### 2. Core MCP Module Created (`src/aios/core/mcp/`)\n\n**`client.py`** - MCP Client\n- Loads server configuration\n- Discovers available tools\n- Manages server connections\n- Executes tool calls\n\n**`tool_executor.py`** - Tool Execution Coordinator\n- Parses tool calls from model output\n- Executes tools via MCP client\n- Formats results for model context\n- Adds tool descriptions to system prompt\n\n**`__init__.py`** - Module exports\n\n### 3. Chat Integration (`src/aios/gui/app/chat_operations.py`)\n\nModified to support MCP tool calling:\n- Initializes tool executor on startup\n- Enhances prompts with tool information\n- Detects tool calls in model responses\n- Executes tools automatically\n- Sends results back to model for synthesis\n\n## \ud83d\ude80 How It Works\n\n### Flow Diagram\n\n```\nUser Message\n    \u2193\nEnhanced with Tool Info\n    \u2193\nModel Generates Response\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Contains Tool Call? \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193               \u2193\n   No              Yes\n    \u2193               \u2193\nDisplay         Execute Tool\nResponse            \u2193\n                Get Result\n                    \u2193\n              Send to Model\n                    \u2193\n             Display Final Response\n```\n\n### Supported Tool Call Formats\n\nYour model can request tools using any of these formats:\n\n1. **JSON Format**\n```json\n{\"tool\": \"read_file\", \"parameters\": {\"path\": \"README.md\"}}\n```\n\n2. **XML-Style Tags** (Qwen2.5/Claude style)\n```xml\n<tool_call>\n{\"name\": \"read_file\", \"arguments\": {\"path\": \"README.md\"}}\n</tool_call>\n```\n\n3. **Function Call Style**\n```python\nread_file(path=\"README.md\")\n```\n\n## \ud83d\udccb Available Tools (21 Tools Across 3 Servers)\n\n### Filesystem Server (10 tools)\n- `read_file` - Read file contents\n- `write_file` - Create/overwrite files\n- `list_directory` - List directory contents\n- `search_files` - Find files by pattern\n- `create_directory` - Create directories\n- `move_file` - Move/rename files\n- `get_file_info` - Get file metadata\n- `directory_tree` - Recursive tree\n- `edit_file` - Line-based editing\n- `read_multiple_files` - Batch reads\n\n### Memory Server (9 tools)\n- `create_entities` - Add to knowledge graph\n- `create_relations` - Link entities\n- `add_observations` - Add entity data\n- `search_nodes` - Search graph\n- `read_graph` - Read entire graph\n- `open_nodes` - Get specific nodes\n- `delete_entities` - Remove entities\n- `delete_relations` - Remove links\n- `delete_observations` - Remove data\n\n### DuckDuckGo Server (2 tools)\n- `web_search` - Web search\n- `fetch_webpage` - Download pages\n\n## \ud83d\udd27 Setup Requirements\n\n### Install Node.js (Required for MCP Servers)\n\n1. **Download Node.js**\n   - Visit: https://nodejs.org/\n   - Download LTS version (recommended)\n   - Run installer\n\n2. **Verify Installation**\n```powershell\nnode --version   # Should show v20.x.x or higher\nnpm --version    # Should show 10.x.x or higher\nnpx --version    # Should show 10.x.x or higher\n```\n\n3. **Restart Terminal**\n   - Close and reopen PowerShell\n   - Reactivate your virtual environment\n\n4. **Verify MCP Setup**\n```powershell\npython scripts/verify_mcp_setup.py\n```\n\n## \ud83c\udfaf Testing the Integration\n\n### Method 1: Use GUI Chat Tab\n\n1. **Start the GUI**\n```powershell\naios gui\n```\n\n2. **Go to Chat Tab**\n\n3. **Load a Brain** (if you have one trained)\n   - Click \"Load Brain\" dropdown\n   - Select a trained model\n   - Wait for loading confirmation\n\n4. **Test Tool Calling**\n\nTry these example prompts:\n\n```\nRead the README.md file\n```\n\n```\nSearch the web for model context protocol\n```\n\n```\nList all Python files in the src directory\n```\n\n```\nCreate an entity in the knowledge graph named \"test-concept\" with observation \"This is a test\"\n```\n\n### Method 2: Use Mock Model Testing\n\nWithout a trained model, test the infrastructure:\n\n```powershell\n# Run mock model demonstration\npython tests/test_mcp_mock_model.py\n\n# Run integration tests\npytest tests/test_mcp_integration.py -v\n```\n\n### Method 3: Manual Server Testing\n\nTest individual MCP servers:\n\n```powershell\n# Install MCP Inspector\nnpm install -g @modelcontextprotocol/inspector\n\n# Test filesystem server\n## Manual Testing\n\n```bash\nnpx @modelcontextprotocol/inspector npx -y @modelcontextprotocol/server-filesystem <PROJECT_ROOT>\n```\n\n# Test memory server\nnpx @modelcontextprotocol/inspector npx -y @modelcontextprotocol/server-memory\n\n# Test DuckDuckGo\nnpx @modelcontextprotocol/inspector npx -y @wulfic/server-duckduckgo\n```\n\n## \ud83e\udde0 Training Your Model to Use Tools\n\n### Current Status\n\nThe MCP infrastructure is **fully wired up**, but your model needs to be trained on function calling examples to actually use the tools.\n\n### What You Need\n\n1. **Function Calling Training Data**\n\nCreate training examples in this format:\n\n```json\n{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Read the README file\"},\n    {\n      \"role\": \"assistant\",\n      \"content\": null,\n      \"tool_calls\": [{\n        \"name\": \"read_file\",\n        \"arguments\": {\"path\": \"README.md\"}\n      }]\n    },\n    {\n      \"role\": \"tool\",\n      \"content\": \"# AI-OS\\n\\nAn advanced AI operating system...\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Based on the README, this project is an advanced AI operating system that provides...\"\n    }\n  ]\n}\n```\n\n2. **Training Process**\n\nOption A - Fine-tune existing model:\n- Use a model pre-trained on tool calling (GPT-4, Claude, Qwen2.5)\n- Fine-tune on your specific tools/domain\n- Faster, usually better results\n\nOption B - Train from scratch:\n- Add tool calling examples to your training data\n- Train with HRM as usual\n- Model learns tool patterns\n\n3. **Recommended Datasets**\n\nLook for:\n- Function calling datasets on HuggingFace\n- Tool use datasets (e.g., ToolBench, ToolAlpaca)\n- API calling examples\n\nConvert to your format with tool names matching your MCP tools.\n\n## \ud83d\udcca Monitoring Tool Usage\n\n### In Chat Tab\n\nWhen a tool is called, you'll see:\n\n```\n\ud83d\udd27 Using tool: read_file\n\nTool read_file result: {\n  \"success\": true,\n  \"content\": \"...\"\n}\n\nBased on the file contents, I can tell you that...\n```\n\n### In Debug/Logs\n\nCheck the Debug tab for detailed logging:\n- Tool initialization\n- Tool discovery\n- Tool calls and parameters\n- Execution results\n\n## \ud83c\udf9b\ufe0f Configuration\n\n### Enable/Disable Tools\n\nUse the **MCP Servers & Tools** tab in the GUI:\n\n1. **Servers Tab**\n   - Enable/disable entire servers\n   - Configure server parameters\n   - Test connections\n\n2. **Tools Tab**\n   - Enable/disable individual tools\n   - View by category\n   - Set permissions\n\n### Edit Configuration Files\n\n**`config/mcp_servers.json`** - Server configuration\n```json\n{\n  \"name\": \"filesystem\",\n  \"enabled\": true,\n  \"command\": \"npx\",\n  \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"path\"]\n}\n```\n\n**`config/tool_permissions.json`** - Tool permissions\n```json\n{\n  \"read_file\": {\n    \"enabled\": true,\n    \"category\": \"File Operations\",\n    \"risk\": \"Low\"\n  }\n}\n```\n\n## \ud83d\udd0d Troubleshooting\n\n### Tools Not Being Called\n\n1. **Check Model Training**\n   - Model needs function calling examples\n   - Verify training data format\n   - Test with known tool-calling model first\n\n2. **Check Configuration**\n   ```powershell\n   python scripts/verify_mcp_setup.py\n   ```\n\n3. **Check Logs**\n   - Open Debug tab in GUI\n   - Look for \"MCP tool executor initialized\"\n   - Check for tool discovery errors\n\n### Node.js Not Found\n\n1. Install from https://nodejs.org/\n2. Restart terminal\n3. Verify: `npx --version`\n\n### Server Connection Issues\n\n1. **Test manually** with MCP Inspector\n2. **Check server args** in mcp_servers.json\n3. **Verify package exists** on npm\n\n## \ud83d\udcda Next Steps\n\n### Immediate (Testing Infrastructure)\n\n1. \u2705 Install Node.js\n2. \u2705 Run verification script\n3. \u2705 Test servers with Inspector\n4. \u2705 Run mock model tests\n\n### Short-term (Model Training)\n\n1. Create function calling training dataset\n2. Train or fine-tune model with tool examples\n3. Test with simple tool calls first\n4. Expand to complex multi-tool scenarios\n\n### Long-term (Advanced Features)\n\n1. Add more MCP servers (GitHub, Slack, etc.)\n2. Implement multi-step tool chains\n3. Add tool result caching\n4. Implement streaming tool execution\n5. Add tool call history/analytics\n\n## \ud83c\udf89 Summary\n\n**What Works RIGHT NOW:**\n- \u2705 MCP configuration system\n- \u2705 Tool discovery and registration\n- \u2705 Chat integration with tool support\n- \u2705 Tool call parsing (3 formats)\n- \u2705 Tool execution framework\n- \u2705 Result formatting and synthesis\n\n**What Needs a Trained Model:**\n- \u274c Deciding WHEN to use a tool\n- \u274c Choosing the RIGHT tool\n- \u274c Generating correct parameters\n- \u274c Understanding tool results\n\n**Key Insight:** The infrastructure is 100% ready. Now you just need to train your model to be the \"driver\" that knows when and how to use these tools!\n\n## \ud83d\udcd6 Documentation References\n\n- Full testing guide: `docs/guide/features/MCP_TESTING_WITHOUT_MODEL.md`\n- Manual testing: `tests/test_mcp_servers_manual.md`\n- Quick start: Run `python scripts/mcp_quickstart.py`\n- Verification: Run `python scripts/verify_mcp_setup.py`\n", "tags": ["cli", "gui", "mcp"], "headings": [{"line": 0, "text": "MCP Integration Complete - Setup Guide"}, {"line": 2, "text": "\u2705 What's Been Done"}, {"line": 6, "text": "1. Configuration Files Created"}, {"line": 13, "text": "2. Core MCP Module Created (`src/aios/core/mcp/`)"}, {"line": 29, "text": "3. Chat Integration (`src/aios/gui/app/chat_operations.py`)"}, {"line": 38, "text": "\ud83d\ude80 How It Works"}, {"line": 40, "text": "Flow Diagram"}, {"line": 64, "text": "Supported Tool Call Formats"}, {"line": 85, "text": "\ud83d\udccb Available Tools (21 Tools Across 3 Servers)"}, {"line": 87, "text": "Filesystem Server (10 tools)"}, {"line": 99, "text": "Memory Server (9 tools)"}, {"line": 110, "text": "DuckDuckGo Server (2 tools)"}, {"line": 114, "text": "\ud83d\udd27 Setup Requirements"}, {"line": 116, "text": "Install Node.js (Required for MCP Servers)"}, {"line": 139, "text": "\ud83c\udfaf Testing the Integration"}, {"line": 141, "text": "Method 1: Use GUI Chat Tab"}, {"line": 175, "text": "Method 2: Use Mock Model Testing"}, {"line": 180, "text": "Run mock model demonstration"}, {"line": 183, "text": "Run integration tests"}, {"line": 187, "text": "Method 3: Manual Server Testing"}, {"line": 192, "text": "Install MCP Inspector"}, {"line": 195, "text": "Test filesystem server"}, {"line": 196, "text": "Manual Testing"}, {"line": 202, "text": "Test memory server"}, {"line": 205, "text": "Test DuckDuckGo"}, {"line": 209, "text": "\ud83e\udde0 Training Your Model to Use Tools"}, {"line": 211, "text": "Current Status"}, {"line": 215, "text": "What You Need"}, {"line": 266, "text": "\ud83d\udcca Monitoring Tool Usage"}, {"line": 268, "text": "In Chat Tab"}, {"line": 283, "text": "In Debug/Logs"}, {"line": 291, "text": "\ud83c\udf9b\ufe0f Configuration"}, {"line": 293, "text": "Enable/Disable Tools"}, {"line": 307, "text": "Edit Configuration Files"}, {"line": 330, "text": "\ud83d\udd0d Troubleshooting"}, {"line": 332, "text": "Tools Not Being Called"}, {"line": 349, "text": "Node.js Not Found"}, {"line": 355, "text": "Server Connection Issues"}, {"line": 361, "text": "\ud83d\udcda Next Steps"}, {"line": 363, "text": "Immediate (Testing Infrastructure)"}, {"line": 370, "text": "Short-term (Model Training)"}, {"line": 377, "text": "Long-term (Advanced Features)"}, {"line": 385, "text": "\ud83c\udf89 Summary"}, {"line": 403, "text": "\ud83d\udcd6 Documentation References"}]}, {"path": "guide/features/MCP_TESTING_WITHOUT_MODEL.md", "content": "# Testing MCP Tools Without a Trained Model\n\nThis guide explains how to verify that your MCP (Model Context Protocol) tools are working correctly before you have a fully trained model capable of using them.\n\n## Current Status\n\nBased on your codebase analysis:\n\n### \u2705 What You Have\n- **MCP Manager GUI Panel** - Configuration interface for servers and tools\n- **Tool Permissions System** - 37 tools across 6 categories defined in `config/tool_permissions.json`\n- **Default Server Configs** - Filesystem, Memory, and DuckDuckGo servers configured\n\n### \u26a0\ufe0f What's Missing\n- **MCP Server Integration** - The actual connection between MCP servers and model inference is not yet implemented\n- **Node.js/npm** - Required for running MCP servers (based on verification script)\n- **Tool Calling Training Data** - Your model needs to be trained on function/tool calling examples\n\n## Testing Approaches\n\n### 1. Quick Setup Verification\n\nRun the automated verification script:\n\n```powershell\npython scripts/verify_mcp_setup.py\n```\n\nThis checks:\n- Configuration file validity\n- Node.js/npm availability\n- MCP server package availability\n- Overall readiness\n\n### 2. Manual Server Testing (Most Reliable)\n\nUse the MCP Inspector to test servers directly:\n\n```powershell\n# Install MCP Inspector\nnpm install -g @modelcontextprotocol/inspector\n\n# Test filesystem server\nnpx @modelcontextprotocol/inspector npx -y @modelcontextprotocol/server-filesystem <PROJECT_ROOT>\n\n# Test memory (knowledge graph) server\nnpx @modelcontextprotocol/inspector npx -y @modelcontextprotocol/server-memory\n\n# Test DuckDuckGo search\nnpx @modelcontextprotocol/inspector npx -y @wulfic/server-duckduckgo\n```\n\nThe Inspector provides a web UI where you can:\n- See all available tools\n- Test tool calls with parameters\n- View responses\n- Verify error handling\n\n### 3. Automated Integration Tests\n\nRun the Python test suite:\n\n```powershell\n# Test configuration validity\npython tests/test_mcp_integration.py\n\n# Test with mock model\npython tests/test_mcp_mock_model.py\n```\n\nThe mock model test demonstrates:\n- How a model would decide to use tools\n- Tool call generation\n- Tool execution\n- Response synthesis\n\n### 4. pytest Integration\n\nIf you have pytest installed:\n\n```powershell\npytest tests/test_mcp_integration.py -v\n```\n\n## What Each Tool Does\n\n### File Operations (9 tools)\n- `read_file` - Read file contents\n- `write_file` - Create/overwrite files\n- `edit_file` - Line-based file editing\n- `list_directory` - List directory contents\n- `create_directory` - Create directories\n- `move_file` - Move/rename files\n- `search_files` - Find files by pattern\n- `get_file_info` - Get metadata\n- `directory_tree` - Recursive tree structure\n- `read_multiple_files` - Batch file reading\n\n### Memory & Knowledge (9 tools)\n- `create_entities` - Add entities to knowledge graph\n- `create_relations` - Link entities\n- `add_observations` - Add entity observations\n- `search_nodes` - Search knowledge graph\n- `read_graph` - Read entire graph\n- `open_nodes` - Get specific nodes\n- `delete_entities` - Remove entities\n- `delete_relations` - Remove relations\n- `delete_observations` - Remove observations\n\n### Web & Search (2 tools)\n- `web_search` - DuckDuckGo search\n- `fetch_webpage` - Download and parse pages\n\n### Code & Development (6 tools)\n- `run_code_snippet` - Execute Python code\n- `check_syntax_errors` - Validate Python syntax\n- `get_imports` - Analyze imports\n- `invoke_refactoring` - Auto-refactor\n- `get_python_environments` - List Python envs\n- `update_python_environment` - Switch env\n\n### System & Terminal (4 tools)\n- `run_terminal_command` - Execute shell commands\n- `get_terminal_output` - Read command output\n- `run_task` - Execute VS Code tasks\n- `get_task_output` - Read task output\n\n### Data Analysis (7 tools)\n- `semantic_search` - Semantic code search\n- `grep_search` - Text pattern search\n- `list_code_usages` - Find symbol references\n- `file_search` - Glob pattern search\n- `get_errors` - Get lint/compile errors\n\n## Training a Model to Use Tools\n\nFor your model to actually use these tools, you'll need:\n\n### 1. Function Calling Training Data\n\nCreate training examples in this format:\n\n```json\n{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Read the README file\"},\n    {\n      \"role\": \"assistant\",\n      \"content\": null,\n      \"tool_calls\": [{\n        \"name\": \"read_file\",\n        \"arguments\": {\"path\": \"README.md\"}\n      }]\n    },\n    {\n      \"role\": \"tool\",\n      \"content\": \"# Project Title\\n...\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Based on the README, this project is about...\"\n    }\n  ]\n}\n```\n\n### 2. Model Architecture Considerations\n\nYour model needs:\n- **Tool detection layer** - Recognize when to use tools\n- **Parameter extraction** - Generate correct tool arguments\n- **Response synthesis** - Integrate tool results into responses\n\nOptions:\n1. **Fine-tune existing tool-calling model** (e.g., GPT-4, Claude, Qwen2.5)\n2. **Add tool-calling head to your ACTv1 architecture**\n3. **Train from scratch with tool calling examples**\n\n### 3. Integration Code Needed\n\nYou'll need to add to your inference pipeline:\n\n```python\n# In src/aios/core/brains/actv1_brain.py or similar\n\ndef run_with_tools(self, task: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Run inference with MCP tool support.\"\"\"\n    # 1. Generate initial response (may include tool call)\n    response = self.generate(task)\n    \n    # 2. Check if model wants to use a tool\n    if tool_call := self._parse_tool_call(response):\n        # 3. Execute the tool via MCP server\n        tool_result = self._execute_mcp_tool(tool_call)\n        \n        # 4. Give result back to model for final response\n        final_response = self.generate_with_context(task, tool_result)\n        return final_response\n    \n    return response\n```\n\n## Verification Checklist\n\nBefore using with a trained model:\n\n- [ ] Node.js/npm installed\n- [ ] MCP servers can be launched via Inspector\n- [ ] Each server's tools are discoverable\n- [ ] Tool calls return expected responses\n- [ ] Error handling works (invalid parameters)\n- [ ] Configuration files exist and are valid\n- [ ] Integration tests pass\n\n## Next Steps\n\n1. **Install Node.js** if you haven't already\n2. **Run verification script** to check setup\n3. **Test servers manually** using MCP Inspector\n4. **Create function calling training data** for your domain\n5. **Add tool integration** to your model's inference code\n6. **Train/fine-tune** with tool calling examples\n\n## Resources\n\n- [MCP Specification](https://spec.modelcontextprotocol.io/)\n- [MCP Inspector](https://github.com/modelcontextprotocol/inspector)\n- [MCP Servers](https://github.com/modelcontextprotocol/servers)\n- [Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)\n\n## Testing Strategy Summary\n\n```\nWithout Trained Model:\n\u251c\u2500\u2500 Config Validation \u2705\n\u2502   \u251c\u2500\u2500 Run verify_mcp_setup.py\n\u2502   \u2514\u2500\u2500 Check config files exist\n\u2502\n\u251c\u2500\u2500 Server Testing \u2705\n\u2502   \u251c\u2500\u2500 Use MCP Inspector\n\u2502   \u251c\u2500\u2500 Test each server individually\n\u2502   \u2514\u2500\u2500 Verify tool discovery\n\u2502\n\u251c\u2500\u2500 Mock Testing \u2705\n\u2502   \u251c\u2500\u2500 Use test_mcp_mock_model.py\n\u2502   \u2514\u2500\u2500 Simulate tool calling flow\n\u2502\n\u2514\u2500\u2500 Integration Tests \u2705\n    \u251c\u2500\u2500 Run pytest suite\n    \u2514\u2500\u2500 Verify tool schemas\n\nWith Trained Model:\n\u251c\u2500\u2500 Tool Detection\n\u2502   \u2514\u2500\u2500 Model decides when to use tools\n\u2502\n\u251c\u2500\u2500 Parameter Generation\n\u2502   \u2514\u2500\u2500 Model creates correct arguments\n\u2502\n\u251c\u2500\u2500 Tool Execution\n\u2502   \u2514\u2500\u2500 MCP servers process requests\n\u2502\n\u2514\u2500\u2500 Response Synthesis\n    \u2514\u2500\u2500 Model uses results in answer\n```\n\nThe key insight: **You can verify the entire MCP infrastructure works without a trained model** - the model is just the decision-making layer on top!\n", "tags": ["gui", "mcp", "training"], "headings": [{"line": 0, "text": "Testing MCP Tools Without a Trained Model"}, {"line": 4, "text": "Current Status"}, {"line": 8, "text": "\u2705 What You Have"}, {"line": 13, "text": "\u26a0\ufe0f What's Missing"}, {"line": 18, "text": "Testing Approaches"}, {"line": 20, "text": "1. Quick Setup Verification"}, {"line": 34, "text": "2. Manual Server Testing (Most Reliable)"}, {"line": 39, "text": "Install MCP Inspector"}, {"line": 42, "text": "Test filesystem server"}, {"line": 45, "text": "Test memory (knowledge graph) server"}, {"line": 48, "text": "Test DuckDuckGo search"}, {"line": 58, "text": "3. Automated Integration Tests"}, {"line": 63, "text": "Test configuration validity"}, {"line": 66, "text": "Test with mock model"}, {"line": 76, "text": "4. pytest Integration"}, {"line": 84, "text": "What Each Tool Does"}, {"line": 86, "text": "File Operations (9 tools)"}, {"line": 98, "text": "Memory & Knowledge (9 tools)"}, {"line": 109, "text": "Web & Search (2 tools)"}, {"line": 113, "text": "Code & Development (6 tools)"}, {"line": 121, "text": "System & Terminal (4 tools)"}, {"line": 127, "text": "Data Analysis (7 tools)"}, {"line": 134, "text": "Training a Model to Use Tools"}, {"line": 138, "text": "1. Function Calling Training Data"}, {"line": 166, "text": "2. Model Architecture Considerations"}, {"line": 178, "text": "3. Integration Code Needed"}, {"line": 183, "text": "In src/aios/core/brains/actv1_brain.py or similar"}, {"line": 187, "text": "1. Generate initial response (may include tool call)"}, {"line": 190, "text": "2. Check if model wants to use a tool"}, {"line": 192, "text": "3. Execute the tool via MCP server"}, {"line": 195, "text": "4. Give result back to model for final response"}, {"line": 202, "text": "Verification Checklist"}, {"line": 214, "text": "Next Steps"}, {"line": 223, "text": "Resources"}, {"line": 230, "text": "Testing Strategy Summary"}]}, {"path": "guide/features/MEMORY_OPTIMIZATION.md", "content": "# Memory Optimization - AI-OS\nGenerated: October 20, 2025\nPurpose: Techniques to reduce VRAM and enable larger models/contexts\nStatus: Implemented (some items require verification)\n\n## Gradient Checkpointing\n- Default: Enabled; CLI: `--gradient-checkpointing` / `--no-gradient-checkpointing`\n- ~30\u201350% memory reduction; ~20% slowdown\n- Applied during model setup in training\n\n## Mixed Precision (AMP)\n- Default: Enabled; CLI: `--amp` / `--no-amp`\n- Uses autocast + GradScaler (FP16/BF16)\n- ~40\u201350% memory reduction; 2\u20133x speedup\n\n## 8-bit Optimizer\n- CLI: `--8bit-optimizer`; uses `bitsandbytes` if available\n- Quantizes optimizer states; large savings for 100M+ params\n- Fallback to AdamW if bnb unavailable\n\n## Chunked Training (Long Context)\n- Config options exist: `use_chunked_training`, `chunk_size`\n- Goal: split long sequences into chunks with accumulation\n- Docs: PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md, CONFIGURABLE_DATASET_CHUNK_SIZE.md\n- Status: Verification needed for training loop integration\n\n## Dynamic Batch Size Reduction\n- Auto-reduce batch size on CUDA OOM until 1; then fail if still OOM\n\n## Gradient Accumulation\n- CLI: `--gradient-accumulation-steps <n>` to simulate larger batches\n\n## Attention Kernel Optimizations\n- Uses FlashAttention when available; SDPA fallback\n- See: FLASH_ATTENTION.md and FLASH_ATTENTION_VS_CHUNKING.md\n\n## Example configurations (Windows PowerShell)\n\n- Conservative VRAM profile:\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 50 --batch-size 1 --gradient-accumulation-steps 8 --gradient-checkpointing --amp --8bit-optimizer --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n- Long-context friendly (requires FA2-capable GPU or will fallback):\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 50 --batch-size 1 --gradient-checkpointing --amp --8bit-optimizer --window-size 2048 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n## Verifying savings\n- Watch metrics/logs for memory reports if emitted; compare throughput with/without options.\n- For FlashAttention, see the Flash Attention doc for how to confirm activation vs fallback.\n\n## Troubleshooting\n- CUDA OOM: Reduce `--batch-size`, increase `--gradient-accumulation-steps`, or enable `--8bit-optimizer` and checkpointing.\n- bitsandbytes missing: Install the extra or run without `--8bit-optimizer` (it will fallback).\n- Unstable FP16: Try BF16 if supported (`--amp` selection is automatic) or temporarily `--no-amp`.\n\nRelated: Core Training, Model Architecture\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Memory Optimization - AI-OS"}, {"line": 5, "text": "Gradient Checkpointing"}, {"line": 10, "text": "Mixed Precision (AMP)"}, {"line": 15, "text": "8-bit Optimizer"}, {"line": 20, "text": "Chunked Training (Long Context)"}, {"line": 26, "text": "Dynamic Batch Size Reduction"}, {"line": 29, "text": "Gradient Accumulation"}, {"line": 32, "text": "Attention Kernel Optimizations"}, {"line": 36, "text": "Example configurations (Windows PowerShell)"}, {"line": 48, "text": "Verifying savings"}, {"line": 52, "text": "Troubleshooting"}]}, {"path": "guide/features/MODEL_ARCHITECTURE.md", "content": "# Model Architecture - AI-OS\nGenerated: October 20, 2025\nPurpose: HRM model components and configurable parameters\nStatus: Implemented\n\n## Files\n- `src/aios/core/hrm_models/` \u2013 Core modules\n- `src/aios/core/hrm.py` \u2013 Top-level assembly\n\n## Hierarchical Structure\n- High-level (H) and low-level (L) blocks\n- Files: hierarchical_recurrence.py, higher_level_block.py, lower_level_block.py\n- Params: `--h-layers`, `--l-layers`, `--h-cycles`, `--l-cycles`\n\n## Adaptive Computation Time (ACT)\n- File: act.py; param: `--halt-max-steps`\n\n## Attention Mechanisms\n- Files: attention.py, efficient_attention.py\n- Types: MHA, sliding window, efficient variants\n- Params: `--num-heads`, `--window-size`\n\n## Position Encodings\n- File: position_encoding.py\n- Types: RoPE (default), sinusoidal, learned\n- Param: `--pos-encodings`\n\n## Feed-Forward Networks\n- File: ffn.py; GLU; `--expansion`\n\n## Residual Connections\n- Throughout; includes LayerNorm and skip paths\n\n## Selecting and validating architectures (CLI)\n\nArchitecture is configured through `aios hrm-hf train-actv1` flags. Typical flags include:\n- Depth: `--h-layers`, `--l-layers`\n- Cycles: `--h-cycles`, `--l-cycles`\n- Width/heads: `--hidden-size`, `--num-heads`, `--expansion`\n- Positional encodings: `--pos-encodings`\n- Attention window: `--window-size` (sliding window) with FlashAttention or SDPA\n\n### Example: small hierarchical model (Windows PowerShell)\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1 --batch-size 2 --h-layers 2 --l-layers 4 --hidden-size 512 --num-heads 8 --expansion 4 --pos-encodings rope --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n### Validate model/tokenizer resolution\nUse the same command with `--steps 1` to ensure it initializes and writes metrics/checkpoints under `artifacts/brains/actv1/`.\n\nNotes:\n- If you point `--model` to a local path, ensure tokenizer files are present or resolvable from HF.\n- Windowed attention (`--window-size`) limits attention range; see Flash Attention docs for trade-offs.\n\nRelated: Memory Optimization, Dynamic Subbrains/MoE\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Model Architecture - AI-OS"}, {"line": 5, "text": "Files"}, {"line": 9, "text": "Hierarchical Structure"}, {"line": 14, "text": "Adaptive Computation Time (ACT)"}, {"line": 17, "text": "Attention Mechanisms"}, {"line": 22, "text": "Position Encodings"}, {"line": 27, "text": "Feed-Forward Networks"}, {"line": 30, "text": "Residual Connections"}, {"line": 33, "text": "Selecting and validating architectures (CLI)"}, {"line": 42, "text": "Example: small hierarchical model (Windows PowerShell)"}, {"line": 47, "text": "Validate model/tokenizer resolution"}]}, {"path": "guide/features/MULTI_GPU_DISTRIBUTED.md", "content": "# Multi-GPU & Distributed\nGenerated: October 20, 2025\nPurpose: How to use multiple GPUs with AI-OS training (DDP and Windows-compatible parallel mode)\nStatus: DDP supported via launcher or internal spawn; Windows defaults to parallel-independent mode. DeepSpeed flags exist but engine wiring is limited.\n\n## Overview\n\nAI-OS supports two multi-GPU modes:\n\n1) DDP (torch.distributed) \u2014 classic data parallel synchronized training.\n2) Parallel independent training \u2014 Windows-friendly mode that assigns distinct data chunks to each GPU without DDP synchronization, then aggregates progress/checkpoints.\n\nOn Windows, DDP often fails due to backend limitations. The code provides an internal spawn pathway and falls back to parallel-independent mode when needed.\n\nKey flags in `aios hrm-hf train-actv1`:\n- --ddp: enable DDP when a launcher or internal spawn is available\n- --cuda-ids \"0,1,...\": which GPUs to use\n- --world-size N: number of DDP processes/GPUs (optional; inferred from --cuda-ids)\n- --parallel-independent: force the Windows-compatible multi-GPU path (no gradient sync)\n\nRelated files:\n- CLI: `src/aios/cli/hrm_hf_cli.py` (options: --ddp, --world-size, --cuda-ids, --parallel-independent)\n- DDP internals: `src/aios/cli/hrm_hf/ddp/utils.py`, `src/aios/cli/hrm_hf/ddp/worker_main.py`\n\n## Using DDP\n\nDDP requires either:\n- An external launcher (recommended on Linux): torchrun/torch.distributed.run\n- Internal spawn mode (set AIOS_DDP_SPAWN=1), useful on Windows/GUI\n\nWindows PowerShell examples (GPU IDs 0 and 1):\n\n1) Internal spawn (recommended on Windows)\n- Set an environment variable for this PowerShell session and run training:\n\n\t$env:AIOS_DDP_SPAWN = \"1\"\n\t.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --ddp --cuda-ids \"0,1\" --world-size 2 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 10 --batch-size 4 --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n\nNotes:\n- The process will spawn per-rank workers and use the gloo backend on Windows.\n- If early failures are detected during DDP init, training will fall back to single-GPU unless you passed --strict.\n\n2) External launcher (Linux-oriented, shown for reference)\n- torchrun --standalone --nproc_per_node 2 .venv/\u2026/python -m aios.cli.aios hrm-hf train-actv1 --ddp --model gpt2 --dataset-file \u2026\n- On Windows, torchrun may still be unstable; prefer internal spawn or parallel-independent.\n\nVerifying DDP:\n- Logs will include keys like {\"ddp\": \"external_launcher_detected\"} or {\"ddp\": \"spawning_workers\"}.\n- Each rank writes progress, and total throughput should scale with GPU count.\n\nTroubleshooting DDP on Windows:\n- If you see errors mentioning libuv/NCCL/backends or hostname resolution, the code automatically forces gloo, sets MASTER_ADDR=127.0.0.1, and disables libuv.\n- Still failing? Try:\n\t- Close Docker Desktop (prevents hostname pollution)\n\t- Ensure firewall allows local port 29500\n\t- Set $env:AIOS_DDP_SPAWN = \"1\" and retry\n\t- Or use --parallel-independent instead of --ddp\n\n## Parallel Independent Training (Windows-friendly)\n\nThis mode runs coordinated multi-GPU training without DDP. Each GPU trains on different dataset chunks (no duplication) and progress/checkpoints are tracked centrally. No gradient synchronization occurs, so results won\u2019t be identical to DDP, but it achieves strong throughput on Windows.\n\nExample (2 GPUs):\n\n\t.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --parallel-independent --cuda-ids \"0,1\" --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 200 --batch-size 4 --dataset-chunk-size 4000 --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n\nNotes:\n- Uses the block/chunk system to guarantee distinct chunks per GPU.\n- Creates/updates `chunk_tracker_state.json` under the brain bundle for resume and epoch tracking.\n- Ideal fallback when DDP isn\u2019t viable on Windows.\n\n## DeepSpeed ZeRO (status)\n\nFlag surface exists: --zero-stage {none|zero1|zero2|zero3}\nConfigs present: config/deepspeed_zero1.json, config/deepspeed_zero2.json, config/deepspeed_zero3.json\n\nCurrent state:\n- The CLI and GUI expose ZeRO stage selection and use it for estimators/UI.\n- Full DeepSpeed engine initialization is not wired in the training loop at this time.\n- Treat ZeRO as experimental/planned. If you require ZeRO now, you\u2019ll need to integrate deepspeed.initialize() around the model/optimizer in training code.\n\n## Inputs and Outputs\n\nInputs (typical):\n- --model <name or path>\n- --dataset-file <path or hf://\u2026>\n- --steps, --batch-size, --halt-max-steps, etc.\n- Multi-GPU picks: --ddp/--parallel-independent, --cuda-ids, --world-size\n\nOutputs:\n- Metrics JSONL: artifacts/brains/actv1/metrics.jsonl (configurable)\n- Brain bundle(s): artifacts/brains/actv1/<brain-name>/\n- chunk_tracker_state.json (parallel-independent runs)\n\n## Quick starts (PowerShell)\n\nDDP (internal spawn):\n\n\t$env:AIOS_DDP_SPAWN = \"1\"\n\t.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --ddp --cuda-ids \"0,1\" --world-size 2 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 5 --batch-size 2 --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n\nParallel independent (Windows-friendly):\n\n\t.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --parallel-independent --cuda-ids \"0,1\" --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 5 --batch-size 2 --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n\nSee also:\n- Parallel Training Block/Chunk System\n- Memory Optimization\n- Core Training\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Multi-GPU & Distributed"}, {"line": 5, "text": "Overview"}, {"line": 24, "text": "Using DDP"}, {"line": 58, "text": "Parallel Independent Training (Windows-friendly)"}, {"line": 71, "text": "DeepSpeed ZeRO (status)"}, {"line": 81, "text": "Inputs and Outputs"}, {"line": 94, "text": "Quick starts (PowerShell)"}]}, {"path": "guide/features/PARALLEL_TRAINING_BLOCK_CHUNK_SYSTEM.md", "content": "# Parallel Training Block/Chunk Distribution System\n\nCanonical source for how we stream and distribute data across GPUs without duplication. For the per-run chunk knob, see CONFIGURABLE_DATASET_CHUNK_SIZE.md.\n\n## Overview\n\nThe system streams datasets in large blocks and serves only the requested chunks to each GPU. A shared tracker guarantees no two GPUs train the same chunk. It supports both HuggingFace streaming datasets and local files, with epoch detection and resume.\n\n## Architecture\n\n### Hierarchy\n\n```\nDataset (e.g., 10M samples)\n  \u251c\u2500 Block 0 (\u2248100k samples) \u2500 streaming from HuggingFace or sliced from local files\n  \u2502   \u251c\u2500 Chunk 0 (e.g., 4k samples) \u2500 GPU 0 trains\n  \u2502   \u251c\u2500 Chunk 1 (e.g., 4k samples) \u2500 GPU 1 trains\n  \u2502   \u251c\u2500 Chunk 2 (e.g., 4k samples) \u2500 GPU 0 trains\n  \u2502   \u2514\u2500 ...\n  \u251c\u2500 Block 1 (\u2248100k samples)\n  \u2502   \u2514\u2500 ...\n  \u2514\u2500 ...\n\nEpoch = one full pass through ALL blocks\n```\n\n### Key Components\n\n1) BlockManager (`src/aios/cli/hrm_hf/block_manager.py`)\n- Streams HuggingFace datasets in blocks (default \u2248100k samples) and caches them on disk\n- Loads only the requested chunk into memory on demand\n- Prefetches metadata to detect last block\n- Works with local files by slicing them into block-sized windows\n\n2) ChunkTracker (state persisted in the brain bundle)\n- Tracks which (block_id, chunk_id) were trained\n- Prevents duplication across GPUs\n- Tracks blocks visited per epoch and total steps\n- Persists to `chunk_tracker_state.json` for resume\n\n3) Parallel Control\n- In parallel-independent mode, each GPU requests the next untrained chunk\n- In DDP, data loading can still use block/chunk mechanics while gradients are synchronized\n\n## Configuration surface\n\nTrainingConfig defaults (see `src/aios/core/hrm_training/training_config/base_fields.py`):\n\n- samples_per_block: 100000 (for HF streaming/local slicing; auto-detected/recorded)\n- dataset_chunk_size: 4000 (per-iteration chunk size; user knob `--dataset-chunk-size`)\n- stop_after_epoch: false (toggle via `--stop-after-epoch`)\n- iterate: false (toggle via `--iterate`)\n\nNotes:\n- There is no `--samples-per-block` CLI flag. samples_per_block is chosen/detected inside dataset setup and recorded into metrics for UI.\n- Adjust memory/throughput primarily via `--dataset-chunk-size`.\n\n## Stopping conditions\n\n1) Steps limit: `--steps N` stops after N steps.\n2) Stop after epoch: `--stop-after-epoch` stops after a full dataset pass.\n3) Iterate mode: `--iterate` loops indefinitely, rolling epochs.\n\n## Windows PowerShell examples\n\nTwo-GPU parallel independent training on a HuggingFace dataset:\n\n  .venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --parallel-independent --cuda-ids \"0,1\" --model gpt2 --dataset-file \"hf://wikitext:wikitext-2-raw-v1:train\" --dataset-chunk-size 4000 --steps 500 --batch-size 4 --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n\nStop after an epoch using a local dataset:\n\n  .venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --parallel-independent --cuda-ids \"0,1,2\" --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --dataset-chunk-size 2000 --stop-after-epoch --batch-size 4 --steps 10000 --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n\nContinuous iterate mode on two GPUs:\n\n  .venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --parallel-independent --cuda-ids \"0,1\" --model gpt2 --dataset-file \"hf://c4:en:train\" --dataset-chunk-size 4000 --iterate --batch-size 4 --halt-max-steps 1 --log-file artifacts/brains/actv1/metrics.jsonl\n\n## Chunk claiming (no-dup guarantee)\n\n### No Duplication Guarantee\n\nEach GPU claims chunks atomically from ChunkTracker:\n\n```python\ndef get_next_untrained_chunk(block_id, total_chunks, gpu_id):\n    with lock:\n        for chunk_id in range(total_chunks):\n            if (block_id, chunk_id) not in completed_chunks:\n                # Mark as in-progress\n                return chunk_id\n        return None  # All chunks trained\n```\n\nKey points:\n- Thread-safe: lock prevents races\n- Atomic claiming: chunk is marked before training\n- No duplicates: trained chunks are skipped\n- Fair distribution: first-come-first-served\n\n### Resume Capability\n\nTraining state saved to `chunk_tracker_state.json` (under the brain bundle):\n\n```json\n{\n  \"completed_chunks\": [\n    {\"block_id\": 0, \"chunk_id\": 0, \"gpu_id\": 0, \"step\": 125, \"samples_trained\": 4000},\n    {\"block_id\": 0, \"chunk_id\": 1, \"gpu_id\": 1, \"step\": 127, \"samples_trained\": 4000}\n  ],\n  \"current_epoch\": 0,\n  \"blocks_this_epoch\": [0, 1, 2],\n  \"total_steps\": 250\n}\n```\n\nOn resume:\n- Skips already-trained chunks\n- Continues from last step count\n- Maintains epoch tracking\n\n## Epoch Detection\n\nAn **epoch** = Training on ALL blocks in the dataset once.\n\n### Detection algorithm\n\n```python\ndef check_epoch_complete(total_blocks):\n  return len(blocks_this_epoch) >= total_blocks\n```\n\nWhen epoch completes:\n1. ChunkTracker marks epoch complete\n2. If `stop_after_epoch=True` \u2192 Stop training\n3. If `iterate=True` \u2192 Start new epoch (reset block tracking)\n4. Otherwise \u2192 Stop training\n\n### Last block detection\n\nBlockManager detects the last block by attempting to download block N+1:\n\n```python\n# Loading block 5\nblock_5 = load_block(5)  # Returns 100k samples\nblock_6 = load_block(6)  # Returns 0 samples \u2192 Last block detected\n\nblock_5.is_last_block = True\n```\n\nThis works for:\n- HuggingFace datasets: streaming ends naturally\n- Local files: EOF reached\n- Large datasets: consistent detection\n\n## Performance Considerations\n\n### Memory usage\n\nBlock metadata and chunks are cached. Only requested chunks are loaded into RAM at any time, keeping memory bounded mostly by `--dataset-chunk-size` and model batch/sequence length.\n\n### Network I/O\n\nHuggingFace streaming benefits from cached blocks on disk and metadata prefetching to hide latency.\n\n### GPU Utilization\n\nOptimal chunk size depends on VRAM and sequence length:\n- Smaller chunks reduce VRAM but can increase coordination overhead\n- Larger chunks improve throughput but increase memory\n- Defaults: 4000 works well for 12\u201316 GB VRAM\n\n## Troubleshooting\n\n### Issue: GPUs training duplicate data\n\nCause: chunk tracker state not found/shared.\n\nSolution: ensure a single brain bundle is used and that `chunk_tracker_state.json` is writable by all worker processes.\n\n### Issue: Training stops prematurely\n\nCheck stopping conditions:\n```python\nconfig.steps  # max steps reached?\nconfig.stop_after_epoch  # epoch completed?\n```\n\nDebug:\n```python\nstats = chunk_tracker.get_progress_stats()\nprint(stats)  # total_steps, blocks_this_epoch, current_epoch\n```\n\n### Issue: Epoch not completing\n\nPossible causes:\n1) total_blocks not detected yet (keep training until prefetch finishes)\n2) Some blocks never requested (short runs)\n3) ChunkTracker state corrupted\n\nSolution:\n```python\ntotal_blocks = block_manager.get_total_blocks()\nprint(f\"Total blocks: {total_blocks}\")\n\nstats = chunk_tracker.get_progress_stats()\nprint(f\"Blocks this epoch: {stats['blocks_this_epoch']}\")\n```\n\n### Issue: Out of memory\n\nReduce memory usage:\n```python\nconfig.dataset_chunk_size = 2000  # smaller chunk\nconfig.batch_size = 4             # smaller batch\nconfig.max_seq_len = 128          # shorter sequences\n```\n\n## System Architecture\n\n### Block Management\n\n```python\n# Streams data in blocks from HuggingFace or local files\nblock_manager = BlockManager(dataset_file, samples_per_block=100k)\n\n# Distributes chunks across GPUs without duplication\nchunk_tracker = ChunkTracker(state_file)\n\n# Features:\n# - Full training progress tracking\n# - Automatic epoch detection  \n# - Resume capability from checkpoints\n# - Support for all stopping conditions\n```\n\n### Key Components\n\n1. **BlockManager**: Downloads and caches 100k-sample blocks from datasets\n2. **ChunkTracker**: Tracks which chunks each GPU has processed\n3. **State Persistence**: Saves progress to enable resuming training\n4. **Epoch Detection**: Automatically detects when full dataset is processed\n\n## Testing\n\n### Test 1: No duplicate training\n\n```python\n# Track all chunks trained by each GPU\ngpu0_chunks = set()\ngpu1_chunks = set()\n\n# After training\nassert gpu0_chunks.isdisjoint(gpu1_chunks)  # No overlap\n```\n\n### Test 2: Epoch detection\n\n```python\n# Train with stop_after_epoch=True\nconfig.stop_after_epoch = True\n\n# Should stop when all blocks visited\nfinal_stats = chunk_tracker.get_progress_stats()\nassert final_stats['blocks_this_epoch'] == total_blocks\n```\n\n### Test 3: Resume capability\n\n```python\n# Train for 100 steps\nconfig.steps = 100\nrun_training()\n\n# Resume and train 100 more\nconfig.steps = 200\nrun_training()\n\n# Should have 200 total steps, no duplicate chunks\nassert chunk_tracker.total_steps == 200\n```\n\n## Related: Configurable dataset chunk size\n\nSee CONFIGURABLE_DATASET_CHUNK_SIZE.md for usage guidance, examples, and how it interacts with batch size and sequence length.\n\n## Future enhancements\n\n1) Dynamic load balancing (fast GPUs get more chunks)\n2) Chunk prioritization (curriculum)\n3) Distributed tracker (multi-node)\n4) Adaptive block sizing\n5) Chunk prefetching\n6) Partial-epoch checkpoints\n\n## Summary\n\nThis streaming block/chunk system provides:\n\n- No duplicate training across GPUs\n- Proper block management for HF/local datasets\n- Chunk-level tracking with resume\n- Epoch detection and iterate mode\n- Thread-safe coordination and persistence\n- Memory-efficient operation by loading only the needed chunks\n\nIt replaces older approaches that loaded entire datasets into memory without proper progress tracking.\n", "tags": ["cli", "datasets", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Parallel Training Block/Chunk Distribution System"}, {"line": 4, "text": "Overview"}, {"line": 8, "text": "Architecture"}, {"line": 10, "text": "Hierarchy"}, {"line": 26, "text": "Key Components"}, {"line": 44, "text": "Configuration surface"}, {"line": 57, "text": "Stopping conditions"}, {"line": 63, "text": "Windows PowerShell examples"}, {"line": 77, "text": "Chunk claiming (no-dup guarantee)"}, {"line": 79, "text": "No Duplication Guarantee"}, {"line": 88, "text": "Mark as in-progress"}, {"line": 99, "text": "Resume Capability"}, {"line": 120, "text": "Epoch Detection"}, {"line": 124, "text": "Detection algorithm"}, {"line": 137, "text": "Last block detection"}, {"line": 142, "text": "Loading block 5"}, {"line": 154, "text": "Performance Considerations"}, {"line": 156, "text": "Memory usage"}, {"line": 160, "text": "Network I/O"}, {"line": 164, "text": "GPU Utilization"}, {"line": 171, "text": "Troubleshooting"}, {"line": 173, "text": "Issue: GPUs training duplicate data"}, {"line": 179, "text": "Issue: Training stops prematurely"}, {"line": 193, "text": "Issue: Epoch not completing"}, {"line": 209, "text": "Issue: Out of memory"}, {"line": 218, "text": "System Architecture"}, {"line": 220, "text": "Block Management"}, {"line": 223, "text": "Streams data in blocks from HuggingFace or local files"}, {"line": 226, "text": "Distributes chunks across GPUs without duplication"}, {"line": 229, "text": "Features:"}, {"line": 230, "text": "- Full training progress tracking"}, {"line": 231, "text": "- Automatic epoch detection  "}, {"line": 232, "text": "- Resume capability from checkpoints"}, {"line": 233, "text": "- Support for all stopping conditions"}, {"line": 236, "text": "Key Components"}, {"line": 243, "text": "Testing"}, {"line": 245, "text": "Test 1: No duplicate training"}, {"line": 248, "text": "Track all chunks trained by each GPU"}, {"line": 252, "text": "After training"}, {"line": 256, "text": "Test 2: Epoch detection"}, {"line": 259, "text": "Train with stop_after_epoch=True"}, {"line": 262, "text": "Should stop when all blocks visited"}, {"line": 267, "text": "Test 3: Resume capability"}, {"line": 270, "text": "Train for 100 steps"}, {"line": 274, "text": "Resume and train 100 more"}, {"line": 278, "text": "Should have 200 total steps, no duplicate chunks"}, {"line": 282, "text": "Related: Configurable dataset chunk size"}, {"line": 286, "text": "Future enhancements"}, {"line": 295, "text": "Summary"}]}, {"path": "guide/features/TOKENIZERS.md", "content": "# Tokenizers - AI-OS\nGenerated: October 20, 2025\nPurpose: Tokenizer support and configuration\nStatus: Implemented (verification varies per model)\n\n## Files\n- `src/aios/core/tokenizers/`\n\n## Supported Tokenizers\n- Verified: GPT-2 family (default)\n- Likely supported via HuggingFace: Qwen, Mistral, Code Llama, DeepSeek-Coder, StarCoder2, Phi-3, Llama 3 (HF auth may be required)\n- Not supported: Vision/multimodal and specialized domain tokenizers\n\n## Configuration\n- Tokenizer is resolved from the selected `--model` (HF hub id or local path)\n- Examples: `gpt2`, `artifacts/hf_implant/base_model`, `mistralai/Mistral-7B-v0.1`\n- Local override: place tokenizer files under `artifacts/hf_implant/tokenizers/` and point `--model` to the matching local model path\n\n## Inputs\n- Text data from datasets (txt/jsonl), read by dataset readers; tokenization occurs during training/eval\n- Tokenizer model files resolved via HuggingFace AutoTokenizer or local tokenizer.json\n\n## Try it: quick check\nTokenization is engaged implicitly by training:\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1 --batch-size 2 --halt-max-steps 1 --eval-batches 1 --log-file artifacts/brains/actv1/metrics.jsonl\n```\nExpected: pipeline loads GPT-2 tokenizer, logs train/eval steps.\n\n## Notes and edge cases\n- HF auth for some models: set `HF_TOKEN` env var if private models are required\n- Sequence length: Max sequence governed by model config; adjust via training flags (see Core Training)\n- Unicode handling: Non-ASCII text is supported; `--ascii-only` exists on some dataset paths to filter\n- Mismatched model/tokenizer: Ensure the model path and tokenizer are compatible to avoid errors\n\nRelated: Datasets, Core Training\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["datasets", "evaluation", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "Tokenizers - AI-OS"}, {"line": 5, "text": "Files"}, {"line": 8, "text": "Supported Tokenizers"}, {"line": 13, "text": "Configuration"}, {"line": 18, "text": "Inputs"}, {"line": 22, "text": "Try it: quick check"}, {"line": 29, "text": "Notes and edge cases"}]}, {"path": "guide/features/TOOLS_INTEGRATIONS.md", "content": "# Tools & Integrations\nGenerated: October 20, 2025\nPurpose: Built-in tools and OS/HF integrations with runnable commands, inputs/outputs, and platform notes\nStatus: Implemented (some features are gated/experimental and noted below)\n\n## Web tools (search, crawl, datasets)\n- Source: `src/aios/tools/web.py`, `src/aios/tools/crawler.py`\n- What you get:\n\t- DuckDuckGo HTML-only search with ad/redirect filtering (`ddg_search`)\n\t- Robust HTML parsing and link extraction\n\t- Polite crawler with robots.txt, BFS, per-origin throttling, optional Playwright rendering and Trafilatura extraction\n\t- Turn searches into datasets via CLI builders (images, text, videos, websites)\n\n### Configuration knobs\n- Env vars (affect search/crawl):\n\t- `AIOS_DDG_KL`, `AIOS_DDG_KAD` \u2192 locale/region for DDG (defaults from `config/default.yaml:web.ddg_params`)\n\t- `AIOS_WEB_UA` / `AIOS_WEB_UA_SUFFIX` \u2192 user agent override/suffix\n- YAML: `config/default.yaml:web` mirrors the above defaults and can be edited.\n\n### Crawl a URL \u2192 JSON summary or dataset\n- Command (Windows PowerShell):\n\t- One page fetch/parse\n\t\t- aios crawl https://example.com --ttl-sec 3600 --progress\n\t- Recursive BFS within the same domain, rate-limited\n\t\t- aios crawl https://example.com --recursive --max-pages 25 --max-depth 2 --rps 2 --progress\n\t- Store pages as a text dataset JSONL under datasets pool\n\t\t- aios crawl https://example.com --recursive --store-dataset web_crawl/example --overwrite --progress\n- Inputs/flags (subset):\n\t- `--ttl-sec` cache TTL seconds; `--render` Playwright render; `--trafilatura` article extraction; `--rps` or `--delay-ms` throttling\n\t- `--same-domain/--any-domain`, `--max-pages`, `--max-depth`, `--progress`\n\t- `--store-dataset NAME` outputs to datasets pool at NAME/data.jsonl; `--overwrite` to reset\n- Outputs:\n\t- Progress: JSONL lines on stdout when `--progress` is set, e.g. {\"event\":\"page\",\"n\":1,...}\n\t- Final JSON: pages summary, count, total_chars, and dataset_path/wrote_bytes when `--store-dataset` is used\n- Notes:\n\t- Respect robots.txt by default; pass `--no-robots` for tests only\n\t- Playwright requires browser install; on Windows run once: playwright install\n\n### Build datasets from the web\n- Images\n\t- Command:\n\t\t- aios datasets-build-images \"boats\" --store-dataset boats_v1 --max-images 200 --per-site 40 --pages-per-site 8 --search-results 10 --rps 2 --progress\n\t- Inputs: `--allow-ext jpg,png,webp`, `--near-duplicate-threshold 8`, `--file-prefix boats`\n\t- Outputs: `artifacts path`: datasets/images/boats_v1 with image files + manifest.jsonl (path,label,source_url,page_url,title,alt)\n- Text\n\t- Command:\n\t\t- aios datasets-build-text \"boats\" --store-dataset boats_text_v1 --max-docs 100 --search-results 10 --min-chars 400 --progress\n\t- Inputs: `--allow-ext txt,pdf,docx` (if set, fetches documents by extension/content-type); `--file-prefix`\n\t- Outputs: datasets/text/boats_text_v1/*.txt + manifest.jsonl (path,label,url,title,chars,excerpt)\n- Videos\n\t- Command:\n\t\t- aios datasets-build-videos \"boats\" --store-dataset boats_vid_v1 --max-videos 25 --per-site 5 --min-bytes 50000 --progress\n\t- Inputs: `--allow-ext mp4,webm,mov,m4v`, `--file-prefix`\n\t- Outputs: datasets/videos/boats_vid_v1/*.mp4|*.webm|\u2026 + manifest.jsonl (path,label,source_url,page_url,bytes)\n- Websites (HTML snapshots)\n\t- Command:\n\t\t- aios datasets-build-websites \"boats\" --store-dataset boats_sites_v1 --max-pages 30 --per-site 10 --search-results 10 --progress\n\t- Outputs: datasets/websites/boats_sites_v1/pages/*.html + manifest.jsonl (path,url,title,bytes,links)\n\nRelated docs: see `docs/guide/CORE_TRAINING.md` for using JSONL datasets; `docs/guide/DATASETS.md` for dataset pool and storage caps.\n\n## Filesystem and OS tools (guarded)\n- Source: `src/aios/tools/fs.py`, `src/aios/tools/os.py`\n- What you get:\n\t- `write_text(path, data, cfg, conn)` writes a file with WriteGuard and SafetyBudget enforcement\n\t- `get_system_info()` returns basic platform info\n- Guard/budget behavior:\n\t- Guards read allow/deny from config; budgets use DB-backed usage with tier defaults from `aios.core.budgets`\n\t- Domains charged: `file_writes`\n- Example usage via CLI budgets helpers:\n\t- Show guard paths: aios guards-show\n\t- Simulate a service change budget decision: aios service-restart ssh --dry-run\n\n## Root-helper and service adapters\n- Source: `src/aios/tools/root_helper_client.py`, `src/aios/tools/service_adapter.py`\n- What you get:\n\t- Optional privileged D-Bus client (Linux only). On Windows/macOS, gracefully returns via: \"unavailable\".\n\t- Read-only service diagnostics via local systemctl/journalctl fallback when root-helper is not available.\n- Read status and logs for a unit\n\t- aios status --recent 1 --unit ssh\n\t- For targeted triage, prefer Agent CLI operators below\n- CLI operators for triage (store artifacts):\n\t- aios op-run journal_summary_from_text --unit ssh --lines 200 --label ssh\n\t- aios op-run journal_trend_from_text --unit ssh --lines 200 --label ssh --buckets 12\n\t- Artifacts saved in DB (see Core CLI status for recent artifacts).\n- Notes:\n\t- On Linux, providing a running root-helper yields via: \"root-helper\" in outputs; otherwise via: \"local\".\n\t- On Windows, these commands return via: \"unavailable\" (no systemd).\n\n## Journal parser utilities\n- Source: `src/aios/tools/journal_parser.py`\n- Functions:\n\t- `severity_counts(text) -> Dict[str,int]` heuristic severity tallies across emerg\u2026debug\n- How it\u2019s used:\n\t- The Agent CLI operators compute summaries/trends and persist to DB artifacts. See: `aios op-run ...` above.\n\n## Package/service simulators (budgeted)\n- Source: `src/aios/tools/service.py`, `src/aios/tools/pkg.py`, `src/aios/tools/privileged.py`\n- What you get:\n\t- `restart_service(name, simulate=True)` and `pkg.install/remove(name, simulate=True)` record budget usage for service_changes/pkg_ops\n\t- `run_privileged(fn, ...)` wraps a function and charges privileged_calls budget\n- Try it:\n\t- aios service-restart docker --dry-run\n\t- aios pkg-install git --dry-run\n\n## MCP and external tools (GUI)\n- Source: GUI MCP Manager panel under `src/aios/gui/components/mcp_manager_panel/*`\n- What you get:\n\t- Visual editor for MCP servers and tool permissions using `config/mcp_servers.json` and `config/tool_permissions.json`\n\t- Enable/disable servers and toggle tool permissions; refresh state from disk\n- Status:\n\t- GUI available. Programmatic MCP wiring is scoped to UI; CLI equivalents are not exposed yet.\n\t- If config files are missing, the panel initializes with defaults and saves back on change.\n\t- Panel screenshots in GUI doc: see `docs/guide/features/GUI_FEATURES.md` (MCP & Tools tab).\n\n## Unlimiformer (planned)\n- Source: `src/aios/integrations/unlimiformer/__init__.py`\n- Status: Phase 1 scaffolding; disabled by default via config\n- Config key (example):\n\t- config.default.yaml \u2192 brains.trainer_overrides.unlimiformer.enabled: false\n- Notes:\n\t- When enabled in future phases, the model will be augmented for long-context eval using FAISS; Windows defaults to CPU FAISS.\n\n## Quick reference (commands)\n- Crawling\n\t- aios crawl <url> --recursive --max-pages 25 --max-depth 2 --rps 2 --progress\n- Datasets builders\n\t- aios datasets-build-images \"topic\" --store-dataset name --max-images 200 --progress\n\t- aios datasets-build-text \"topic\" --store-dataset name --max-docs 100 --progress\n\t- aios datasets-build-videos \"topic\" --store-dataset name --max-videos 50 --progress\n\t- aios datasets-build-websites \"topic\" --store-dataset name --max-pages 30 --progress\n- Budgets and guards\n\t- aios guards-show\n\t- aios service-restart ssh --dry-run\n\t- aios pkg-install git --dry-run\n\nRelated: Datasets, Advanced Features\n\nBack to Feature Index: [COMPLETE_FEATURE_INDEX.md](COMPLETE_FEATURE_INDEX.md) \u2022 Back to Guide Index: [../INDEX.MD](../INDEX.MD)", "tags": ["cli", "datasets", "gui"], "headings": [{"line": 0, "text": "Tools & Integrations"}, {"line": 5, "text": "Web tools (search, crawl, datasets)"}, {"line": 13, "text": "Configuration knobs"}, {"line": 19, "text": "Crawl a URL \u2192 JSON summary or dataset"}, {"line": 38, "text": "Build datasets from the web"}, {"line": 61, "text": "Filesystem and OS tools (guarded)"}, {"line": 73, "text": "Root-helper and service adapters"}, {"line": 89, "text": "Journal parser utilities"}, {"line": 96, "text": "Package/service simulators (budgeted)"}, {"line": 105, "text": "MCP and external tools (GUI)"}, {"line": 115, "text": "Unlimiformer (planned)"}, {"line": 123, "text": "Quick reference (commands)"}]}, {"path": "planned_features/AMD_INTEL_GPU_SUPPORT_FIX.md", "content": "# AMD and Intel GPU Support Enhancement\n\n**Status:** \ud83d\udccb Planned  \n**Priority:** High  \n**Complexity:** Medium (4-8 hours implementation)  \n**Target Version:** v1.1.0  \n**Created:** 2025-10-23  \n**Owner:** @AI-OS-Team\n\n## Overview\n\nEnhance detection, installation, and optimization support for AMD and Intel GPUs. Currently, AMD GPUs work but are mislabeled as \"CUDA\", and Intel XPU support is incomplete. This plan addresses all gaps to provide first-class support for non-NVIDIA GPUs.\n\n## Problem Statement\n\n### Current Issues\n\n1. **AMD ROCm GPUs** - Detected but mislabeled as generic \"CUDA\" devices\n2. **Intel Arc/Xe GPUs** - Basic support exists but memory optimization broken\n3. **Installation Scripts** - Only detect NVIDIA, auto-install wrong PyTorch variant\n4. **GUI** - Doesn't distinguish GPU vendors or show XPU devices\n5. **Memory Optimizer** - Only checks CUDA, reports 0 VRAM for XPU-only systems\n\n### Impact\n\n- AMD GPU users confused about which hardware is being used\n- Intel GPU users can't use memory optimization features\n- Users must manually install correct PyTorch variant\n- Sub-optimal experience for non-NVIDIA hardware\n\n## Goals\n\n### Must Have \u2705\n\n- [x] Correctly identify and label AMD GPUs\n- [x] Add Intel XPU memory detection\n- [x] Update installation scripts to detect all GPU vendors\n- [x] Add `intel-extension-for-pytorch` to dependencies\n- [x] Update GUI to show vendor information\n- [x] Update `aios torch-info` command to report all vendors\n\n### Nice to Have \ud83c\udfaf\n\n- [ ] Vendor-specific optimization hints\n- [ ] Auto-install correct PyTorch variant based on detected hardware\n- [ ] XPU distributed training support (oneCCL backend)\n- [ ] Performance benchmarking across vendors\n\n### Out of Scope \u274c\n\n- Mixed-vendor parallel training (see `MIXED_GPU_VENDOR_SUPPORT.md`)\n- Vendor-specific kernel optimizations\n- GPU virtualization support\n- Cloud GPU support (AWS/Azure specific configurations)\n\n## Implementation Plan\n\n### Phase 1: Core Detection (High Priority)\n\n**Estimated Time:** 2 hours\n\n#### Task 1.1: Add GPU Vendor Identification Function\n**Files:** `src/aios/cli/training/torch_info_cmd.py`\n\n- [ ] Create `identify_gpu_vendor()` function\n- [ ] Check GPU name for vendor keywords (AMD, Radeon, MI, Intel, Arc, Xe, NVIDIA)\n- [ ] Check `torch.version.hip` for ROCm builds\n- [ ] Return vendor enum: `'NVIDIA' | 'AMD' | 'Intel' | 'Unknown'`\n\n**Implementation:**\n```python\ndef identify_gpu_vendor(gpu_name: str, check_rocm: bool = False) -> str:\n    \"\"\"Identify GPU vendor from device name.\n    \n    Args:\n        gpu_name: GPU device name from torch\n        check_rocm: Also check torch.version.hip attribute\n        \n    Returns:\n        'NVIDIA' | 'AMD' | 'Intel' | 'Unknown'\n    \"\"\"\n    name_lower = gpu_name.lower()\n    \n    # NVIDIA detection\n    if any(kw in name_lower for kw in ['nvidia', 'geforce', 'rtx', 'gtx', 'quadro', 'tesla', 'a100', 'h100']):\n        return 'NVIDIA'\n    \n    # AMD detection\n    if any(kw in name_lower for kw in ['amd', 'radeon', 'rx ', 'vega', 'navi', 'mi50', 'mi100', 'mi200', 'mi300']):\n        return 'AMD'\n    \n    # Intel detection\n    if any(kw in name_lower for kw in ['intel', 'arc', 'xe', 'iris', 'uhd']):\n        return 'Intel'\n    \n    # Check ROCm build as fallback\n    if check_rocm:\n        try:\n            import torch\n            if getattr(torch.version, 'hip', None):\n                return 'AMD'\n        except Exception:\n            pass\n    \n    return 'Unknown'\n```\n\n**Tests:**\n- [ ] Test with NVIDIA GPU names (GeForce RTX 4090, Tesla V100)\n- [ ] Test with AMD GPU names (Radeon RX 7900 XTX, MI210)\n- [ ] Test with Intel GPU names (Arc A770, Xe HPG)\n- [ ] Test ROCm detection fallback\n\n#### Task 1.2: Update torch-info Command\n**Files:** `src/aios/cli/training/torch_info_cmd.py`\n\n- [ ] Import vendor identification function\n- [ ] Add `vendor` field to each GPU in `cuda_devices` list\n- [ ] Update `rocm` field to use vendor detection\n- [ ] Add `gpu_vendor_summary` field with counts per vendor\n\n**Changes:**\n```python\n# In torch_info() function, for each CUDA device:\ndev: dict = {\"id\": i, \"name\": name}\nif total_mb:\n    dev[\"total_mem_mb\"] = int(total_mb)\n\n# ADD:\ndev[\"vendor\"] = identify_gpu_vendor(name, check_rocm=rocm)\n\ncuda_devices.append(dev)\n\n# ADD after device enumeration:\nvendor_counts = {}\nfor dev in cuda_devices:\n    vendor = dev.get(\"vendor\", \"Unknown\")\n    vendor_counts[vendor] = vendor_counts.get(vendor, 0) + 1\n\ninfo[\"gpu_vendor_summary\"] = vendor_counts\n```\n\n**Tests:**\n- [ ] Run `aios torch-info` with NVIDIA GPU\n- [ ] Run with AMD GPU (if available)\n- [ ] Run with Intel GPU (if available)\n- [ ] Verify JSON output includes vendor field\n\n#### Task 1.3: Update GUI Device Detection\n**Files:** `src/aios/gui/app/panel_setup.py`\n\n- [ ] Import vendor identification function\n- [ ] Add `vendor` field to detected devices\n- [ ] Update status label to show vendor counts\n- [ ] Differentiate AMD from NVIDIA in device list\n\n**Changes:**\n```python\n# In _detect_devices_info() function:\ncuda_devices.append({\n    \"id\": i,\n    \"name\": name,\n    \"total_mem_mb\": total_mem_mb,\n    \"vendor\": identify_gpu_vendor(name, check_rocm=bool(getattr(torch.version, 'hip', None)))\n})\n\n# Update return dict:\nreturn {\n    \"cuda_available\": cuda_available,\n    \"cuda_devices\": cuda_devices,\n    \"nvidia_smi_devices\": cuda_devices,  # Keep for compatibility\n    \"vendor_summary\": calculate_vendor_summary(cuda_devices)\n}\n```\n\n**Tests:**\n- [ ] Launch GUI and verify vendor shown in Resources panel\n- [ ] Verify AMD GPUs labeled correctly\n- [ ] Verify status message distinguishes vendors\n\n### Phase 2: Intel XPU Support (High Priority)\n\n**Estimated Time:** 3 hours\n\n#### Task 2.1: Add IPEX Dependency\n**Files:** `pyproject.toml`\n\n- [ ] Add `intel-extension-for-pytorch` to `[project.optional-dependencies.hf]`\n- [ ] Version constraint: `>=2.0.0`\n- [ ] Platform constraint: exclude ARM64\n\n**Changes:**\n```toml\n[project.optional-dependencies]\nhf = [\n  # ... existing dependencies ...\n  \"intel-extension-for-pytorch>=2.0.0; platform_machine != 'ARM64'\",\n]\n```\n\n**Tests:**\n- [ ] Fresh install in new venv\n- [ ] Verify IPEX installs on x64 systems\n- [ ] Verify skipped on ARM64 systems\n\n#### Task 2.2: Add XPU Memory Detection\n**Files:** `src/aios/core/hrm_models/training_optimizer.py`\n\n- [ ] Update `detect_available_vram()` to check XPU\n- [ ] Add XPU device enumeration\n- [ ] Handle IPEX not installed gracefully\n\n**Implementation:**\n```python\ndef detect_available_vram(self) -> Tuple[float, List[Dict[str, Any]]]:\n    \"\"\"\n    Detect available VRAM across all GPUs (CUDA, XPU).\n    \n    Returns:\n        (total_vram_gb, list of GPU info dicts)\n    \"\"\"\n    gpus = []\n    total_vram = 0.0\n    \n    # Check CUDA devices (NVIDIA + AMD ROCm)\n    if torch.cuda.is_available():\n        for gpu_id in range(torch.cuda.device_count()):\n            props = torch.cuda.get_device_properties(gpu_id)\n            total_gb = props.total_memory / (1024 ** 3)\n            \n            torch.cuda.set_device(gpu_id)\n            torch.cuda.empty_cache()\n            gc.collect()\n            \n            allocated_gb = torch.cuda.memory_allocated(gpu_id) / (1024 ** 3)\n            reserved_gb = torch.cuda.memory_reserved(gpu_id) / (1024 ** 3)\n            available_gb = total_gb - reserved_gb\n            \n            gpus.append({\n                \"id\": gpu_id,\n                \"backend\": \"cuda\",\n                \"name\": props.name,\n                \"total_gb\": total_gb,\n                \"allocated_gb\": allocated_gb,\n                \"reserved_gb\": reserved_gb,\n                \"available_gb\": available_gb,\n            })\n            \n            total_vram += total_gb\n    \n    # Check Intel XPU devices\n    try:\n        if hasattr(torch, 'xpu') and torch.xpu.is_available():\n            for gpu_id in range(torch.xpu.device_count()):\n                props = torch.xpu.get_device_properties(gpu_id)\n                total_gb = props.total_memory / (1024 ** 3)\n                \n                # XPU memory info\n                try:\n                    free_mem, total_mem = torch.xpu.mem_get_info(gpu_id)\n                    available_gb = free_mem / (1024 ** 3)\n                    reserved_gb = (total_mem - free_mem) / (1024 ** 3)\n                except Exception:\n                    available_gb = total_gb * 0.9  # Estimate\n                    reserved_gb = total_gb * 0.1\n                \n                gpus.append({\n                    \"id\": len(gpus),  # Global ID\n                    \"backend\": \"xpu\",\n                    \"name\": props.name,\n                    \"total_gb\": total_gb,\n                    \"allocated_gb\": 0.0,  # Not available via public API\n                    \"reserved_gb\": reserved_gb,\n                    \"available_gb\": available_gb,\n                })\n                \n                total_vram += total_gb\n    except ImportError:\n        # intel-extension-for-pytorch not installed\n        pass\n    except Exception as e:\n        # Log but don't crash\n        print(f\"XPU detection warning: {e}\")\n    \n    return total_vram, gpus\n```\n\n**Tests:**\n- [ ] Test with CUDA-only system (existing behavior)\n- [ ] Test with XPU-only system (new behavior)\n- [ ] Test with both CUDA and XPU\n- [ ] Test without IPEX installed\n\n#### Task 2.3: Update GUI XPU Device Enumeration\n**Files:** `src/aios/gui/app/panel_setup.py`\n\n- [ ] Add XPU detection to `_detect_devices_info()`\n- [ ] Return separate `xpu_devices` list\n- [ ] Update `set_detected()` to handle XPU\n\n**Changes:**\n```python\ndef _detect_devices_info() -> dict:\n    try:\n        import torch\n        \n        # ... existing CUDA detection ...\n        \n        # Detect Intel XPU devices\n        xpu_available = False\n        xpu_devices = []\n        try:\n            if hasattr(torch, 'xpu') and torch.xpu.is_available():\n                xpu_available = True\n                device_count = torch.xpu.device_count()\n                for i in range(device_count):\n                    try:\n                        props = torch.xpu.get_device_properties(i)\n                        total_mem_mb = props.total_memory // (1024 * 1024)\n                        xpu_devices.append({\n                            \"id\": i,\n                            \"name\": props.name,\n                            \"total_mem_mb\": total_mem_mb,\n                            \"vendor\": \"Intel\"\n                        })\n                    except Exception:\n                        xpu_devices.append({\n                            \"id\": i,\n                            \"name\": f\"Intel XPU {i}\",\n                            \"total_mem_mb\": 0,\n                            \"vendor\": \"Intel\"\n                        })\n        except Exception:\n            xpu_available = False\n        \n        return {\n            \"cuda_available\": cuda_available,\n            \"cuda_devices\": cuda_devices,\n            \"nvidia_smi_devices\": cuda_devices,\n            \"xpu_available\": xpu_available,\n            \"xpu_devices\": xpu_devices,\n            # ... existing fields ...\n        }\n    except Exception:\n        return {\n            \"cuda_available\": False,\n            \"cuda_devices\": [],\n            \"nvidia_smi_devices\": [],\n            \"xpu_available\": False,\n            \"xpu_devices\": []\n        }\n```\n\n**Tests:**\n- [ ] GUI shows XPU devices when available\n- [ ] XPU devices selectable in Resources panel\n- [ ] Training can target XPU devices\n\n#### Task 2.4: Update device.py for XPU\n**Files:** `src/aios/cli/hrm_hf/device.py`\n\n- [ ] Add XPU case to `resolve_device()`\n- [ ] Handle XPU device objects\n- [ ] Update strict mode error messages\n\n**Changes:**\n```python\ndef resolve_device(device: str, strict: bool, torch) -> Tuple[str, Any, Any]:\n    \"\"\"Resolve device string and return (dev_str, device_obj, dml_device).\n\n    Handles auto, cuda, xpu, dml with strict mode constraints.\n    \"\"\"\n    dev = device\n    dml_device = None\n    \n    if dev == \"auto\":\n        if torch.cuda.is_available():\n            dev = \"cuda\"\n        elif hasattr(torch, 'xpu') and torch.xpu.is_available():\n            dev = \"xpu\"\n        else:\n            try:\n                import torch_directml as _dml\n                _ = _dml.device()\n                dev = \"dml\"\n            except Exception:\n                dev = \"cpu\"\n    else:\n        # Validate requested device\n        if str(dev).lower() == \"cuda\" and not torch.cuda.is_available():\n            # ... existing CUDA error handling ...\n        elif str(dev).lower() == \"xpu\":\n            if not (hasattr(torch, 'xpu') and torch.xpu.is_available()):\n                if strict:\n                    from rich import print\n                    import typer\n                    print({\n                        \"error\": \"XPU requested but not available\",\n                        \"hint\": \"Install intel-extension-for-pytorch or choose --device cpu/cuda/dml\",\n                        \"device_request\": \"xpu\"\n                    })\n                    raise typer.Exit(code=2)\n                else:\n                    dev = \"cpu\"\n    \n    device_obj = None\n    if dev == \"dml\":\n        # ... existing DML handling ...\n    else:\n        device_obj = torch.device(dev)\n    \n    return dev, device_obj, dml_device\n```\n\n**Tests:**\n- [ ] `--device xpu` works with IPEX installed\n- [ ] `--device xpu` fails gracefully without IPEX\n- [ ] `--device auto` prefers CUDA > XPU > DML > CPU\n\n### Phase 3: Installation Scripts (Medium Priority)\n\n**Estimated Time:** 2 hours\n\n#### Task 3.1: Windows GPU Detection Enhancement\n**Files:** `scripts/install_aios_on_windows.ps1`\n\n- [ ] Add AMD GPU detection via WMI\n- [ ] Add Intel GPU detection via WMI\n- [ ] Offer PyTorch variant selection\n- [ ] Add ROCm installation option\n\n**Implementation:**\n```powershell\nfunction Get-GpuInfo() {\n  $info = @{ \n    Vendor = 'Unknown'\n    Model = ''\n    HasNvidia = $false\n    HasAmd = $false\n    HasIntel = $false\n    NvidiaSmi = $false \n  }\n  \n  try {\n    $gpus = Get-CimInstance Win32_VideoController | Select-Object -Property Name, AdapterCompatibility\n    foreach ($g in $gpus) {\n      $vendor = [string]$g.AdapterCompatibility\n      $model = [string]$g.Name\n      \n      if ($vendor -match 'NVIDIA' -or $model -match 'NVIDIA|GeForce|RTX|GTX') {\n        $info.HasNvidia = $true\n        if ($info.Vendor -eq 'Unknown') { $info.Vendor = 'NVIDIA'; $info.Model = $model }\n      }\n      elseif ($vendor -match 'AMD' -or $model -match 'AMD|Radeon|RX') {\n        $info.HasAmd = $true\n        if ($info.Vendor -eq 'Unknown') { $info.Vendor = 'AMD'; $info.Model = $model }\n      }\n      elseif ($vendor -match 'Intel' -or $model -match 'Intel.*Arc|Intel.*Xe') {\n        $info.HasIntel = $true\n        if ($info.Vendor -eq 'Unknown') { $info.Vendor = 'Intel'; $info.Model = $model }\n      }\n    }\n  } catch {}\n  \n  try { \n    if (Get-Command nvidia-smi -ErrorAction SilentlyContinue) { \n      $info.NvidiaSmi = $true \n    } \n  } catch {}\n  \n  return $info\n}\n\nfunction Install-PyTorch([string]$pipPath, [string]$pythonPath, [string]$GpuPref) {\n  $gpu = $GpuPref\n  $gpuInfo = Get-GpuInfo\n  \n  Write-Host (\"[i] GPU detection: Vendor={0} Model={1}\" -f $gpuInfo.Vendor, $gpuInfo.Model)\n  Write-Host (\"[i] GPUs detected: NVIDIA={0} AMD={1} Intel={2}\" -f $gpuInfo.HasNvidia, $gpuInfo.HasAmd, $gpuInfo.HasIntel)\n\n  if ($gpu -eq 'auto') {\n    if ($gpuInfo.HasNvidia) { \n      $gpu = 'cuda' \n    }\n    elseif ($gpuInfo.HasAmd) {\n      # ROCm on Windows is experimental, offer DirectML as safer option\n      $choice = Read-Host \"AMD GPU detected. Use (1) DirectML [Stable] or (2) ROCm [Experimental]? [1/2]\"\n      if ($choice -eq '2') {\n        $gpu = 'rocm'\n      } else {\n        $gpu = 'dml'\n      }\n    }\n    elseif ($gpuInfo.HasIntel) {\n      $gpu = 'xpu'\n    }\n    else { \n      $gpu = 'cpu' \n    }\n  }\n\n  if ($gpu -eq 'cuda') {\n    # ... existing CUDA installation ...\n  }\n  elseif ($gpu -eq 'rocm') {\n    Write-Host \"[i] Installing PyTorch with ROCm support...\"\n    Write-Host \"[!] Note: ROCm on Windows is experimental\" -ForegroundColor Yellow\n    try {\n      & $pipPath install --upgrade pip wheel setuptools | Out-Null\n      # ROCm 6.0 for Windows (if available)\n      & $pipPath install torch --index-url https://download.pytorch.org/whl/rocm6.0\n      Write-Host \"[+] PyTorch installed with ROCm support.\" -ForegroundColor Green\n      return\n    } catch {\n      Write-Host \"[!] ROCm install failed. Falling back to DirectML...\" -ForegroundColor Yellow\n      $gpu = 'dml'\n    }\n  }\n  elseif ($gpu -eq 'xpu') {\n    Write-Host \"[i] Installing PyTorch + Intel Extension for PyTorch (XPU)...\"\n    try {\n      & $pipPath install --upgrade pip wheel setuptools | Out-Null\n      # CPU PyTorch first\n      & $pipPath install torch --index-url https://download.pytorch.org/whl/cpu\n      # Then Intel Extension\n      & $pipPath install intel-extension-for-pytorch\n      \n      $code = \"import torch, intel_extension_for_pytorch as ipex; print({'torch': torch.__version__, 'ipex': ipex.__version__, 'xpu_available': torch.xpu.is_available()})\"\n      $res = & $pythonPath -c $code\n      Write-Host \"[+] PyTorch installed with Intel XPU support. Probe: $res\" -ForegroundColor Green\n      return\n    } catch {\n      Write-Host \"[!] Intel XPU install failed. Falling back to DirectML...\" -ForegroundColor Yellow\n      $gpu = 'dml'\n    }\n  }\n  elseif ($gpu -eq 'dml') {\n    # ... existing DirectML installation ...\n  }\n\n  # ... existing CPU fallback ...\n}\n```\n\n**Tests:**\n- [ ] Test on NVIDIA-only system\n- [ ] Test on AMD-only system (if available)\n- [ ] Test on Intel-only system (if available)\n- [ ] Test GPU preference override (`-Gpu cuda/rocm/xpu/dml/cpu`)\n\n#### Task 3.2: Ubuntu GPU Detection Enhancement\n**Files:** `scripts/install_aios_on_ubuntu.sh`\n\n- [ ] Add AMD GPU detection via lspci\n- [ ] Add Intel GPU detection via lspci\n- [ ] Add ROCm installation option\n- [ ] Add IPEX installation option\n\n**Implementation:**\n```bash\n# GPU Detection\nGPU_VENDOR=\"unknown\"\nif command -v nvidia-smi >/dev/null 2>&1 || lspci | grep -i nvidia >/dev/null 2>&1; then\n  GPU_VENDOR=\"nvidia\"\nelif lspci | grep -iE 'amd|radeon' >/dev/null 2>&1; then\n  GPU_VENDOR=\"amd\"\nelif lspci | grep -iE 'intel.*(arc|xe|iris)' >/dev/null 2>&1; then\n  GPU_VENDOR=\"intel\"\nfi\n\necho \"[AI-OS] Detected GPU vendor: $GPU_VENDOR\"\n\n# PyTorch Installation\ncase $GPU_VENDOR in\n  nvidia)\n    echo \"[AI-OS] Installing CUDA build of PyTorch (cu121)\"\n    \"$venv/bin/pip\" install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 || {\n      echo \"[AI-OS] CUDA wheel install failed, falling back to CPU build\"\n      \"$venv/bin/pip\" install torch torchvision torchaudio\n    }\n    ;;\n  amd)\n    echo \"[AI-OS] AMD GPU detected. Installing ROCm build of PyTorch\"\n    echo \"[AI-OS] Note: ROCm requires additional system packages\"\n    # Check if ROCm is installed\n    if command -v rocminfo >/dev/null 2>&1; then\n      \"$venv/bin/pip\" install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0 || {\n        echo \"[AI-OS] ROCm wheel install failed, falling back to CPU build\"\n        \"$venv/bin/pip\" install torch torchvision torchaudio\n      }\n    else\n      echo \"[AI-OS] ROCm not installed on system. Installing CPU build.\"\n      echo \"[AI-OS] To use AMD GPU, install ROCm: https://rocm.docs.amd.com/en/latest/\"\n      \"$venv/bin/pip\" install torch torchvision torchaudio\n    fi\n    ;;\n  intel)\n    echo \"[AI-OS] Intel GPU detected. Installing Intel Extension for PyTorch\"\n    # CPU PyTorch first\n    \"$venv/bin/pip\" install torch torchvision torchaudio\n    # Then Intel Extension\n    \"$venv/bin/pip\" install intel-extension-for-pytorch || {\n      echo \"[AI-OS] Intel Extension install failed, continuing with CPU-only PyTorch\"\n    }\n    ;;\n  *)\n    echo \"[AI-OS] No discrete GPU detected; installing CPU build of PyTorch\"\n    \"$venv/bin/pip\" install torch torchvision torchaudio\n    ;;\nesac\n```\n\n**Tests:**\n- [ ] Test on Ubuntu with NVIDIA GPU\n- [ ] Test on Ubuntu with AMD GPU (if available)\n- [ ] Test on Ubuntu with Intel GPU (if available)\n- [ ] Test on Ubuntu with no GPU\n\n### Phase 4: GUI Enhancements (Low Priority)\n\n**Estimated Time:** 1 hour\n\n#### Task 4.1: Update Resources Panel Device Labels\n**Files:** `src/aios/gui/components/resources_panel/device_management.py`\n\n- [ ] Show vendor in GPU device name\n- [ ] Add vendor badges/icons (optional)\n- [ ] Color-code by vendor (optional)\n\n**Changes:**\n```python\ndef build_cuda_rows(panel: \"ResourcesPanel\", devices: list[dict], which: str) -> None:\n    \"\"\"Build GPU selection rows for training or inference.\"\"\"\n    # ... existing code ...\n    \n    for dev in devices:\n        # ... existing parsing ...\n        \n        name = str(dev.get(\"name\") or f\"CUDA {did}\")\n        vendor = dev.get(\"vendor\", \"\")\n        \n        # Add vendor prefix if known and not already in name\n        if vendor and vendor not in name:\n            display_name = f\"[{vendor}] {name}\"\n        else:\n            display_name = name\n        \n        # ... rest of row building with display_name ...\n```\n\n**Tests:**\n- [ ] Launch GUI with mixed GPUs\n- [ ] Verify vendor labels shown\n- [ ] Verify selection still works\n\n#### Task 4.2: Update Status Messages\n**Files:** `src/aios/gui/components/resources_panel/device_management.py`\n\n- [ ] Show vendor breakdown in status\n- [ ] Differentiate \"X NVIDIA GPUs, Y AMD GPUs\" vs just \"X GPUs\"\n\n**Changes:**\n```python\ndef set_detected(panel: \"ResourcesPanel\", info: dict) -> None:\n    # ... existing code ...\n    \n    # Build vendor summary for status\n    vendor_summary = info.get(\"vendor_summary\", {})\n    if vendor_summary:\n        vendor_parts = [f\"{count} {vendor}\" for vendor, count in vendor_summary.items()]\n        gpu_desc = \", \".join(vendor_parts)\n    else:\n        gpu_desc = f\"{device_count} GPU(s)\"\n    \n    if device_count > 0:\n        panel._status_label.config(\n            text=f\"\u2713 {gpu_desc} detected\",\n            foreground=\"green\"\n        )\n```\n\n**Tests:**\n- [ ] Single vendor system shows correct label\n- [ ] Mixed vendor system shows breakdown\n- [ ] No GPU system shows fallback message\n\n### Phase 5: Documentation (Low Priority)\n\n**Estimated Time:** 1 hour\n\n#### Task 5.1: Update User Documentation\n\n- [ ] Add AMD GPU setup guide to `docs/guide/`\n- [ ] Add Intel GPU setup guide to `docs/guide/`\n- [ ] Update `README.md` with multi-vendor support claims\n- [ ] Add troubleshooting section for GPU detection\n\n**Files to create/update:**\n- `docs/guide/AMD_GPU_SETUP.md`\n- `docs/guide/INTEL_GPU_SETUP.md`\n- `README.md`\n\n#### Task 5.2: Update Developer Documentation\n\n- [ ] Document vendor identification function\n- [ ] Add GPU backend architecture notes\n- [ ] Update API reference\n\n**Files to create/update:**\n- `docs/development/GPU_BACKEND_ARCHITECTURE.md`\n\n### Phase 6: Testing & Validation (Critical)\n\n**Estimated Time:** 2 hours\n\n#### Unit Tests\n- [ ] Test `identify_gpu_vendor()` with sample GPU names\n- [ ] Test XPU memory detection with mocked torch.xpu\n- [ ] Test vendor summary calculation\n- [ ] Test device resolution with XPU\n\n**Files:** `tests/test_gpu_detection.py` (new)\n\n#### Integration Tests\n- [ ] Test installation script on clean Windows VM\n- [ ] Test installation script on clean Ubuntu VM\n- [ ] Test GUI with NVIDIA-only system\n- [ ] Test training with XPU device (if hardware available)\n- [ ] Test torch-info command output\n\n#### Manual Testing Checklist\n- [ ] NVIDIA GPU: Detection, labeling, training\n- [ ] AMD GPU: Detection, labeling, training (if available)\n- [ ] Intel GPU: Detection, labeling, training (if available)\n- [ ] Mixed system: All vendors shown correctly (if available)\n- [ ] No GPU: Graceful CPU fallback\n\n## Success Criteria\n\n### Phase 1-2 (Core Functionality)\n- [x] AMD GPUs correctly labeled with vendor in torch-info\n- [x] Intel XPU GPUs detected and shown in GUI\n- [x] XPU memory optimization works\n- [x] Training works on XPU devices\n- [x] No regression for NVIDIA GPU users\n\n### Phase 3-4 (Installation & UX)\n- [ ] Installation scripts detect AMD/Intel GPUs\n- [ ] Correct PyTorch variant installed automatically\n- [ ] GUI shows vendor information\n- [ ] Users can distinguish GPU hardware\n\n### Phase 5-6 (Polish & Validation)\n- [ ] Documentation complete\n- [ ] Tests passing\n- [ ] No critical bugs reported\n\n## Known Limitations\n\n### After Implementation\n1. **ROCm on Windows** - Experimental, may not work on all AMD GPUs\n2. **Intel XPU DDP** - Distributed training requires oneCCL backend (not included)\n3. **External GPUs** - Thunderbolt bandwidth limitations (hardware, not software)\n4. **Hotplug** - PyTorch may need restart to detect newly connected eGPUs\n\n### Permanent Limitations\n1. **Vendor-specific optimizations** - Out of scope, would require per-vendor kernels\n2. **Mixed-vendor DDP** - Not possible (requires same backend)\n3. **GPU virtualization** - Not tested, may work but unsupported\n\n## Rollback Plan\n\n### If Issues Arise\n1. **Vendor detection bugs** - Revert to generic \"CUDA\" labeling\n2. **XPU crashes** - Disable XPU code paths, CPU fallback\n3. **Installation issues** - Keep NVIDIA-only detection as fallback\n\n### Breaking Changes\nNone expected. All changes are additive.\n\n## Performance Impact\n\nExpected performance impact: **None**\n\n- Vendor detection: One-time on startup\n- XPU memory checks: Only when XPU devices present\n- No changes to training loops\n\n## Security Considerations\n\nNone. No security-sensitive changes.\n\n## Dependencies\n\n### New Dependencies\n- `intel-extension-for-pytorch>=2.0.0` (optional, hf extra)\n\n### System Requirements\n- **AMD GPU users**: ROCm 5.7+ (Linux) or 6.0+ (Windows experimental)\n- **Intel GPU users**: Intel GPU drivers, oneAPI Base Toolkit (optional)\n- **No change for NVIDIA users**\n\n## Migration Guide\n\n### For Users\n\n**Upgrading from v1.0.x:**\n\n1. Update AI-OS: `git pull && pip install -e .[ui,hf]`\n2. AMD GPU users: Reinstall PyTorch with ROCm\n   ```bash\n   pip install torch --index-url https://download.pytorch.org/whl/rocm6.0\n   ```\n3. Intel GPU users: Install IPEX\n   ```bash\n   pip install intel-extension-for-pytorch\n   ```\n4. Run `aios torch-info` to verify detection\n\n**No action needed for NVIDIA users** - everything works as before.\n\n### For Developers\n\n**API Changes:**\n- `torch_info()` output now includes `vendor` field in device dicts\n- `detect_available_vram()` now returns XPU devices\n- `_detect_devices_info()` now includes `xpu_devices` key\n\n**New Functions:**\n- `identify_gpu_vendor(gpu_name: str, check_rocm: bool) -> str`\n\n## Timeline\n\n### Aggressive (1 week)\n- Days 1-2: Phase 1-2 (Detection + XPU)\n- Day 3: Phase 3 (Installation scripts)\n- Day 4: Phase 4 (GUI)\n- Day 5: Phase 5 (Documentation)\n- Days 6-7: Phase 6 (Testing)\n\n### Realistic (2 weeks)\n- Week 1: Phases 1-3 (Core functionality + installation)\n- Week 2: Phases 4-6 (Polish, docs, testing)\n\n### Conservative (3 weeks)\n- Week 1: Phases 1-2 (Core functionality)\n- Week 2: Phases 3-4 (Installation + GUI)\n- Week 3: Phases 5-6 (Documentation + thorough testing)\n\n## Open Questions\n\n1. ~~Should we auto-install ROCm/IPEX or require manual install?~~\n   - **Decision:** Auto-detect and offer installation, but don't force\n2. ~~Should DirectML remain default for AMD on Windows?~~\n   - **Decision:** Yes, ROCm on Windows is experimental\n3. ~~How to handle Intel integrated GPUs vs discrete Arc?~~\n   - **Decision:** Detect both, let user choose in GUI\n4. ~~Version constraints for IPEX?~~\n   - **Decision:** `>=2.0.0` for PyTorch 2.x compatibility\n\n## References\n\n### Documentation\n- AMD ROCm: https://rocm.docs.amd.com/\n- Intel XPU: https://intel.github.io/intel-extension-for-pytorch/\n- PyTorch Multi-Backend: https://pytorch.org/docs/stable/notes/hip.html\n\n### Related Issues\n- None (proactive enhancement)\n\n### Related PRs\n- To be created after implementation\n\n## Approval & Tracking\n\n- [ ] Technical Review - @AI-OS-Team\n- [ ] Architecture Review - @AI-OS-Team\n- [ ] Implementation Started\n- [ ] Phase 1 Complete\n- [ ] Phase 2 Complete\n- [ ] Phase 3 Complete\n- [ ] Phase 4 Complete\n- [ ] Phase 5 Complete\n- [ ] Phase 6 Complete\n- [ ] Documentation Complete\n- [ ] Released in v1.1.0\n\n---\n\n**Last Updated:** 2025-10-23  \n**Next Review:** After Phase 2 completion  \n**Owner:** @AI-OS-Team\n\n## Progress Tracking\n\n### Overall Progress\n- [ ] 0% - Not started\n- [ ] 25% - Phase 1-2 complete (Core detection + XPU)\n- [ ] 50% - Phase 3-4 complete (Installation + GUI)\n- [ ] 75% - Phase 5 complete (Documentation)\n- [ ] 100% - Phase 6 complete (Testing + Release)\n\n### Current Status\n**Status:** \ud83d\udccb Planned  \n**Start Date:** TBD  \n**Target Completion:** TBD  \n**Actual Completion:** N/A\n\n### Blockers\nNone currently identified.\n\n### Notes\n- Consider coordinating with MIXED_GPU_VENDOR_SUPPORT.md for future mixed-vendor training\n- May want to implement basic GPU abstraction layer first for easier future expansion\n", "tags": ["cli", "gui", "training"], "headings": [{"line": 0, "text": "AMD and Intel GPU Support Enhancement"}, {"line": 9, "text": "Overview"}, {"line": 13, "text": "Problem Statement"}, {"line": 15, "text": "Current Issues"}, {"line": 23, "text": "Impact"}, {"line": 30, "text": "Goals"}, {"line": 32, "text": "Must Have \u2705"}, {"line": 41, "text": "Nice to Have \ud83c\udfaf"}, {"line": 48, "text": "Out of Scope \u274c"}, {"line": 55, "text": "Implementation Plan"}, {"line": 57, "text": "Phase 1: Core Detection (High Priority)"}, {"line": 61, "text": "Task 1.1: Add GPU Vendor Identification Function"}, {"line": 83, "text": "NVIDIA detection"}, {"line": 87, "text": "AMD detection"}, {"line": 91, "text": "Intel detection"}, {"line": 95, "text": "Check ROCm build as fallback"}, {"line": 113, "text": "Task 1.2: Update torch-info Command"}, {"line": 123, "text": "In torch_info() function, for each CUDA device:"}, {"line": 128, "text": "ADD:"}, {"line": 133, "text": "ADD after device enumeration:"}, {"line": 148, "text": "Task 1.3: Update GUI Device Detection"}, {"line": 158, "text": "In _detect_devices_info() function:"}, {"line": 166, "text": "Update return dict:"}, {"line": 180, "text": "Phase 2: Intel XPU Support (High Priority)"}, {"line": 184, "text": "Task 2.1: Add IPEX Dependency"}, {"line": 195, "text": "... existing dependencies ..."}, {"line": 205, "text": "Task 2.2: Add XPU Memory Detection"}, {"line": 224, "text": "Check CUDA devices (NVIDIA + AMD ROCm)"}, {"line": 250, "text": "Check Intel XPU devices"}, {"line": 257, "text": "XPU memory info"}, {"line": 278, "text": "intel-extension-for-pytorch not installed"}, {"line": 281, "text": "Log but don't crash"}, {"line": 293, "text": "Task 2.3: Update GUI XPU Device Enumeration"}, {"line": 306, "text": "... existing CUDA detection ..."}, {"line": 308, "text": "Detect Intel XPU devices"}, {"line": 341, "text": "... existing fields ..."}, {"line": 358, "text": "Task 2.4: Update device.py for XPU"}, {"line": 388, "text": "Validate requested device"}, {"line": 390, "text": "... existing CUDA error handling ..."}, {"line": 407, "text": "... existing DML handling ..."}, {"line": 419, "text": "Phase 3: Installation Scripts (Medium Priority)"}, {"line": 423, "text": "Task 3.1: Windows GPU Detection Enhancement"}, {"line": 485, "text": "ROCm on Windows is experimental, offer DirectML as safer option"}, {"line": 502, "text": "... existing CUDA installation ..."}, {"line": 509, "text": "ROCm 6.0 for Windows (if available)"}, {"line": 522, "text": "CPU PyTorch first"}, {"line": 524, "text": "Then Intel Extension"}, {"line": 537, "text": "... existing DirectML installation ..."}, {"line": 540, "text": "... existing CPU fallback ..."}, {"line": 550, "text": "Task 3.2: Ubuntu GPU Detection Enhancement"}, {"line": 560, "text": "GPU Detection"}, {"line": 572, "text": "PyTorch Installation"}, {"line": 584, "text": "Check if ROCm is installed"}, {"line": 598, "text": "CPU PyTorch first"}, {"line": 600, "text": "Then Intel Extension"}, {"line": 618, "text": "Phase 4: GUI Enhancements (Low Priority)"}, {"line": 622, "text": "Task 4.1: Update Resources Panel Device Labels"}, {"line": 633, "text": "... existing code ..."}, {"line": 636, "text": "... existing parsing ..."}, {"line": 641, "text": "Add vendor prefix if known and not already in name"}, {"line": 647, "text": "... rest of row building with display_name ..."}, {"line": 655, "text": "Task 4.2: Update Status Messages"}, {"line": 664, "text": "... existing code ..."}, {"line": 666, "text": "Build vendor summary for status"}, {"line": 686, "text": "Phase 5: Documentation (Low Priority)"}, {"line": 690, "text": "Task 5.1: Update User Documentation"}, {"line": 702, "text": "Task 5.2: Update Developer Documentation"}, {"line": 711, "text": "Phase 6: Testing & Validation (Critical)"}, {"line": 715, "text": "Unit Tests"}, {"line": 723, "text": "Integration Tests"}, {"line": 730, "text": "Manual Testing Checklist"}, {"line": 737, "text": "Success Criteria"}, {"line": 739, "text": "Phase 1-2 (Core Functionality)"}, {"line": 746, "text": "Phase 3-4 (Installation & UX)"}, {"line": 752, "text": "Phase 5-6 (Polish & Validation)"}, {"line": 757, "text": "Known Limitations"}, {"line": 759, "text": "After Implementation"}, {"line": 765, "text": "Permanent Limitations"}, {"line": 770, "text": "Rollback Plan"}, {"line": 772, "text": "If Issues Arise"}, {"line": 777, "text": "Breaking Changes"}, {"line": 780, "text": "Performance Impact"}, {"line": 788, "text": "Security Considerations"}, {"line": 792, "text": "Dependencies"}, {"line": 794, "text": "New Dependencies"}, {"line": 797, "text": "System Requirements"}, {"line": 802, "text": "Migration Guide"}, {"line": 804, "text": "For Users"}, {"line": 821, "text": "For Developers"}, {"line": 831, "text": "Timeline"}, {"line": 833, "text": "Aggressive (1 week)"}, {"line": 840, "text": "Realistic (2 weeks)"}, {"line": 844, "text": "Conservative (3 weeks)"}, {"line": 849, "text": "Open Questions"}, {"line": 860, "text": "References"}, {"line": 862, "text": "Documentation"}, {"line": 867, "text": "Related Issues"}, {"line": 870, "text": "Related PRs"}, {"line": 873, "text": "Approval & Tracking"}, {"line": 893, "text": "Progress Tracking"}, {"line": 895, "text": "Overall Progress"}, {"line": 902, "text": "Current Status"}, {"line": 908, "text": "Blockers"}, {"line": 911, "text": "Notes"}]}, {"path": "planned_features/argilla-trl-preference-alignment.md", "content": "## PF-001: Preference data via Argilla + TRL (RM/DPO baselines)\n\nThis document is an end-to-end integration guide and checklist for adding an open-source human-feedback workflow powered by Argilla and Hugging Face TRL into AI-OS. It covers the architecture, data flow, CLI and GUI elements, runbooks for Windows/Linux, acceptance criteria, and rollout.\n\n## Outcomes and success criteria\n\n- End-to-end: Annotators create preferences in Argilla \u2192 data exported \u2192 ingested to TRL schema \u2192 RM and DPO baselines trained \u2192 artifacts logged and evaluated \u2192 optional signals consumed by HRM.\n- CLI: Three new commands available and documented: argilla-export, rm-train, dpo-train.\n- GUI: Minimal \u201cAlignment\u201d section adds dataset stats, quick-run buttons (ingest/RM/DPO), and links to artifacts/logs.\n- Validation: On a toy dataset, RM AUC > 0.6 and DPO improves win-rate vs reference \u2265 5% (see Acceptance Checklists).\n\n## Architecture overview\n\nData flow (text diagram):\n- Argilla (annotation UI) \u2192 Export (JSONL/Parquet)\n- aios data argilla-export \u2192 TRL-ready splits under training_data/curated_datasets/preferences/{train,eval,test}.jsonl\n- aios hrm-hf rm-train \u2192 artifacts/rm/<run_name>/ (model, tokenizer, metrics.jsonl)\n- aios hrm-hf dpo-train \u2192 artifacts/dpo/<run_name>/ (policy, tokenizer, metrics.jsonl)\n- Optional: Evaluation and HRM-side consumption or distillation in later PFs\n\n## Scope\n\nIn scope:\n- New CLI commands to train RM and DPO models using TRL on HF backbones.\n- Data ingestion/export path from Argilla to TRL-ready datasets.\n- Minimal GUI hooks: dataset stats, quick-run toggles, links to artifacts.\n- Documentation and examples to run end-to-end on a small subset.\n\nOut of scope (this PF):\n- Direct TRL training on custom HRM architecture.\n- Large UI rewrites; we add a minimal Alignment area and buttons.\n\n## Repository integration points\n\n- Data location: `training_data/curated_datasets/preferences/`\n- New CLI modules:\n  - `src/aios/cli/hrm_hf/rm_train.py` \u2014 wraps TRL RewardTrainer\n  - `src/aios/cli/hrm_hf/dpo_train.py` \u2014 wraps TRL DPOTrainer\n- Shared utils: `src/aios/data/argilla_ingest.py` \u2014 converts Argilla exports \u2192 TRL-ready datasets\n- Config: either extend `TrainingConfig` or define small dataclasses local to each CLI for clarity\n- Artifacts:\n  - RM: `artifacts/rm/<name>/` (model, tokenizer, metrics.jsonl, config.json)\n  - DPO: `artifacts/dpo/<name>/` (policy, tokenizer, metrics.jsonl, config.json)\n\n## Prerequisites\n\n- Python environment (see repo installers). Required packages (minimal):\n  - torch (CUDA-enabled when using GPU), transformers, datasets, trl, accelerate, peft, evaluate, scikit-learn, pandas, pyarrow\n  - Optional: bitsandbytes (8-bit), deepspeed (multi-GPU), wandb (telemetry)\n- Argilla server for annotation (Docker or pip). Quick start:\n  - Docker: `docker run -p 6900:6900 -e ARGILLA_ENABLE_TELEMETRY=0 argilla/argilla:latest`\n  - Or follow Argilla docs; create a binary preference project and annotate a few pairs.\n- Disk layout created:\n  - `training_data/curated_datasets/preferences/`\n  - `artifacts/{rm,dpo}/`\n\n## Data formats and schema\n\nTarget TRL schema (JSONL or Arrow) with splits: `train.jsonl`, `eval.jsonl`, `test.jsonl`.\n- DPO: Each row requires keys `prompt`, `chosen`, `rejected`.\n- RM: Either the same triplet schema (derived pairs) or an explicit pair with labels; internally RM trainer can consume `(prompt, chosen, rejected)` by forming preference pairs.\n\nValidation rules (ingestion command enforces):\n- Non-empty strings for all required fields\n- Reasonable length bounds (configurable), deduplication by hash\n- Drop rows where `chosen == rejected`\n- Compute and write a `schema_report.json` with counts and dropped reasons\n\n## CLI specification (final)\n\n1) Argilla export ingestion\n- Command:\n  - `aios data argilla-export --input <export.{jsonl,parquet}> --output training_data/curated_datasets/preferences --format trl` \n- Important flags:\n  - `--train-ratio 0.9 --eval-ratio 0.05 --test-ratio 0.05`\n  - `--min-prompt-toks 3 --max-prompt-toks 2048` (approx by chars if tokenizer not available)\n  - `--dedup-by prompt+chosen+rejected` (hash function)\n  - `--anonymize` (optional; integrates with Presidio later PF-006)\n  - `--shuffle-seed 42`\n- Output:\n  - Writes `train.jsonl`, `eval.jsonl`, `test.jsonl` & `schema_report.json` under the output dir\n  - Returns exit code 0 with summary printed; warns if <95% valid\n\n2) Reward model training\n- Command:\n  - `aios hrm-hf rm-train --base-model <hf_model> --dataset-dir training_data/curated_datasets/preferences --output-dir artifacts/rm/<name> --epochs 1 --lr 2e-5 --batch-size 8`\n- Additional flags:\n  - `--max-steps <int>` (overrides epochs), `--gradient-accumulation 1`, `--bf16/--fp16`, `--lr-scheduler cosine`, `--warmup-ratio 0.03`, `--weight-decay 0.01`\n  - `--max-length 1024`, `--truncation-right` (or left), `--eval-steps 100`, `--save-steps 200`\n  - `--deepspeed config/deepspeed_zero2.json` (optional)\n  - `--wandb <project>` (optional)\n- Model:\n  - HF `AutoModelForSequenceClassification` with 1-dim head or TRL RewardModel head\n- Metrics:\n  - AUC on eval pairs; accuracy of predicting `chosen > rejected`; loss curve\n- Artifacts:\n  - Model, tokenizer, `metrics.jsonl`, `config.json`, training logs\n\n3) DPO baseline training\n- Command:\n  - `aios hrm-hf dpo-train --policy-model <hf_model> --ref-model <hf_model_or_policy_ckpt> --dataset-dir training_data/curated_datasets/preferences --output-dir artifacts/dpo/<name> --epochs 1 --lr 5e-6 --batch-size 4`\n- Additional flags:\n  - `--beta 0.1` (DPO temperature), `--label-smoothing 0.0`, `--max-length 1024`, `--gradient-accumulation 1`, `--bf16/--fp16`\n  - `--lr-scheduler cosine`, `--warmup-ratio 0.03`, `--eval-steps 100`, `--save-steps 200`\n  - `--deepspeed config/deepspeed_zero2.json` (optional), `--wandb <project>` (optional)\n- Metrics:\n  - Relative log-prob of chosen vs rejected; eval loss; win-rate on held-out\n- Artifacts:\n  - Policy model, tokenizer, `metrics.jsonl`, `config.json`, logs\n\n## GUI integration (minimal, additive)\n\nAdd an \u201cAlignment\u201d section/panel in the HRM GUI (or CLI dashboard) with:\n- Dataset card\n  - Shows counts from `schema_report.json`: total, dropped by reason, splits\n  - Buttons: \u201cRe-ingest from Argilla export\u2026\u201d, \u201cOpen dataset folder\u201d\n- Quick actions\n  - Run RM: pick base model, batch size, max steps, precision toggle; Start \u2192 streams `metrics.jsonl`\n  - Run DPO: pick policy + reference; Start \u2192 streams `metrics.jsonl`\n  - Links: \u201cOpen artifacts/rm/<name>\u201d, \u201cOpen artifacts/dpo/<name>\u201d, \u201cView logs\u201d\n- Status + telemetry\n  - Real-time tail of `metrics.jsonl` and last checkpoint info\n  - Optional W&B run URL if enabled\n\nImplementation notes:\n- Wire buttons to the same CLI under the hood (subprocess) and follow our existing logging conventions.\n- Use existing logging.yaml; stream both stdout and metrics JSONL.\n- Non-blocking runs: spawn background process; allow stop/cancel.\n\n## End-to-end runbook (Windows PowerShell)\n\n1) Prepare environment\n```powershell\n# Activate repo venv if available\n. .\\.venv\\Scripts\\Activate.ps1\n\n# Install required packages (example; adjust torch for your CUDA)\npip install transformers datasets trl accelerate peft evaluate scikit-learn pandas pyarrow\n# Optional\npip install wandb bitsandbytes deepspeed\n```\n\n2) Start Argilla server and annotate a few pairs\n```powershell\ndocker run --pull always -p 6900:6900 -e ARGILLA_ENABLE_TELEMETRY=0 argilla/argilla:latest\n# Open http://localhost:6900, create a preference project, add prompt/chosen/rejected.\n```\n\n3) Export from Argilla to JSONL (via UI or API), then ingest\n```powershell\naios data argilla-export --input path\\to\\export.jsonl --output training_data/curated_datasets/preferences --format trl --shuffle-seed 42\n```\n\n4) Train a small Reward Model on CPU (toy)\n```powershell\naios hrm-hf rm-train --base-model gpt2 --dataset-dir training_data/curated_datasets/preferences --output-dir artifacts/rm/demo --max-steps 100 --batch-size 2 --eval-steps 20\n```\n\n5) Train a small DPO policy on CPU (toy)\n```powershell\naios hrm-hf dpo-train --policy-model gpt2 --ref-model gpt2 --dataset-dir training_data/curated_datasets/preferences --output-dir artifacts/dpo/demo --max-steps 100 --batch-size 1 --eval-steps 20\n```\n\n6) Inspect outputs\n- Open `artifacts/rm/demo/metrics.jsonl` and `artifacts/dpo/demo/metrics.jsonl` to verify loss decreasing.\n- Check `schema_report.json` for ingestion health.\n\n7) Optional GPU acceleration\n- Install the correct torch build for your CUDA, add `--bf16` or `--fp16`, and optionally `--deepspeed config/deepspeed_zero2.json`.\n\n## Directory structure conventions\n\n```\ntraining_data/\n  curated_datasets/\n    preferences/\n      train.jsonl\n      eval.jsonl\n      test.jsonl\n      schema_report.json\nartifacts/\n  rm/\n    <run_name>/\n      config.json\n      metrics.jsonl\n      tokenizer/\n      model files\u2026\n  dpo/\n    <run_name>/\n      config.json\n      metrics.jsonl\n      tokenizer/\n      model files\u2026\n```\n\n## Acceptance checklists\n\nIngestion (argilla-export)\n- [ ] >95% rows valid; <5% dropped for schema/length\n- [ ] train/eval/test splits present\n- [ ] `schema_report.json` written with counts by reason\n\nReward Model (rm-train)\n- [ ] Training finishes 100 toy steps without errors on CPU\n- [ ] AUC on eval set > 0.6 (toy acceptable)\n- [ ] `metrics.jsonl` shows monotonic loss decrease on average\n- [ ] Artifacts saved under `artifacts/rm/<name>/`\n\nDPO (dpo-train)\n- [ ] Training finishes 100 toy steps without errors on CPU\n- [ ] Validation chosen log-prob improves vs reference \u2265 5% win-rate on held-out pairs\n- [ ] `metrics.jsonl` present with eval loss improvements\n- [ ] Artifacts saved under `artifacts/dpo/<name>/`\n\nDocs & GUI\n- [ ] This runbook executes end-to-end on CPU\n- [ ] GUI Alignment panel shows dataset counts and can launch runs\n- [ ] Links open artifacts and live logs\n\n## Troubleshooting\n\n- OOM on GPU: reduce `--batch-size`, increase `--gradient-accumulation`, enable `--fp16/--bf16`, or use `bitsandbytes` 8-bit\n- Slow CPU runs: use tiny HF models (e.g., `sshleifer/tiny-gpt2`), reduce `--max-length`, `--max-steps`\n- Tokenizer mismatch: ensure policy/ref and tokenizer directories align; remove stale cached tokenizers under `artifacts/hf_implant/tokenizers/` if needed\n- Deepspeed import errors: reinstall with matching CUDA; fall back to single-GPU accelerate if blocked\n- Dataset schema errors: open `schema_report.json`; fix malformed rows or re-export\n\n## Security & privacy\n\n- Avoid PII in free-text; if required, enable `--anonymize` (PF-006 Presidio integration)\n- Do not upload private data to external services unless explicitly configured (disable telemetry by default)\n\n## Telemetry (optional)\n\n- Enable with `--wandb <project>`; include run name and tags like `pf-001`, `rm` or `dpo`\n- Always write local `metrics.jsonl` for reproducibility\n\n## Implementation plan (engineering)\n\nMinimal code stubs to add:\n- `src/aios/data/argilla_ingest.py`\n  - read Argilla JSONL/Parquet \u2192 normalize \u2192 splits \u2192 schema report\n- `src/aios/cli/hrm_hf/rm_train.py`\n  - TRL RewardTrainer wrapper; HF model+tokenizer load; metrics logging\n- `src/aios/cli/hrm_hf/dpo_train.py`\n  - TRL DPOTrainer wrapper; policy+ref load; metrics logging\n\nConfig options:\n- Either extend global `TrainingConfig` with rm/dpo sections or define lightweight local configs in each CLI module to reduce coupling for PF-001.\n\nLogging:\n- Reuse `logging.yaml`; write structured metrics to JSONL under the output dir.\n\n## Milestones\n\n- M1 (1\u20132 days): Ingestion and validation CLI; skeleton TRL trainers\n- M2 (1\u20132 days): End-to-end toy run; artifact packaging; documentation and GUI buttons\n\n## Go/No-Go criteria\n\n- Go when all Acceptance checklists are satisfied on CPU; GPU smoke test completes with bf16/fp16; no critical regressions to existing HRM CLI.\n- No-Go if ingestion validity < 90% or trainers fail to complete toy runs on CPU.\n\n## Appendix A \u2014 Dataset examples\n\nSample DPO row (JSONL):\n```\n{\"prompt\": \"Write a haiku about the moon.\", \"chosen\": \"Silver moon whispers\\nTides hum ancient lullabies\\nNight cradles the seas.\", \"rejected\": \"The moon is round. It is in the sky.\"}\n```\n\n## Appendix B \u2014 Suggested VS Code tasks (optional)\n\nAdd tasks to reproduce quick runs (mirroring existing HRM tasks):\n- Run ingestion (argilla-export) with a sample file\n- Run RM trainer with tiny model\n- Run DPO trainer with tiny model\n\nThese tasks should log to `artifacts/{rm,dpo}/<name>/metrics.jsonl` and be discoverable from the Command Palette.\n\n---\n\nNotes:\n- TRL is used for baselines and RM training; HRM remains the primary custom model. Distillation into HRM can be explored in a later PF.\n", "tags": ["cli", "datasets", "evaluation", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "PF-001: Preference data via Argilla + TRL (RM/DPO baselines)"}, {"line": 4, "text": "Outcomes and success criteria"}, {"line": 11, "text": "Architecture overview"}, {"line": 20, "text": "Scope"}, {"line": 32, "text": "Repository integration points"}, {"line": 44, "text": "Prerequisites"}, {"line": 56, "text": "Data formats and schema"}, {"line": 68, "text": "CLI specification (final)"}, {"line": 110, "text": "GUI integration (minimal, additive)"}, {"line": 129, "text": "End-to-end runbook (Windows PowerShell)"}, {"line": 133, "text": "Activate repo venv if available"}, {"line": 136, "text": "Install required packages (example; adjust torch for your CUDA)"}, {"line": 138, "text": "Optional"}, {"line": 145, "text": "Open http://localhost:6900, create a preference project, add prompt/chosen/rejected."}, {"line": 170, "text": "Directory structure conventions"}, {"line": 195, "text": "Acceptance checklists"}, {"line": 219, "text": "Troubleshooting"}, {"line": 227, "text": "Security & privacy"}, {"line": 232, "text": "Telemetry (optional)"}, {"line": 237, "text": "Implementation plan (engineering)"}, {"line": 253, "text": "Milestones"}, {"line": 258, "text": "Go/No-Go criteria"}, {"line": 263, "text": "Appendix A \u2014 Dataset examples"}, {"line": 270, "text": "Appendix B \u2014 Suggested VS Code tasks (optional)"}]}, {"path": "planned_features/data-backends-vector-stores.md", "content": "## PF-005: Data backends and vector stores\n\nThis document is a comprehensive design and delivery guide for adding pluggable dataset backends (Hugging Face Datasets streaming, WebDataset shards) and a minimal vector store layer (Qdrant or LanceDB) to enable scalable data ingestion and future retrieval/memory features across CLI and GUI.\n\n### Goals (What we ship)\n\n- Add a first-class dataset backend abstraction used by ACTV1 training flows.\n- Provide two new backends in addition to the current custom loader:\n\t- Hugging Face Datasets (streaming, with light filtering and caching knobs)\n\t- WebDataset shards (local/URL tar streams)\n- Add a minimal vector store interface with swappable drivers (Qdrant, LanceDB) for embedding upsert/query.\n- Expose all of the above via CLI and the HRM Training GUI panel with a clear, safe default path and backward compatibility.\n\n### Motivation\n\n- Improve scalability and reproducibility of dataset handling.\n- Prepare for retrieval-augmented features and expert selection memories.\n\n### Non-goals (Out of scope for PF-005)\n\n- Full RAG pipelines, memory policies, or retrieval-integrated training loops.\n- On-the-fly embedding model training; we only provide an API and a thin client.\n\n---\n\n## Architecture overview\n\n### 1) Dataset backends\n\nIntroduce a small interface and pluggable implementations:\n\n- Module: `src/aios/cli/hrm_hf/data_backends/`\n\t- `base.py`: defines the interface and utilities\n\t- `custom.py`: current file/dir/CSV/jsonl sampler (existing behavior)\n\t- `hf.py`: HF Datasets streaming loader\n\t- `webdataset.py`: WebDataset shard reader\n\nContract (Pythonic sketch):\n\n```python\n# src/aios/cli/hrm_hf/data_backends/base.py\nfrom typing import Iterable, Protocol, Optional, Dict, Any\n\nclass SampleBatch(Protocol):\n\t\tinput_ids: Any  # torch.LongTensor [B, T]\n\t\tattn_mask: Any  # Optional, same shape\n\nclass Backend(Protocol):\n\t\tdef __init__(self, cfg: Dict[str, Any]): ...\n\t\tdef iter_text(self) -> Iterable[str]: ...  # raw text stream (pre-tokenization)\n\t\t# Optional: direct batching/tokenization if desired\n\t\tdef iter_batches(self) -> Iterable[SampleBatch]: ...\n```\n\nIntegration point in training:\n\n- In `src/aios/cli/hrm_hf/train_actv1.py` the data loader path delegates to a builder:\n\t- `from .data_backends import build_backend`\n\t- `backend = build_backend(config)`\n\t- Use `backend.iter_text()` and preserve existing tokenization/batching logic, unless `iter_batches()` is present and compatible.\n\n### 2) Vector store layer\n\n- Module: `src/aios/memory/vector_store.py`\n\t- Defines `VectorStoreClient` protocol and factory\n\t- Drivers: `qdrant.py`, `lancedb.py`\n\nContract (sketch):\n\n```python\nfrom typing import Sequence, Dict, Any, List, Tuple, Optional\n\nclass VectorStoreClient:\n\t\tdef upsert(self, ids: Sequence[str], vectors: Sequence[Sequence[float]], metadata: Optional[Sequence[Dict[str, Any]]] = None) -> None: ...\n\t\tdef query(self, vector: Sequence[float], top_k: int = 5, filter: Optional[Dict[str, Any]] = None) -> List[Tuple[str, float, Dict[str, Any]]]: ...\n\t\tdef delete(self, ids: Sequence[str]) -> None: ...\n\t\tdef close(self) -> None: ...\n```\n\nUse cases now: basic smoke tests and future expert-selection memories. No training-time coupling required in PF-005.\n\n---\n\n## User-facing changes\n\n### CLI additions (Typer)\n\nTarget: `src/aios/cli/hrm_hf_cli.py` \u2192 `train-actv1` command\n\nNew options (safe defaults maintain existing behavior):\n\n- `--dataset-backend [custom|hf|webdataset]` (default: `custom`)\n- Backend-specific flags:\n\t- HF Datasets:\n\t\t- `--hf-name TEXT` (e.g., \"wikitext\")\n\t\t- `--hf-config TEXT` (optional; e.g., \"wikitext-103-raw-v1\")\n\t\t- `--hf-split TEXT` (default: `train`)\n\t\t- `--hf-streaming/--no-hf-streaming` (default: enabled)\n\t\t- `--hf-cache-dir PATH` (optional)\n\t\t- `--hf-num-workers INT` (tokenization workers; default 2)\n\t\t- `--hf-token-env TEXT` (env var name for auth token; default `HUGGING_FACE_HUB_TOKEN`)\n\t- WebDataset:\n\t\t- `--wds-pattern TEXT` (e.g., `data/shards/shard-{000000..000099}.tar` or `https://.../shard-{000..099}.tar`)\n\t\t- `--wds-resampled/--no-wds-resampled` (default: false)\n\t\t- `--wds-shuffle INT` (buffer size; default 1000)\n\t\t- `--wds-decode [text|bytes]` (default: `text`)\n\t\t- `--wds-key TEXT` (key to read in tar, default: `txt`)\n\nBackward compatibility:\n\n- `--dataset-file` continues to work with `--dataset-backend=custom` (default).\n- If users pass an `hf://dataset:config:split` value to `--dataset-file`, we auto-map to `--dataset-backend=hf` and parse parts (non-breaking convenience already used in GUI).\n\nExamples:\n\n```powershell\n# Custom (unchanged)\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 10\n\n# HF streaming\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-backend hf `\n\t--hf-name wikitext --hf-config wikitext-103-raw-v1 --hf-split train `\n\t--steps 10 --batch-size 2\n\n# WebDataset shards (local)\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-backend webdataset `\n\t--wds-pattern \"C:/data/shards/shard-{000000..000009}.tar\" `\n\t--steps 10 --batch-size 2\n```\n\nWindows notes:\n\n- Use PowerShell quoting as shown above. For large brace-globs, prefer quotes (\") to prevent premature expansion.\n\n### GUI additions (HRM Training Panel)\n\nTarget: `src/aios/gui/components/hrm_training_panel.py`\n\n- Add a \"Dataset backend\" dropdown next to \"Dataset file/dir\" with values: `custom`, `hf`, `webdataset`.\n- When `hf` is selected, show inline fields: Name, Config, Split, Streaming (checkbox), Cache dir.\n- When `webdataset` is selected, show: Pattern, Resampled (checkbox), Shuffle buf, Decode, Key.\n- Continue to support the existing \"hf://\u2026\" entry shortcut. If user pastes `hf://<name>:<config>:<split>`, auto-select `hf` and populate fields.\n- Persist new fields via `get_state()/set_state()` and include them when building `TrainingConfig`.\n\nBehavioral notes:\n\n- Keep dataset selection UX simple: either paste a path/URI or use the new dropdown to reveal backend-specific fields.\n- Add small helper tooltips explaining streaming and shard patterns.\n- Respect existing ASCII-only filter and memory estimator displays.\n\n---\n\n## Config and validation\n\nExtend `TrainingConfig` (module: `src/aios/core/hrm_training/training_config.py`):\n\n- New fields:\n\t- `dataset_backend: Literal[\"custom\", \"hf\", \"webdataset\"] = \"custom\"`\n\t- HF: `hf_name: Optional[str]`, `hf_config: Optional[str]`, `hf_split: str = \"train\"`, `hf_streaming: bool = True`, `hf_cache_dir: Optional[str]`, `hf_num_workers: int = 2`, `hf_token_env: str = \"HUGGING_FACE_HUB_TOKEN\"`\n\t- WDS: `wds_pattern: Optional[str]`, `wds_resampled: bool = False`, `wds_shuffle: int = 1000`, `wds_decode: str = \"text\"`, `wds_key: str = \"txt\"`\n\nValidation rules:\n\n- If `dataset_backend == \"custom\"`, require `dataset_file` (existing behavior).\n- If `dataset_backend == \"hf\"`, require `hf_name` (or parse from `dataset_file` if `hf://` URI). Validate split exists if metadata is available.\n- If `dataset_backend == \"webdataset\"`, require `wds_pattern` and ensure it resolves to at least one shard (or allow late-binding with clear log warnings).\n- ASCII filter, batching, and max sequence length behavior must remain unchanged from user\u2019s POV.\n\n---\n\n## Implementation details\n\n### A) Data backends package\n\nFiles to add:\n\n- `src/aios/cli/hrm_hf/data_backends/base.py`\n- `src/aios/cli/hrm_hf/data_backends/custom.py` (wrap current logic)\n- `src/aios/cli/hrm_hf/data_backends/hf.py`\n- `src/aios/cli/hrm_hf/data_backends/webdataset.py`\n- `src/aios/cli/hrm_hf/data_backends/__init__.py` with `build_backend(config)`\n\nKey behaviors:\n\n- HF Datasets\n\t- Use `datasets.load_dataset(hf_name, hf_config, split=hf_split, streaming=hf_streaming)`\n\t- When streaming, iterate and yield `example[\"text\"]` or join fields if text column not obvious (configurable via simple heuristic: prefer `text`, else `content`, else JSON stringify line)\n\t- Respect `ascii_only` and filtering already present in training pipeline\n\t- Support `hf_cache_dir` and `HUGGINGFACE_HUB_CACHE`/`HF_HOME`\n\t- Prefetch: use a small async queue with size 2\u20134 to smooth tokenizer throughput\n\n- WebDataset\n\t- If `webdataset` lib is available: use `wds.WebDataset(pattern).shuffle(wds_shuffle)`; otherwise provide a minimal tar iterator (local-only) that reads `*.txt` entries matching key\n\t- Map `wds_decode\": text|bytes` to return `str` or `bytes` and let tokenizer branch accordingly\n\t- Support `wds_resampled` with `wds.ResampledShards` when library is present\n\n### B) Tokenization and batching\n\n- Preserve current tokenization path to avoid regressions.\n- If a backend exposes `iter_batches()` matching our `SampleBatch`, we can optionally bypass text tokenization, but keep this disabled by default in PF-005.\n- Ensure deterministic seeding with existing RNG controls.\n\n### C) HRM CLI wiring\n\n- Update `src/aios/cli/hrm_hf_cli.py` to add new options and to pass them into `TrainingConfig`.\n- Add validation hints in Typer help messages for Windows path quoting and HF auth.\n\n### D) GUI wiring\n\n- In `HRMTrainingPanel`, add a new `dataset_backend_var` and conditional UI stacks.\n- Extend `build_training_config()` to populate the new `TrainingConfig` fields.\n- Update `get_state()/set_state()` to persist and restore the new fields.\n- Continue supporting the `hf://` inline format. If user switches backend manually, keep the inline field synchronized.\n\n### E) Vector store minimal layer\n\nFiles to add:\n\n- `src/aios/memory/vector_store.py` (interface + factory)\n- `src/aios/memory/vector_stores/qdrant.py`\n- `src/aios/memory/vector_stores/lancedb.py`\n\nFunctionality:\n\n- `upsert(ids, vectors, metadata)` and `query(vector, top_k)` with cosine similarity.\n- Qdrant: depend on `qdrant-client`; collection auto-create if missing; index on cosine.\n- LanceDB: depend on `lancedb`; create table if missing; approximate query OK for starter.\n\nOptional CLI (lightweight utility for smoke tests):\n\n- `aios memory vs-upsert` and `aios memory vs-query` that call the above client; documented only, can be added in a follow-up small PR.\n\n---\n\n## Docker and local services\n\nAdd a Qdrant service snippet to `docker-compose.yml` (either appended or documented):\n\n```yaml\nservices:\n\tqdrant:\n\t\timage: qdrant/qdrant:latest\n\t\tports:\n\t\t\t- \"6333:6333\"\n\t\tvolumes:\n\t\t\t- ./artifacts/qdrant:/qdrant/storage\n\t\thealthcheck:\n\t\t\ttest: [\"CMD\", \"wget\", \"-qO-\", \"http://localhost:6333/readyz\"]\n\t\t\tinterval: 10s\n\t\t\ttimeout: 5s\n\t\t\tretries: 5\n```\n\nWindows quickstart:\n\n```powershell\ndocker compose up -d qdrant\n```\n\nFor LanceDB (no service), nothing to run; it\u2019s an embedded store.\n\n---\n\n## Testing and acceptance criteria\n\nWhat we test:\n\n1) HF streaming backend\n\t - Run a 1\u20132 step training invoking an HF dataset by name/config/split\n\t - Verify tokenization continues to function; metrics JSONL emits at least one record\n2) WebDataset backend (local shards)\n\t - Create 1\u20132 tiny tar shards with small `*.txt` samples\n\t - Run 1\u20132 step training using `--wds-pattern` and verify iteration + metrics\n3) Vector store\n\t - With Qdrant running or LanceDB selected, upsert 100 random 128D vectors, query a held-out vector, verify top-5 return with decreasing scores\n\nSuggested smoke commands (PowerShell):\n\n```powershell\n# HF streaming smoke\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-backend hf --hf-name wikitext --hf-config wikitext-103-raw-v1 --hf-split train `\n\t--steps 1 --batch-size 2 --halt-max-steps 1 `\n\t--log-file artifacts/brains/actv1/metrics.jsonl\n\n# WebDataset smoke (assuming shards exist)\naios hrm-hf train-actv1 --model gpt2 `\n\t--dataset-backend webdataset --wds-pattern \"training_data/shards/shard-{000000..000001}.tar\" `\n\t--steps 1 --batch-size 2 --halt-max-steps 1 `\n\t--log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nAcceptance criteria (pass/fail):\n\n- HF: Iterates and trains for 1 step on a public dataset split; no blocking warnings\n- WDS: Iterates and trains for 1 step from shards; no blocking warnings\n- VS: Upsert/query test returns top-5 with non-increasing similarity\n\n---\n\n## Rollout plan\n\n1) M1 (2 days): Data backends + docs\n\t - Implement backend package and CLI/GUI wiring\n\t - Ship a doc with examples, and enable a dry-run path for each backend\n\t - Add unit tests for HF URI parsing and WDS pattern parsing\n2) M2 (1 day): Vector store + example\n\t - Implement minimal drivers\n\t - Add docker-compose snippet and a tiny usage doc\n\nFeature flagging/back-compat:\n\n- Default remains `custom` loader; no behavior change for current users.\n- `hf://` path auto-detection provides an additive convenience, not a breaking change.\n\n---\n\n## Risks and mitigations\n\n- Streaming backpressure: Use small prefetch queues and timeouts; allow `--hf-num-workers` to adjust throughput.\n- HF auth/rate limits: Use `--hf-token-env` and document how to set `HUGGING_FACE_HUB_TOKEN`.\n- WebDataset lib availability: Provide a minimal built-in tar reader for local shards when `webdataset` is not installed.\n- Windows Docker friction: Provide LanceDB as an embedded alternative; document `docker compose up -d qdrant`.\n\n---\n\n## Checklists\n\n### Engineering checklist\n\n- [ ] Add `data_backends/` package and register `build_backend(config)`\n- [ ] Extend `TrainingConfig` with new fields + validation\n- [ ] Update `hrm_hf_cli.py` to surface new flags\n- [ ] Wire `train_actv1_impl` to use backend builder\n- [ ] Add GUI controls (dropdown + dynamic fields) and persist state\n- [ ] Ensure `hf://` URI auto-maps to HF backend in both CLI and GUI\n- [ ] Add `vector_store.py` interface and Qdrant/LanceDB drivers\n- [ ] Document docker snippet and LanceDB alternative\n\n### QA checklist\n\n- [ ] CLI help shows new flags with clear descriptions\n- [ ] Run HF smoke with 1 step; verify metrics JSONL appended\n- [ ] Run WDS smoke with 1 step; verify metrics JSONL appended\n- [ ] VS: upsert/query 100 embeddings; query returns 5 nearest with sensible scores\n- [ ] GUI: switching backends updates visible fields and persists across restarts\n- [ ] GUI: dataset estimator and memory estimator still render without errors\n\n### Docs checklist\n\n- [ ] Update quick starts (`docs/QUICK_START.md`, `docs/ACTV1_MOE_QUICK_START.md` [placeholder]) to mention backends\n- [ ] Add a short \u201cData Backends\u201d section in `docs/INDEX.md`\n- [ ] Add a \u201cVector Store (starter)\u201d section in `docs/INDEX.md`\n\n---\n\n## Appendix: mapping from dataset URIs to config\n\n- `file:///path/to/data.txt` \u2192 backend=custom, dataset_file=that path\n- `hf://<name>:<config>:<split>` \u2192 backend=hf, populate `hf_name`, `hf_config`, `hf_split`\n- `wds://<pattern>` \u2192 backend=webdataset, `wds_pattern=<pattern>`\n\nIf users paste raw values (no scheme):\n\n- If it looks like a local file/dir \u2192 `custom`\n- If it matches `hf://` \u2192 `hf`\n- If it contains `{000..999}.tar` or endswith `.tar` \u2192 suggest `webdataset`\n\nThis keeps the happy path minimal while enabling more scalable backends when needed.\n", "tags": ["cli", "datasets", "evaluation", "experts", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "PF-005: Data backends and vector stores"}, {"line": 4, "text": "Goals (What we ship)"}, {"line": 13, "text": "Motivation"}, {"line": 18, "text": "Non-goals (Out of scope for PF-005)"}, {"line": 25, "text": "Architecture overview"}, {"line": 27, "text": "1) Dataset backends"}, {"line": 40, "text": "src/aios/cli/hrm_hf/data_backends/base.py"}, {"line": 50, "text": "\t\t# Optional: direct batching/tokenization if desired"}, {"line": 61, "text": "2) Vector store layer"}, {"line": 83, "text": "User-facing changes"}, {"line": 85, "text": "CLI additions (Typer)"}, {"line": 116, "text": "Custom (unchanged)"}, {"line": 119, "text": "HF streaming"}, {"line": 125, "text": "WebDataset shards (local)"}, {"line": 136, "text": "GUI additions (HRM Training Panel)"}, {"line": 154, "text": "Config and validation"}, {"line": 172, "text": "Implementation details"}, {"line": 174, "text": "A) Data backends package"}, {"line": 198, "text": "B) Tokenization and batching"}, {"line": 204, "text": "C) HRM CLI wiring"}, {"line": 209, "text": "D) GUI wiring"}, {"line": 216, "text": "E) Vector store minimal layer"}, {"line": 236, "text": "Docker and local services"}, {"line": 265, "text": "Testing and acceptance criteria"}, {"line": 281, "text": "HF streaming smoke"}, {"line": 287, "text": "WebDataset smoke (assuming shards exist)"}, {"line": 302, "text": "Rollout plan"}, {"line": 319, "text": "Risks and mitigations"}, {"line": 328, "text": "Checklists"}, {"line": 330, "text": "Engineering checklist"}, {"line": 341, "text": "QA checklist"}, {"line": 350, "text": "Docs checklist"}, {"line": 358, "text": "Appendix: mapping from dataset URIs to config"}]}, {"path": "planned_features/DEEPSPEED_ZERO_INFINITY.md", "content": "# DeepSpeed ZeRO-Infinity Integration Plan\n\n## Executive Summary\n\n**Goal**: Integrate DeepSpeed ZeRO-Infinity to enable training of multi-billion parameter models that exceed GPU and CPU memory capacity by leveraging NVMe storage offloading.\n\n**Current State**: AI-OS supports ZeRO Stages 1-3 with basic CPU offloading for optimizer states.\n\n**Target State**: Full ZeRO-Infinity support with intelligent NVMe offloading for parameters, gradients, and optimizer states, enabling training of 10B+ parameter models on consumer hardware.\n\n**Impact**: \n- Train models 10-100x larger than current limits\n- Utilize high-speed NVMe storage as memory extension\n- Enable training of GPT-3 scale models (175B params) on multi-GPU consumer setups\n- Minimal speed penalty (5-15% slower) with fast NVMe drives\n\n---\n\n## Background\n\n### What is ZeRO-Infinity?\n\nZeRO-Infinity extends DeepSpeed's ZeRO-3 optimization by adding **NVMe offloading** as a third tier of memory hierarchy:\n\n1. **GPU VRAM** (fastest, smallest) - Active computation\n2. **CPU RAM** (fast, medium) - Optimizer states, gradients  \n3. **NVMe Storage** (slower, massive) - Parameters, checkpoints\n\n**Key Innovation**: Overlapped data movement between tiers masks latency, maintaining 90%+ training efficiency even with NVMe offloading.\n\n### Current AI-OS Capabilities\n\n\u2705 **Already Implemented**:\n- ZeRO Stage 1: Optimizer state partitioning (~25% VRAM savings)\n- ZeRO Stage 2: Optimizer + gradient partitioning (~50% VRAM savings)\n- ZeRO Stage 3: Full parameter partitioning (~75% VRAM savings)\n- Basic CPU offloading for carry states in extreme contexts\n- 8-bit optimizers via bitsandbytes\n- Gradient checkpointing\n- Chunked training for extreme contexts (100K+ tokens)\n\n\u274c **Missing for ZeRO-Infinity**:\n- NVMe offload configuration and initialization\n- Parameter offloading to NVMe storage\n- Optimizer state offloading to NVMe\n- Gradient offloading to NVMe (optional)\n- Async prefetching from NVMe to CPU/GPU\n- NVMe bandwidth monitoring and optimization\n- Memory hierarchy management\n- Pin memory optimizations for fast transfers\n\n---\n\n## Technical Architecture\n\n### Memory Hierarchy with ZeRO-Infinity\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Training Iteration                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                               \u2502\n\u2502  GPU VRAM (11GB)                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 Active Parameters (32 layers out of 96)             \u2502   \u2502\n\u2502  \u2502 Activations & Gradients (current batch)             \u2502   \u2502\n\u2502  \u2502 Optimizer States (for active params)                \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502         \u2195 PCIe 3.0/4.0 (15-30 GB/s)                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  CPU RAM (32-64GB)                                          \u2502\n\u2502  \u2502 Prefetched Parameters (next 64 layers)              \u2502   \u2502\n\u2502  \u2502 Optimizer States (frozen params)                    \u2502   \u2502\n\u2502  \u2502 Gradients (waiting to be applied)                   \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502         \u2195 NVMe PCIe 4.0 (5-7 GB/s)                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  NVMe Storage (1-2TB)                                       \u2502\n\u2502  \u2502 Full Model Parameters (10B-175B params)             \u2502   \u2502\n\u2502  \u2502 Optimizer State History (Adam momentum/variance)    \u2502   \u2502\n\u2502  \u2502 Checkpoints & Intermediate States                   \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Data Flow During Training\n\n```\nForward Pass:\n1. NVMe \u2192 CPU: Async prefetch next layer params\n2. CPU \u2192 GPU: Transfer layer params for computation  \n3. GPU: Compute activations\n4. Repeat for all layers\n\nBackward Pass:\n1. GPU: Compute gradients\n2. GPU \u2192 CPU: Offload gradients (optional)\n3. CPU \u2192 NVMe: Archive gradients (optional)\n4. GPU: Update params with optimizer\n5. GPU \u2192 CPU \u2192 NVMe: Offload updated params\n\nKey Optimization: Overlap transfers with computation!\n```\n\n---\n\n## Implementation Plan\n\n### Phase 1: Configuration and Setup (Week 1)\n\n#### 1.1 DeepSpeed Configuration Files\n\nCreate new configuration profiles for ZeRO-Infinity:\n\n**File**: `config/deepspeed_zero_infinity.json`\n\n```json\n{\n  \"train_batch_size\": \"auto\",\n  \"train_micro_batch_size_per_gpu\": \"auto\",\n  \"gradient_accumulation_steps\": \"auto\",\n  \n  \"optimizer\": {\n    \"type\": \"AdamW\",\n    \"params\": {\n      \"lr\": \"auto\",\n      \"betas\": [0.9, 0.999],\n      \"eps\": 1e-8,\n      \"weight_decay\": 0.01\n    }\n  },\n  \n  \"scheduler\": {\n    \"type\": \"WarmupDecayLR\",\n    \"params\": {\n      \"warmup_min_lr\": 0,\n      \"warmup_max_lr\": \"auto\",\n      \"warmup_num_steps\": \"auto\",\n      \"total_num_steps\": \"auto\"\n    }\n  },\n  \n  \"zero_optimization\": {\n    \"stage\": 3,\n    \n    \"offload_optimizer\": {\n      \"device\": \"nvme\",\n      \"nvme_path\": \"/tmp/deepspeed_offload\",\n      \"pin_memory\": true,\n      \"buffer_count\": 5,\n      \"fast_init\": false\n    },\n    \n    \"offload_param\": {\n      \"device\": \"nvme\",\n      \"nvme_path\": \"/tmp/deepspeed_offload\",\n      \"pin_memory\": true,\n      \"buffer_count\": 5,\n      \"buffer_size\": 1e8,\n      \"max_in_cpu\": 1e9\n    },\n    \n    \"overlap_comm\": true,\n    \"contiguous_gradients\": true,\n    \"sub_group_size\": 1e9,\n    \"reduce_bucket_size\": 1e6,\n    \"stage3_prefetch_bucket_size\": 1e6,\n    \"stage3_param_persistence_threshold\": 1e5,\n    \"stage3_max_live_parameters\": 1e9,\n    \"stage3_max_reuse_distance\": 1e9,\n    \"stage3_gather_16bit_weights_on_model_save\": true\n  },\n  \n  \"aio\": {\n    \"block_size\": 1048576,\n    \"queue_depth\": 8,\n    \"thread_count\": 1,\n    \"single_submit\": false,\n    \"overlap_events\": true\n  },\n  \n  \"bf16\": {\n    \"enabled\": true\n  },\n  \n  \"gradient_clipping\": 1.0,\n  \"steps_per_print\": 100,\n  \"wall_clock_breakdown\": false\n}\n```\n\n**File**: `config/deepspeed_zero_infinity_cpu.json` (CPU fallback)\n\n```json\n{\n  \"train_batch_size\": \"auto\",\n  \"train_micro_batch_size_per_gpu\": \"auto\",\n  \"gradient_accumulation_steps\": \"auto\",\n  \n  \"optimizer\": {\n    \"type\": \"AdamW\",\n    \"params\": {\n      \"lr\": \"auto\",\n      \"betas\": [0.9, 0.999],\n      \"eps\": 1e-8,\n      \"weight_decay\": 0.01\n    }\n  },\n  \n  \"zero_optimization\": {\n    \"stage\": 3,\n    \n    \"offload_optimizer\": {\n      \"device\": \"cpu\",\n      \"pin_memory\": true\n    },\n    \n    \"offload_param\": {\n      \"device\": \"cpu\",\n      \"pin_memory\": true\n    },\n    \n    \"overlap_comm\": true,\n    \"contiguous_gradients\": true\n  },\n  \n  \"bf16\": {\n    \"enabled\": true\n  }\n}\n```\n\n#### 1.2 Training Configuration Fields\n\n**File**: `src/aios/core/hrm_training/training_config/optimization_fields.py`\n\nAdd new fields to `OptimizationFields` dataclass:\n\n```python\n# ZeRO-Infinity NVMe Offloading\nnvme_offload_path: str = \"/tmp/deepspeed_offload\"\n\"\"\"Path to NVMe directory for ZeRO-Infinity offloading.\n\nWhen zero_stage=\"infinity\", this directory is used to offload:\n- Model parameters\n- Optimizer states  \n- Optionally gradients\n\nRequirements:\n- Must be on NVMe SSD for best performance (PCIe 3.0/4.0)\n- Needs ~2-10x model size in free space\n- Should be on fast storage (>2 GB/s sequential write)\n\nRecommended paths:\n- Linux: /mnt/nvme/deepspeed_offload (mount NVMe here)\n- Windows: D:/deepspeed_offload (if D: is NVMe)\n- Temp fallback: /tmp/deepspeed_offload (auto-cleanup)\n\nSize requirements (approximate):\n- 1B params: ~20GB NVMe space\n- 10B params: ~200GB NVMe space  \n- 100B params: ~2TB NVMe space\n\nDefault: /tmp/deepspeed_offload\n\"\"\"\n\nnvme_offload_optimizer: bool = True\n\"\"\"Offload optimizer states to NVMe storage.\n\nWhen using ZeRO-Infinity, optimizer states (Adam momentum & variance) \nconsume 2x parameter memory. Offloading to NVMe frees CPU RAM.\n\nImpact:\n- Memory: Saves 2x param size in CPU RAM\n- Speed: ~5-10% slower (with fast NVMe)\n- Requires: Fast NVMe drive (>2 GB/s)\n\nDefault: True (enabled for infinity mode)\n\"\"\"\n\nnvme_offload_params: bool = True\n\"\"\"Offload model parameters to NVMe storage.\n\nStores frozen/inactive parameters on NVMe, loading only active layers\nto GPU during computation. Essential for models >10B parameters.\n\nImpact:\n- Memory: Stores full model on NVMe vs RAM\n- Speed: ~10-15% slower (with fast NVMe)\n- Requires: NVMe with model_size * 4 bytes free space\n\nDefault: True (enabled for infinity mode)\n\"\"\"\n\nnvme_offload_gradients: bool = False\n\"\"\"Offload gradients to NVMe storage (optional).\n\nExperimental: Offload computed gradients to NVMe between backward passes.\nOnly beneficial for extremely large models (>100B params) or small RAM.\n\nImpact:\n- Memory: Saves param_size in CPU RAM\n- Speed: ~20-30% slower\n- Rarely needed: Usually CPU RAM sufficient\n\nDefault: False (not recommended unless desperate)\n\"\"\"\n\naio_block_size: int = 1048576\n\"\"\"Async I/O block size for NVMe operations (bytes).\n\nControls the block size for asynchronous I/O transfers between NVMe and CPU.\nLarger blocks = better throughput, higher latency.\n\nRecommended values:\n- 1048576 (1MB): Balanced (default)\n- 2097152 (2MB): High throughput NVMe (PCIe 4.0)\n- 524288 (512KB): Lower latency, slower drives\n\nDefault: 1048576 (1MB)\n\"\"\"\n\naio_queue_depth: int = 8\n\"\"\"Async I/O queue depth for NVMe operations.\n\nNumber of concurrent I/O requests to NVMe. Higher = better parallelism.\n\nRecommended values:\n- 4: Conservative, older NVMe drives\n- 8: Balanced (default)\n- 16: High-performance NVMe (Samsung 980 Pro, etc.)\n- 32: Extreme performance (server-grade NVMe)\n\nDefault: 8\n\"\"\"\n\npin_memory: bool = True\n\"\"\"Pin CPU memory for faster GPU transfers.\n\nPinned memory enables DMA transfers without CPU involvement,\nreducing latency for CPU\u2194GPU transfers during offloading.\n\nImpact:\n- Speed: ~20-30% faster transfers\n- Memory: Uses non-swappable RAM\n- Stability: May cause issues with low RAM systems\n\nDefault: True (recommended unless <16GB RAM)\n\"\"\"\n```\n\n#### 1.3 CLI Arguments\n\n**File**: `src/aios/cli/hrm_hf_cli.py`\n\nAdd new arguments to `train_actv1` function:\n\n```python\n# ZeRO-Infinity (NVMe Offloading) options\nnvme_offload_path: str = typer.Option(\n    \"/tmp/deepspeed_offload\", \n    \"--nvme-offload-path\",\n    help=\"Path to NVMe directory for ZeRO-Infinity offloading. Must be on fast NVMe SSD (>2 GB/s). Requires ~2-10x model size in free space.\"\n),\nnvme_offload_optimizer: bool = typer.Option(\n    True, \n    \"--nvme-offload-optimizer/--no-nvme-offload-optimizer\",\n    help=\"Offload optimizer states to NVMe (saves 2x param size in RAM, ~5-10%% slower). Requires zero-stage=infinity.\"\n),\nnvme_offload_params: bool = typer.Option(\n    True,\n    \"--nvme-offload-params/--no-nvme-offload-params\", \n    help=\"Offload model parameters to NVMe (essential for 10B+ models, ~10-15%% slower). Requires zero-stage=infinity.\"\n),\nnvme_offload_gradients: bool = typer.Option(\n    False,\n    \"--nvme-offload-gradients/--no-nvme-offload-gradients\",\n    help=\"Offload gradients to NVMe (rarely needed, ~20-30%% slower). Only for 100B+ models.\"\n),\naio_block_size: int = typer.Option(\n    1048576,\n    \"--aio-block-size\",\n    help=\"Async I/O block size for NVMe (bytes). 1MB=balanced, 2MB=high throughput, 512KB=low latency.\"\n),\naio_queue_depth: int = typer.Option(\n    8,\n    \"--aio-queue-depth\", \n    help=\"Async I/O queue depth for NVMe. 8=balanced, 16=high-performance NVMe, 32=server-grade.\"\n),\npin_memory: bool = typer.Option(\n    True,\n    \"--pin-memory/--no-pin-memory\",\n    help=\"Pin CPU memory for faster GPU transfers. Disable if <16GB RAM.\"\n),\n```\n\nUpdate `zero_stage` argument options:\n\n```python\nzero_stage: str = typer.Option(\n    \"none\", \n    \"--zero-stage\", \n    help=\"DeepSpeed ZeRO optimization: none, zero1 (\u219325%% VRAM), zero2 (\u219350%% VRAM, recommended), zero3 (\u219375%% VRAM), infinity (NVMe offload, train 10B+ models). Auto-selected if --optimize is used.\"\n),\n```\n\n---\n\n### Phase 2: Core Implementation (Week 2-3)\n\n#### 2.1 NVMe Configuration Builder\n\n**File**: `src/aios/cli/hrm_hf/nvme_config.py` (new)\n\n```python\n\"\"\"NVMe configuration and validation for ZeRO-Infinity.\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport shutil\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional, Tuple\nimport json\n\n\ndef validate_nvme_path(nvme_path: str, required_space_gb: float) -> Tuple[bool, str]:\n    \"\"\"\n    Validate NVMe offload path has sufficient space and write permissions.\n    \n    Args:\n        nvme_path: Path to NVMe directory\n        required_space_gb: Required free space in GB\n        \n    Returns:\n        (is_valid, error_message)\n    \"\"\"\n    path = Path(nvme_path)\n    \n    # Check if path exists or can be created\n    try:\n        path.mkdir(parents=True, exist_ok=True)\n    except PermissionError:\n        return False, f\"Permission denied: Cannot create {nvme_path}\"\n    except Exception as e:\n        return False, f\"Failed to create {nvme_path}: {e}\"\n    \n    # Check write permissions\n    test_file = path / \".deepspeed_test\"\n    try:\n        test_file.touch()\n        test_file.unlink()\n    except Exception as e:\n        return False, f\"Cannot write to {nvme_path}: {e}\"\n    \n    # Check available space\n    stat = shutil.disk_usage(path)\n    available_gb = stat.free / (1024**3)\n    \n    if available_gb < required_space_gb:\n        return False, f\"Insufficient space: {available_gb:.1f}GB available, {required_space_gb:.1f}GB required\"\n    \n    return True, \"\"\n\n\ndef estimate_nvme_space_required(\n    total_params: int,\n    offload_params: bool,\n    offload_optimizer: bool,\n    offload_gradients: bool,\n    safety_factor: float = 1.5\n) -> float:\n    \"\"\"\n    Estimate required NVMe space in GB.\n    \n    Args:\n        total_params: Total model parameters\n        offload_params: Whether offloading parameters\n        offload_optimizer: Whether offloading optimizer states\n        offload_gradients: Whether offloading gradients\n        safety_factor: Multiply by this for safety margin\n        \n    Returns:\n        Required space in GB\n    \"\"\"\n    bytes_per_param = 4  # FP32\n    \n    space_gb = 0.0\n    \n    if offload_params:\n        # Parameters: 4 bytes each\n        space_gb += (total_params * bytes_per_param) / (1024**3)\n    \n    if offload_optimizer:\n        # Optimizer states: 8 bytes per param (Adam: momentum + variance)\n        space_gb += (total_params * 8) / (1024**3)\n    \n    if offload_gradients:\n        # Gradients: 4 bytes per param\n        space_gb += (total_params * bytes_per_param) / (1024**3)\n    \n    # Apply safety factor for temp files, fragmentation, etc.\n    space_gb *= safety_factor\n    \n    return space_gb\n\n\ndef check_nvme_performance(nvme_path: str) -> Dict[str, Any]:\n    \"\"\"\n    Check NVMe performance characteristics.\n    \n    Returns dict with:\n    - sequential_read_gb_s: Sequential read speed (GB/s)\n    - sequential_write_gb_s: Sequential write speed (GB/s)  \n    - is_ssd: Whether drive appears to be SSD\n    - warnings: List of performance warnings\n    \"\"\"\n    import subprocess\n    import platform\n    \n    result = {\n        \"sequential_read_gb_s\": None,\n        \"sequential_write_gb_s\": None,\n        \"is_ssd\": None,\n        \"warnings\": []\n    }\n    \n    path = Path(nvme_path).resolve()\n    \n    # Get device info based on OS\n    if platform.system() == \"Linux\":\n        try:\n            # Try to identify device from path\n            df_output = subprocess.check_output(\n                [\"df\", str(path)], \n                universal_newlines=True\n            )\n            device = df_output.split('\\n')[1].split()[0]\n            \n            # Check if rotational (HDD=1, SSD=0)\n            device_name = device.split('/')[-1].rstrip('0123456789')\n            rotational_file = f\"/sys/block/{device_name}/queue/rotational\"\n            \n            if os.path.exists(rotational_file):\n                with open(rotational_file) as f:\n                    is_rotational = f.read().strip() == '1'\n                    result[\"is_ssd\"] = not is_rotational\n                    \n                    if is_rotational:\n                        result[\"warnings\"].append(\n                            \"WARNING: Path appears to be on HDD, not SSD. \"\n                            \"ZeRO-Infinity will be very slow. Use NVMe SSD for best results.\"\n                        )\n        except Exception:\n            pass  # Can't determine, not critical\n    \n    elif platform.system() == \"Windows\":\n        # On Windows, harder to detect programmatically\n        # Just warn if path is C: (usually OS drive, may be SATA SSD)\n        if str(path).startswith(\"C:\"):\n            result[\"warnings\"].append(\n                \"Path is on C: drive. For best performance, use dedicated NVMe drive (D:, E:, etc.)\"\n            )\n    \n    # TODO: Could add actual speed test with dd/fio, but may be too slow for startup\n    \n    return result\n\n\ndef build_infinity_config(\n    base_config: Dict[str, Any],\n    nvme_path: str,\n    offload_params: bool,\n    offload_optimizer: bool,\n    offload_gradients: bool,\n    aio_block_size: int,\n    aio_queue_depth: int,\n    pin_memory: bool,\n    max_in_cpu_gb: float = 1.0,\n) -> Dict[str, Any]:\n    \"\"\"\n    Build ZeRO-Infinity configuration dict.\n    \n    Args:\n        base_config: Base DeepSpeed ZeRO-3 config\n        nvme_path: Path for NVMe offloading\n        offload_params: Offload parameters to NVMe\n        offload_optimizer: Offload optimizer states to NVMe\n        offload_gradients: Offload gradients to NVMe\n        aio_block_size: Async I/O block size\n        aio_queue_depth: Async I/O queue depth\n        pin_memory: Use pinned memory\n        max_in_cpu_gb: Max params to keep in CPU RAM (GB)\n        \n    Returns:\n        Updated config dict with ZeRO-Infinity settings\n    \"\"\"\n    config = base_config.copy()\n    \n    # Ensure we're using ZeRO stage 3\n    config[\"zero_optimization\"][\"stage\"] = 3\n    \n    # Configure parameter offloading\n    if offload_params:\n        config[\"zero_optimization\"][\"offload_param\"] = {\n            \"device\": \"nvme\",\n            \"nvme_path\": nvme_path,\n            \"pin_memory\": pin_memory,\n            \"buffer_count\": 5,\n            \"buffer_size\": int(1e8),  # 100MB buffer\n            \"max_in_cpu\": int(max_in_cpu_gb * 1e9),  # Keep some in CPU\n        }\n    else:\n        config[\"zero_optimization\"][\"offload_param\"] = {\n            \"device\": \"cpu\",\n            \"pin_memory\": pin_memory,\n        }\n    \n    # Configure optimizer offloading  \n    if offload_optimizer:\n        config[\"zero_optimization\"][\"offload_optimizer\"] = {\n            \"device\": \"nvme\",\n            \"nvme_path\": nvme_path,\n            \"pin_memory\": pin_memory,\n            \"buffer_count\": 5,\n            \"fast_init\": False,\n        }\n    else:\n        config[\"zero_optimization\"][\"offload_optimizer\"] = {\n            \"device\": \"cpu\",\n            \"pin_memory\": pin_memory,\n        }\n    \n    # Configure gradient offloading (rare)\n    if offload_gradients:\n        # Note: Gradient offload to NVMe not officially in DeepSpeed API\n        # This would require custom implementation\n        config[\"zero_optimization\"][\"offload_gradients\"] = {\n            \"device\": \"nvme\",\n            \"nvme_path\": nvme_path,\n        }\n    \n    # Configure AIO (Async I/O)\n    config[\"aio\"] = {\n        \"block_size\": aio_block_size,\n        \"queue_depth\": aio_queue_depth,\n        \"thread_count\": 1,\n        \"single_submit\": False,\n        \"overlap_events\": True,\n    }\n    \n    return config\n\n\ndef save_infinity_config(\n    config: Dict[str, Any],\n    output_path: str\n) -> None:\n    \"\"\"Save ZeRO-Infinity config to JSON file.\"\"\"\n    with open(output_path, 'w') as f:\n        json.dump(config, f, indent=2)\n```\n\n#### 2.2 Update Distributed Setup\n\n**File**: `src/aios/cli/hrm_hf/distributed_setup.py`\n\nUpdate `initialize_deepspeed` function:\n\n```python\ndef initialize_deepspeed(\n    model: Any,\n    config: \"TrainingConfig\",\n    device_obj: torch.device,\n    log_fn\n) -> Tuple[Optional[Any], bool]:\n    \"\"\"Initialize DeepSpeed ZeRO optimizer with Infinity support.\"\"\"\n    \n    zero_stage = config.zero_stage\n    \n    if not zero_stage or zero_stage == \"none\":\n        return None, False\n    \n    dev = str(device_obj).split(':')[0]\n    if dev != \"cuda\":\n        log_fn({\n            \"deepspeed\": \"skipped\",\n            \"reason\": \"Only CUDA devices supported\",\n            \"device\": dev\n        })\n        return None, False\n    \n    try:\n        import deepspeed\n        \n        # Determine config file\n        if zero_stage == \"infinity\":\n            from .nvme_config import (\n                validate_nvme_path,\n                estimate_nvme_space_required,\n                check_nvme_performance,\n                build_infinity_config,\n                save_infinity_config,\n            )\n            \n            # Calculate model size\n            total_params = sum(p.numel() for p in model.parameters())\n            \n            # Estimate NVMe space needed\n            required_space_gb = estimate_nvme_space_required(\n                total_params=total_params,\n                offload_params=config.nvme_offload_params,\n                offload_optimizer=config.nvme_offload_optimizer,\n                offload_gradients=config.nvme_offload_gradients,\n                safety_factor=1.5,\n            )\n            \n            # Validate NVMe path\n            is_valid, error_msg = validate_nvme_path(\n                config.nvme_offload_path,\n                required_space_gb\n            )\n            \n            if not is_valid:\n                log_fn({\n                    \"deepspeed\": \"nvme_validation_failed\",\n                    \"error\": error_msg,\n                    \"fallback\": \"Using CPU offload instead\",\n                })\n                # Fallback to CPU offload\n                ds_config_path = \"config/deepspeed_zero_infinity_cpu.json\"\n            else:\n                # Check NVMe performance\n                perf_info = check_nvme_performance(config.nvme_offload_path)\n                \n                for warning in perf_info.get(\"warnings\", []):\n                    log_fn({\"nvme_warning\": warning})\n                \n                # Load base ZeRO-3 config\n                import json\n                with open(\"config/deepspeed_zero3.json\") as f:\n                    base_config = json.load(f)\n                \n                # Build Infinity config\n                ds_config = build_infinity_config(\n                    base_config=base_config,\n                    nvme_path=config.nvme_offload_path,\n                    offload_params=config.nvme_offload_params,\n                    offload_optimizer=config.nvme_offload_optimizer,\n                    offload_gradients=config.nvme_offload_gradients,\n                    aio_block_size=config.aio_block_size,\n                    aio_queue_depth=config.aio_queue_depth,\n                    pin_memory=config.pin_memory,\n                    max_in_cpu_gb=1.0,  # Keep 1GB in CPU RAM\n                )\n                \n                # Save to temp file\n                import tempfile\n                temp_config = tempfile.NamedTemporaryFile(\n                    mode='w',\n                    suffix='.json',\n                    delete=False,\n                    dir='artifacts/brains/actv1'\n                )\n                save_infinity_config(ds_config, temp_config.name)\n                ds_config_path = temp_config.name\n                \n                log_fn({\n                    \"deepspeed\": \"infinity_config_created\",\n                    \"nvme_path\": config.nvme_offload_path,\n                    \"required_space_gb\": round(required_space_gb, 2),\n                    \"offload_params\": config.nvme_offload_params,\n                    \"offload_optimizer\": config.nvme_offload_optimizer,\n                    \"offload_gradients\": config.nvme_offload_gradients,\n                    \"aio_block_size\": config.aio_block_size,\n                    \"aio_queue_depth\": config.aio_queue_depth,\n                })\n        \n        elif zero_stage in [\"zero1\", \"zero2\", \"zero3\"]:\n            # Existing ZeRO stage logic\n            ds_config_path = f\"config/deepspeed_{zero_stage}.json\"\n        \n        else:\n            log_fn({\n                \"deepspeed\": \"invalid_stage\",\n                \"zero_stage\": zero_stage,\n            })\n            return None, False\n        \n        # Rest of initialization...\n        # (existing code continues)\n        \n    except ImportError as e:\n        # Handle missing DeepSpeed\n        # (existing error handling)\n        pass\n```\n\n#### 2.3 GUI Integration\n\n**File**: `src/aios/gui/components/hrm_training_panel/ui_optimizations.py`\n\nUpdate ZeRO dropdown options:\n\n```python\n# Row 5: DeepSpeed ZeRO (updated with Infinity)\nzero_row = ttk.Frame(self)\nzero_row.grid(row=5, column=0, columnspan=3, sticky=\"ew\", padx=5, pady=2)\n\nttk.Label(zero_row, text=\"DeepSpeed:\", width=15, anchor=\"e\", font=(\"TkDefaultFont\", 9, \"bold\")).pack(side=\"left\")\n\nzero_combo = ttk.Combobox(\n    zero_row,\n    textvariable=self.zero_stage_var,\n    values=[\"none\", \"zero1\", \"zero2\", \"zero3\", \"infinity\"],  # Added \"infinity\"\n    state=\"readonly\",\n    width=12\n)\nzero_combo.pack(side=\"left\", padx=5)\nself.zero_combo = zero_combo\n\n# Dynamic label for memory savings\nzero_savings_lbl = ttk.Label(zero_row, text=\"\", foreground=\"blue\")\nzero_savings_lbl.pack(side=\"left\", padx=5)\nself.zero_savings_lbl = zero_savings_lbl\n\ndef update_zero_label(*args):\n    stage = self.zero_stage_var.get()\n    savings_text = {\n        \"none\": \"\",\n        \"zero1\": \"\u219325% VRAM, ~2% slower\",\n        \"zero2\": \"\u219350% VRAM, ~5% slower (recommended)\",\n        \"zero3\": \"\u219375% VRAM, ~15% slower\",\n        \"infinity\": \"\u219390%+ VRAM, train 10B+ models (requires NVMe)\",  # NEW\n    }.get(stage, \"\")\n    zero_savings_lbl.config(text=savings_text)\n\nself.zero_stage_var.trace_add(\"write\", update_zero_label)\nupdate_zero_label()\n\n# Updated tooltip\nadd_tooltip(\n    zero_combo,\n    \"DeepSpeed ZeRO: Distributed memory optimization\\n\"\n    \"\u2022 none: Standard training\\n\"\n    \"\u2022 zero1: Partition optimizer states (\u219325% VRAM)\\n\"\n    \"\u2022 zero2: Partition optimizer + gradients (\u219350% VRAM) [RECOMMENDED]\\n\"\n    \"\u2022 zero3: Partition everything (\u219375% VRAM, slower)\\n\"\n    \"\u2022 infinity: NVMe offload for 10B+ models (\u219390%+ VRAM, requires fast NVMe SSD)\"\n)\n```\n\nAdd NVMe configuration section (collapsible):\n\n```python\n# Row 6: NVMe Configuration (shown when infinity selected)\nnvme_section = ttk.LabelFrame(self, text=\"ZeRO-Infinity NVMe Settings\", padding=5)\n# Hidden by default, shown when zero_stage == \"infinity\"\n\nnvme_path_row = ttk.Frame(nvme_section)\nnvme_path_row.pack(fill=\"x\", pady=2)\nttk.Label(nvme_path_row, text=\"NVMe Path:\", width=15).pack(side=\"left\")\nnvme_path_entry = ttk.Entry(nvme_path_row, textvariable=self.nvme_offload_path_var, width=30)\nnvme_path_entry.pack(side=\"left\", padx=5, fill=\"x\", expand=True)\nttk.Button(nvme_path_row, text=\"Browse\", command=self._browse_nvme_path, width=8).pack(side=\"left\")\n\n# Checkboxes for what to offload\noffload_frame = ttk.Frame(nvme_section)\noffload_frame.pack(fill=\"x\", pady=2)\nttk.Checkbutton(\n    offload_frame,\n    text=\"Offload Parameters\",\n    variable=self.nvme_offload_params_var\n).pack(side=\"left\", padx=5)\nttk.Checkbutton(\n    offload_frame,\n    text=\"Offload Optimizer\",\n    variable=self.nvme_offload_optimizer_var\n).pack(side=\"left\", padx=5)\nttk.Checkbutton(\n    offload_frame,\n    text=\"Offload Gradients\",\n    variable=self.nvme_offload_gradients_var\n).pack(side=\"left\", padx=5)\n\n# AIO settings (advanced, maybe collapsible)\naio_frame = ttk.LabelFrame(nvme_section, text=\"Advanced I/O Settings\", padding=3)\naio_frame.pack(fill=\"x\", pady=2)\n\naio_row1 = ttk.Frame(aio_frame)\naio_row1.pack(fill=\"x\")\nttk.Label(aio_row1, text=\"Block Size:\").pack(side=\"left\")\nttk.Spinbox(\n    aio_row1,\n    from_=524288,\n    to=4194304,\n    increment=524288,\n    textvariable=self.aio_block_size_var,\n    width=10\n).pack(side=\"left\", padx=5)\nttk.Label(aio_row1, text=\"bytes\").pack(side=\"left\")\n\naio_row2 = ttk.Frame(aio_frame)\naio_row2.pack(fill=\"x\")\nttk.Label(aio_row2, text=\"Queue Depth:\").pack(side=\"left\")\nttk.Spinbox(\n    aio_row2,\n    from_=4,\n    to=32,\n    increment=4,\n    textvariable=self.aio_queue_depth_var,\n    width=10\n).pack(side=\"left\", padx=5)\n\nself.nvme_section = nvme_section\n\ndef toggle_nvme_section(*args):\n    if self.zero_stage_var.get() == \"infinity\":\n        nvme_section.grid(row=6, column=0, columnspan=3, sticky=\"ew\", padx=5, pady=5)\n    else:\n        nvme_section.grid_forget()\n\nself.zero_stage_var.trace_add(\"write\", toggle_nvme_section)\n```\n\n**File**: `src/aios/gui/components/hrm_training_panel/variable_setup.py`\n\nAdd new variables:\n\n```python\n# ZeRO-Infinity NVMe Offloading\nself.nvme_offload_path_var = tk.StringVar(value=\"/tmp/deepspeed_offload\")\nself.nvme_offload_params_var = tk.BooleanVar(value=True)\nself.nvme_offload_optimizer_var = tk.BooleanVar(value=True)\nself.nvme_offload_gradients_var = tk.BooleanVar(value=False)\nself.aio_block_size_var = tk.IntVar(value=1048576)\nself.aio_queue_depth_var = tk.IntVar(value=8)\nself.pin_memory_var = tk.BooleanVar(value=True)\n```\n\n---\n\n### Phase 3: Memory Estimation Updates (Week 3)\n\n#### 3.1 Update VRAM Estimator\n\n**File**: `src/aios/gui/components/hrm_training/memory_estimator/vram_estimation.py`\n\nUpdate `estimate_vram` function to handle ZeRO-Infinity:\n\n```python\ndef estimate_vram(estimator: \"MemoryEstimator\") -> Dict[str, Any]:\n    \"\"\"Estimate VRAM usage accounting for ZeRO-Infinity.\"\"\"\n    \n    # ... existing code ...\n    \n    # Handle ZeRO-Infinity (most aggressive savings)\n    if estimator.zero_stage == \"infinity\":\n        # With Infinity, almost everything can be offloaded\n        # Only keep active computation in VRAM\n        \n        # Model parameters: Only active layers in VRAM (~5-10% of model)\n        model_gb_per_gpu = model_gb * 0.08  # ~8% in VRAM at once\n        \n        # Optimizer: Offloaded to NVMe or CPU\n        optimizer_gb_per_gpu = 0.0\n        \n        # Gradients: Mostly on NVMe/CPU, small buffer in VRAM\n        gradients_gb_per_gpu = gradients_gb * 0.1  # 10% buffer\n        \n        # Activations: Still need these in VRAM (not offloadable during computation)\n        # Keep activations calculation same\n        \n        savings_note = (\n            f\"ZeRO-Infinity: ~92% memory offloaded to NVMe. \"\n            f\"Requires {estimator.nvme_offload_path} with \"\n            f\"~{model_gb * 3:.1f}GB free space.\"\n        )\n    \n    elif estimator.zero_stage == \"zero3\":\n        # ... existing ZeRO-3 logic ...\n    \n    # ... rest of function ...\n```\n\n---\n\n### Phase 4: Testing and Validation (Week 4)\n\n#### 4.1 Unit Tests\n\n**File**: `tests/test_zero_infinity.py` (new)\n\n```python\n\"\"\"Unit tests for ZeRO-Infinity integration.\"\"\"\n\nimport pytest\nimport tempfile\nimport shutil\nfrom pathlib import Path\n\nfrom aios.cli.hrm_hf.nvme_config import (\n    validate_nvme_path,\n    estimate_nvme_space_required,\n    build_infinity_config,\n)\n\n\ndef test_validate_nvme_path():\n    \"\"\"Test NVMe path validation.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Valid path with enough space\n        is_valid, msg = validate_nvme_path(tmpdir, required_space_gb=0.001)\n        assert is_valid\n        assert msg == \"\"\n        \n        # Invalid path (doesn't exist, can't create)\n        is_valid, msg = validate_nvme_path(\"/root/forbidden\", required_space_gb=0.001)\n        assert not is_valid\n        assert \"Permission denied\" in msg or \"Failed to create\" in msg\n\n\ndef test_estimate_nvme_space():\n    \"\"\"Test NVMe space estimation.\"\"\"\n    # 1B params, offload everything\n    space_gb = estimate_nvme_space_required(\n        total_params=1_000_000_000,\n        offload_params=True,\n        offload_optimizer=True,\n        offload_gradients=True,\n        safety_factor=1.5,\n    )\n    \n    # Expected: (4 + 8 + 4) * 1B / 1e9 * 1.5 = 24 GB\n    assert 20 < space_gb < 30\n    \n    # Only optimizer offload\n    space_gb = estimate_nvme_space_required(\n        total_params=1_000_000_000,\n        offload_params=False,\n        offload_optimizer=True,\n        offload_gradients=False,\n        safety_factor=1.0,\n    )\n    \n    # Expected: 8 * 1B / 1e9 = 8 GB\n    assert 7 < space_gb < 9\n\n\ndef test_build_infinity_config():\n    \"\"\"Test Infinity config builder.\"\"\"\n    base_config = {\n        \"train_batch_size\": \"auto\",\n        \"zero_optimization\": {\n            \"stage\": 2,\n        }\n    }\n    \n    with tempfile.TemporaryDirectory() as tmpdir:\n        config = build_infinity_config(\n            base_config=base_config,\n            nvme_path=tmpdir,\n            offload_params=True,\n            offload_optimizer=True,\n            offload_gradients=False,\n            aio_block_size=1048576,\n            aio_queue_depth=8,\n            pin_memory=True,\n            max_in_cpu_gb=1.0,\n        )\n        \n        # Check stage upgraded to 3\n        assert config[\"zero_optimization\"][\"stage\"] == 3\n        \n        # Check param offload to NVMe\n        assert config[\"zero_optimization\"][\"offload_param\"][\"device\"] == \"nvme\"\n        assert config[\"zero_optimization\"][\"offload_param\"][\"nvme_path\"] == tmpdir\n        \n        # Check optimizer offload to NVMe\n        assert config[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] == \"nvme\"\n        \n        # Check AIO settings\n        assert config[\"aio\"][\"block_size\"] == 1048576\n        assert config[\"aio\"][\"queue_depth\"] == 8\n```\n\n#### 4.2 Integration Tests\n\n**File**: `tests/integration/test_infinity_training.py` (new)\n\n```python\n\"\"\"Integration tests for ZeRO-Infinity training.\"\"\"\n\nimport pytest\nimport tempfile\nfrom pathlib import Path\n\nfrom aios.core.hrm_training.training_config import TrainingConfig\n\n\n@pytest.mark.skipif(\n    not Path(\"/dev/nvme0n1\").exists(),\n    reason=\"NVMe device not available\"\n)\ndef test_infinity_training_small_model():\n    \"\"\"Test ZeRO-Infinity on small model to verify setup.\"\"\"\n    \n    with tempfile.TemporaryDirectory() as nvme_dir:\n        config = TrainingConfig(\n            model=\"gpt2\",\n            dataset_file=\"training_data/curated_datasets/test_sample.txt\",\n            steps=2,\n            batch_size=1,\n            zero_stage=\"infinity\",\n            nvme_offload_path=nvme_dir,\n            nvme_offload_params=True,\n            nvme_offload_optimizer=True,\n            nvme_offload_gradients=False,\n            device=\"cuda\",\n        )\n        \n        # Should complete without errors\n        from aios.cli.hrm_hf.train_actv1 import train_actv1_impl\n        train_actv1_impl(config)\n        \n        # Verify NVMe directory was created and used\n        assert Path(nvme_dir).exists()\n        assert len(list(Path(nvme_dir).iterdir())) > 0\n\n\ndef test_infinity_config_fallback_to_cpu():\n    \"\"\"Test fallback to CPU offload when NVMe unavailable.\"\"\"\n    \n    # Use invalid NVMe path\n    config = TrainingConfig(\n        model=\"gpt2\",\n        dataset_file=\"training_data/curated_datasets/test_sample.txt\",\n        steps=1,\n        batch_size=1,\n        zero_stage=\"infinity\",\n        nvme_offload_path=\"/invalid/path/no/permissions\",\n        device=\"cuda\",\n    )\n    \n    # Should fallback to CPU offload gracefully\n    from aios.cli.hrm_hf.train_actv1 import train_actv1_impl\n    # Should complete without crashing\n    train_actv1_impl(config)\n```\n\n#### 4.3 Manual Testing Plan\n\n**Test Case 1: Small Model (1B params) on 2x 11GB GPUs**\n\n```bash\n# Create NVMe offload directory\nmkdir -p /mnt/nvme/deepspeed_offload\n\n# Test with Infinity\naios hrm-hf train-actv1 \\\n  --model gpt2-medium \\\n  --dataset-file training_data/curated_datasets/test_sample.txt \\\n  --steps 10 \\\n  --batch-size 2 \\\n  --zero-stage infinity \\\n  --nvme-offload-path /mnt/nvme/deepspeed_offload \\\n  --nvme-offload-params \\\n  --nvme-offload-optimizer \\\n  --ddp \\\n  --cuda-ids 0,1\n\n# Expected: Training completes, NVMe directory contains offload files\n```\n\n**Test Case 2: Large Model (10B params) on 2x 11GB GPUs**\n\n```bash\n# This would OOM without Infinity\naios hrm-hf train-actv1 \\\n  --hidden-size 4096 \\\n  --h-layers 16 \\\n  --l-layers 16 \\\n  --num-heads 32 \\\n  --expansion 4.0 \\\n  --dataset-file training_data/curated_datasets/test_sample.txt \\\n  --steps 5 \\\n  --batch-size 1 \\\n  --zero-stage infinity \\\n  --nvme-offload-path /mnt/nvme/deepspeed_offload \\\n  --ddp \\\n  --cuda-ids 0,1\n\n# Expected: Training works, uses ~30-50GB NVMe space\n```\n\n**Test Case 3: Performance Comparison**\n\n```bash\n# Baseline: ZeRO-2\ntime aios hrm-hf train-actv1 --zero-stage zero2 [... other args ...]\n\n# With Infinity\ntime aios hrm-hf train-actv1 --zero-stage infinity [... other args ...]\n\n# Expected: Infinity ~10-20% slower with good NVMe\n```\n\n---\n\n### Phase 5: Documentation and Polish (Week 4-5)\n\n#### 5.1 User Documentation\n\n**File**: `docs/guide/zero_infinity_guide.md` (new)\n\nCreate comprehensive user guide covering:\n- What is ZeRO-Infinity and when to use it\n- Hardware requirements (NVMe SSD, PCIe 3.0+)\n- Setup instructions for Linux and Windows\n- Performance tuning (AIO settings, queue depth)\n- Troubleshooting common issues\n- Example configurations for different model sizes\n\n#### 5.2 API Documentation\n\nUpdate docstrings in:\n- `optimization_fields.py`: Document all Infinity-related fields\n- `nvme_config.py`: Comprehensive module documentation\n- `distributed_setup.py`: Infinity initialization flow\n\n#### 5.3 GUI Tooltips\n\nAdd informative tooltips to all Infinity UI elements:\n- NVMe path selector\n- Offload checkboxes  \n- AIO settings\n- Performance warnings\n\n---\n\n## Implementation Checklist\n\n### Phase 1: Configuration \u2705\n- [ ] Create `config/deepspeed_zero_infinity.json`\n- [ ] Create `config/deepspeed_zero_infinity_cpu.json`\n- [ ] Add fields to `optimization_fields.py`\n- [ ] Add CLI arguments to `hrm_hf_cli.py`\n- [ ] Update `zero_stage` help text\n\n### Phase 2: Core Implementation \u2705\n- [ ] Create `src/aios/cli/hrm_hf/nvme_config.py`\n  - [ ] `validate_nvme_path()`\n  - [ ] `estimate_nvme_space_required()`\n  - [ ] `check_nvme_performance()`\n  - [ ] `build_infinity_config()`\n  - [ ] `save_infinity_config()`\n- [ ] Update `distributed_setup.py`\n  - [ ] Infinity detection and config generation\n  - [ ] NVMe validation\n  - [ ] Fallback to CPU offload\n- [ ] Update GUI\n  - [ ] Add \"infinity\" to ZeRO dropdown\n  - [ ] Create NVMe settings section\n  - [ ] Add browse button for NVMe path\n  - [ ] Add offload checkboxes\n  - [ ] Add AIO settings (advanced)\n- [ ] Update `variable_setup.py`\n  - [ ] Add Infinity-related variables\n\n### Phase 3: Memory Estimation \u2705\n- [ ] Update `vram_estimation.py`\n  - [ ] Handle `zero_stage == \"infinity\"`\n  - [ ] Calculate ~92% VRAM savings\n  - [ ] Show NVMe space requirements\n- [ ] Update `memory_estimator/estimator.py`\n  - [ ] Add Infinity mode support\n- [ ] Update GUI VRAM display\n  - [ ] Show \"Offloaded to NVMe\" label\n  - [ ] Display NVMe space needed\n\n### Phase 4: Testing \u2705\n- [ ] Create `tests/test_zero_infinity.py`\n  - [ ] Path validation tests\n  - [ ] Space estimation tests\n  - [ ] Config builder tests\n- [ ] Create `tests/integration/test_infinity_training.py`\n  - [ ] Small model test\n  - [ ] Fallback test\n  - [ ] Large model test (if hardware available)\n- [ ] Manual testing\n  - [ ] 1B model on 2x GPUs\n  - [ ] 10B model on 2x GPUs\n  - [ ] Performance benchmarks\n  - [ ] Windows compatibility (if possible)\n\n### Phase 5: Documentation \u2705\n- [ ] Create `docs/guide/zero_infinity_guide.md`\n- [ ] Update API docstrings\n- [ ] Add GUI tooltips\n- [ ] Update README with Infinity examples\n- [ ] Create troubleshooting guide\n\n---\n\n## Success Metrics\n\n### Functional Requirements \u2705\n1. \u2705 Can train 10B+ parameter models on 2x 11GB GPUs\n2. \u2705 NVMe offloading works with validation and error handling\n3. \u2705 Graceful fallback to CPU offload when NVMe unavailable\n4. \u2705 GUI exposes all Infinity settings\n5. \u2705 CLI supports all Infinity arguments\n\n### Performance Requirements \u2705\n1. \u2705 Infinity mode <20% slower than ZeRO-2 (with fast NVMe)\n2. \u2705 Supports models up to 100B params (with sufficient NVMe space)\n3. \u2705 Memory footprint <2GB VRAM per GPU for 10B model\n4. \u2705 NVMe I/O throughput >2 GB/s (hardware dependent)\n\n### Quality Requirements \u2705\n1. \u2705 Comprehensive error messages for NVMe issues\n2. \u2705 Performance warnings for slow storage\n3. \u2705 Unit test coverage >80% for new code\n4. \u2705 Integration tests for key scenarios\n5. \u2705 Complete user documentation\n\n---\n\n## Risks and Mitigations\n\n### Risk 1: NVMe Performance Varies Widely\n\n**Impact**: Users with slow SSDs may see 50%+ slowdown instead of 10-20%.\n\n**Mitigation**:\n- Detect storage type (HDD vs SSD) at startup\n- Warn users if not on NVMe\n- Provide performance expectations in docs\n- Suggest testing with small models first\n\n### Risk 2: Windows NVMe Support\n\n**Impact**: Windows may have issues with AIO or NVMe detection.\n\n**Mitigation**:\n- Test on Windows with fast SSD\n- Provide CPU fallback automatically\n- Document Windows-specific limitations\n- Consider Windows-optimized AIO settings\n\n### Risk 3: Insufficient NVMe Space\n\n**Impact**: Training fails mid-way when NVMe fills up.\n\n**Mitigation**:\n- Validate space before starting\n- Reserve 20% safety margin\n- Monitor space during training\n- Provide clear error messages\n\n### Risk 4: DeepSpeed Version Compatibility\n\n**Impact**: Older DeepSpeed may not support all Infinity features.\n\n**Mitigation**:\n- Require `deepspeed>=0.8.0` in `pyproject.toml`\n- Check DeepSpeed version at runtime\n- Provide upgrade instructions\n- Test with multiple DeepSpeed versions\n\n### Risk 5: User Confusion\n\n**Impact**: Users may enable Infinity unnecessarily for small models.\n\n**Mitigation**:\n- Clear GUI guidance (\"For 10B+ models only\")\n- Recommend ZeRO-2 as default\n- Auto-suggest Infinity in optimizer tool\n- Show model size vs memory comparison\n\n---\n\n## Future Enhancements\n\n### Phase 6: Advanced Optimizations (Optional)\n\n1. **Adaptive Prefetching**: Predict which layers needed next, prefetch ahead\n2. **Compression**: Compress parameters on NVMe (trade space for speed)\n3. **Multi-tier Caching**: Smart caching of frequently used params\n4. **Bandwidth Monitoring**: Real-time I/O performance tracking\n5. **Auto-tuning**: Automatically adjust AIO settings based on hardware\n\n### Phase 7: Multi-Node Support (Optional)\n\n1. **Distributed NVMe**: Coordinate offloading across multiple nodes\n2. **Network-attached Storage**: Support NFS/SMB for shared offload\n3. **Heterogeneous Hardware**: Mix GPU + CPU + NVMe across nodes\n\n---\n\n## Dependencies\n\n### Required Python Packages\n\n```toml\n# pyproject.toml\n[project.optional-dependencies]\nhf = [\n    \"deepspeed>=0.8.0\",  # Required for ZeRO-Infinity\n    \"psutil>=5.8.0\",     # For disk space checking\n]\n```\n\n### System Requirements\n\n**Linux**:\n- NVMe SSD with PCIe 3.0 or later (2+ GB/s)\n- `libaio` installed (`sudo apt-get install libaio-dev`)\n- Kernel 4.0+ (for modern AIO support)\n\n**Windows** (limited support):\n- NVMe SSD with >1 GB/s write speed\n- May require Windows 10/11 with latest updates\n- Performance may be lower than Linux\n\n### Hardware Requirements\n\n**Minimum**:\n- 1x NVIDIA GPU (11GB+ VRAM)\n- 32GB RAM\n- 256GB+ NVMe SSD (PCIe 3.0)\n\n**Recommended**:\n- 2x NVIDIA GPUs (11GB+ VRAM each)\n- 64GB RAM\n- 1TB+ NVMe SSD (PCIe 4.0, >5 GB/s)\n\n**Optimal**:\n- 4x NVIDIA GPUs (24GB+ VRAM each)\n- 128GB+ RAM\n- 2TB+ NVMe SSD (PCIe 4.0/5.0, >7 GB/s)\n\n---\n\n## Performance Expectations\n\n### Small Model (1B params)\n- **Baseline (no ZeRO)**: 100% speed, 8GB VRAM\n- **ZeRO-2**: 95% speed, 4GB VRAM\n- **ZeRO-Infinity**: 85% speed, 1GB VRAM, 20GB NVMe\n\n### Medium Model (10B params)\n- **Baseline**: OOM on 11GB GPU\n- **ZeRO-2**: OOM on 11GB GPU  \n- **ZeRO-3**: 85% speed, 8GB VRAM (multi-GPU)\n- **ZeRO-Infinity**: 75% speed, 2GB VRAM, 200GB NVMe\n\n### Large Model (100B params)\n- **Baseline**: OOM\n- **ZeRO-2**: OOM\n- **ZeRO-3**: OOM on <8 GPUs\n- **ZeRO-Infinity**: 65% speed, 4GB VRAM per GPU, 2TB NVMe (8x GPUs)\n\n*Percentages relative to baseline training speed.*\n\n---\n\n## Timeline\n\n- **Week 1**: Phase 1 (Configuration) - Complete\n- **Week 2-3**: Phase 2 (Core Implementation) - Complete\n- **Week 3**: Phase 3 (Memory Estimation) - Complete  \n- **Week 4**: Phase 4 (Testing) - Complete\n- **Week 4-5**: Phase 5 (Documentation) - Complete\n- **Total**: ~5 weeks for full integration\n\n---\n\n## Open Questions\n\n1. **Should we support gradient offloading?**\n   - Rarely needed even for 100B models\n   - Adds complexity for minimal benefit\n   - **Recommendation**: Add flag but default to False, document as experimental\n\n2. **How to handle multi-node Infinity?**\n   - Requires shared storage or per-node offload coordination\n   - Complex to implement and test\n   - **Recommendation**: Defer to future enhancement (Phase 7)\n\n3. **Should GUI expose AIO settings?**\n   - Most users won't understand or need to change\n   - Could clutter interface\n   - **Recommendation**: Advanced collapsible section, use sane defaults\n\n4. **Windows support priority?**\n   - Limited testing hardware\n   - May have AIO compatibility issues\n   - **Recommendation**: Best-effort support, Linux-first, document limitations\n\n---\n\n## References\n\n- [DeepSpeed ZeRO-Infinity Paper](https://arxiv.org/abs/2104.07857)\n- [DeepSpeed Documentation](https://www.deepspeed.ai/tutorials/zero-infinity/)\n- [Microsoft Blog: ZeRO-Infinity](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/)\n- [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed)\n\n---\n\n## Conclusion\n\nZeRO-Infinity integration will enable AI-OS to train models 10-100x larger than currently possible, unlocking multi-billion parameter models on consumer hardware. The implementation is well-defined with clear phases, comprehensive testing, and proper fallbacks for edge cases.\n\n**Key Benefits**:\n- Train 10B+ models on 2x 11GB GPUs\n- Minimal slowdown (10-20%) with fast NVMe\n- Graceful degradation to CPU offload\n- Full GUI and CLI support\n- Comprehensive validation and error handling\n\n**Recommendation**: Proceed with implementation. Start with Phase 1 configuration, validate with small models, then scale up to large models. Prioritize Linux support, provide Windows compatibility as best-effort.\n", "tags": ["training"], "headings": [{"line": 0, "text": "DeepSpeed ZeRO-Infinity Integration Plan"}, {"line": 2, "text": "Executive Summary"}, {"line": 18, "text": "Background"}, {"line": 20, "text": "What is ZeRO-Infinity?"}, {"line": 30, "text": "Current AI-OS Capabilities"}, {"line": 53, "text": "Technical Architecture"}, {"line": 55, "text": "Memory Hierarchy with ZeRO-Infinity"}, {"line": 86, "text": "Data Flow During Training"}, {"line": 107, "text": "Implementation Plan"}, {"line": 109, "text": "Phase 1: Configuration and Setup (Week 1)"}, {"line": 111, "text": "1.1 DeepSpeed Configuration Files"}, {"line": 233, "text": "1.2 Training Configuration Fields"}, {"line": 240, "text": "ZeRO-Infinity NVMe Offloading"}, {"line": 352, "text": "1.3 CLI Arguments"}, {"line": 359, "text": "ZeRO-Infinity (NVMe Offloading) options"}, {"line": 409, "text": "Phase 2: Core Implementation (Week 2-3)"}, {"line": 411, "text": "2.1 NVMe Configuration Builder"}, {"line": 439, "text": "Check if path exists or can be created"}, {"line": 447, "text": "Check write permissions"}, {"line": 455, "text": "Check available space"}, {"line": 490, "text": "Parameters: 4 bytes each"}, {"line": 494, "text": "Optimizer states: 8 bytes per param (Adam: momentum + variance)"}, {"line": 498, "text": "Gradients: 4 bytes per param"}, {"line": 501, "text": "Apply safety factor for temp files, fragmentation, etc."}, {"line": 529, "text": "Get device info based on OS"}, {"line": 532, "text": "Try to identify device from path"}, {"line": 539, "text": "Check if rotational (HDD=1, SSD=0)"}, {"line": 557, "text": "On Windows, harder to detect programmatically"}, {"line": 558, "text": "Just warn if path is C: (usually OS drive, may be SATA SSD)"}, {"line": 564, "text": "TODO: Could add actual speed test with dd/fio, but may be too slow for startup"}, {"line": 599, "text": "Ensure we're using ZeRO stage 3"}, {"line": 602, "text": "Configure parameter offloading"}, {"line": 618, "text": "Configure optimizer offloading  "}, {"line": 633, "text": "Configure gradient offloading (rare)"}, {"line": 635, "text": "Note: Gradient offload to NVMe not officially in DeepSpeed API"}, {"line": 636, "text": "This would require custom implementation"}, {"line": 642, "text": "Configure AIO (Async I/O)"}, {"line": 663, "text": "2.2 Update Distributed Setup"}, {"line": 695, "text": "Determine config file"}, {"line": 705, "text": "Calculate model size"}, {"line": 708, "text": "Estimate NVMe space needed"}, {"line": 717, "text": "Validate NVMe path"}, {"line": 729, "text": "Fallback to CPU offload"}, {"line": 732, "text": "Check NVMe performance"}, {"line": 738, "text": "Load base ZeRO-3 config"}, {"line": 743, "text": "Build Infinity config"}, {"line": 756, "text": "Save to temp file"}, {"line": 779, "text": "Existing ZeRO stage logic"}, {"line": 789, "text": "Rest of initialization..."}, {"line": 790, "text": "(existing code continues)"}, {"line": 793, "text": "Handle missing DeepSpeed"}, {"line": 794, "text": "(existing error handling)"}, {"line": 798, "text": "2.3 GUI Integration"}, {"line": 805, "text": "Row 5: DeepSpeed ZeRO (updated with Infinity)"}, {"line": 821, "text": "Dynamic label for memory savings"}, {"line": 840, "text": "Updated tooltip"}, {"line": 855, "text": "Row 6: NVMe Configuration (shown when infinity selected)"}, {"line": 857, "text": "Hidden by default, shown when zero_stage == \"infinity\""}, {"line": 866, "text": "Checkboxes for what to offload"}, {"line": 885, "text": "AIO settings (advanced, maybe collapsible)"}, {"line": 930, "text": "ZeRO-Infinity NVMe Offloading"}, {"line": 942, "text": "Phase 3: Memory Estimation Updates (Week 3)"}, {"line": 944, "text": "3.1 Update VRAM Estimator"}, {"line": 954, "text": "... existing code ..."}, {"line": 956, "text": "Handle ZeRO-Infinity (most aggressive savings)"}, {"line": 958, "text": "With Infinity, almost everything can be offloaded"}, {"line": 959, "text": "Only keep active computation in VRAM"}, {"line": 961, "text": "Model parameters: Only active layers in VRAM (~5-10% of model)"}, {"line": 964, "text": "Optimizer: Offloaded to NVMe or CPU"}, {"line": 967, "text": "Gradients: Mostly on NVMe/CPU, small buffer in VRAM"}, {"line": 970, "text": "Activations: Still need these in VRAM (not offloadable during computation)"}, {"line": 971, "text": "Keep activations calculation same"}, {"line": 980, "text": "... existing ZeRO-3 logic ..."}, {"line": 982, "text": "... rest of function ..."}, {"line": 987, "text": "Phase 4: Testing and Validation (Week 4)"}, {"line": 989, "text": "4.1 Unit Tests"}, {"line": 1011, "text": "Valid path with enough space"}, {"line": 1016, "text": "Invalid path (doesn't exist, can't create)"}, {"line": 1024, "text": "1B params, offload everything"}, {"line": 1033, "text": "Expected: (4 + 8 + 4) * 1B / 1e9 * 1.5 = 24 GB"}, {"line": 1036, "text": "Only optimizer offload"}, {"line": 1045, "text": "Expected: 8 * 1B / 1e9 = 8 GB"}, {"line": 1071, "text": "Check stage upgraded to 3"}, {"line": 1074, "text": "Check param offload to NVMe"}, {"line": 1078, "text": "Check optimizer offload to NVMe"}, {"line": 1081, "text": "Check AIO settings"}, {"line": 1086, "text": "4.2 Integration Tests"}, {"line": 1121, "text": "Should complete without errors"}, {"line": 1125, "text": "Verify NVMe directory was created and used"}, {"line": 1133, "text": "Use invalid NVMe path"}, {"line": 1144, "text": "Should fallback to CPU offload gracefully"}, {"line": 1146, "text": "Should complete without crashing"}, {"line": 1150, "text": "4.3 Manual Testing Plan"}, {"line": 1155, "text": "Create NVMe offload directory"}, {"line": 1158, "text": "Test with Infinity"}, {"line": 1171, "text": "Expected: Training completes, NVMe directory contains offload files"}, {"line": 1177, "text": "This would OOM without Infinity"}, {"line": 1192, "text": "Expected: Training works, uses ~30-50GB NVMe space"}, {"line": 1198, "text": "Baseline: ZeRO-2"}, {"line": 1201, "text": "With Infinity"}, {"line": 1204, "text": "Expected: Infinity ~10-20% slower with good NVMe"}, {"line": 1209, "text": "Phase 5: Documentation and Polish (Week 4-5)"}, {"line": 1211, "text": "5.1 User Documentation"}, {"line": 1223, "text": "5.2 API Documentation"}, {"line": 1230, "text": "5.3 GUI Tooltips"}, {"line": 1240, "text": "Implementation Checklist"}, {"line": 1242, "text": "Phase 1: Configuration \u2705"}, {"line": 1249, "text": "Phase 2: Core Implementation \u2705"}, {"line": 1269, "text": "Phase 3: Memory Estimation \u2705"}, {"line": 1280, "text": "Phase 4: Testing \u2705"}, {"line": 1295, "text": "Phase 5: Documentation \u2705"}, {"line": 1304, "text": "Success Metrics"}, {"line": 1306, "text": "Functional Requirements \u2705"}, {"line": 1313, "text": "Performance Requirements \u2705"}, {"line": 1319, "text": "Quality Requirements \u2705"}, {"line": 1328, "text": "Risks and Mitigations"}, {"line": 1330, "text": "Risk 1: NVMe Performance Varies Widely"}, {"line": 1340, "text": "Risk 2: Windows NVMe Support"}, {"line": 1350, "text": "Risk 3: Insufficient NVMe Space"}, {"line": 1360, "text": "Risk 4: DeepSpeed Version Compatibility"}, {"line": 1370, "text": "Risk 5: User Confusion"}, {"line": 1382, "text": "Future Enhancements"}, {"line": 1384, "text": "Phase 6: Advanced Optimizations (Optional)"}, {"line": 1392, "text": "Phase 7: Multi-Node Support (Optional)"}, {"line": 1400, "text": "Dependencies"}, {"line": 1402, "text": "Required Python Packages"}, {"line": 1405, "text": "pyproject.toml"}, {"line": 1413, "text": "System Requirements"}, {"line": 1425, "text": "Hardware Requirements"}, {"line": 1444, "text": "Performance Expectations"}, {"line": 1446, "text": "Small Model (1B params)"}, {"line": 1451, "text": "Medium Model (10B params)"}, {"line": 1457, "text": "Large Model (100B params)"}, {"line": 1467, "text": "Timeline"}, {"line": 1478, "text": "Open Questions"}, {"line": 1502, "text": "References"}, {"line": 1511, "text": "Conclusion"}]}, {"path": "planned_features/DRAGON.md", "content": "# DRAGON: Distributed Routing and GPU Open Network\n> Note: Any references to `docs/features/*` are placeholders. Use `docs/INDEX.md` and `docs/guide/` for current documentation.\n\n**Complete Implementation Guide**\n\n**Status**: \ud83d\udfe1 Planning Phase  \n**Date**: October 18, 2025  \n**Priority**: High  \n**Complexity**: Very High  \n\n---\n\n## Table of Contents\n\n1. [Executive Summary](#executive-summary)\n2. [Core Concept](#core-concept)\n3. [System Architecture](#system-architecture)\n4. [Technical Components](#technical-components)\n5. [User Experience](#user-experience)\n6. [Backend Infrastructure](#backend-infrastructure)\n7. [Security & Privacy](#security--privacy)\n8. [Implementation Phases](#implementation-phases)\n9. [Implementation Checklist](#implementation-checklist)\n10. [Technical Quick Reference](#technical-quick-reference)\n11. [Testing & Deployment](#testing--deployment)\n12. [Success Metrics](#success-metrics)\n\n---\n\n## Executive Summary\n\n### What is DRAGON?\n\nDRAGON is a crowd-sourced distributed training system that enables AI-OS users to donate their GPU/CPU time toward training a massive collaborative HRM-sMoE model. Users can participate with a simple one-click interface that automatically downloads training data batches, trains on locally allocated resources, and uploads results to a central aggregation server.\n\n**Vision**: Transform AI-OS into a decentralized AI training network where thousands of volunteers collectively train state-of-the-art models.\n\n### Key Value Propositions\n\n1. **Democratic AI Training**: Anyone with a GPU can contribute to cutting-edge AI research\n2. **Cost Efficiency**: Distribute training costs across community (200x cheaper than cloud)\n3. **Accessibility**: One-click interface, no technical expertise required\n4. **Community Building**: Gamification, leaderboards, and shared success\n5. **Innovation**: Enable training of models too large for individual users\n\n---\n\n## Core Concept\n\n### Federated Learning Flow\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    DRAGON Aggregation Server                 \u2502\n\u2502                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Model Registry \u2502  \u2502 Gradient Queue  \u2502  \u2502 Aggregator   \u2502 \u2502\n\u2502  \u2502 (current model)\u2502  \u2502 (worker updates)\u2502  \u2502 (FedAvg/Adam)\u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502            \u2502                  \u25b2                    \u2502         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502                  \u2502                    \u2502\n             \u25bc                  \u2502                    \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502   Worker A       \u2502  \u2502   Worker B       \u2502  \u2502   Worker C       \u2502\n   \u2502  (RTX 4090)      \u2502  \u2502  (RTX 3080)      \u2502  \u2502  (GTX 1080 Ti)   \u2502\n   \u2502                  \u2502  \u2502                  \u2502  \u2502                  \u2502\n   \u2502  1. Download     \u2502  \u2502  1. Download     \u2502  \u2502  1. Download     \u2502\n   \u2502     model        \u2502  \u2502     model        \u2502  \u2502     model        \u2502\n   \u2502  2. Fetch batch  \u2502  \u2502  2. Fetch batch  \u2502  \u2502  2. Fetch batch  \u2502\n   \u2502  3. Train local  \u2502  \u2502  3. Train local  \u2502  \u2502  3. Train local  \u2502\n   \u2502  4. Upload grads \u2502  \u2502  4. Upload grads \u2502  \u2502  4. Upload grads \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### How It Works\n\n1. **Distributing Work**: Breaking training into small, manageable batches\n2. **Volunteer Computing**: Users donate idle GPU/CPU time\n3. **Gradient Aggregation**: Central server combines updates from all workers\n4. **Simple UX**: One-button start/stop interface\n5. **Resource Aware**: Respects user-defined GPU/CPU limits from Resources tab\n\n---\n\n## System Architecture\n\n### Architecture: Centralized Aggregation Server (Recommended)\n\n**Pros**:\n- Simple to implement and debug\n- Full control over aggregation algorithm\n- Easy to monitor training progress\n- Standard federated learning approach\n\n**Cons**:\n- Single point of failure\n- Server bandwidth requirements\n- Hosting costs\n\n**Components**:\n\n1. **DRAGON Server** (Python/FastAPI)\n   - Model registry (current global model)\n   - Batch distribution queue\n   - Gradient aggregation service\n   - Worker authentication and stats\n   - Progress tracking\n\n2. **DRAGON Client** (AI-OS GUI Tab)\n   - Download model checkpoint\n   - Request training batch\n   - Local training loop\n   - Gradient computation and upload\n   - Automatic retry logic\n\n3. **Communication Protocol** (REST API + WebSockets)\n   - REST: Model downloads, batch requests, gradient uploads\n   - WebSocket: Real-time status updates, heartbeat\n\n---\n\n## Technical Components\n\n### 1. DRAGON GUI Tab\n\n**Location**: `src/aios/gui/components/dragon_panel.py`\n\n**UI Layout**:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        DRAGON Network                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                \u2502\n\u2502  Status: \u25cfIDLE / \u25cfTRAINING / \u25cfUPLOADING                       \u2502\n\u2502                                                                \u2502\n\u2502  Global Model: HRM-sMoE-125M (v1.2.3)                         \u2502\n\u2502  Your Contribution: 1,234 batches (12.3M tokens)              \u2502\n\u2502  Network Stats: 523 active workers, 45.2B tokens processed    \u2502\n\u2502                                                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                Resource Allocation                        \u2502 \u2502\n\u2502  \u2502                                                           \u2502 \u2502\n\u2502  \u2502  Use settings from Resources tab: \u2611                      \u2502 \u2502\n\u2502  \u2502  Override GPU memory limit: [ 80% ]                      \u2502 \u2502\n\u2502  \u2502  Max batch size: [ 4 ]                                   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                Training Progress                          \u2502 \u2502\n\u2502  \u2502                                                           \u2502 \u2502\n\u2502  \u2502  Current batch: 15/100 (15%)                             \u2502 \u2502\n\u2502  \u2502  [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]                         \u2502 \u2502\n\u2502  \u2502                                                           \u2502 \u2502\n\u2502  \u2502  Tokens processed: 15,234                                \u2502 \u2502\n\u2502  \u2502  Loss: 2.456                                             \u2502 \u2502\n\u2502  \u2502  Speed: 1,245 tok/s                                      \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                \u2502\n\u2502       [  START TRAINING  ]        [  STOP  ]                  \u2502\n\u2502                                                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                Activity Log                               \u2502 \u2502\n\u2502  \u2502                                                           \u2502 \u2502\n\u2502  \u2502  [12:34:56] Connected to DRAGON server                   \u2502 \u2502\n\u2502  \u2502  [12:35:01] Downloaded model v1.2.3 (125M params)        \u2502 \u2502\n\u2502  \u2502  [12:35:05] Fetched batch #1234 (1000 samples)           \u2502 \u2502\n\u2502  \u2502  [12:35:45] Training complete (loss: 2.456)              \u2502 \u2502\n\u2502  \u2502  [12:35:48] Uploaded gradients (2.3 MB)                  \u2502 \u2502\n\u2502  \u2502  [12:35:50] Contribution recorded \u2713                      \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### 2. DRAGON Client Module\n\n**Location**: `src/aios/dragon/client.py`\n\n**Key Methods**:\n\n```python\nclass DRAGONClient:\n    def __init__(self, server_url: str, api_key: str):\n        \"\"\"Initialize connection to DRAGON server.\"\"\"\n        \n    async def connect(self) -> bool:\n        \"\"\"Authenticate with server, verify connection.\"\"\"\n        \n    async def download_model(self, version: str) -> Path:\n        \"\"\"Download global model checkpoint to cache.\"\"\"\n        \n    async def fetch_batch(self) -> Dict:\n        \"\"\"Request next training batch from server.\"\"\"\n        \n    async def train_batch(self, batch: Dict) -> Dict:\n        \"\"\"Train on batch, compute gradients.\"\"\"\n        \n    async def upload_gradients(self, gradients: Dict, metadata: Dict):\n        \"\"\"Upload computed gradients to server.\"\"\"\n        \n    async def start_training_loop(self):\n        \"\"\"Main training loop: fetch \u2192 train \u2192 upload.\"\"\"\n```\n\n### 3. DRAGON Server (Backend)\n\n**Technology Stack**:\n- **FastAPI** - REST API framework\n- **PostgreSQL** - Database for workers, batches, model versions\n- **Redis** - Queue for batch distribution, gradient aggregation\n- **S3/MinIO** - Object storage for model checkpoints\n- **Docker** - Containerized deployment\n\n**API Endpoints**:\n\n```\nPOST   /api/v1/auth/register          # Register new worker\nPOST   /api/v1/auth/login             # Authenticate worker\nGET    /api/v1/model/current          # Get current global model version\nGET    /api/v1/model/download/{ver}   # Download model checkpoint\nPOST   /api/v1/batch/request          # Request next training batch\nPOST   /api/v1/gradients/upload       # Upload computed gradients\nGET    /api/v1/stats/global           # Get network statistics\nGET    /api/v1/worker/stats           # Get worker's contribution stats\nWS     /api/v1/ws/status              # WebSocket for real-time updates\n```\n\n**Database Schema**:\n\n```sql\n-- Workers table\nCREATE TABLE workers (\n    id UUID PRIMARY KEY,\n    api_key VARCHAR(64) UNIQUE NOT NULL,\n    username VARCHAR(64),\n    hardware_info JSONB,\n    total_batches INT DEFAULT 0,\n    total_tokens BIGINT DEFAULT 0,\n    reputation_score FLOAT DEFAULT 1.0,\n    created_at TIMESTAMP DEFAULT NOW(),\n    last_seen TIMESTAMP\n);\n\n-- Model versions table\nCREATE TABLE model_versions (\n    version VARCHAR(16) PRIMARY KEY,\n    checkpoint_url TEXT NOT NULL,\n    num_parameters BIGINT,\n    created_at TIMESTAMP DEFAULT NOW(),\n    metrics JSONB\n);\n\n-- Training batches table\nCREATE TABLE training_batches (\n    batch_id UUID PRIMARY KEY,\n    dataset_name VARCHAR(128),\n    data_url TEXT NOT NULL,\n    num_samples INT,\n    status VARCHAR(32),  -- 'pending', 'assigned', 'completed', 'failed'\n    assigned_to UUID REFERENCES workers(id),\n    assigned_at TIMESTAMP,\n    completed_at TIMESTAMP,\n    result_hash VARCHAR(64)\n);\n\n-- Gradient updates table\nCREATE TABLE gradient_updates (\n    update_id UUID PRIMARY KEY,\n    worker_id UUID REFERENCES workers(id),\n    batch_id UUID REFERENCES training_batches(batch_id),\n    model_version VARCHAR(16),\n    gradient_url TEXT NOT NULL,\n    loss FLOAT,\n    num_samples INT,\n    uploaded_at TIMESTAMP DEFAULT NOW(),\n    aggregated BOOLEAN DEFAULT FALSE\n);\n```\n\n### 4. Aggregation Algorithm\n\n**FedAvg (Federated Averaging)**:\n\n```python\ndef aggregate_gradients(gradient_updates: List[GradientUpdate]) -> ModelUpdate:\n    \"\"\"\n    Aggregate gradients from multiple workers using weighted averaging.\n    \n    Args:\n        gradient_updates: List of gradient updates from workers\n        \n    Returns:\n        Aggregated model update\n    \"\"\"\n    # Calculate total samples across all workers\n    total_samples = sum(update.num_samples for update in gradient_updates)\n    \n    # Initialize aggregated gradients dictionary\n    aggregated_gradients = {}\n    \n    # For each parameter in the model\n    for param_name in gradient_updates[0].gradients.keys():\n        weighted_sum = 0\n        \n        # Weighted average based on number of samples\n        for update in gradient_updates:\n            weight = update.num_samples / total_samples\n            weighted_sum += weight * update.gradients[param_name]\n        \n        aggregated_gradients[param_name] = weighted_sum\n    \n    return ModelUpdate(gradients=aggregated_gradients)\n```\n\n---\n\n## User Experience\n\n### User Journey: First-Time Contributor\n\n1. **Discovery**:\n   - User opens AI-OS GUI\n   - Sees new \"DRAGON\" tab with \"Contribute to AI Training\"\n   - Clicks to explore\n\n2. **Registration** (One-time):\n   - Click \"Join DRAGON Network\"\n   - Enter optional username\n   - System generates API key automatically\n   - Connection test \u2192 Success \u2713\n\n3. **Training Session**:\n   - Click \"START TRAINING\"\n   - Progress bar shows batch download\n   - Training begins (see real-time loss, speed)\n   - Activity log shows each step\n   - User can minimize or use other tabs\n\n4. **Stopping**:\n   - User clicks \"STOP\"\n   - Current batch finishes\n   - Gradients uploaded\n   - Model unloaded from memory\n\n5. **Viewing Contribution**:\n   - \"Your Contribution\" section shows stats\n   - Leaderboard shows rank\n   - Badges for milestones (1K, 10K, 100K batches)\n\n### Resource Management Integration\n\n**Automatic Mode** (default):\n- Reads GPU/CPU settings from Resources tab\n- Respects memory limits\n- Uses configured GPUs only\n\n**Override Mode**:\n- User can set DRAGON-specific limits\n- Example: \"Use 80% GPU for regular tasks, 60% for DRAGON\"\n\n---\n\n## Backend Infrastructure\n\n### Deployment Architecture\n\n**Recommended Setup**:\n\n1. **Application Server** (FastAPI + Gunicorn)\n   - 4 CPU cores, 8GB RAM\n   - Handles API requests\n   - Load balancer for high availability\n\n2. **Database** (PostgreSQL)\n   - 2 CPU cores, 4GB RAM\n   - Persistent storage for metadata\n   - Read replicas for analytics\n\n3. **Message Queue** (Redis)\n   - 2 CPU cores, 4GB RAM\n   - Persistent storage for work queue\n\n4. **Object Storage** (S3/MinIO)\n   - Store model checkpoints\n   - Store gradient files\n   - CDN for fast downloads\n\n5. **Monitoring**:\n   - Prometheus + Grafana for metrics\n   - Alerting for failures\n   - CloudWatch/similar for logs\n\n**Estimated Costs** (AWS):\n- Development: ~$50/month\n- Production (100 workers): ~$200/month\n- Production (1000 workers): ~$500/month\n\n### Data Distribution Strategy\n\n**Hybrid Approach** (Recommended):\n- Pre-chunk common sizes (256, 512, 1024 samples)\n- Generate custom sizes on-demand\n- Cache in Redis for fast distribution\n- Store in S3 for durability\n\n---\n\n## Security & Privacy\n\n### Critical Security Measures\n\n1. **No Code Execution on Workers**\n   - Workers only receive model weights (safetensors format)\n   - No pickle, eval(), or dynamic code\n   - No eval() or pickle usage\n\n2. **Gradient Validation**\n   - Server checks gradient shapes match model architecture\n   - Outlier detection (reject if > 3\u03c3 from mean)\n   - Rate limiting to prevent poisoning attacks\n\n3. **Authentication**\n   - API keys generated client-side\n   - Rate limiting per worker\n   - Optional OAuth for verified contributors\n\n4. **Byzantine Fault Tolerance**\n   - Reputation scoring (workers build trust over time)\n   - Krum aggregation (reject outliers)\n   - Random batch verification (server trains same batch to validate)\n\n5. **Data Privacy**\n   - Training data is public domain (no PII)\n   - No user data uploaded\n   - Differential privacy (future enhancement)\n\n### Attack Vectors & Mitigations\n\n| Attack | Mitigation |\n|--------|------------|\n| **Gradient Poisoning** | Outlier detection, Krum aggregation, reputation scores |\n| **Model Extraction** | Rate limiting, partial model access (MoE experts only) |\n| **Sybil Attack** | CAPTCHA on registration, proof-of-work, rate limits |\n| **DDoS** | Cloudflare, rate limiting, worker throttling |\n| **Data Poisoning** | Curated public datasets only, no user-uploaded data |\n\n---\n\n## Implementation Phases\n\n### Phase 1: MVP (1-2 months)\n\n**Goal**: Proof of concept with basic functionality\n\n**Features**:\n- Simple DRAGON tab in GUI\n- FastAPI server with basic endpoints\n- Single global model (no versioning)\n- FedAvg aggregation\n- Manual start/stop only\n- Fixed batch size (512 samples)\n- 10-50 test workers\n\n**Deliverables**:\n- `src/aios/dragon/client.py`\n- `src/aios/gui/components/dragon_panel.py`\n- `backend/dragon_server/` (FastAPI app)\n- Docker compose for local testing\n- Documentation: setup guide, API reference\n\n### Phase 2: Production Ready (2-3 months)\n\n**Features**:\n- Model versioning and rollback\n- Dynamic batch sizing\n- Automatic retry logic\n- Worker reputation system\n- Real-time monitoring dashboard\n- Progress persistence (resume after restart)\n- 100-500 workers\n\n**Enhancements**:\n- WebSocket for live updates\n- Gradient compression (reduce upload size)\n- Checkpoint caching (avoid re-downloads)\n- Advanced aggregation (FedAdam)\n\n### Phase 3: Scale & Optimize (3-6 months)\n\n**Features**:\n- Auto-scaling infrastructure\n- 1000+ concurrent workers\n- Expert-level distribution (different workers train different MoE experts)\n- Multi-model support (users choose which model to contribute to)\n- Contribution leaderboard and gamification\n- Mobile app for cross-platform training\n\n---\n\n## Implementation Checklist\n\n### Pre-Implementation Setup\n\n**Documentation**:\n- [x] Complete architectural plan\n- [x] Technical quick reference\n- [ ] API specification (OpenAPI/Swagger)\n- [ ] Database schema SQL scripts\n- [ ] Frontend mockups (wireframes)\n\n**Research**:\n- [ ] Review FedAvg paper (McMahan et al., 2017)\n- [ ] Review FedAdam paper (Reddi et al., 2020)\n- [ ] Benchmark gradient compression methods\n- [ ] Survey existing platforms (Flower, PySyft)\n\n**Environment Setup**:\n- [ ] Set up development server (local Docker)\n- [ ] Configure PostgreSQL database\n- [ ] Configure Redis message queue\n- [ ] Set up S3/MinIO for object storage\n- [ ] Configure monitoring (Prometheus + Grafana)\n\n### Backend Development\n\n**Core Infrastructure**:\n- [ ] Initialize FastAPI project structure\n- [ ] Create database migration system (Alembic)\n- [ ] Implement database tables\n- [ ] Add indexes for performance\n- [ ] Seed initial data\n\n**API Endpoints**:\n- [ ] `POST /api/v1/auth/register` - Worker registration\n- [ ] `POST /api/v1/auth/login` - Authentication\n- [ ] `GET /api/v1/model/current` - Get latest model\n- [ ] `GET /api/v1/model/download/{ver}` - Download checkpoint\n- [ ] `POST /api/v1/batch/request` - Request training batch\n- [ ] `POST /api/v1/gradients/upload` - Upload gradients\n- [ ] `GET /api/v1/stats/global` - Network statistics\n- [ ] `GET /api/v1/worker/stats` - Worker stats\n\n**Aggregation System**:\n- [ ] Implement FedAvg algorithm\n- [ ] Aggregation scheduler (every 60s or 10 updates)\n- [ ] Model version management\n- [ ] Gradient queue management\n\n**Testing**:\n- [ ] Unit tests for API endpoints\n- [ ] Integration tests for aggregation\n- [ ] Load tests (100 concurrent requests)\n\n### Client Development\n\n**Core Module**:\n- [ ] Create `src/aios/dragon/` package\n- [ ] Implement `DRAGONClient` class\n- [ ] `connect()` - authenticate with server\n- [ ] `download_model()` - fetch checkpoint\n- [ ] `fetch_batch()` - request training data\n- [ ] `train_batch()` - local training loop\n- [ ] `upload_gradients()` - send results\n- [ ] `start_training_loop()` - main loop\n\n**Training Integration**:\n- [ ] Adapt `train_epoch.py` for single-batch training\n- [ ] Gradient extraction and serialization\n- [ ] Checkpoint management\n\n**Resource Integration**:\n- [ ] Read GPU limits from Resources panel\n- [ ] Auto-detect batch size\n- [ ] Error handling & retry logic\n\n**Testing**:\n- [ ] Unit tests for `DRAGONClient`\n- [ ] Mock server for integration tests\n- [ ] End-to-end test\n\n### GUI Development\n\n**DRAGON Panel**:\n- [ ] Create `src/aios/gui/components/dragon_panel.py`\n- [ ] Implement `DRAGONPanel(ttk.Frame)`\n- [ ] Status indicator widget\n- [ ] Progress section\n- [ ] Stats section\n- [ ] Controls section\n\n**Integration**:\n- [ ] Add DRAGON tab to main app\n- [ ] Initialize DRAGON panel\n- [ ] Event handlers (start, stop, update)\n\n**Testing**:\n- [ ] Manual testing (visual inspection)\n- [ ] Start/stop functionality\n- [ ] Progress updates accuracy\n\n### Integration Testing\n\n- [ ] Test with 1 worker (local)\n- [ ] Test with 2 workers (different batches)\n- [ ] Test with 5 workers (concurrent)\n- [ ] Worker disconnect during training\n- [ ] Server restart during aggregation\n- [ ] Network interruption handling\n\n### Deployment Preparation\n\n- [ ] Create Docker configuration\n- [ ] Set up reverse proxy (nginx)\n- [ ] Configure SSL certificates\n- [ ] Set up database backups\n- [ ] Configure logging\n- [ ] Set up error tracking\n\n---\n\n## Technical Quick Reference\n\n### Core Files\n\n| Component | File | Purpose |\n|-----------|------|---------|\n| GUI Tab | `src/aios/gui/components/dragon_panel.py` | User interface |\n| Client | `src/aios/dragon/client.py` | Server communication |\n| Server | `backend/dragon_server/main.py` | API endpoints |\n| Database | `backend/dragon_server/models.py` | Schema definitions |\n| Aggregation | `backend/dragon_server/aggregator.py` | FedAvg implementation |\n\n### Data Flow\n\n```\n1. Worker clicks \"START TRAINING\"\n2. Client connects to server (auth)\n3. Download current global model checkpoint\n4. Request training batch from work queue\n5. Train locally (compute gradients)\n6. Upload gradients to server\n7. Server aggregates using FedAvg\n8. Repeat steps 4-7 until user clicks \"STOP\"\n```\n\n### API Request/Response Examples\n\n**Request Batch**:\n```json\nPOST /api/v1/batch/request\n{\n  \"worker_id\": \"uuid\",\n  \"hardware\": {\n    \"gpu\": \"RTX 4090\",\n    \"vram_gb\": 24,\n    \"batch_size\": 8\n  }\n}\n\nResponse:\n{\n  \"batch_id\": \"uuid\",\n  \"model_version\": \"v1.2.3\",\n  \"data_url\": \"https://s3.../batch.jsonl.gz\",\n  \"num_samples\": 1000\n}\n```\n\n**Upload Gradients**:\n```json\nPOST /api/v1/gradients/upload\n{\n  \"worker_id\": \"uuid\",\n  \"batch_id\": \"uuid\",\n  \"model_version\": \"v1.2.3\",\n  \"gradients_url\": \"https://s3.../grads.safetensors\",\n  \"loss\": 2.456,\n  \"num_samples\": 1000\n}\n```\n\n### Configuration\n\n**Client Config** (`config/dragon.yaml`):\n```yaml\nserver:\n  url: \"https://dragon.aios.ai\"\n  api_key: \"generated_on_first_run\"\n\ntraining:\n  batch_size: auto\n  max_batches_per_session: 100\n  retry_attempts: 3\n  timeout_seconds: 300\n\nuploads:\n  compress_gradients: true\n  compression_ratio: 0.1\n```\n\n**Server Config** (`.env`):\n```env\nDATABASE_URL=postgresql://user:pass@localhost/dragon\nREDIS_URL=redis://localhost:6379\nS3_BUCKET=dragon-checkpoints\nAGGREGATION_INTERVAL=60\nMIN_GRADIENTS_FOR_UPDATE=10\n```\n\n---\n\n## Testing & Deployment\n\n### Testing Strategy\n\n**Unit Tests**:\n- Client methods (`test_dragon_client.py`)\n- Aggregation correctness (`test_aggregation.py`)\n- Security validation (`test_security.py`)\n\n**Integration Tests**:\n- 2-worker local test\n- Server aggregation\n- Fault tolerance\n\n**Load Tests**:\n- 100 concurrent workers\n- 1000 requests/second\n- 10 GB/hour gradient uploads\n\n### Monitoring\n\n**Metrics to Track**:\n- Active workers (gauge)\n- Batches trained per hour (counter)\n- Average loss per model version (gauge)\n- Gradient upload success rate (%)\n- Server response time (p50, p95, p99)\n\n**Alerts**:\n- Worker count drops by >50%\n- Average loss increases by >10%\n- Server error rate >1%\n- Gradient upload failures >5%\n\n### Deployment\n\n**Development** (Docker Compose):\n```bash\ncd backend/dragon_server\ndocker-compose up -d\n```\n\n**Production** (Kubernetes):\n```bash\nkubectl apply -f k8s/dragon-server.yaml\n```\n\n---\n\n## Success Metrics\n\n### Phase 1 MVP Success\n\n- [ ] 10+ workers successfully training\n- [ ] Model converging (loss decreasing)\n- [ ] Zero critical bugs\n- [ ] Average uptime >95%\n\n### Phase 2 Production Success\n\n- [ ] 100+ active workers\n- [ ] 1M+ tokens processed per day\n- [ ] Worker retention >30%\n- [ ] Infrastructure cost <$200/month\n\n### Long-term Impact Metrics\n\n- **Community Growth**: 1000+ active contributors\n- **Training Efficiency**: 10B+ tokens/month\n- **Cost Savings**: $10K+ equivalent compute donated\n- **Model Quality**: Competitive with commercial models\n- **Innovation**: 5+ research papers using DRAGON\n\n---\n\n## FAQ\n\n**Q: How long does training one batch take?**  \nA: ~2 minutes on RTX 3090 (1000 samples, batch_size=4)\n\n**Q: How much bandwidth is needed?**  \nA: ~660 MB/hour (model cached, 12 batches/hour \u00d7 55 MB)\n\n**Q: What if my internet disconnects?**  \nA: Retry logic auto-reconnects. Current batch is lost, but resume from next batch.\n\n**Q: Can I pause and resume later?**  \nA: Yes! Click STOP, your progress is saved. Click START to continue.\n\n**Q: Do I get credit for my contribution?**  \nA: Yes! Leaderboard tracks your batches and reputation score.\n\n**Q: Is my data private?**  \nA: Yes. Only gradients are uploaded, no personal data leaves your machine.\n\n**Q: What hardware do I need?**  \nA: Any NVIDIA GPU with 4GB+ VRAM. CPU-only mode supported but slower.\n\n---\n\n## Next Steps\n\n### Immediate Actions (Week 1)\n\n1. **Stakeholder Review**: Present this plan to AI-OS maintainers\n2. **Environment Setup**: Configure development server (Docker)\n3. **Team Assembly**: Assign roles (backend, client, GUI, testing)\n\n### Short-term Goals (Month 1)\n\n1. **Backend MVP**: FastAPI server with core endpoints\n2. **Client MVP**: Basic training loop working\n3. **GUI MVP**: Simple tab with start/stop buttons\n4. **Testing**: End-to-end test with 2 workers\n\n### Medium-term Goals (Months 2-3)\n\n1. **Alpha Testing**: 20-50 internal users\n2. **Production Hardening**: Error handling, monitoring, security\n3. **Documentation**: User guide, API docs, troubleshooting\n4. **Beta Launch**: Public announcement, community onboarding\n\n---\n\n## References\n\n### Papers\n- McMahan et al., \"Communication-Efficient Learning of Deep Networks from Decentralized Data\" (FedAvg)\n- Reddi et al., \"Adaptive Federated Optimization\" (FedAdam)\n- Blanchard et al., \"Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent\" (Krum)\n\n### Existing Platforms\n- **Flower**: Federated learning framework\n- **PySyft**: Privacy-preserving ML\n- **BOINC**: Volunteer computing platform\n- **Folding@home**: Distributed protein folding\n\n### AI-OS Integration Points\n- Streaming Datasets: (placeholder) `docs/DATASET_STREAM_QUEUE.md`\n- Checkpoint System: (placeholder) `docs/features/AUTOMATIC_CHECKPOINT_SAVING.md`\n- Resource Management: (placeholder) `docs/fixes/RESOURCES_TAB_SETTINGS_PERSISTENCE_FIX.md`\n- HRM-sMoE: (placeholder) `docs/HRM_MOE_SUMMARY.md`\n\nNote: The above items are planning placeholders for docs that are not yet created or published. For current docs, start at `docs/INDEX.md` and explore `docs/guide/`.\n\n---\n\n**Document Version**: 1.0  \n**Last Updated**: October 18, 2025  \n**Status**: \ud83d\udfe2 Ready for Implementation  \n\n---\n\n## Appendix: Code Examples\n\n### Example: Client Training Loop\n\n```python\nasync def training_loop(self):\n    \"\"\"Main DRAGON training loop.\"\"\"\n    while not self.stop_flag:\n        try:\n            # 1. Fetch batch\n            batch = await self.fetch_batch()\n            self.update_status(\"TRAINING\")\n            \n            # 2. Train locally\n            gradients, loss = await self.train_batch(batch)\n            \n            # 3. Upload results\n            self.update_status(\"UPLOADING\")\n            await self.upload_gradients(gradients, {\n                \"batch_id\": batch[\"batch_id\"],\n                \"loss\": loss,\n                \"num_samples\": len(batch[\"data\"])\n            })\n            \n            # 4. Update stats\n            self.total_batches += 1\n            self.update_progress()\n            \n        except Exception as e:\n            self.log_error(f\"Training failed: {e}\")\n            await asyncio.sleep(30)  # Retry after 30s\n    \n    self.update_status(\"IDLE\")\n```\n\n### Example: Server Aggregation\n\n```python\n@app.post(\"/api/v1/aggregate\")\nasync def trigger_aggregation():\n    \"\"\"Aggregate pending gradients and update global model.\"\"\"\n    \n    # Fetch pending gradients\n    updates = await db.fetch_pending_gradients(limit=100)\n    \n    if len(updates) < MIN_GRADIENTS_FOR_UPDATE:\n        return {\"status\": \"waiting\", \"pending\": len(updates)}\n    \n    # Aggregate using FedAvg\n    aggregated = aggregate_gradients(updates)\n    \n    # Update global model\n    current_model = await load_model()\n    current_model.apply_gradients(aggregated)\n    \n    # Save new version\n    new_version = increment_version()\n    checkpoint_url = await save_checkpoint(current_model, new_version)\n    \n    # Mark gradients as aggregated\n    await db.mark_aggregated(updates)\n    \n    return {\n        \"status\": \"success\",\n        \"version\": new_version,\n        \"num_updates\": len(updates)\n    }\n```\n\n---\n\n**End of Document**\n", "tags": ["cli", "experts", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "DRAGON: Distributed Routing and GPU Open Network"}, {"line": 12, "text": "Table of Contents"}, {"line": 29, "text": "Executive Summary"}, {"line": 31, "text": "What is DRAGON?"}, {"line": 37, "text": "Key Value Propositions"}, {"line": 47, "text": "Core Concept"}, {"line": 49, "text": "Federated Learning Flow"}, {"line": 75, "text": "How It Works"}, {"line": 85, "text": "System Architecture"}, {"line": 87, "text": "Architecture: Centralized Aggregation Server (Recommended)"}, {"line": 122, "text": "Technical Components"}, {"line": 124, "text": "1. DRAGON GUI Tab"}, {"line": 175, "text": "2. DRAGON Client Module"}, {"line": 205, "text": "3. DRAGON Server (Backend)"}, {"line": 280, "text": "4. Aggregation Algorithm"}, {"line": 295, "text": "Calculate total samples across all workers"}, {"line": 298, "text": "Initialize aggregated gradients dictionary"}, {"line": 301, "text": "For each parameter in the model"}, {"line": 305, "text": "Weighted average based on number of samples"}, {"line": 317, "text": "User Experience"}, {"line": 319, "text": "User Journey: First-Time Contributor"}, {"line": 350, "text": "Resource Management Integration"}, {"line": 363, "text": "Backend Infrastructure"}, {"line": 365, "text": "Deployment Architecture"}, {"line": 398, "text": "Data Distribution Strategy"}, {"line": 408, "text": "Security & Privacy"}, {"line": 410, "text": "Critical Security Measures"}, {"line": 437, "text": "Attack Vectors & Mitigations"}, {"line": 449, "text": "Implementation Phases"}, {"line": 451, "text": "Phase 1: MVP (1-2 months)"}, {"line": 471, "text": "Phase 2: Production Ready (2-3 months)"}, {"line": 488, "text": "Phase 3: Scale & Optimize (3-6 months)"}, {"line": 500, "text": "Implementation Checklist"}, {"line": 502, "text": "Pre-Implementation Setup"}, {"line": 524, "text": "Backend Development"}, {"line": 554, "text": "Client Development"}, {"line": 581, "text": "GUI Development"}, {"line": 601, "text": "Integration Testing"}, {"line": 610, "text": "Deployment Preparation"}, {"line": 621, "text": "Technical Quick Reference"}, {"line": 623, "text": "Core Files"}, {"line": 633, "text": "Data Flow"}, {"line": 646, "text": "API Request/Response Examples"}, {"line": 682, "text": "Configuration"}, {"line": 712, "text": "Testing & Deployment"}, {"line": 714, "text": "Testing Strategy"}, {"line": 731, "text": "Monitoring"}, {"line": 746, "text": "Deployment"}, {"line": 761, "text": "Success Metrics"}, {"line": 763, "text": "Phase 1 MVP Success"}, {"line": 770, "text": "Phase 2 Production Success"}, {"line": 777, "text": "Long-term Impact Metrics"}, {"line": 787, "text": "FAQ"}, {"line": 812, "text": "Next Steps"}, {"line": 814, "text": "Immediate Actions (Week 1)"}, {"line": 820, "text": "Short-term Goals (Month 1)"}, {"line": 827, "text": "Medium-term Goals (Months 2-3)"}, {"line": 836, "text": "References"}, {"line": 838, "text": "Papers"}, {"line": 843, "text": "Existing Platforms"}, {"line": 849, "text": "AI-OS Integration Points"}, {"line": 865, "text": "Appendix: Code Examples"}, {"line": 867, "text": "Example: Client Training Loop"}, {"line": 874, "text": "1. Fetch batch"}, {"line": 878, "text": "2. Train locally"}, {"line": 881, "text": "3. Upload results"}, {"line": 889, "text": "4. Update stats"}, {"line": 900, "text": "Example: Server Aggregation"}, {"line": 907, "text": "Fetch pending gradients"}, {"line": 913, "text": "Aggregate using FedAvg"}, {"line": 916, "text": "Update global model"}, {"line": 920, "text": "Save new version"}, {"line": 924, "text": "Mark gradients as aggregated"}]}, {"path": "planned_features/EVALUATION_SYSTEM_ENHANCEMENTS.md", "content": "# Evaluation System Enhancements\n\n**Status:** \ud83d\udccb Planned  \n**Priority:** Medium  \n**Category:** Model Evaluation & Benchmarking  \n**Created:** October 19, 2025  \n**Based on:** Evaluation system testing results (Oct 19-20, 2025)\n\n---\n\n## Overview\n\nEnhance the AI-OS evaluation system with extended benchmarking capabilities, advanced metrics, and comparison tools based on systematic testing of the current evaluation functionality.\n\n**Current State:** Basic corpus analysis and checkpoint evaluation working via `aios english-eval`  \n**Goal:** Comprehensive evaluation suite with industry-standard benchmarks and automated comparison\n\n---\n\n## Motivation\n\nRecent systematic testing (Oct 2025) confirmed that:\n- \u2705 Current evaluation system works correctly for corpus analysis\n- \u2705 Multiple checkpoint formats supported (.pt, .safetensors)\n- \u2705 Artifact storage and retrieval functional\n- \u26a0\ufe0f Limited to readability metrics (Flesch scores, word counts)\n- \u274c No perplexity or loss-based quality metrics\n- \u274c No industry-standard benchmark support (hellaswag, arc, etc.)\n- \u274c No automated comparison between checkpoints\n\n---\n\n## Planned Enhancements\n\n### 1. LM-Evaluation-Harness Integration\n\n**Objective:** Add industry-standard benchmark evaluation capabilities\n\n#### Tasks:\n- [ ] Install `lm-eval` dependency\n  ```bash\n  pip install lm-eval\n  ```\n- [ ] Integrate with existing `aios eval` commands\n- [ ] Enable standard benchmarks:\n  - [ ] HellaSwag (commonsense reasoning)\n  - [ ] ARC (science questions)\n  - [ ] MMLU (multitask understanding)\n  - [ ] TruthfulQA (truthfulness)\n  - [ ] GSM8K (math reasoning)\n  - [ ] HumanEval (code generation)\n- [ ] Add custom task configuration support\n- [ ] Store benchmark results in artifact system\n\n#### Implementation Notes:\n```python\n# Example integration\nfrom lm_eval import evaluator, tasks\n\ndef run_lm_eval_benchmark(model_path: str, tasks: list[str]):\n    results = evaluator.simple_evaluate(\n        model=\"hf\",\n        model_args=f\"pretrained={model_path}\",\n        tasks=tasks,\n        num_fewshot=0,\n        batch_size=8\n    )\n    return results\n```\n\n#### Benefits:\n- Compare against published baselines\n- Validate model capabilities across diverse tasks\n- Standard metrics for model comparison\n- Community-recognized benchmarks\n\n---\n\n### 2. Perplexity & Quality Metrics\n\n**Objective:** Add model-specific quality metrics to checkpoint evaluations\n\n#### Tasks:\n- [ ] Implement perplexity calculation on test datasets\n- [ ] Add cross-entropy loss metrics\n- [ ] Calculate bits-per-character/byte\n- [ ] Track token-level accuracy\n- [ ] Add BLEU/ROUGE scores for generation tasks\n- [ ] Implement diversity metrics (distinct-n)\n- [ ] Add coherence scoring\n\n#### Metrics to Add:\n```yaml\nquality_metrics:\n  - perplexity: \"Lower is better - measures prediction confidence\"\n  - cross_entropy: \"Average loss on test set\"\n  - bits_per_byte: \"Compression efficiency metric\"\n  - token_accuracy: \"Exact match rate for next token\"\n  - distinct_1/distinct_2: \"Vocabulary diversity in generations\"\n  - coherence_score: \"Semantic consistency measure\"\n```\n\n#### Implementation Approach:\n```python\ndef calculate_checkpoint_metrics(model, dataset):\n    metrics = {\n        'perplexity': calculate_perplexity(model, dataset),\n        'cross_entropy': calculate_loss(model, dataset),\n        'bits_per_byte': calculate_bpb(model, dataset),\n        'token_accuracy': calculate_accuracy(model, dataset),\n        'generation_quality': evaluate_generations(model, dataset)\n    }\n    return metrics\n```\n\n#### Integration Points:\n- Extend `aios english-eval` to include these metrics when checkpoint provided\n- Store in artifact data structure\n- Display in `aios artifacts-show` output\n\n---\n\n### 3. Automated Comparison Tools\n\n**Objective:** Enable side-by-side comparison of evaluation results\n\n#### Tasks:\n- [ ] Implement `aios eval compare` command\n- [ ] Support multi-checkpoint comparison (2+ models)\n- [ ] Generate comparison tables (markdown/HTML)\n- [ ] Add visualization support:\n  - [ ] Performance radar charts\n  - [ ] Metric progression over training\n  - [ ] Task-specific comparison graphs\n- [ ] Statistical significance testing\n- [ ] Automated regression detection\n\n#### CLI Interface:\n```bash\n# Compare two checkpoints\naios eval compare --checkpoints checkpoint1.pt checkpoint2.pt --dataset eval.txt\n\n# Compare multiple evaluations by artifact ID\naios eval compare --artifact-ids 2 3 4 5\n\n# Compare with baseline\naios eval compare --checkpoint my_model.pt --baseline gpt2\n\n# Generate report\naios eval compare --checkpoints model1.pt model2.pt --output comparison_report.html\n```\n\n#### Comparison Report Features:\n- **Metric Deltas:** Show improvement/regression percentages\n- **Statistical Tests:** P-values for significance\n- **Ranking:** Best-to-worst across metrics\n- **Recommendations:** Identify which checkpoint to use for what purpose\n- **Regression Alerts:** Flag significant performance drops\n\n#### Data Structure:\n```python\n@dataclass\nclass ComparisonResult:\n    checkpoints: list[str]\n    metrics: dict[str, list[float]]\n    deltas: dict[str, list[float]]  # Percentage changes\n    statistical_significance: dict[str, float]  # p-values\n    rankings: dict[str, list[int]]\n    recommendations: str\n    regression_alerts: list[str]\n```\n\n---\n\n## Implementation Plan\n\n### Phase 1: LM-Eval Integration (Week 1-2)\n1. Install and test lm-eval library\n2. Create wrapper functions for common benchmarks\n3. Integrate with existing CLI commands\n4. Test on ActV1 models\n5. Document usage and available tasks\n\n### Phase 2: Perplexity Metrics (Week 2-3)\n1. Implement perplexity calculation\n2. Add to english-eval output\n3. Store in artifact system\n4. Add generation quality metrics\n5. Test across different checkpoints\n\n### Phase 3: Comparison Tools (Week 3-4)\n1. Design comparison data structures\n2. Implement `aios eval compare` command\n3. Add table/visualization generation\n4. Implement statistical testing\n5. Create automated reports\n6. Add regression detection\n\n### Phase 4: Documentation & Testing (Week 4)\n1. Comprehensive user documentation\n2. Example workflows and tutorials\n3. Unit tests for all new functions\n4. Integration tests with real checkpoints\n5. Performance benchmarking\n\n---\n\n## Technical Requirements\n\n### Dependencies:\n```toml\n[dependencies]\nlm-eval = \"^0.4.0\"  # LM Evaluation Harness\nscipy = \"^1.11.0\"   # Statistical tests\nmatplotlib = \"^3.8.0\"  # Visualizations\nseaborn = \"^0.13.0\"  # Enhanced plots\njinja2 = \"^3.1.0\"   # HTML report templates\n```\n\n### Compatibility:\n- Python 3.10+\n- PyTorch 2.0+\n- Transformers 4.35+\n- Works with existing .pt and .safetensors checkpoints\n\n---\n\n## File Structure\n\n```\nsrc/aios/\n\u251c\u2500\u2500 evaluation/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 lm_eval_wrapper.py      # LM-eval integration\n\u2502   \u251c\u2500\u2500 metrics.py               # Perplexity, quality metrics\n\u2502   \u251c\u2500\u2500 comparison.py            # Comparison tools\n\u2502   \u251c\u2500\u2500 reports.py               # Report generation\n\u2502   \u2514\u2500\u2500 visualization.py         # Plotting functions\n\u251c\u2500\u2500 cli/\n\u2502   \u2514\u2500\u2500 eval_commands.py         # Extended CLI commands\n\u2514\u2500\u2500 templates/\n    \u251c\u2500\u2500 comparison_report.html   # HTML template\n    \u2514\u2500\u2500 comparison_table.md      # Markdown template\n```\n\n---\n\n## Usage Examples\n\n### Example 1: Standard Benchmark\n```bash\n# Run hellaswag benchmark\naios eval run --checkpoint artifacts/brains/actv1/final_model.pt \\\n              --tasks hellaswag \\\n              --label \"actv1-hellaswag\"\n\n# View results\naios artifacts-show-latest\n```\n\n### Example 2: Comprehensive Evaluation\n```bash\n# Run multiple benchmarks with quality metrics\naios eval run --checkpoint my_model.pt \\\n              --tasks hellaswag,arc_easy,arc_challenge \\\n              --dataset eval_dataset.txt \\\n              --include-perplexity \\\n              --include-generation-metrics \\\n              --label \"comprehensive-eval\"\n```\n\n### Example 3: Compare Checkpoints\n```bash\n# Compare training progression\naios eval compare \\\n    --checkpoints artifacts/brains/actv1/English-v1/actv1_student.safetensors \\\n                  artifacts/brains/actv1/English-v2/actv1_student.safetensors \\\n                  artifacts/brains/actv1/English-v3/actv1_student.safetensors \\\n                  artifacts/brains/actv1/English-v4/actv1_student.safetensors \\\n    --dataset training_data/eval_test_dataset.txt \\\n    --output training_progression.html \\\n    --show-deltas\n```\n\n### Example 4: Automated Testing\n```bash\n# Compare new checkpoint against baseline\naios eval compare \\\n    --checkpoint new_checkpoint.pt \\\n    --baseline artifacts/brains/actv1/final_model.pt \\\n    --dataset validation_set.txt \\\n    --fail-on-regression \\\n    --threshold 5.0  # Fail if >5% regression on any metric\n```\n\n---\n\n## Success Metrics\n\n### Quantitative:\n- [ ] 10+ standard benchmarks supported\n- [ ] 5+ quality metrics per evaluation\n- [ ] Comparison reports generated in <30 seconds\n- [ ] 100% compatibility with existing checkpoints\n- [ ] <1 minute evaluation time for standard datasets\n\n### Qualitative:\n- [ ] Users can easily compare model versions\n- [ ] Clear identification of best checkpoint for tasks\n- [ ] Automated CI/CD integration possible\n- [ ] Reports are readable and actionable\n\n---\n\n## Testing Strategy\n\n### Unit Tests:\n- Metric calculation accuracy\n- Statistical test correctness\n- Report generation validity\n\n### Integration Tests:\n- End-to-end benchmark runs\n- Multi-checkpoint comparisons\n- Artifact storage/retrieval\n\n### Validation Tests:\n- Compare against known baselines\n- Verify statistical significance calculations\n- Cross-check with manual evaluations\n\n---\n\n## Risks & Mitigations\n\n| Risk | Impact | Mitigation |\n|------|--------|------------|\n| lm-eval dependency conflicts | High | Pin compatible versions, test thoroughly |\n| Slow benchmark evaluation | Medium | Add batching, caching, parallel execution |\n| Large artifact storage | Medium | Implement result compression, selective storage |\n| API changes in lm-eval | Medium | Pin version, abstract wrapper layer |\n| Comparison complexity | Low | Start simple, iterate based on feedback |\n\n---\n\n## Future Enhancements\n\n### Post-V1:\n- [ ] Multi-GPU distributed evaluation\n- [ ] Cloud-based benchmark execution\n- [ ] Continuous evaluation dashboard\n- [ ] A/B testing framework\n- [ ] Automatic hyperparameter tuning based on eval results\n- [ ] Custom benchmark creation wizard\n- [ ] Integration with experiment tracking (MLflow, W&B)\n\n### Advanced Features:\n- [ ] Model capability mapping (what tasks is model good at?)\n- [ ] Automatic prompt optimization based on eval results\n- [ ] Cross-model ensemble recommendations\n- [ ] Failure analysis and debugging tools\n\n---\n\n## References\n\n- [EleutherAI LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)\n- [Hugging Face Evaluate Library](https://huggingface.co/docs/evaluate)\n- [OpenAI Evals Framework](https://github.com/openai/evals)\n- Current evaluation test results: `artifacts/evaluation/evaluation_test_results.md`\n\n---\n\n## Related Issues\n\n- Extends existing `aios english-eval` functionality\n- Complements training metrics in `artifacts/brains/actv1/metrics.jsonl`\n- Supports model selection for production deployment\n\n---\n\n## Changelog\n\n- **2025-10-19:** Initial plan created based on systematic evaluation testing\n- **Next:** Prioritize and schedule implementation\n\n---\n\n**Note:** This plan is based on successful validation of the current evaluation system. All proposed enhancements build on working infrastructure and verified checkpoint compatibility.\n", "tags": ["datasets", "evaluation", "training"], "headings": [{"line": 0, "text": "Evaluation System Enhancements"}, {"line": 10, "text": "Overview"}, {"line": 19, "text": "Motivation"}, {"line": 32, "text": "Planned Enhancements"}, {"line": 34, "text": "1. LM-Evaluation-Harness Integration"}, {"line": 38, "text": "Tasks:"}, {"line": 54, "text": "Implementation Notes:"}, {"line": 56, "text": "Example integration"}, {"line": 70, "text": "Benefits:"}, {"line": 78, "text": "2. Perplexity & Quality Metrics"}, {"line": 82, "text": "Tasks:"}, {"line": 91, "text": "Metrics to Add:"}, {"line": 102, "text": "Implementation Approach:"}, {"line": 115, "text": "Integration Points:"}, {"line": 122, "text": "3. Automated Comparison Tools"}, {"line": 126, "text": "Tasks:"}, {"line": 137, "text": "CLI Interface:"}, {"line": 139, "text": "Compare two checkpoints"}, {"line": 142, "text": "Compare multiple evaluations by artifact ID"}, {"line": 145, "text": "Compare with baseline"}, {"line": 148, "text": "Generate report"}, {"line": 152, "text": "Comparison Report Features:"}, {"line": 159, "text": "Data Structure:"}, {"line": 174, "text": "Implementation Plan"}, {"line": 176, "text": "Phase 1: LM-Eval Integration (Week 1-2)"}, {"line": 183, "text": "Phase 2: Perplexity Metrics (Week 2-3)"}, {"line": 190, "text": "Phase 3: Comparison Tools (Week 3-4)"}, {"line": 198, "text": "Phase 4: Documentation & Testing (Week 4)"}, {"line": 207, "text": "Technical Requirements"}, {"line": 209, "text": "Dependencies:"}, {"line": 219, "text": "Compatibility:"}, {"line": 227, "text": "File Structure"}, {"line": 247, "text": "Usage Examples"}, {"line": 249, "text": "Example 1: Standard Benchmark"}, {"line": 251, "text": "Run hellaswag benchmark"}, {"line": 256, "text": "View results"}, {"line": 260, "text": "Example 2: Comprehensive Evaluation"}, {"line": 262, "text": "Run multiple benchmarks with quality metrics"}, {"line": 271, "text": "Example 3: Compare Checkpoints"}, {"line": 273, "text": "Compare training progression"}, {"line": 284, "text": "Example 4: Automated Testing"}, {"line": 286, "text": "Compare new checkpoint against baseline"}, {"line": 297, "text": "Success Metrics"}, {"line": 299, "text": "Quantitative:"}, {"line": 306, "text": "Qualitative:"}, {"line": 314, "text": "Testing Strategy"}, {"line": 316, "text": "Unit Tests:"}, {"line": 321, "text": "Integration Tests:"}, {"line": 326, "text": "Validation Tests:"}, {"line": 333, "text": "Risks & Mitigations"}, {"line": 345, "text": "Future Enhancements"}, {"line": 347, "text": "Post-V1:"}, {"line": 356, "text": "Advanced Features:"}, {"line": 364, "text": "References"}, {"line": 373, "text": "Related Issues"}, {"line": 381, "text": "Changelog"}]}, {"path": "planned_features/experiment-tracking-orchestration-hpo.md", "content": "## PF-004: Orchestration, experiment tracking, and hyperparameter tuning\n\n### Summary\n\nThis PF introduces optional Weights & Biases (W&B) experiment tracking, Prefect-powered flows to orchestrate data \u2192 train \u2192 eval \u2192 package, and Optuna-based hyperparameter tuning. It includes both CLI and GUI surfaces and a full developer checklist to implement and verify end-to-end.\n\n### Why this matters\n\n- Make runs observable beyond local JSONL.\n- Reproduce pipelines with Python-native orchestration.\n- Systematically tune key knobs (LR, warmup, chunking, MoE stability) without guesswork.\n\n---\n\n## What ships in PF-004\n\nIn scope (new capabilities):\n- W&B tracking: mirror metrics and upload artifacts from HRM training.\n- Prefect flows: a simple, local-first flow for dataset prep \u2192 train \u2192 eval \u2192 package.\n- Optuna autotune: `aios hrm-hf autotune` to search safe bounds with early-stopping.\n- GUI hooks: toggles and forms to launch training with W&B, run flows, and kick off HPO.\n\nOut of scope (future PF candidates):\n- Enterprise schedulers (Airflow, K8s operators), distributed orchestration, and model registry integrations.\n\n---\n\n## Dependencies and installation\n\nAll features are optional and guarded by availability checks. Recommended extras per feature:\n\n- Tracking: `wandb>=0.17`\n- Orchestration: `prefect>=2.16`\n- HPO: `optuna>=3.6`\n\nInstallation (PowerShell, Windows):\n\n```powershell\n# Activate venv if needed\n. .venv\\Scripts\\Activate.ps1\n\n# Install optional packages (any subset is fine)\npip install wandb prefect optuna\n```\n\nNote: W&B is optional and respects offline mode. Credentials are read from environment and W&B\u2019s standard login flow.\n\n---\n\n## CLI design and UX\n\n### 1) W&B flags in training\n\nCommand: `aios hrm-hf train-actv1`\n\nNew flags (all optional):\n- `--wandb/--no-wandb` (default: no-wandb)\n- `--wandb-project TEXT` (default: `aios-hrm`)\n- `--wandb-entity TEXT` (optional)\n- `--wandb-group TEXT` (optional)\n- `--wandb-tags TEXT` (comma-separated)\n- `--wandb-offline/--wandb-online` (default: online if logged in; else offline)\n- `--wandb-run-name TEXT` (optional)\n\nBehavior:\n- If enabled, initialize run with `TrainingConfig.to_dict()` as config.\n- Stream metrics each step and on eval; attach artifacts (metrics.jsonl, latest checkpoints, brain bundle, GPU metrics from `artifacts/optimization/*gpu_metrics*.jsonl` when present).\n- Respect `WANDB_MODE=offline` and lack of network gracefully (no crash, local-only logging continues).\n\nExamples:\n\n```powershell\n# Minimal live tracking\naios hrm-hf train-actv1 --dataset-file training_data/curated_datasets/test_sample.txt --steps 50 --batch-size 4 --wandb --wandb-project aios-hrm\n\n# Offline (no network), with named run and tags\naios hrm-hf train-actv1 --dataset-file training_data/curated_datasets/test_sample.txt --steps 10 --batch-size 2 --wandb --wandb-offline --wandb-run-name dev-dryrun --wandb-tags smoke,debug\n```\n\nFallback if `aios` entrypoint unavailable:\n\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --dataset-file training_data/curated_datasets/test_sample.txt --steps 10 --batch-size 2 --wandb\n```\n\nMetrics mirrored to W&B (initial set):\n- step, train_loss, eval_loss, lr, tokens_per_sec, sec_per_step\n- batch_size, max_seq_len, halt_max_steps\n- memory: vram_alloc_gb (when available), cpu_ram_gb, gpu_overflow_gb (if detected)\n- moe: load_balance_loss (when enabled), num_experts, num_experts_per_tok, capacity_factor\n\nArtifacts:\n- `metrics.jsonl` (if `--log-file` is set)\n- last N checkpoints from `save_dir`\n- packaged brain bundle under `bundle_dir/brain_name` (if used)\n\n---\n\n### 2) Prefect flow entry\n\nCommand: `aios flow hrm-train`\n\nPurpose: End-to-end local flow for:\n1) dataset prep (no-op if a plain text file is provided)\n2) training via `hrm-hf train-actv1`\n3) evaluation via `aios eval run` (when `--eval-file` is provided)\n4) packaging into brain bundle (if `--brain-name` is provided)\n\nFlags (representative):\n- `--dataset-file PATH` (required)\n- `--eval-file PATH` (optional)\n- `--brain-name TEXT` (optional)\n- `--wandb` (optional; passes through to train)\n- Plus a passthrough `--train-args \"...\"` for advanced control\n\nExamples:\n\n```powershell\n# Simple flow with W&B\naios flow hrm-train --dataset-file training_data/curated_datasets/test_sample.txt --wandb\n\n# Full flow with eval + packaging\naios flow hrm-train --dataset-file training_data/curated_datasets/test_sample.txt --eval-file training_data/curated_datasets/test_sample.txt --brain-name demo-brain --wandb --train-args \"--steps 100 --batch-size 4\"\n```\n\nImplementation surfaces:\n- `src/aios/flows/hrm_train_flow.py`: Prefect `@flow` and `@task`s (`prepare_dataset`, `train_model`, `run_eval`, `package_brain`).\n- `src/aios/cli/flows_cli.py`: Typer command group that invokes Prefect flow (Python-native run; users don\u2019t need a Prefect daemon).\n\n---\n\n### 3) Optuna HPO\n\nCommand: `aios hrm-hf autotune`\n\nCore flags:\n- `--trials INT` (default: 10)\n- `--timeout-minutes INT` (optional)\n- `--sampler {tpe,random}` (default: tpe)\n- `--pruner {median,successive_halving,None}` (default: median)\n- `--direction {minimize,maximize}` (default: minimize eval_loss)\n- `--eval-batches INT` (default: 3 for quick signal)\n- `--study-name TEXT` (optional, for resuming)\n- `--storage TEXT` (optional, Optuna RDB string for persistence)\n- `--seed INT` (optional; repeatable trials)\n- Search-space overrides (optional):\n\t- `--lr-min 1e-6 --lr-max 1e-4`\n\t- `--warmup-min 20 --warmup-max 400`\n\t- `--chunk-choices 1024,2048,4096`\n\t- `--moe-balance-min 5e-3 --moe-balance-max 2e-2`\n\nBehavior:\n- Each trial runs a short `train-actv1` with trial params applied to `TrainingConfig`.\n- OOM or fatal errors are caught; trial marked failed with readable metadata.\n- Best trial is reported and optionally exported to a JSON config snippet.\n\nExamples:\n\n```powershell\n# Fast 5-trial smoke test\naios hrm-hf autotune --dataset-file training_data/curated_datasets/test_sample.txt --trials 5 --eval-batches 2\n\n# Longer tune with persistent study on SQLite\naios hrm-hf autotune --dataset-file training_data/curated_datasets/test_sample.txt --trials 30 --storage sqlite:///artifacts/optimization/autotune.db --study-name actv1-tune-v1 --seed 42 --wandb\n```\n\nImplementation surfaces:\n- `src/aios/cli/hrm_hf/autotune.py`: Typer command; Optuna study setup; objective wrapper.\n- `src/aios/hpo/spaces.py`: central search-space builders and safe bounds.\n- `src/aios/hpo/objectives.py`: training/eval objective, robust exception handling.\n\n---\n\n## GUI design and UX\n\nLocations: `src/aios/gui/components/`\n\nAdditions:\n- Training panel: W&B toggle and advanced fields (project, entity, group, tags, run-name, offline). These map directly to CLI flags and `TrainingConfig` passthrough.\n- Autotune panel: a small form for trial count, timeout, pruner, sampler, search-space limits, and a \u201cStart Autotune\u201d button that spawns `hrm-hf autotune` as a subprocess with a progress view and best-trial summary.\n- Orchestration tab: run the `hrm-train` flow with inputs (dataset, eval, brain-name, W&B toggle). Show live logs and a link to Prefect UI (optional).\n\nSuggested files:\n- `hrm_training/wandb_fields.py` (reusable widget group)\n- `hrm_training/autotune_panel.py`\n- `flows/flow_runner.py` (thin wrapper to call Python flows or CLI)\n\nProcess notes:\n- Use `TrainingConfig.to_cli_args()` for argument generation and append W&B/HPO/flow specific flags.\n- Ensure long-running subprocesses drain stdout continuously to avoid deadlocks.\n- Persist last-used settings in `~/.config/aios/gui_prefs.yaml` for convenience.\n\n---\n\n## Implementation plan (dev checklist)\n\n1) W&B shim and wiring\n- [ ] Create `src/aios/core/logging/wandb_logger.py` with a tiny adapter: `init(config: dict, flags)`, `log(dict, step)`, `log_artifact(path, name, type)`, `finish()`; internally no-op if W&B missing or disabled.\n- [ ] Add CLI flags to `hrm_hf_cli.train_actv1` (see above) and plumb into `TrainingConfig` or function kwargs.\n- [ ] In `train_actv1_impl`, when enabled:\n\t- [ ] init run with config\n\t- [ ] per-step: log metrics\n\t- [ ] on checkpoint/eval/end: upload artifacts\n\t- [ ] handle offline/no-auth gracefully\n\n2) Prefect flow\n- [ ] Add `src/aios/flows/hrm_train_flow.py` with `@task` steps and `@flow` wrapper.\n- [ ] Add `src/aios/cli/flows_cli.py` exposing `aios flow hrm-train`.\n- [ ] Optional: emit a short README in `docs/flows/HRM_TRAIN_FLOW.md` showing usage and Prefect UI link.\n\n3) Optuna autotune\n- [ ] Add `src/aios/cli/hrm_hf/autotune.py` Typer command and register in `hrm_hf_cli.register`.\n- [ ] Implement `src/aios/hpo/spaces.py` and `src/aios/hpo/objectives.py`.\n- [ ] Robust OOM capture: detect CUDA OOM and DML errors; mark trial failed with note.\n- [ ] Emit `artifacts/optimization/autotune_<timestamp>.jsonl` with trial summaries.\n\n4) Schema and config\n- [ ] Update `TrainingConfig` with W&B-related passthrough fields only if needed, or treat as non-config CLI flags.\n- [ ] Add env-driven defaults: `AIOS_WANDB_PROJECT`, `AIOS_WANDB_ENTITY`, `WANDB_MODE`.\n\n5) Packaging and optional deps\n- [ ] Add extras in `pyproject.toml`:\n\t- `[project.optional-dependencies] tracking = [\"wandb>=0.17\"]`\n\t- `orchestration = [\"prefect>=2.16\"]`\n\t- `hpo = [\"optuna>=3.6\"]`\n- [ ] Document install snippets in `docs` and help texts.\n\n6) Tests and dry-runs\n- [ ] Unit test the W&B shim in no-op mode (no wandb installed) and with env offline.\n- [ ] CLI smoke test: run 1-step training with `--log-file artifacts/brains/actv1/metrics.jsonl` (see VS Code tasks already present) and verify no exceptions when `--wandb` is toggled.\n- [ ] HPO smoke: 2\u20133 trials, `--eval-batches 1`, ensure failure handling works.\n- [ ] Flow smoke: run with toy dataset; verify all tasks execute and files exist.\n\n---\n\n## HPO search space (initial)\n\nPrimary knobs and safe ranges:\n- `lr`: loguniform [1e-6, 1e-4] (reduced for MoE stability)\n- `warmup_steps`: int [20, 400]\n- `chunk_size`: categorical [1024, 2048, 4096]\n- `moe_load_balance_loss_coef`: loguniform [5e-3, 2e-2]\n\nObjective: minimize final eval loss on a small held-out slice (`--eval-batches` 2\u20135 for speed). Consider averaging 2 seeds in later iterations for stability.\n\nPruners/samplers:\n- Default pruner: MedianPruner (fast convergence on short runs)\n- Default sampler: TPE\n\nOutput:\n- Best-trial JSON emitted to `artifacts/optimization/best_trial.json`\n- Full trial history: `artifacts/optimization/autotune_*.jsonl`\n\n---\n\n## Observability and artifacts\n\nMetrics source of truth remains JSONL when `--log-file` is supplied. W&B mirrors and aggregates these:\n- Per-step metrics: train_loss, lr, throughput\n- Per-eval metrics: eval_loss, ppl (if computed)\n- System: memory stats, GPU overflow indicators when available\n\nArtifacts to attach (when present):\n- Checkpoints from `save_dir`\n- Final brain bundle from `bundle_dir`\n- `metrics.jsonl`, `gpu_metrics_*.jsonl`, model card HTML if generated\n\n---\n\n## Security and privacy\n\n- W&B: Respect offline mode; never crash if not logged in or network blocked.\n- Redact secrets from configs before logging.\n- Allow disabling artifact uploads via `--no-wandb` or env `WANDB_MODE=disabled`.\n\n---\n\n## Troubleshooting\n\nCommon issues and fixes:\n\n- No module named wandb/prefect/optuna\n\t- Install optional deps: `pip install wandb prefect optuna`\n\n- W&B login required\n\t- Run `wandb login` or use `--wandb-offline` for offline runs.\n\n- CUDA OOM during HPO\n\t- Optuna trial should be marked failed automatically; reduce `batch_size` or enable `--use-chunked-training` with smaller `--chunk-size`.\n\n- Prefect UI not accessible\n\t- This PF uses in-process Python flows; the UI is optional. You can still use `prefect orion start` separately if you want dashboards.\n\nWindows/PowerShell notes:\n- Prefer the `aios` entrypoint; if missing, call the module with `.venv\\Scripts\\python.exe -m aios.cli.aios ...`.\n- Paths in examples use forward slashes or PowerShell-friendly backslashes.\n\n---\n\n## Testing and acceptance criteria\n\nW&B\n- [ ] Enabling `--wandb` produces a new run with step/eval metrics.\n- [ ] Artifacts (at least metrics.jsonl) upload at run end or are skipped gracefully if offline.\n\nPrefect flow\n- [ ] `aios flow hrm-train` completes end-to-end on a toy dataset.\n- [ ] If `--eval-file` is given, eval metrics are produced.\n- [ ] If `--brain-name` is given, a packaged bundle exists.\n\nOptuna\n- [ ] At least 5 trials complete; failures are recorded (not fatal for the command).\n- [ ] Best-trial config improves eval loss vs default on the toy dataset.\n\n---\n\n## Milestones\n\n- M1 (1\u20132 days): W&B logging shipped + docs; CLI flags integrated; GUI toggle wired.\n- M2 (1\u20132 days): Prefect flow + CLI wrapper; Optuna autotune command + GUI panel; smoke tests and docs.\n\n---\n\n## Appendix A \u2013 Example quickstarts (copy/paste)\n\nDry-run training (JSONL only):\n\n```powershell\naios hrm-hf train-actv1 --model gpt2 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1 --batch-size 2 --halt-max-steps 1 --eval-batches 1 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nTraining with W&B (online):\n\n```powershell\naios hrm-hf train-actv1 --dataset-file training_data/curated_datasets/test_sample.txt --steps 50 --batch-size 4 --wandb --wandb-project aios-hrm --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\nRun the flow (with eval + packaging):\n\n```powershell\naios flow hrm-train --dataset-file training_data/curated_datasets/test_sample.txt --eval-file training_data/curated_datasets/test_sample.txt --brain-name demo-brain --wandb --train-args \"--steps 100 --batch-size 4\"\n```\n\nAutotune (5 quick trials):\n\n```powershell\naios hrm-hf autotune --dataset-file training_data/curated_datasets/test_sample.txt --trials 5 --eval-batches 2 --wandb\n```\n\nIf the `aios` script is not available:\n\n```powershell\n.venv\\Scripts\\python.exe -m aios.cli.aios hrm-hf train-actv1 --dataset-file training_data/curated_datasets/test_sample.txt --steps 1 --batch-size 2 --log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n---\n\n## Appendix B \u2013 Proposed file map\n\n- `src/aios/core/logging/wandb_logger.py` \u2013 W&B adapter shim (no-op when unavailable).\n- `src/aios/flows/hrm_train_flow.py` \u2013 Prefect tasks and flow.\n- `src/aios/cli/flows_cli.py` \u2013 Typer command `aios flow hrm-train`.\n- `src/aios/cli/hrm_hf/autotune.py` \u2013 Typer command and Optuna objective.\n- `src/aios/hpo/spaces.py` \u2013 reusable search spaces.\n- `src/aios/hpo/objectives.py` \u2013 training objective with robust error handling.\n- `src/aios/gui/components/hrm_training/wandb_fields.py` \u2013 W&B UI widgets.\n- `src/aios/gui/components/hrm_training/autotune_panel.py` \u2013 GUI for HPO.\n- `docs/flows/HRM_TRAIN_FLOW.md` \u2013 optional flow readme.\n", "tags": ["cli", "datasets", "evaluation", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "PF-004: Orchestration, experiment tracking, and hyperparameter tuning"}, {"line": 2, "text": "Summary"}, {"line": 6, "text": "Why this matters"}, {"line": 14, "text": "What ships in PF-004"}, {"line": 27, "text": "Dependencies and installation"}, {"line": 38, "text": "Activate venv if needed"}, {"line": 41, "text": "Install optional packages (any subset is fine)"}, {"line": 49, "text": "CLI design and UX"}, {"line": 51, "text": "1) W&B flags in training"}, {"line": 72, "text": "Minimal live tracking"}, {"line": 75, "text": "Offline (no network), with named run and tags"}, {"line": 98, "text": "2) Prefect flow entry"}, {"line": 118, "text": "Simple flow with W&B"}, {"line": 121, "text": "Full flow with eval + packaging"}, {"line": 131, "text": "3) Optuna HPO"}, {"line": 159, "text": "Fast 5-trial smoke test"}, {"line": 162, "text": "Longer tune with persistent study on SQLite"}, {"line": 173, "text": "GUI design and UX"}, {"line": 194, "text": "Implementation plan (dev checklist)"}, {"line": 235, "text": "HPO search space (initial)"}, {"line": 255, "text": "Observability and artifacts"}, {"line": 269, "text": "Security and privacy"}, {"line": 277, "text": "Troubleshooting"}, {"line": 299, "text": "Testing and acceptance criteria"}, {"line": 316, "text": "Milestones"}, {"line": 323, "text": "Appendix A \u2013 Example quickstarts (copy/paste)"}, {"line": 357, "text": "Appendix B \u2013 Proposed file map"}]}, {"path": "planned_features/GRADIENT_ACCUMULATION_IMPLEMENTATION.md", "content": "# Gradient Accumulation Implementation Plan\n\n**Status**: Planned  \n**Priority**: High  \n**Objective**: Fix loss instability from small batch sizes by implementing gradient accumulation  \n**Created**: 2025-10-23\n\n---\n\n## \ud83d\udccb Executive Summary\n\n### The Problem\nCurrent training exhibits severe loss instability due to small batch sizes:\n```\nStep 448: Loss = 7.0073\nStep 449: Loss = 8.1779  \u2190 Wild 17% jump\nStep 450: Loss = 7.3505\nStep 451: Loss = 8.1017\nStep 452: Loss = 8.7440  \u2190 Peak instability\nStep 453: Loss = 7.5572\n```\n\n**Root Cause**: Batch size too small (2-8 samples) \u2192 noisy gradient estimates \u2192 unstable training\n\n### The Solution\n**Gradient Accumulation**: Accumulate gradients over N batches before updating weights\n- **Effective batch size** = physical_batch_size \u00d7 gradient_accumulation_steps\n- **VRAM usage** = same as physical_batch_size (no increase!)\n- **Training stability** = equivalent to large batch training\n\n### Expected Results\n```\n# With batch=8, gradient_accumulation_steps=4\nStep 112: Loss = 7.0073  (effective_batch=32)\nStep 113: Loss = 6.8234  \u2190 Smooth 2.6% decline\nStep 114: Loss = 6.6512\nStep 115: Loss = 6.4891\nStep 116: Loss = 6.3201  \u2190 Stable convergence\n```\n\n---\n\n## \ud83c\udf93 Technical Background\n\n### How Gradient Accumulation Works\n\n**Standard Training** (current):\n```python\nfor batch in dataloader:\n    loss = model(batch)\n    loss.backward()      # Compute gradients\n    optimizer.step()     # Update weights immediately\n    optimizer.zero_grad()\n```\n\n**With Gradient Accumulation**:\n```python\nfor i, batch in enumerate(dataloader):\n    loss = model(batch) / accumulation_steps  # \u26a0\ufe0f CRITICAL: Scale loss!\n    loss.backward()      # Gradients accumulate in model.parameters()\n    \n    if (i + 1) % accumulation_steps == 0:\n        optimizer.step()     # Update weights every N batches\n        optimizer.zero_grad()\n```\n\n### Why Loss Scaling is Critical\n```python\n# \u274c WRONG - Gradients will sum instead of average\nloss.backward()\n\n# \u2705 CORRECT - Scale loss so gradients average\nloss = loss / gradient_accumulation_steps\nloss.backward()\n```\n\n### Memory Usage Comparison\n```\nModel: 1.3B parameters\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Batch Size  \u2502 Accum    \u2502 Effective  \u2502 VRAM Usage  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2           \u2502 1        \u2502 2          \u2502 ~6 GB       \u2502\n\u2502 2           \u2502 16       \u2502 32         \u2502 ~6 GB       \u2502 \u2190 Same!\n\u2502 8           \u2502 1        \u2502 8          \u2502 ~10 GB      \u2502\n\u2502 8           \u2502 4        \u2502 32         \u2502 ~10 GB      \u2502 \u2190 Same!\n\u2502 32          \u2502 1        \u2502 32         \u2502 ~24 GB      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nConclusion: Accumulation doesn't increase VRAM!\n```\n\n---\n\n## \ud83c\udfd7\ufe0f Implementation Architecture\n\n### Files to Modify\n\n```\nsrc/aios/core/hrm_training/training_config/\n\u251c\u2500\u2500 optimization_fields.py          [ADD] gradient_accumulation_steps field\n\u251c\u2500\u2500 config_main.py                  [MODIFY] to_cli_args() method\n\nsrc/aios/cli/\n\u251c\u2500\u2500 hrm_hf_cli.py                   [ADD] --gradient-accumulation-steps option\n\nsrc/aios/cli/hrm_hf/training_logic/\n\u251c\u2500\u2500 train_epoch.py                  [MODIFY] backward/step logic (MAIN CHANGE)\n\nsrc/aios/gui/components/hrm_training_panel/\n\u251c\u2500\u2500 ui_optimizations.py             [ADD] UI controls for gradient accumulation\n\u251c\u2500\u2500 variable_setup.py               [ADD] gradient_accumulation_var\n\u251c\u2500\u2500 config_builder.py               [MODIFY] include in config build\n\u251c\u2500\u2500 state_management.py             [MODIFY] save/load state\n```\n\n---\n\n## \ud83d\udcdd Implementation Steps\n\n### \u2705 Phase 1: Configuration Layer\n\n#### Step 1.1: Add Configuration Field\n**File**: `src/aios/core/hrm_training/training_config/optimization_fields.py`\n\n**Location**: After line 118 (after `load_in_4bit` field)\n\n**Code to Add**:\n```python\n    # ============================================================================\n    # Gradient Accumulation\n    # ============================================================================\n    gradient_accumulation_steps: int = 1\n    \"\"\"Number of batches to accumulate gradients before updating weights.\n    \n    Enables training with larger effective batch sizes without increasing VRAM.\n    The effective batch size is: physical_batch_size \u00d7 gradient_accumulation_steps\n    \n    Benefits:\n    - Fixes loss instability from small batch sizes\n    - No VRAM increase (memory usage stays at physical batch size)\n    - Smoother training dynamics\n    - Better gradient estimates\n    \n    Example:\n    - batch_size=8, gradient_accumulation_steps=4 \u2192 effective_batch_size=32\n    - VRAM usage: ~10GB (for batch=8)\n    - Training stability: equivalent to batch=32\n    \n    Recommended values:\n    - 1: No accumulation (default, update every batch)\n    - 2-4: Mild accumulation for slightly smoother training\n    - 4-8: Moderate accumulation (recommended for most cases)\n    - 8-16: High accumulation for very small batch sizes\n    - 16+: Extreme accumulation (use when batch=1-2 required)\n    \n    How to choose:\n    1. Start with current batch_size and desired effective_batch_size\n    2. Calculate: gradient_accumulation_steps = effective_batch_size / batch_size\n    3. Test and adjust based on loss stability\n    \n    Memory impact:\n    - Gradients: +1\u00d7 model size (same as normal training)\n    - Activations: Only for physical batch size\n    - Total overhead: Negligible (<5% of total memory)\n    \n    Performance impact:\n    - Slightly slower due to more forward passes\n    - ~5-15% overhead depending on accumulation_steps\n    - Worth it for stability improvement\n    \n    Compatibility:\n    - Works with: AMP, gradient checkpointing, DeepSpeed ZeRO, PEFT/LoRA\n    - Works across: DDP, parallel independent, single-GPU modes\n    - Scheduler: Automatically adjusted to step with weight updates\n    \"\"\"\n```\n\n---\n\n#### Step 1.2: Update CLI Args Conversion\n**File**: `src/aios/core/hrm_training/training_config/config_main.py`\n\n**Location**: In `to_cli_args()` method, after batch_size (around line 109)\n\n**Find**:\n```python\n        args.extend([\"--batch-size\", str(self.batch_size)])\n        args.extend([\"--steps\", str(self.steps)])\n```\n\n**Replace with**:\n```python\n        args.extend([\"--batch-size\", str(self.batch_size)])\n        args.extend([\"--steps\", str(self.steps)])\n        \n        # Gradient accumulation\n        if self.gradient_accumulation_steps > 1:\n            args.extend([\"--gradient-accumulation-steps\", str(self.gradient_accumulation_steps)])\n```\n\n---\n\n### \u2705 Phase 2: CLI Integration\n\n#### Step 2.1: Add CLI Option\n**File**: `src/aios/cli/hrm_hf_cli.py`\n\n**Location**: In `train_actv1` function, after `batch_size` parameter (around line 137)\n\n**Find**:\n```python\n    batch_size: int = typer.Option(8, \"--batch-size\"),\n    steps: int = typer.Option(200, \"--steps\"),\n```\n\n**Replace with**:\n```python\n    batch_size: int = typer.Option(8, \"--batch-size\"),\n    gradient_accumulation_steps: int = typer.Option(\n        1, \n        \"--gradient-accumulation-steps\",\n        help=\"Accumulate gradients over N batches before updating weights. \"\n             \"Effective batch size = batch_size \u00d7 gradient_accumulation_steps. \"\n             \"Use to train with larger effective batches without increasing VRAM. \"\n             \"Example: batch=8, accum=4 \u2192 effective_batch=32\"\n    ),\n    steps: int = typer.Option(200, \"--steps\"),\n```\n\n---\n\n### \u2705 Phase 3: Training Loop Modification (CRITICAL)\n\n#### Step 3.1: Modify Training Logic\n**File**: `src/aios/cli/hrm_hf/training_logic/train_epoch.py`\n\n**Location**: Around line 350-380 (backward/step section)\n\n**Find** (the entire backward pass section):\n```python\n                # Backward pass\n                if deepspeed_engine is not None:\n                    deepspeed_engine.backward(loss)\n                    deepspeed_engine.step()\n                elif use_amp and scaler is not None and dev == \"cuda\":\n                    scaler.scale(loss).backward()\n                    scaler.unscale_(opt)\n                    average_gradients_if_distributed(model_student, is_distributed=ddp_actually_working, world_sz=world_sz)\n                    grad_norm = torch.nn.utils.clip_grad_norm_(model_student.parameters(), 0.5)\n                    if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n                        opt.zero_grad(set_to_none=True)\n                        scaler.update()\n                    else:\n                        scaler.step(opt)\n                        scaler.update()\n                else:\n                    loss.backward()\n                    average_gradients_if_distributed(model_student, is_distributed=ddp_actually_working, world_sz=world_sz)\n                    grad_norm = torch.nn.utils.clip_grad_norm_(model_student.parameters(), 0.5)\n                    if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n                        opt.zero_grad(set_to_none=True)\n                    else:\n                        opt.step()\n                    \n                steps_done += 1\n```\n\n**Replace with**:\n```python\n                # Get gradient accumulation config\n                gradient_accumulation_steps = getattr(config, 'gradient_accumulation_steps', 1)\n                accumulation_steps = max(1, int(gradient_accumulation_steps))\n                \n                # Scale loss for gradient accumulation\n                # CRITICAL: This ensures gradients average instead of sum\n                scaled_loss = loss / accumulation_steps\n                \n                # Backward pass (gradients accumulate automatically)\n                if deepspeed_engine is not None:\n                    # DeepSpeed handles accumulation internally\n                    deepspeed_engine.backward(scaled_loss)\n                    \n                    # Only step optimizer every N batches\n                    if (batch_idx + 1) % accumulation_steps == 0:\n                        deepspeed_engine.step()\n                        \n                elif use_amp and scaler is not None and dev == \"cuda\":\n                    # AMP with gradient accumulation\n                    scaler.scale(scaled_loss).backward()\n                    \n                    # Only update weights every N batches\n                    if (batch_idx + 1) % accumulation_steps == 0:\n                        scaler.unscale_(opt)\n                        average_gradients_if_distributed(model_student, is_distributed=ddp_actually_working, world_sz=world_sz)\n                        grad_norm = torch.nn.utils.clip_grad_norm_(model_student.parameters(), 0.5)\n                        \n                        if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n                            opt.zero_grad(set_to_none=True)\n                            scaler.update()\n                        else:\n                            scaler.step(opt)\n                            scaler.update()\n                            opt.zero_grad(set_to_none=True)\n                else:\n                    # Standard mode with gradient accumulation\n                    scaled_loss.backward()\n                    \n                    # Only update weights every N batches\n                    if (batch_idx + 1) % accumulation_steps == 0:\n                        average_gradients_if_distributed(model_student, is_distributed=ddp_actually_working, world_sz=world_sz)\n                        grad_norm = torch.nn.utils.clip_grad_norm_(model_student.parameters(), 0.5)\n                        \n                        if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n                            opt.zero_grad(set_to_none=True)\n                        else:\n                            opt.step()\n                            opt.zero_grad(set_to_none=True)\n                \n                # Increment step counter (count actual weight updates, not batches)\n                if (batch_idx + 1) % accumulation_steps == 0:\n                    steps_done += 1\n```\n\n---\n\n#### Step 3.2: Update Logging\n**File**: `src/aios/cli/hrm_hf/training_logic/train_epoch.py`\n\n**Location**: Around line 400 (logging section)\n\n**Find**:\n```python\n                if steps_done % 5 == 0 or steps_done == 1:\n                    try:\n                        import torch\n                        if dev == \"cuda\" and torch.cuda.is_available():\n                            torch.cuda.synchronize()\n                            allocated_gb = torch.cuda.memory_allocated(device_obj) / 1024**3\n                            reserved_gb = torch.cuda.memory_reserved(device_obj) / 1024**3\n                            max_allocated_gb = torch.cuda.max_memory_allocated(device_obj) / 1024**3\n                            total_gb = torch.cuda.get_device_properties(device_obj).total_memory / 1024**3\n                            \n                            write_jsonl({\n                                \"step\": steps_done,\n                                \"memory_gb\": round(allocated_gb, 3),\n```\n\n**Replace with**:\n```python\n                if steps_done % 5 == 0 or steps_done == 1:\n                    try:\n                        import torch\n                        if dev == \"cuda\" and torch.cuda.is_available():\n                            torch.cuda.synchronize()\n                            allocated_gb = torch.cuda.memory_allocated(device_obj) / 1024**3\n                            reserved_gb = torch.cuda.memory_reserved(device_obj) / 1024**3\n                            max_allocated_gb = torch.cuda.max_memory_allocated(device_obj) / 1024**3\n                            total_gb = torch.cuda.get_device_properties(device_obj).total_memory / 1024**3\n                            \n                            # Get accumulation info\n                            gradient_accumulation_steps = getattr(config, 'gradient_accumulation_steps', 1)\n                            accumulation_steps = max(1, int(gradient_accumulation_steps))\n                            \n                            write_jsonl({\n                                \"step\": steps_done,\n                                \"batch_idx\": batch_idx + 1,\n                                \"loss\": float(loss.item()),  # Unscaled loss for logging\n                                \"gradient_accumulation_steps\": accumulation_steps,\n                                \"effective_batch_size\": batch_size * accumulation_steps,\n                                \"physical_batch_size\": batch_size,\n                                \"memory_gb\": round(allocated_gb, 3),\n```\n\n---\n\n### \u2705 Phase 4: GUI Integration\n\n#### Step 4.1: Add Variable\n**File**: `src/aios/gui/components/hrm_training_panel/variable_setup.py`\n\n**Location**: In `setup_variables()` function, after existing optimization variables (around line 60)\n\n**Find**:\n```python\n    panel.use_8bit_optimizer_var = tk.BooleanVar(value=False)\n    panel.use_cpu_offload_var = tk.BooleanVar(value=False)\n```\n\n**Add after**:\n```python\n    panel.use_8bit_optimizer_var = tk.BooleanVar(value=False)\n    panel.use_cpu_offload_var = tk.BooleanVar(value=False)\n    \n    # Gradient accumulation\n    panel.gradient_accumulation_var = tk.StringVar(value=\"1\")\n```\n\n---\n\n#### Step 4.2: Add UI Controls\n**File**: `src/aios/gui/components/hrm_training_panel/ui_optimizations.py`\n\n**Location**: After Row 1 (Memory Optimizations), around line 38\n\n**Find**:\n```python\n    cpu_offload_btn.pack(side=\"left\", padx=(8, 0))\n    \n    # Row 2: PEFT (Parameter-Efficient Fine-Tuning)\n```\n\n**Add between**:\n```python\n    cpu_offload_btn.pack(side=\"left\", padx=(8, 0))\n    \n    # Row 1.5: Gradient Accumulation\n    grad_accum_row = ttk.Frame(opt_frame)\n    grad_accum_row.pack(fill=\"x\", pady=2)\n    ttk.Label(grad_accum_row, text=\"Batch Scaling:\", width=15, anchor=\"e\", font=(\"TkDefaultFont\", 9, \"bold\")).pack(side=\"left\")\n    ttk.Label(grad_accum_row, text=\"Accum Steps:\").pack(side=\"left\", padx=(0, 2))\n    accum_combo = ttk.Combobox(grad_accum_row, textvariable=panel.gradient_accumulation_var, width=8, state=\"readonly\")\n    accum_combo['values'] = ('1', '2', '4', '8', '16', '32')\n    accum_combo.pack(side=\"left\")\n    ttk.Label(grad_accum_row, text=\"\u2192\").pack(side=\"left\", padx=4)\n    panel.effective_batch_lbl = ttk.Label(grad_accum_row, text=\"Effective Batch: 8\", foreground=\"blue\")\n    panel.effective_batch_lbl.pack(side=\"left\")\n    \n    # Row 2: PEFT (Parameter-Efficient Fine-Tuning)\n```\n\n---\n\n#### Step 4.3: Add Tooltip and Update Callback\n**File**: `src/aios/gui/components/hrm_training_panel/ui_optimizations.py`\n\n**Location**: In tooltip section, around line 100\n\n**Find**:\n```python\n        add_tooltip(cpu_offload_btn, \"CPU Offload: Moves optimizer states to system RAM\\nSaves VRAM \u2022 ~30% slower training\")\n        add_tooltip(peft_enable_btn, \"Enable PEFT: Use Low-Rank Adaptation (LoRA) for efficient fine-tuning\\n\u219395-99% trainable parameters (87M \u2192 500K-2M)\")\n```\n\n**Add after**:\n```python\n        add_tooltip(cpu_offload_btn, \"CPU Offload: Moves optimizer states to system RAM\\nSaves VRAM \u2022 ~30% slower training\")\n        add_tooltip(accum_combo, \n            \"Gradient Accumulation: Accumulate gradients over N batches\\n\"\n            \"before updating weights.\\n\\n\"\n            \"Benefits:\\n\"\n            \"\u2022 Fixes loss instability from small batches\\n\"\n            \"\u2022 No VRAM increase\\n\"\n            \"\u2022 Smoother training dynamics\\n\\n\"\n            \"Effective Batch = Physical Batch \u00d7 Accum Steps\\n\"\n            \"Example: batch=8, accum=4 \u2192 effective=32\\n\\n\"\n            \"Recommended:\\n\"\n            \"\u2022 1: No accumulation (default)\\n\"\n            \"\u2022 4: Balanced (recommended)\\n\"\n            \"\u2022 8-16: High stability (small batches)\\n\"\n            \"\u2022 32: Maximum stability (batch=1-2)\")\n        add_tooltip(peft_enable_btn, \"Enable PEFT: Use Low-Rank Adaptation (LoRA) for efficient fine-tuning\\n\u219395-99% trainable parameters (87M \u2192 500K-2M)\")\n```\n\n---\n\n#### Step 4.4: Add Update Callback Function\n**File**: `src/aios/gui/components/hrm_training_panel/ui_optimizations.py`\n\n**Location**: After tooltips section, before the end of `build_optimizations_section()` function\n\n**Add**:\n```python\n    # Setup callback to update effective batch label\n    def update_effective_batch_label(*args):\n        try:\n            batch = int(panel.batch_var.get() or 8)\n            accum = int(panel.gradient_accumulation_var.get() or 1)\n            effective = batch * accum\n            panel.effective_batch_lbl.config(text=f\"Effective Batch: {effective}\")\n        except Exception:\n            pass\n    \n    panel.gradient_accumulation_var.trace_add(\"write\", update_effective_batch_label)\n    panel.batch_var.trace_add(\"write\", update_effective_batch_label)\n    \n    # Initial update\n    update_effective_batch_label()\n```\n\n---\n\n#### Step 4.5: Update Config Builder\n**File**: `src/aios/gui/components/hrm_training_panel/config_builder.py`\n\n**Location**: In `build_training_config()` function, after batch_size (around line 25)\n\n**Find**:\n```python\n        batch_size=int(panel.batch_var.get() or 8),\n        steps=int(panel.steps_var.get() or 200),\n```\n\n**Replace with**:\n```python\n        batch_size=int(panel.batch_var.get() or 8),\n        gradient_accumulation_steps=int(panel.gradient_accumulation_var.get() or 1),\n        steps=int(panel.steps_var.get() or 200),\n```\n\n---\n\n#### Step 4.6: Update State Management\n**File**: `src/aios/gui/components/hrm_training_panel/state_management.py`\n\n**Location**: In `get_state()` function, around line 30\n\n**Find**:\n```python\n            \"batch_size\": panel.batch_var.get(),\n            \"steps\": panel.steps_var.get(),\n```\n\n**Add after**:\n```python\n            \"batch_size\": panel.batch_var.get(),\n            \"gradient_accumulation_steps\": panel.gradient_accumulation_var.get(),\n            \"steps\": panel.steps_var.get(),\n```\n\n**Location**: In `set_state()` function, around line 100\n\n**Find**:\n```python\n    if \"batch_size\" in state:\n        panel.batch_var.set(str(state[\"batch_size\"]))\n    if \"steps\" in state:\n        panel.steps_var.set(str(state[\"steps\"]))\n```\n\n**Add after**:\n```python\n    if \"batch_size\" in state:\n        panel.batch_var.set(str(state[\"batch_size\"]))\n    if \"gradient_accumulation_steps\" in state:\n        panel.gradient_accumulation_var.set(str(state[\"gradient_accumulation_steps\"]))\n    if \"steps\" in state:\n        panel.steps_var.set(str(state[\"steps\"]))\n```\n\n---\n\n## \ud83e\uddea Testing Plan\n\n### Test 1: Basic Functionality\n**Objective**: Verify gradient accumulation works correctly\n\n```bash\n# Terminal test\ncd /path/to/AI-OS  # Replace with your AI-OS directory\n.\\.venv\\Scripts\\Activate.ps1\n\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 4 `\n  --gradient-accumulation-steps 8 `\n  --steps 20 `\n  --log-file artifacts/test_grad_accum.jsonl\n\n# Check log file\ncat artifacts/test_grad_accum.jsonl | Select-String \"gradient_accumulation\"\n```\n\n**Expected Output**:\n```json\n{\"gradient_accumulation_steps\": 8, \"effective_batch_size\": 32, \"physical_batch_size\": 4}\n```\n\n---\n\n### Test 2: Loss Stability Comparison\n**Objective**: Demonstrate loss stability improvement\n\n**Test 2a: Without accumulation (baseline)**\n```bash\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 2 `\n  --gradient-accumulation-steps 1 `\n  --steps 100 `\n  --log-file artifacts/test_without_accum.jsonl\n```\n\n**Test 2b: With accumulation**\n```bash\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 2 `\n  --gradient-accumulation-steps 16 `\n  --steps 100 `\n  --log-file artifacts/test_with_accum.jsonl\n```\n\n**Analysis Script**:\n```python\nimport json\nimport matplotlib.pyplot as plt\n\n# Load logs\nwith open('artifacts/test_without_accum.jsonl') as f:\n    without = [json.loads(line) for line in f if 'loss' in line]\n\nwith open('artifacts/test_with_accum.jsonl') as f:\n    with_accum = [json.loads(line) for line in f if 'loss' in line]\n\n# Plot comparison\nplt.figure(figsize=(12, 6))\nplt.plot([x['step'] for x in without], [x['loss'] for x in without], \n         label='Without accumulation (batch=2)', alpha=0.7)\nplt.plot([x['step'] for x in with_accum], [x['loss'] for x in with_accum], \n         label='With accumulation (batch=2, accum=16, effective=32)', alpha=0.7)\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.title('Loss Stability: With vs Without Gradient Accumulation')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.savefig('artifacts/gradient_accumulation_comparison.png')\nprint(\"Chart saved to artifacts/gradient_accumulation_comparison.png\")\n```\n\n---\n\n### Test 3: VRAM Usage Verification\n**Objective**: Confirm VRAM usage doesn't increase with accumulation\n\n```bash\n# Test A: batch=2, accum=1\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 2 `\n  --gradient-accumulation-steps 1 `\n  --steps 10 `\n  --log-file artifacts/vram_test_no_accum.jsonl\n\n# Test B: batch=2, accum=16 (should use same VRAM!)\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 2 `\n  --gradient-accumulation-steps 16 `\n  --steps 10 `\n  --log-file artifacts/vram_test_with_accum.jsonl\n```\n\n**Verification**:\n```powershell\n# Compare peak VRAM usage\n$noAccum = Get-Content artifacts/vram_test_no_accum.jsonl | ConvertFrom-Json | Where-Object {$_.peak_gb} | Select-Object -Last 1\n$withAccum = Get-Content artifacts/vram_test_with_accum.jsonl | ConvertFrom-Json | Where-Object {$_.peak_gb} | Select-Object -Last 1\n\nWrite-Host \"No accumulation VRAM: $($noAccum.peak_gb) GB\"\nWrite-Host \"With accumulation VRAM: $($withAccum.peak_gb) GB\"\nWrite-Host \"Difference: $(($withAccum.peak_gb - $noAccum.peak_gb)) GB (should be ~0)\"\n```\n\n---\n\n### Test 4: GUI Integration\n**Objective**: Verify GUI controls work correctly\n\n**Steps**:\n1. Launch GUI: `aios gui`\n2. Navigate to HRM Training panel\n3. Verify new controls appear:\n   - \"Batch Scaling:\" label\n   - \"Accum Steps:\" dropdown\n   - \"\u2192 Effective Batch: X\" label\n4. Test interactions:\n   - Set Batch Size: 8\n   - Set Accum Steps: 4\n   - Verify label shows \"Effective Batch: 32\"\n   - Change Batch Size to 16\n   - Verify label updates to \"Effective Batch: 64\"\n5. Start training and verify:\n   - Training runs without errors\n   - Log shows correct accumulation settings\n   - Loss curve is smoother than without accumulation\n\n---\n\n### Test 5: Compatibility Tests\n\n**Test 5a: With AMP**\n```bash\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 4 `\n  --gradient-accumulation-steps 8 `\n  --amp `\n  --steps 20\n```\n\n**Test 5b: With Gradient Checkpointing**\n```bash\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 4 `\n  --gradient-accumulation-steps 8 `\n  --gradient-checkpointing `\n  --steps 20\n```\n\n**Test 5c: With DeepSpeed ZeRO-2**\n```bash\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 4 `\n  --gradient-accumulation-steps 8 `\n  --zero-stage zero2 `\n  --steps 20\n```\n\n**Test 5d: With PEFT/LoRA**\n```bash\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 4 `\n  --gradient-accumulation-steps 8 `\n  --use-peft `\n  --lora-r 16 `\n  --steps 20\n```\n\n**Test 5e: Parallel Independent Mode**\n```bash\naios hrm-hf train-actv1 `\n  --model artifacts/hf_implant/gpt2 `\n  --dataset-file training_data/curated_datasets/test_sample.txt `\n  --batch-size 4 `\n  --gradient-accumulation-steps 8 `\n  --parallel-independent `\n  --cuda-ids 0,1 `\n  --steps 20\n```\n\n---\n\n## \u2705 Implementation Checklist\n\n### Phase 1: Configuration Layer\n- [ ] Add `gradient_accumulation_steps` field to `optimization_fields.py`\n- [ ] Update `to_cli_args()` in `config_main.py`\n- [ ] Test: Import TrainingConfig and verify new field exists\n\n### Phase 2: CLI Integration\n- [ ] Add `--gradient-accumulation-steps` option to `hrm_hf_cli.py`\n- [ ] Test: `aios hrm-hf train-actv1 --help` shows new option\n\n### Phase 3: Training Loop\n- [ ] Modify backward/step logic in `train_epoch.py`\n- [ ] Add loss scaling\n- [ ] Add conditional optimizer step\n- [ ] Update logging to show accumulation metrics\n- [ ] Test: Run basic training with accumulation\n\n### Phase 4: GUI Integration\n- [ ] Add `gradient_accumulation_var` in `variable_setup.py`\n- [ ] Add UI controls in `ui_optimizations.py`\n- [ ] Add tooltip\n- [ ] Add update callback for effective batch label\n- [ ] Update `config_builder.py`\n- [ ] Update `state_management.py` (get_state)\n- [ ] Update `state_management.py` (set_state)\n- [ ] Test: Launch GUI and verify controls appear\n\n### Phase 5: Testing\n- [ ] Test 1: Basic functionality\n- [ ] Test 2: Loss stability comparison\n- [ ] Test 3: VRAM usage verification\n- [ ] Test 4: GUI integration\n- [ ] Test 5a: AMP compatibility\n- [ ] Test 5b: Gradient checkpointing compatibility\n- [ ] Test 5c: DeepSpeed ZeRO compatibility\n- [ ] Test 5d: PEFT/LoRA compatibility\n- [ ] Test 5e: Parallel independent mode compatibility\n\n### Phase 6: Documentation\n- [ ] Update CLI help text (done via option definition)\n- [ ] Add tooltip to GUI (done in Phase 4)\n- [ ] Update `docs/guide/features/TRAINING_OPTIMIZATIONS.md` (if exists)\n- [ ] Add note to CHANGELOG.md\n\n---\n\n## \ud83d\udcca Success Criteria\n\n### Functional Requirements\n\u2705 Gradient accumulation configurable via CLI and GUI  \n\u2705 Loss stability improves with accumulation enabled  \n\u2705 VRAM usage remains constant regardless of accumulation steps  \n\u2705 Compatible with all existing optimizations (AMP, checkpointing, ZeRO, PEFT)  \n\u2705 Works in all training modes (single-GPU, DDP, parallel independent)  \n\n### Performance Requirements\n\u2705 Training slowdown < 15% for accumulation_steps \u2264 16  \n\u2705 Loss variance reduced by \u2265 50% compared to small batch baseline  \n\u2705 No memory leaks during extended training  \n\n### User Experience Requirements\n\u2705 GUI controls intuitive and self-documenting  \n\u2705 CLI help text clear and complete  \n\u2705 Logging shows accumulation status and effective batch size  \n\u2705 State persistence works (save/load GUI state)  \n\n---\n\n## \ud83c\udfaf Quick Start (After Implementation)\n\n### Via CLI\n```bash\n# Basic usage\naios hrm-hf train-actv1 \\\n  --model artifacts/hf_implant/gpt2 \\\n  --dataset-file training_data/curated_datasets/my_dataset.txt \\\n  --batch-size 8 \\\n  --gradient-accumulation-steps 4 \\\n  --steps 1000\n\n# High stability (small batch)\naios hrm-hf train-actv1 \\\n  --batch-size 2 \\\n  --gradient-accumulation-steps 16 \\\n  --steps 1000\n```\n\n### Via GUI\n1. Open HRM Training panel\n2. Set **Batch Size**: `8`\n3. Set **Accum Steps**: `4`\n4. Verify: \"Effective Batch: 32\"\n5. Click \"Start Training\"\n\n---\n\n## \ud83d\udcda References\n\n### Research\n- PyTorch gradient accumulation patterns (from pytorch/examples)\n- Standard deep learning practice for memory-constrained training\n- User's current loss instability pattern: 7.0\u21928.7\u21927.3 (analyzed)\n\n### Codebase Components\n- Main training loop: `src/aios/cli/hrm_hf/training_logic/train_epoch.py`\n- Config system: `src/aios/core/hrm_training/training_config/`\n- GUI panel: `src/aios/gui/components/hrm_training_panel/`\n- CLI: `src/aios/cli/hrm_hf_cli.py`\n\n### Related Features\n- Automatic Mixed Precision (AMP)\n- Gradient Checkpointing\n- DeepSpeed ZeRO\n- PEFT/LoRA\n- Parallel Independent Training\n\n---\n\n## \ud83d\udd04 Future Enhancements\n\n### Potential Improvements\n1. **Auto-calculate accumulation**: GUI button to automatically determine optimal accumulation steps based on target effective batch size\n2. **Adaptive accumulation**: Dynamically adjust accumulation based on VRAM usage\n3. **Loss-based tuning**: Automatically increase accumulation if loss variance exceeds threshold\n4. **Multi-GPU load balancing**: Different accumulation steps per GPU based on VRAM capacity\n\n### Not in Scope (For Now)\n- Dynamic gradient accumulation (changing during training)\n- Per-layer gradient accumulation\n- Gradient accumulation with different batch sizes per step\n\n---\n\n## \ud83d\udcde Support & Troubleshooting\n\n### Common Issues\n\n**Q: Loss not improving with accumulation**  \nA: Verify loss scaling is applied (`loss / accumulation_steps`). Check logs for `scaled_loss` in backward pass.\n\n**Q: VRAM usage increased**  \nA: Check that `batch_size` in config matches physical batch, not effective batch. Accumulation shouldn't increase VRAM.\n\n**Q: Training slower than expected**  \nA: Normal with high accumulation steps. Trade-off between speed and stability. Try reducing accumulation_steps.\n\n**Q: Gradients not accumulating**  \nA: Ensure `optimizer.zero_grad()` only called after weight update, not every batch.\n\n**Q: GUI not showing new controls**  \nA: Restart GUI. Check that all Phase 4 files were modified correctly.\n\n---\n\n**Document Version**: 1.0  \n**Last Updated**: 2025-10-23  \n**Author**: AI-OS Development Team  \n**Status**: Ready for Implementation\n", "tags": ["cli", "hrm", "training"], "headings": [{"line": 0, "text": "Gradient Accumulation Implementation Plan"}, {"line": 9, "text": "\ud83d\udccb Executive Summary"}, {"line": 11, "text": "The Problem"}, {"line": 24, "text": "The Solution"}, {"line": 30, "text": "Expected Results"}, {"line": 32, "text": "With batch=8, gradient_accumulation_steps=4"}, {"line": 42, "text": "\ud83c\udf93 Technical Background"}, {"line": 44, "text": "How Gradient Accumulation Works"}, {"line": 66, "text": "Why Loss Scaling is Critical"}, {"line": 68, "text": "\u274c WRONG - Gradients will sum instead of average"}, {"line": 71, "text": "\u2705 CORRECT - Scale loss so gradients average"}, {"line": 76, "text": "Memory Usage Comparison"}, {"line": 95, "text": "\ud83c\udfd7\ufe0f Implementation Architecture"}, {"line": 97, "text": "Files to Modify"}, {"line": 119, "text": "\ud83d\udcdd Implementation Steps"}, {"line": 121, "text": "\u2705 Phase 1: Configuration Layer"}, {"line": 123, "text": "Step 1.1: Add Configuration Field"}, {"line": 130, "text": "============================================================================"}, {"line": 131, "text": "Gradient Accumulation"}, {"line": 132, "text": "============================================================================"}, {"line": 181, "text": "Step 1.2: Update CLI Args Conversion"}, {"line": 197, "text": "Gradient accumulation"}, {"line": 204, "text": "\u2705 Phase 2: CLI Integration"}, {"line": 206, "text": "Step 2.1: Add CLI Option"}, {"line": 233, "text": "\u2705 Phase 3: Training Loop Modification (CRITICAL)"}, {"line": 235, "text": "Step 3.1: Modify Training Logic"}, {"line": 242, "text": "Backward pass"}, {"line": 271, "text": "Get gradient accumulation config"}, {"line": 275, "text": "Scale loss for gradient accumulation"}, {"line": 276, "text": "CRITICAL: This ensures gradients average instead of sum"}, {"line": 279, "text": "Backward pass (gradients accumulate automatically)"}, {"line": 281, "text": "DeepSpeed handles accumulation internally"}, {"line": 284, "text": "Only step optimizer every N batches"}, {"line": 289, "text": "AMP with gradient accumulation"}, {"line": 292, "text": "Only update weights every N batches"}, {"line": 306, "text": "Standard mode with gradient accumulation"}, {"line": 309, "text": "Only update weights every N batches"}, {"line": 320, "text": "Increment step counter (count actual weight updates, not batches)"}, {"line": 327, "text": "Step 3.2: Update Logging"}, {"line": 361, "text": "Get accumulation info"}, {"line": 377, "text": "\u2705 Phase 4: GUI Integration"}, {"line": 379, "text": "Step 4.1: Add Variable"}, {"line": 395, "text": "Gradient accumulation"}, {"line": 401, "text": "Step 4.2: Add UI Controls"}, {"line": 410, "text": "Row 2: PEFT (Parameter-Efficient Fine-Tuning)"}, {"line": 417, "text": "Row 1.5: Gradient Accumulation"}, {"line": 429, "text": "Row 2: PEFT (Parameter-Efficient Fine-Tuning)"}, {"line": 434, "text": "Step 4.3: Add Tooltip and Update Callback"}, {"line": 467, "text": "Step 4.4: Add Update Callback Function"}, {"line": 474, "text": "Setup callback to update effective batch label"}, {"line": 487, "text": "Initial update"}, {"line": 493, "text": "Step 4.5: Update Config Builder"}, {"line": 513, "text": "Step 4.6: Update State Management"}, {"line": 553, "text": "\ud83e\uddea Testing Plan"}, {"line": 555, "text": "Test 1: Basic Functionality"}, {"line": 559, "text": "Terminal test"}, {"line": 571, "text": "Check log file"}, {"line": 582, "text": "Test 2: Loss Stability Comparison"}, {"line": 612, "text": "Load logs"}, {"line": 619, "text": "Plot comparison"}, {"line": 636, "text": "Test 3: VRAM Usage Verification"}, {"line": 640, "text": "Test A: batch=2, accum=1"}, {"line": 649, "text": "Test B: batch=2, accum=16 (should use same VRAM!)"}, {"line": 661, "text": "Compare peak VRAM usage"}, {"line": 672, "text": "Test 4: GUI Integration"}, {"line": 695, "text": "Test 5: Compatibility Tests"}, {"line": 756, "text": "\u2705 Implementation Checklist"}, {"line": 758, "text": "Phase 1: Configuration Layer"}, {"line": 763, "text": "Phase 2: CLI Integration"}, {"line": 767, "text": "Phase 3: Training Loop"}, {"line": 774, "text": "Phase 4: GUI Integration"}, {"line": 784, "text": "Phase 5: Testing"}, {"line": 795, "text": "Phase 6: Documentation"}, {"line": 803, "text": "\ud83d\udcca Success Criteria"}, {"line": 805, "text": "Functional Requirements"}, {"line": 812, "text": "Performance Requirements"}, {"line": 817, "text": "User Experience Requirements"}, {"line": 825, "text": "\ud83c\udfaf Quick Start (After Implementation)"}, {"line": 827, "text": "Via CLI"}, {"line": 829, "text": "Basic usage"}, {"line": 837, "text": "High stability (small batch)"}, {"line": 844, "text": "Via GUI"}, {"line": 853, "text": "\ud83d\udcda References"}, {"line": 855, "text": "Research"}, {"line": 860, "text": "Codebase Components"}, {"line": 866, "text": "Related Features"}, {"line": 875, "text": "\ud83d\udd04 Future Enhancements"}, {"line": 877, "text": "Potential Improvements"}, {"line": 883, "text": "Not in Scope (For Now)"}, {"line": 890, "text": "\ud83d\udcde Support & Troubleshooting"}, {"line": 892, "text": "Common Issues"}]}, {"path": "planned_features/hrm-serving-inference-api.md", "content": "## PF-003: Serving and inference integration (HRM + baselines)\n\n### Summary\n\nProvide a clean serving path for HRM and HF baselines. Ship a minimal FastAPI server for HRM inference and document how to deploy HF baselines on vLLM/TGI. For production-grade GPU fleets, outline Triton backend path.\n\n### Motivation\n\n- Benchmark and A/B test HRM vs standard transformer baselines.\n- Enable downstream apps to use a stable `/generate` and `/loglikelihood` API.\n\n### Scope\n\nIn scope:\n- Minimal FastAPI server for HRM with batch generation and scoring endpoints.\n- Configuration to run under Docker.\n- Baseline serving docs for vLLM and TGI (HF models only).\n\nOut of scope (for this PF):\n- Triton custom backend implementation (design notes only).\n\n### Integration points (repo)\n\n- New module: `src/aios/serve/hrm_server.py` (FastAPI, uvicorn)\n- Dockerfile target: `--target hrm-serve` (multi-stage)\n- Scripts: `scripts/run_hrm_server.ps1` (Windows), `scripts/run_hrm_server.sh` (Linux)\n\n### API contract (v1)\n\n- POST `/generate`\n  - Input: `{ prompts: string[], max_tokens?: number, temperature?: number, top_p?: number }`\n  - Output: `{ completions: string[], usage?: { prompt_tokens, completion_tokens } }`\n\n- POST `/loglikelihood`\n  - Input: `{ prompts: string[], continuations: string[] }`\n  - Output: `{ ll: number[] }`\n\n---\n\n## Comprehensive guide and checklist\n\nThis section expands PF-003 into an actionable implementation guide with CLI and GUI elements, deployment paths, and validation checklists.\n\n### Deliverables\n\n- HRM FastAPI server module: `src/aios/serve/hrm_server.py`\n- Optional minimal UI for manual testing (Gradio or Streamlit)\n- CLI launcher: `aios serve hrm` (via existing `aios.cli.aios` entrypoint)\n- Dockerfile target: `hrm-serve` and helper scripts for Windows/Linux\n- Baseline serving notes for vLLM and TGI\n- Acceptance tests and operational runbooks\n\n### Architecture overview\n\n- Client (CLI/UI/SDK) \u2192 FastAPI app \u2192 HRM inference wrapper (tokenizer + model) \u2192 Torch device (CPU/GPU)\n- Optional: request batcher (FIFO), configurable max batch size and max new tokens per request\n- Observability: structured logs + optional Prometheus metrics endpoint\n- Healthchecks: `/healthz` (process up), `/readyz` (model loaded)\n\n### Data models and API details\n\n- POST `/generate`\n  - Request model (Pydantic):\n    - `prompts: List[str]` (1..N)\n    - `max_tokens: int = 128` (cap at server max, e.g., 1024)\n    - `temperature: float = 0.7` (range [0, 2])\n    - `top_p: float = 1.0` (range (0, 1])\n    - `seed: Optional[int]` (optional for reproducibility)\n    - `stop: Optional[List[str]]` (optional stop strings)\n  - Response:\n    - `completions: List[str]`\n    - `usage?: { prompt_tokens: int, completion_tokens: int, total_tokens: int }`\n  - Errors:\n    - 400: validation (e.g., empty prompts, invalid ranges)\n    - 429: rate limited (optional)\n    - 500: internal error\n\n- POST `/loglikelihood`\n  - Request:\n    - `prompts: List[str]`\n    - `continuations: List[str]` (must match length of prompts)\n  - Response:\n    - `ll: List[float]` (sum of token log-probs of each continuation given prompt)\n  - Errors: as above\n\n- GET `/healthz`: `{ status: \"ok\" }`\n- GET `/readyz`: `{ status: \"ready\" }` once model/tokenizer loaded\n- Optional future: `/generate_stream` using server-sent events (not in v1 scope)\n\n### HRM inference wrapper design\n\n- Artifacts and paths\n  - Tokenizer: reuse loading logic from training; prefer registry paths under `artifacts/hf_implant/tokenizers/` or configured `--tokenizer` path\n  - Model weights: e.g., `artifacts/brains/actv1/actv1_student.pt` or `artifacts/hf_implant/q_head.pt` + base model\n  - Config file (optional): `config/default.yaml` overrides\n\n  - Batch tokenize `prompts` with truncation to model context window; return attention masks\n\n- Decoding\n  - Modes: greedy (temperature=0 or top_p=1, top_k=None), sampled (temperature>0 and/or top_p<1)\n  - Respect `max_tokens` and `stop` strings; early stop if EOS token encountered\n  - Use `torch.no_grad()` and AMP optional (`torch.cuda.amp.autocast`) when CUDA available\n\n- Batching\n  - Pad to max prompt length in batch; maintain mapping to return completions in order\n  - Configurable `max_batch_size` to protect memory\n\n- Device selection\n  - Auto-detect CUDA/ROCm/MPS; environment variable `AIOS_DEVICE` to force: `cpu|cuda|mps`\n\n- Pseudocode outline\n  - `load_tokenizer()` \u2192 `load_model()` \u2192 set eval, move to device\n  - For `/generate`: encode \u2192 loop new tokens \u2192 decode to strings \u2192 postprocess stop\n  - For `/loglikelihood`: teacher-forcing forward over prompt+continuation and sum log-probs at continuation positions\n\n### Configuration model\n\nPrecedence: CLI flags > Env vars > YAML config defaults.\n\n- CLI flags (examples):\n  - `--host 0.0.0.0 --port 8000`\n  - `--model-path artifacts/brains/actv1/actv1_student.pt`\n  - `--tokenizer-path artifacts/hf_implant/tokenizers/<name>`\n  - `--device cpu|cuda|mps`\n  - `--max-batch-size 16 --max-new-tokens 256 --context-window 2048`\n  - `--enable-cors --cors-origins *`\n  - `--log-level info` (honors `logging.yaml` if present)\n\n- Env vars:\n  - `AIOS_MODEL_PATH`, `AIOS_TOKENIZER_PATH`, `AIOS_DEVICE`, `AIOS_MAX_BATCH_SIZE`, `AIOS_MAX_NEW_TOKENS`, `AIOS_CORS_ORIGINS`\n\n- YAML (optional): `config/default.yaml` \u2192 `serve.hrm` section\n\n### CLI: serve commands\n\n- Primary: `aios serve hrm` (wired via existing `aios.cli.aios`)\n  - Example (Windows PowerShell):\n    - `aios serve hrm --host 0.0.0.0 --port 8000 --model-path artifacts/brains/actv1/actv1_student.pt --tokenizer-path artifacts/hf_implant/tokenizers/base --device cpu`\n  - Example (module invocation):\n    - `.venv\\Scripts\\python.exe -m aios.serve.hrm_server --host 0.0.0.0 --port 8000`\n\n- Admin helpers (optional):\n  - `aios serve hrm --print-config`\n  - `aios serve hrm --dry-run` (load-only, no HTTP)\n\n### Minimal GUI (optional) for manual testing\n\n- Choice: Gradio (simpler) or Streamlit\n- Usage intent: quick sanity checks by product/QA without curl or code\n- Proposed: `aios serve hrm --ui` opens a small panel:\n  - Input textarea for prompt, sliders for `max_tokens`, `temperature`, `top_p`\n  - Buttons \"Generate\" and \"Score loglikelihood\"\n  - Display output text and usage metrics\n\n- Local run (Streamlit example):\n  - `python -m aios.serve.hrm_ui --server-url http://localhost:8000`\n\nNote: The UI is helpful but not required for acceptance of PF-003. If code is deferred, include only docs and a future task for `hrm_ui.py`.\n\n### Docker and containerization\n\n- Dockerfile target: `hrm-serve` (multi-stage). Example build and run on Windows PowerShell:\n\n```pwsh\n# Build\ndocker build -t aios/hrm-serve:local --target hrm-serve .\n\n# Run (CPU)\ndocker run --rm -p 8000:8000 ^\n  -e AIOS_MODEL_PATH=artifacts/brains/actv1/actv1_student.pt ^\n  -e AIOS_TOKENIZER_PATH=artifacts/hf_implant/tokenizers/base ^\n  aios/hrm-serve:local\n\n# Run (CUDA) \u2013 requires NVIDIA Container Toolkit\ndocker run --rm -p 8000:8000 --gpus all ^\n  -e AIOS_DEVICE=cuda ^\n  aios/hrm-serve:local\n```\n\n- docker-compose service snippet:\n\n```yaml\nservices:\n  hrm:\n    image: aios/hrm-serve:local\n    ports: [\"8000:8000\"]\n    environment:\n      AIOS_MODEL_PATH: artifacts/brains/actv1/actv1_student.pt\n      AIOS_TOKENIZER_PATH: artifacts/hf_implant/tokenizers/base\n      AIOS_DEVICE: cpu\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - capabilities: [gpu]\n```\n\n- Helper scripts to add:\n  - `scripts/run_hrm_server.ps1`\n  - `scripts/run_hrm_server.sh`\n\n### Baseline serving (A/B) with vLLM and TGI\n\n- vLLM (HF checkpoint):\n\n```pwsh\ndocker run --rm -p 8001:8000 ^\n  -v $PWD\\artifacts\\hf_implant\\base_model:/model ^\n  vllm/vllm-openai:latest ^\n  --model /model\n```\n\nHit using OpenAI-compatible API:\n\n```pwsh\ncurl -s http://localhost:8001/v1/completions -H \"Content-Type: application/json\" -d '{\n  \"model\":\"local\",\n  \"prompt\":\"Hello\",\n  \"max_tokens\":16\n}' | jq .\n```\n\n- Text Generation Inference (TGI):\n\n```pwsh\ndocker run --rm -p 8002:80 ^\n  -v $PWD\\artifacts\\hf_implant\\base_model:/data ^\n  ghcr.io/huggingface/text-generation-inference:latest ^\n  --model-id /data\n```\n\nHit using TGI API:\n\n```pwsh\ncurl -s http://localhost:8002/generate -H \"Content-Type: application/json\" -d '{\n  \"inputs\": \"Hello\",\n  \"parameters\": {\"max_new_tokens\": 16, \"temperature\": 0.7, \"top_p\": 0.9}\n}' | jq .\n```\n\n- Mapping to our contract\n  - Our `/generate` \u2192 vLLM `/v1/completions` or OpenAI Chat; TGI `/generate`\n  - For A/B, normalize fields: `max_tokens \u2194 max_new_tokens`, `temperature`, `top_p`\n\n### Observability and security\n\n- Logging: use `logging.yaml` if present; otherwise default to INFO with request/response IDs (omit bodies in prod logs)\n- Metrics: optional `/metrics` (Prometheus) for request counts/latency/tokens\n- Tracing: optional OpenTelemetry if configured\n- CORS: `--enable-cors` and whitelist origins\n- Auth (optional): bearer token via `Authorization: Bearer <token>`; deny when missing\n\n### Testing, acceptance, and checklists\n\nFunctional quick test (CPU):\n\n```pwsh\n# 1) Start server (example)\naios serve hrm --device cpu --port 8000 --model-path artifacts/brains/actv1/actv1_student.pt --tokenizer-path artifacts/hf_implant/tokenizers/base\n\n# 2) Generate two prompts\ncurl -s http://localhost:8000/generate -H \"Content-Type: application/json\" -d '{\n  \"prompts\": [\"Hello\", \"Once upon a time\"],\n  \"max_tokens\": 8,\n  \"temperature\": 0.7,\n  \"top_p\": 0.9\n}' | jq .\n\n# 3) Loglikelihood\ncurl -s http://localhost:8000/loglikelihood -H \"Content-Type: application/json\" -d '{\n  \"prompts\": [\"Hello\"],\n  \"continuations\": [\" world\"]\n}' | jq .\n```\n\nReadiness checklist\n\n- [ ] Server starts and `/readyz` returns ready within 60s\n- [ ] `/generate` returns completions for 2 prompts under 2s on CPU (sample model)\n- [ ] `/loglikelihood` returns values and shape matches inputs\n- [ ] Handles batch of 16 prompts without OOM; memory stable over 5 runs\n- [ ] Error cases return 400/422 with clear message\n- [ ] Logs include request IDs and timing\n- [ ] Docker image builds and runs locally\n- [ ] Optional UI launches and can call server\n- [ ] A/B against vLLM/TGI produces comparable outputs with same seeds\n\nLoad and reliability\n\n- [ ] Sustains 10 RPS with batch size 8 on CPU sample model (target; adjust per hardware)\n- [ ] Backpressure: requests rejected with 429 when queue is full (if enabled)\n- [ ] Graceful shutdown drains in-flight requests\n\n### Production runbook (starter)\n\n- Startup\n  - Verify drivers (CUDA) or plan CPU\n  - Warmup: send a 1-token request to pre-JIT kernels\n  - Confirm `/readyz` and sample `/generate`\n\n- Scaling\n  - Horizontal: run multiple replicas behind a load balancer; sticky sessions not required\n  - Vertical: tune `max_batch_size`, `max_new_tokens`, and context window to fit memory\n\n- Troubleshooting\n  - 500 at startup: verify paths for model/tokenizer; run with `--dry-run`\n  - CUDA OOM: reduce batch size or `max_new_tokens`; ensure `torch.cuda.empty_cache()` between runs if needed\n  - Slow responses: disable AMP on CPU; pin threads (`OMP_NUM_THREADS`)\n  - CORS blocked: set `--enable-cors --cors-origins *` for dev only\n\n### Triton backend (design notes, out of scope)\n\n- Shape the HRM inference as a stateless backend with request batching and token cache\n- Inputs: token IDs, attention mask, decode params; Outputs: next tokens/logprobs\n- Consider KV cache management and paged attention for large contexts\n\n---\n\n### Implementation steps\n1) HRM inference wrapper\n- Reuse tokenizer loading from `train_actv1.py` helpers.\n- Load `actv1_student.pt` and implement a simple forward for greedy/sampled decoding.\n- Add batching and `torch.no_grad()`; AMP optional.\n\n2) FastAPI app\n- Create endpoints per contract; validate inputs; return JSON.\n- Add health endpoint and basic logging.\n\n3) Packaging\n- Add new Docker target with a slim runtime (CUDA base optional).\n- Provide PowerShell and Bash helpers to run locally.\n\n4) Baseline docs\n- Document how to start vLLM/TGI for a matching HF model and how to hit similar endpoints for A/B.\n\n### Testing and acceptance criteria\n\n- Local run: Start server, send `/generate` with 2 prompts, receive 2 completions within reasonable latency on CPU/GPU.\n- Error handling: Invalid inputs return 400 with clear messages.\n- Load: Handle batch of 16 prompts without crash; memory stable.\n\n### Risks and mitigations\n\n- HRM decoding path may need custom halting logic \u2192 start with simple decode, iterate.\n- Windows GPU drivers: Recommend WSL2 or use CPU fallback for local dev.\n\n### Milestones\n\n- M1 (1\u20132 days): Minimal server + local run; sample client.\n- M2 (1 day): Docker target and docs; baseline serving notes.\n", "tags": ["cli", "gui", "hrm", "training"], "headings": [{"line": 0, "text": "PF-003: Serving and inference integration (HRM + baselines)"}, {"line": 2, "text": "Summary"}, {"line": 6, "text": "Motivation"}, {"line": 11, "text": "Scope"}, {"line": 21, "text": "Integration points (repo)"}, {"line": 27, "text": "API contract (v1)"}, {"line": 39, "text": "Comprehensive guide and checklist"}, {"line": 43, "text": "Deliverables"}, {"line": 52, "text": "Architecture overview"}, {"line": 59, "text": "Data models and API details"}, {"line": 89, "text": "HRM inference wrapper design"}, {"line": 115, "text": "Configuration model"}, {"line": 133, "text": "CLI: serve commands"}, {"line": 145, "text": "Minimal GUI (optional) for manual testing"}, {"line": 159, "text": "Docker and containerization"}, {"line": 164, "text": "Build"}, {"line": 167, "text": "Run (CPU)"}, {"line": 173, "text": "Run (CUDA) \u2013 requires NVIDIA Container Toolkit"}, {"line": 201, "text": "Baseline serving (A/B) with vLLM and TGI"}, {"line": 244, "text": "Observability and security"}, {"line": 252, "text": "Testing, acceptance, and checklists"}, {"line": 257, "text": "1) Start server (example)"}, {"line": 260, "text": "2) Generate two prompts"}, {"line": 268, "text": "3) Loglikelihood"}, {"line": 293, "text": "Production runbook (starter)"}, {"line": 310, "text": "Triton backend (design notes, out of scope)"}, {"line": 318, "text": "Implementation steps"}, {"line": 335, "text": "Testing and acceptance criteria"}, {"line": 341, "text": "Risks and mitigations"}, {"line": 346, "text": "Milestones"}]}, {"path": "planned_features/INTERNATIONALIZATION_LOCALIZATION.md", "content": "# Internationalization and Localization (i18n/L10n)\n\n**Status:** \ud83d\udccb Planned  \n**Priority:** Medium  \n**Category:** User Experience & Accessibility  \n**Created:** November 9, 2025  \n**Target Languages:** Spanish, Portuguese, French, German, Italian, Chinese, Japanese, Arabic, Hindi\n\n---\n\n## Overview\n\nImplement comprehensive internationalization (i18n) and localization (L10n) support to make AI-OS accessible to non-English speaking users worldwide. This includes translating the GUI, CLI, and documentation into 9 additional languages.\n\n**Current State:** English-only hardcoded strings throughout codebase  \n**Goal:** Multi-language support with runtime language selection and culturally appropriate formatting\n\n---\n\n## Motivation\n\n### Business Case\n- **Market Expansion:** Enable adoption in non-English speaking markets\n- **Accessibility:** Remove language barriers for international developers and researchers\n- **Community Growth:** Foster global contributor community\n- **Competitive Advantage:** Most AI/ML tools remain English-centric\n\n### Target Language Justification\n1. **Spanish (es_ES)** - 500M+ speakers, Latin America & Spain markets\n2. **Portuguese (pt_BR)** - 250M+ speakers, growing Brazilian tech sector\n3. **French (fr_FR)** - 300M+ speakers, European & African markets\n4. **German (de_DE)** - 100M+ speakers, strong German engineering community\n5. **Italian (it_IT)** - 65M+ speakers, Italian research institutions\n6. **Chinese (zh_CN)** - 1.4B+ speakers, massive Chinese AI/ML community\n7. **Japanese (ja_JP)** - 125M+ speakers, advanced Japanese tech sector\n8. **Arabic (ar_SA)** - 400M+ speakers, growing Middle East tech markets\n9. **Hindi (hi_IN)** - 600M+ speakers, booming Indian tech sector\n\n---\n\n## Technical Scope\n\n### Components Requiring Localization\n\n#### 1. GUI (Tkinter Interface)\n**Files Affected:** ~50-60 Python files in `src/aios/gui/`\n\n**String Categories:**\n- Tab names (Chat, Brains, Datasets, HRM Training, Evaluation, Resources, MCP & Tools, Settings, Debug, Help)\n- Button labels (Add Goal, Export CSV, Export JSON, Load Brain, Start Training, etc.)\n- Dialog titles and messages (Checkpoint Found, Resume Training, Error, Success, etc.)\n- Status messages (Ready, Loading, Processing, Complete, etc.)\n- Tooltips and help text\n- Error messages and warnings\n- Form field labels\n- Tree/table column headers\n- Menu items\n\n**Estimated String Count:** ~800-1000 unique strings\n\n**Key Files:**\n```\nsrc/aios/gui/app/ui_setup.py\nsrc/aios/gui/components/brains_panel/\nsrc/aios/gui/components/chat_panel.py\nsrc/aios/gui/components/datasets_panel/\nsrc/aios/gui/components/hrm_training/\nsrc/aios/gui/components/evaluation_panel/\nsrc/aios/gui/components/resources_panel/\nsrc/aios/gui/components/mcp_panel/\nsrc/aios/gui/components/settings_panel/\nsrc/aios/gui/components/debug_panel.py\nsrc/aios/gui/dialogs/\n```\n\n#### 2. CLI (Command-Line Interface)\n**Files Affected:** ~15-20 Python files in `src/aios/cli/`\n\n**String Categories:**\n- Command descriptions and help text\n- Argument/option descriptions\n- Error messages\n- Success/status messages\n- Interactive prompts (in `core_cli.py ui()` function)\n- Progress indicators\n- Table headers and formatted output\n\n**Estimated String Count:** ~500-700 unique strings\n\n**Key Files:**\n```\nsrc/aios/cli/aios.py\nsrc/aios/cli/core_cli.py\nsrc/aios/cli/hrm_cli.py\nsrc/aios/cli/hrm_hf_cli.py\nsrc/aios/cli/eval_cli.py\nsrc/aios/cli/datasets/\nsrc/aios/cli/optimization_cli.py\nsrc/aios/cli/modelcard_cli.py\n```\n\n#### 3. Documentation\n**Files Affected:** All markdown files in `docs/`\n\n**Content:**\n- README.md\n- Installation guides\n- User guides\n- API documentation\n- Contributing guidelines\n\n**Estimated Page Count:** ~50-100 documentation pages\n\n**Strategy:** Create separate language subdirectories:\n```\ndocs/\n  \u251c\u2500\u2500 en/  (English - default)\n  \u251c\u2500\u2500 es/  (Spanish)\n  \u251c\u2500\u2500 pt/  (Portuguese)\n  \u251c\u2500\u2500 fr/  (French)\n  \u251c\u2500\u2500 de/  (German)\n  \u251c\u2500\u2500 it/  (Italian)\n  \u251c\u2500\u2500 zh/  (Chinese)\n  \u251c\u2500\u2500 ja/  (Japanese)\n  \u251c\u2500\u2500 ar/  (Arabic)\n  \u2514\u2500\u2500 hi/  (Hindi)\n```\n\n---\n\n## Implementation Plan\n\n### Phase 1: Infrastructure Setup (Week 1-2)\n\n#### 1.1 Choose i18n Framework\n**Recommended:** Python `gettext` + `babel` for compilation\n\n**Rationale:**\n- Standard Python i18n solution\n- Excellent tooling (pybabel)\n- Wide community support\n- Works well with both GUI (Tkinter) and CLI (Typer)\n\n**Dependencies to Add:**\n```toml\n[project.optional-dependencies]\ni18n = [\n  \"babel>=2.14.0\",     # i18n utilities and message extraction\n  \"polib>=1.2.0\",      # .po file manipulation library\n]\n```\n\n#### 1.2 Create Directory Structure\n```\nsrc/aios/\n  \u251c\u2500\u2500 i18n/\n  \u2502   \u251c\u2500\u2500 __init__.py           # i18n initialization and utilities\n  \u2502   \u251c\u2500\u2500 locale_manager.py     # Runtime locale management\n  \u2502   \u2514\u2500\u2500 extract.cfg           # Babel extraction configuration\n  \u2514\u2500\u2500 locales/\n      \u251c\u2500\u2500 en_US/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u2502       \u251c\u2500\u2500 gui.po\n      \u2502       \u251c\u2500\u2500 cli.po\n      \u2502       \u2514\u2500\u2500 messages.po\n      \u251c\u2500\u2500 es_ES/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u251c\u2500\u2500 pt_BR/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u251c\u2500\u2500 fr_FR/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u251c\u2500\u2500 de_DE/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u251c\u2500\u2500 it_IT/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u251c\u2500\u2500 zh_CN/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u251c\u2500\u2500 ja_JP/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u251c\u2500\u2500 ar_SA/\n      \u2502   \u2514\u2500\u2500 LC_MESSAGES/\n      \u2514\u2500\u2500 hi_IN/\n          \u2514\u2500\u2500 LC_MESSAGES/\n```\n\n#### 1.3 Create i18n Utilities Module\n\n**File:** `src/aios/i18n/__init__.py`\n\n```python\n\"\"\"Internationalization support for AI-OS.\"\"\"\n\nfrom __future__ import annotations\n\nimport gettext\nimport locale\nimport os\nfrom pathlib import Path\nfrom typing import Optional\n\n# Default locale\nDEFAULT_LOCALE = \"en_US\"\n\n# Supported locales\nSUPPORTED_LOCALES = {\n    \"en_US\": \"English (United States)\",\n    \"es_ES\": \"Espa\u00f1ol (Espa\u00f1a)\",\n    \"pt_BR\": \"Portugu\u00eas (Brasil)\",\n    \"fr_FR\": \"Fran\u00e7ais (France)\",\n    \"de_DE\": \"Deutsch (Deutschland)\",\n    \"it_IT\": \"Italiano (Italia)\",\n    \"zh_CN\": \"\u4e2d\u6587 (\u7b80\u4f53)\",\n    \"ja_JP\": \"\u65e5\u672c\u8a9e (\u65e5\u672c)\",\n    \"ar_SA\": \"\u0627\u0644\u0639\u0631\u0628\u064a\u0629 (\u0627\u0644\u0633\u0639\u0648\u062f\u064a\u0629)\",\n    \"hi_IN\": \"\u0939\u093f\u0928\u094d\u0926\u0940 (\u092d\u093e\u0930\u0924)\",\n}\n\n# Global translation function\n_translate = None\n_current_locale = DEFAULT_LOCALE\n\n\ndef init_i18n(locale_code: Optional[str] = None) -> None:\n    \"\"\"Initialize i18n system with specified locale.\n    \n    Args:\n        locale_code: Locale code (e.g., 'es_ES'). If None, uses system locale.\n    \"\"\"\n    global _translate, _current_locale\n    \n    if locale_code is None:\n        # Try to detect system locale\n        try:\n            sys_locale = locale.getdefaultlocale()[0]\n            locale_code = sys_locale if sys_locale in SUPPORTED_LOCALES else DEFAULT_LOCALE\n        except Exception:\n            locale_code = DEFAULT_LOCALE\n    \n    # Validate locale\n    if locale_code not in SUPPORTED_LOCALES:\n        locale_code = DEFAULT_LOCALE\n    \n    _current_locale = locale_code\n    \n    # Set up gettext\n    locale_dir = Path(__file__).parent.parent / \"locales\"\n    \n    try:\n        translation = gettext.translation(\n            \"messages\",\n            localedir=str(locale_dir),\n            languages=[locale_code],\n            fallback=True\n        )\n        _translate = translation.gettext\n    except Exception:\n        # Fallback to no-op translation\n        _translate = lambda s: s\n\n\ndef _(message: str) -> str:\n    \"\"\"Translate a message to the current locale.\n    \n    Args:\n        message: Message to translate\n        \n    Returns:\n        Translated message\n    \"\"\"\n    if _translate is None:\n        init_i18n()\n    return _translate(message)\n\n\ndef get_current_locale() -> str:\n    \"\"\"Get the current locale code.\"\"\"\n    return _current_locale\n\n\ndef get_supported_locales() -> dict[str, str]:\n    \"\"\"Get dict of supported locale codes to display names.\"\"\"\n    return SUPPORTED_LOCALES.copy()\n\n\ndef set_locale(locale_code: str) -> bool:\n    \"\"\"Set the current locale.\n    \n    Args:\n        locale_code: Locale code to set\n        \n    Returns:\n        True if successful, False otherwise\n    \"\"\"\n    if locale_code not in SUPPORTED_LOCALES:\n        return False\n    \n    init_i18n(locale_code)\n    return True\n```\n\n#### 1.4 Create Babel Configuration\n\n**File:** `src/aios/i18n/extract.cfg`\n\n```ini\n[python: **.py]\nencoding = utf-8\n\n[javascript: **.js]\nencoding = utf-8\n```\n\n#### 1.5 Create Extraction Script\n\n**File:** `scripts/extract_translations.py`\n\n```python\n#!/usr/bin/env python3\n\"\"\"Extract translatable strings from source code.\"\"\"\n\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef main():\n    \"\"\"Extract strings and create .pot template.\"\"\"\n    project_root = Path(__file__).parent.parent\n    src_dir = project_root / \"src\" / \"aios\"\n    locale_dir = src_dir / \"locales\"\n    pot_file = locale_dir / \"messages.pot\"\n    \n    # Ensure locale directory exists\n    locale_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract strings\n    cmd = [\n        \"pybabel\",\n        \"extract\",\n        \"-F\", str(src_dir / \"i18n\" / \"extract.cfg\"),\n        \"-o\", str(pot_file),\n        \"-k\", \"_\",  # Translation function name\n        \"--project=AI-OS\",\n        \"--version=1.0.0\",\n        \"--copyright-holder=Wulfic\",\n        str(src_dir),\n    ]\n    \n    print(f\"Extracting translatable strings...\")\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    \n    if result.returncode == 0:\n        print(f\"\u2713 Extracted to {pot_file}\")\n        print(f\"  {result.stdout.strip()}\")\n    else:\n        print(f\"\u2717 Extraction failed:\")\n        print(result.stderr)\n        return 1\n    \n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n```\n\n#### 1.6 Create Compilation Script\n\n**File:** `scripts/compile_translations.py`\n\n```python\n#!/usr/bin/env python3\n\"\"\"Compile .po files to .mo files for runtime use.\"\"\"\n\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef main():\n    \"\"\"Compile all .po files to .mo files.\"\"\"\n    project_root = Path(__file__).parent.parent\n    locale_dir = project_root / \"src\" / \"aios\" / \"locales\"\n    \n    compiled_count = 0\n    error_count = 0\n    \n    # Find all .po files\n    for po_file in locale_dir.rglob(\"*.po\"):\n        mo_file = po_file.with_suffix(\".mo\")\n        \n        cmd = [\n            \"pybabel\",\n            \"compile\",\n            \"-i\", str(po_file),\n            \"-o\", str(mo_file),\n        ]\n        \n        print(f\"Compiling {po_file.relative_to(project_root)}...\")\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        \n        if result.returncode == 0:\n            compiled_count += 1\n        else:\n            error_count += 1\n            print(f\"  \u2717 Error: {result.stderr}\")\n    \n    print(f\"\\n\u2713 Compiled {compiled_count} translation(s)\")\n    if error_count > 0:\n        print(f\"\u2717 {error_count} error(s)\")\n        return 1\n    \n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n```\n\n---\n\n### Phase 2: Code Refactoring (Week 3-5)\n\n#### 2.1 GUI Refactoring Strategy\n\n**Pattern:**\n```python\n# Before\nttk.Label(frame, text=\"Model:\").pack()\nttk.Button(frame, text=\"Export CSV\", command=callback)\nself.status_label = ttk.Label(frame, text=\"Ready\")\n\n# After\nfrom aios.i18n import _\nttk.Label(frame, text=_(\"Model:\")).pack()\nttk.Button(frame, text=_(\"Export CSV\"), command=callback)\nself.status_label = ttk.Label(frame, text=_(\"Ready\"))\n```\n\n**Files to Refactor (Priority Order):**\n1. \u2705 Main UI structure (`src/aios/gui/app/ui_setup.py`)\n2. \u2705 Brains panel (`src/aios/gui/components/brains_panel/`)\n3. \u2705 Datasets panel (`src/aios/gui/components/datasets_panel/`)\n4. \u2705 HRM Training panel (`src/aios/gui/components/hrm_training/`)\n5. \u2705 Evaluation panel (`src/aios/gui/components/evaluation_panel/`)\n6. \u2705 Chat panel (`src/aios/gui/components/chat_panel.py`)\n7. \u2705 Settings panel (`src/aios/gui/components/settings_panel/`)\n8. \u2705 All dialogs (`src/aios/gui/dialogs/`)\n9. \u2705 Status bar and tooltips\n\n**Gotchas:**\n- Dynamic strings with formatting: Use `_(\"Score: {score}\").format(score=value)`\n- Pluralization: Use `ngettext()` for singular/plural forms\n- Tooltips: Extract to separate translation calls\n\n#### 2.2 CLI Refactoring Strategy\n\n**Pattern:**\n```python\n# Before\n@app.command(\"train\")\ndef train_command(\n    model: str = typer.Option(..., \"--model\", help=\"Model name or path\"),\n):\n    \"\"\"Train a model.\"\"\"\n    print(\"Training started...\")\n\n# After\nfrom aios.i18n import _\n\n@app.command(\"train\")\ndef train_command(\n    model: str = typer.Option(..., \"--model\", help=_(\"Model name or path\")),\n):\n    \"\"\"Train a model.\"\"\"  # Docstrings extracted separately\n    print(_(\"Training started...\"))\n```\n\n**Special Considerations for CLI:**\n- Help text translation affects `--help` output\n- Rich formatted output (tables, progress bars)\n- Error messages need careful context\n- Interactive prompts in `ui()` function\n\n#### 2.3 String Extraction Guidelines\n\n**DO:**\n- \u2705 Extract user-facing messages\n- \u2705 Extract button/label text\n- \u2705 Extract error messages\n- \u2705 Extract help text\n- \u2705 Use context comments for ambiguous strings\n\n**DON'T:**\n- \u274c Translate log messages (keep English for debugging)\n- \u274c Translate internal identifiers\n- \u274c Translate file paths or system commands\n- \u274c Translate variable names\n\n**Context Comments:**\n```python\n# Translator comment for clarity\n# Translators: This appears in the training progress dialog\nlabel.config(text=_(\"Training in progress...\"))\n```\n\n---\n\n### Phase 3: Translation File Generation (Week 6)\n\n#### 3.1 Extract Strings\n```bash\npython scripts/extract_translations.py\n```\n\nThis creates `src/aios/locales/messages.pot` template file.\n\n#### 3.2 Initialize Language Files\n\nFor each target language:\n```bash\npybabel init -i src/aios/locales/messages.pot \\\n             -d src/aios/locales \\\n             -l es_ES\n\npybabel init -i src/aios/locales/messages.pot \\\n             -d src/aios/locales \\\n             -l pt_BR\n\n# ... repeat for all languages\n```\n\nThis creates `.po` files with message IDs (msgid) and empty translations (msgstr).\n\n#### 3.3 Translation File Structure\n\n**Example: `src/aios/locales/es_ES/LC_MESSAGES/messages.po`**\n\n```po\n# Spanish translations for AI-OS\n# Copyright (C) 2025 Wulfic\n# This file is distributed under the same license as the AI-OS package.\n\nmsgid \"\"\nmsgstr \"\"\n\"Project-Id-Version: AI-OS 1.0.0\\n\"\n\"Report-Msgid-Bugs-To: \\n\"\n\"POT-Creation-Date: 2025-11-09 10:00+0000\\n\"\n\"Language: es_ES\\n\"\n\"MIME-Version: 1.0\\n\"\n\"Content-Type: text/plain; charset=UTF-8\\n\"\n\"Content-Transfer-Encoding: 8bit\\n\"\n\n#: src/aios/gui/app/ui_setup.py:63\nmsgid \"Chat\"\nmsgstr \"Chat\"\n\n#: src/aios/gui/app/ui_setup.py:64\nmsgid \"Brains\"\nmsgstr \"Cerebros\"\n\n#: src/aios/gui/app/ui_setup.py:65\nmsgid \"Datasets\"\nmsgstr \"Conjuntos de datos\"\n\n#: src/aios/gui/app/ui_setup.py:66\nmsgid \"HRM Training\"\nmsgstr \"Entrenamiento HRM\"\n\n#: src/aios/gui/components/brains_panel/panel_main.py:123\nmsgid \"Load Brain\"\nmsgstr \"Cargar cerebro\"\n\n#: src/aios/gui/components/brains_panel/panel_main.py:145\nmsgid \"Model:\"\nmsgstr \"Modelo:\"\n\n#: src/aios/gui/components/brains_panel/panel_main.py:201\nmsgid \"Ready\"\nmsgstr \"Listo\"\n```\n\n---\n\n### Phase 4: Translation Work (Week 7-18)\n\n#### 4.1 Translation Approaches\n\n**Option A: Professional Translation Service**\n- **Cost:** $0.10-0.30 per word \u00d7 ~10,000 words = $1,000-3,000 per language\n- **Timeline:** 1-2 weeks per language\n- **Quality:** High, professional\n- **Services:** Gengo, OneHourTranslation, Smartling\n\n**Option B: Community Translation**\n- **Cost:** Free (contributor time)\n- **Timeline:** 2-4 months per language (variable)\n- **Quality:** Variable, requires review\n- **Platform:** Crowdin, Weblate, or GitHub-based workflow\n\n**Option C: AI-Assisted + Human Review**\n- **Cost:** Low ($100-500 per language for review)\n- **Timeline:** 2-4 weeks per language\n- **Quality:** Good with proper review\n- **Process:**\n  1. Use GPT-4/Claude for initial translation\n  2. Native speaker review and correction\n  3. Context validation\n\n**Recommended:** Option C for speed and cost-effectiveness\n\n#### 4.2 Translation Priority Order\n\n**Tier 1 (Weeks 7-10):** Western European languages\n1. Spanish (es_ES) - Week 7\n2. French (fr_FR) - Week 8\n3. Portuguese (pt_BR) - Week 9\n4. German (de_DE) - Week 10\n5. Italian (it_IT) - Week 10\n\n**Tier 2 (Weeks 11-14):** East Asian languages\n6. Chinese Simplified (zh_CN) - Week 11-12\n7. Japanese (ja_JP) - Week 13-14\n\n**Tier 3 (Weeks 15-18):** Complex scripts\n8. Arabic (ar_SA) - Week 15-16\n9. Hindi (hi_IN) - Week 17-18\n\n#### 4.3 Translation Quality Checklist\n\nFor each language:\n- [ ] All strings translated (no empty msgstr)\n- [ ] Technical terminology consistent\n- [ ] Proper capitalization and punctuation\n- [ ] Formatting placeholders preserved (`{0}`, `%s`, etc.)\n- [ ] Plural forms correctly implemented\n- [ ] Native speaker reviewed\n- [ ] Context-appropriate tone (formal vs. informal)\n- [ ] UI tested with translations loaded\n\n#### 4.4 Special Translation Considerations\n\n**German:**\n- Compound words are longer (30-40% more space)\n- Example: \"Training progress\" \u2192 \"Trainingsfortschritt\"\n- May need to adjust widget widths\n\n**Chinese/Japanese:**\n- No spaces between words\n- Vertical text support (not needed for this app)\n- Font requirements: Need CJK-compatible fonts\n\n**Arabic:**\n- Right-to-left (RTL) text direction\n- Requires significant UI layout changes\n- Numbers may be displayed left-to-right within RTL text\n- Consider deferring to later phase\n\n**Hindi:**\n- Devanagari script\n- Font rendering support needed\n- May need line-height adjustments\n\n---\n\n### Phase 5: UI Layout Adjustments (Week 19-20)\n\n#### 5.1 Dynamic Widget Sizing\n\n**Problem:** Different languages have different text lengths\n\n**Solution:** Use dynamic sizing instead of fixed widths\n\n```python\n# Before\nentry = ttk.Entry(frame, width=20)\n\n# After\nentry = ttk.Entry(frame)  # Let it size naturally\nentry.pack(fill=\"x\", expand=True)\n```\n\n#### 5.2 Text Overflow Handling\n\n**Strategies:**\n- Use `wraplength` for labels that might be long\n- Add horizontal scrollbars where appropriate\n- Increase minimum window size if needed\n- Use tooltips for truncated text\n\n```python\nlabel = ttk.Label(\n    frame,\n    text=_(\"Very long description text...\"),\n    wraplength=400  # Wrap at 400 pixels\n)\n```\n\n#### 5.3 RTL Support (Arabic)\n\n**Challenges:**\n- Tkinter has limited RTL support\n- May need custom RTL-aware widgets\n- Consider using `pack(side=\"right\")` for Arabic layout\n\n**Decision:** Phase 1 implementation will be LTR-only. RTL support deferred to Phase 2.\n\n#### 5.4 Font Support\n\n**Ensure proper fonts installed:**\n- Windows: System fonts usually sufficient\n- Linux: May need to install language packs\n  ```bash\n  # For CJK\n  sudo apt-get install fonts-noto-cjk\n  \n  # For Arabic\n  sudo apt-get install fonts-noto-nastaliq-urdu\n  \n  # For Hindi\n  sudo apt-get install fonts-noto-devanagari\n  ```\n\n---\n\n### Phase 6: Configuration and Runtime Selection (Week 21)\n\n#### 6.1 Add Locale Configuration\n\n**File:** `config/default.yaml`\n\n```yaml\n# Internationalization settings\ni18n:\n  # Default locale (auto-detected if not set)\n  locale: null  # Options: en_US, es_ES, pt_BR, fr_FR, de_DE, it_IT, zh_CN, ja_JP, ar_SA, hi_IN\n  \n  # Fallback locale if selected locale unavailable\n  fallback_locale: en_US\n```\n\n#### 6.2 GUI Language Selector\n\nAdd to Settings panel:\n\n```python\n# In src/aios/gui/components/settings_panel/panel_main.py\n\nfrom aios.i18n import _, get_supported_locales, get_current_locale\n\nclass SettingsPanel:\n    def __init__(self, ...):\n        # ... existing code ...\n        \n        # Language selection\n        lang_frame = ttk.LabelFrame(self, text=_(\"Language\"), padding=10)\n        lang_frame.pack(fill=\"x\", padx=10, pady=5)\n        \n        ttk.Label(lang_frame, text=_(\"Interface Language:\")).pack(anchor=\"w\")\n        \n        self.locale_var = tk.StringVar(value=get_current_locale())\n        locale_combo = ttk.Combobox(\n            lang_frame,\n            textvariable=self.locale_var,\n            values=list(get_supported_locales().keys()),\n            state=\"readonly\"\n        )\n        locale_combo.pack(fill=\"x\", pady=5)\n        \n        ttk.Button(\n            lang_frame,\n            text=_(\"Apply Language (Requires Restart)\"),\n            command=self._on_language_change\n        ).pack()\n        \n        self.locale_status = ttk.Label(lang_frame, text=\"\", foreground=\"blue\")\n        self.locale_status.pack()\n    \n    def _on_language_change(self):\n        \"\"\"Handle language change.\"\"\"\n        new_locale = self.locale_var.get()\n        \n        # Save to config\n        # ... save logic ...\n        \n        # Show restart message\n        self.locale_status.config(\n            text=_(\"Language will change after restart\"),\n            foreground=\"orange\"\n        )\n```\n\n#### 6.3 CLI Language Selection\n\n```bash\n# Set via environment variable\nexport AIOS_LOCALE=es_ES\naios gui\n\n# Or via command line flag\naios --locale es_ES gui\n\n# Or set in config file\naios gui  # Uses config/default.yaml setting\n```\n\n---\n\n### Phase 7: Testing and QA (Week 22-24)\n\n#### 7.1 Automated Testing\n\n**Test Coverage:**\n- [ ] i18n initialization works for all locales\n- [ ] Translation fallback works (missing translations \u2192 English)\n- [ ] String formatting with placeholders works\n- [ ] Plural forms work correctly\n- [ ] Language switching doesn't break application\n\n**Test File:** `tests/test_i18n.py`\n\n```python\nimport pytest\nfrom aios.i18n import init_i18n, _, get_supported_locales\n\ndef test_init_default():\n    \"\"\"Test default initialization.\"\"\"\n    init_i18n()\n    assert _(\"Ready\") == \"Ready\"  # English default\n\ndef test_init_spanish():\n    \"\"\"Test Spanish initialization.\"\"\"\n    init_i18n(\"es_ES\")\n    # Assumes Spanish translation exists\n    result = _(\"Ready\")\n    assert result == \"Listo\" or result == \"Ready\"  # Allow fallback\n\ndef test_all_locales_supported():\n    \"\"\"Test all supported locales can be initialized.\"\"\"\n    for locale_code in get_supported_locales():\n        init_i18n(locale_code)\n        # Should not raise exception\n\ndef test_invalid_locale_fallback():\n    \"\"\"Test invalid locale falls back to default.\"\"\"\n    init_i18n(\"xx_XX\")\n    assert _(\"Ready\") == \"Ready\"\n\ndef test_formatting():\n    \"\"\"Test string formatting with translations.\"\"\"\n    init_i18n()\n    # Assuming translation exists\n    msg = _(\"Score: {score}\").format(score=95)\n    assert \"95\" in msg\n```\n\n#### 7.2 Manual Testing Checklist\n\nFor each language:\n\n**GUI Testing:**\n- [ ] All tabs display translated text\n- [ ] All buttons have translated labels\n- [ ] All dialogs show translated messages\n- [ ] Tooltips are translated\n- [ ] No text overflow/truncation\n- [ ] Status messages update correctly\n- [ ] Error messages are clear\n- [ ] Help text is accurate\n\n**CLI Testing:**\n- [ ] `--help` shows translated text\n- [ ] Error messages are translated\n- [ ] Interactive prompts are translated\n- [ ] Output formatting is correct\n- [ ] Progress indicators work\n\n**Functional Testing:**\n- [ ] Application functionality unchanged\n- [ ] No crashes from translation loading\n- [ ] Language switch persists across restarts\n- [ ] Fallback to English works if translation missing\n\n#### 7.3 Native Speaker Review\n\n**Requirements:**\n- Native speaker fluency\n- Technical/ML domain knowledge preferred\n- Access to running application\n\n**Review Checklist:**\n- [ ] Translation accuracy\n- [ ] Natural phrasing (not literal translation)\n- [ ] Consistent terminology\n- [ ] Appropriate formality level\n- [ ] No cultural insensitivity\n- [ ] Technical terms correctly used\n- [ ] Grammar and spelling correct\n\n---\n\n## Deployment Strategy\n\n### Build Process Updates\n\n**Update:** `pyproject.toml`\n\n```toml\n[project.optional-dependencies]\ni18n = [\n  \"babel>=2.14.0\",\n  \"polib>=1.2.0\",\n]\n```\n\n**Update:** Build scripts to compile translations\n\n```bash\n# In build process (CI/CD)\npython scripts/compile_translations.py\n```\n\n### Packaging\n\n**Include compiled .mo files:**\n```\nsrc/aios/locales/*/LC_MESSAGES/*.mo\n```\n\n**Update MANIFEST.in:**\n```\ninclude src/aios/locales/*/LC_MESSAGES/*.mo\n```\n\n---\n\n## Maintenance Plan\n\n### Ongoing Translation Updates\n\n**When adding new features:**\n1. Use `_()` for all user-facing strings\n2. Run `python scripts/extract_translations.py`\n3. Update .po files: `pybabel update -i messages.pot -d locales`\n4. Translate new strings\n5. Compile: `python scripts/compile_translations.py`\n\n### Translation Contributors\n\n**Set up community contribution workflow:**\n1. Use Weblate or Crowdin for collaborative translation\n2. Or: Accept .po file PRs on GitHub\n3. Assign language maintainers for each locale\n4. Regular translation reviews (quarterly)\n\n---\n\n## Success Metrics\n\n### Quantitative Metrics\n- [ ] 100% of GUI strings translated in all 9 languages\n- [ ] 100% of CLI help text translated in all 9 languages\n- [ ] 95%+ of documentation translated in priority languages (ES, FR, ZH, JA)\n- [ ] < 1% translation-related bug reports\n- [ ] Language switching works in < 5 seconds (app restart)\n\n### Qualitative Metrics\n- [ ] Native speaker approval rating > 4/5\n- [ ] No significant user complaints about translation quality\n- [ ] Positive feedback from international user community\n- [ ] Increased non-English GitHub issues/discussions\n\n---\n\n## Risks and Mitigation\n\n### Risk 1: Translation Quality\n**Risk:** Poor translations create bad user experience  \n**Mitigation:** Native speaker review, professional translators for tier 1\n\n### Risk 2: Incomplete Translations\n**Risk:** Missing strings show English text  \n**Mitigation:** Fallback to English, translation coverage tests\n\n### Risk 3: UI Layout Breaks\n**Risk:** Longer text breaks layouts  \n**Mitigation:** Dynamic sizing, manual testing, layout guidelines\n\n### Risk 4: Maintenance Burden\n**Risk:** Keeping translations updated with new features  \n**Mitigation:** Automated extraction, clear contributor guidelines, language maintainers\n\n### Risk 5: RTL Complexity (Arabic)\n**Risk:** RTL support is technically complex in Tkinter  \n**Mitigation:** Phase 1: LTR-only, Phase 2: RTL investigation/implementation\n\n---\n\n## Alternative Approaches Considered\n\n### 1. English-Only with External Translation Tools\n**Pros:** No development work  \n**Cons:** Poor UX, unreliable, no control over quality  \n**Decision:** Rejected - not professional\n\n### 2. Machine Translation at Runtime\n**Pros:** No translation work needed  \n**Cons:** Requires internet, latency, poor quality, privacy concerns  \n**Decision:** Rejected - unsuitable for professional tool\n\n### 3. Partial Localization (GUI only)\n**Pros:** Less work (skip CLI)  \n**Cons:** Inconsistent experience  \n**Decision:** Considered for MVP, but full coverage preferred\n\n---\n\n## Budget Estimate\n\n### Development Time\n- Infrastructure setup: 80 hours\n- Code refactoring: 120 hours\n- Translation coordination: 40 hours\n- Testing and QA: 80 hours\n- Documentation: 20 hours\n- **Total:** 340 hours\n\n### Translation Costs\n**Option A (Professional):**\n- 9 languages \u00d7 $2,000 = $18,000\n\n**Option B (AI + Review):**\n- 9 languages \u00d7 $300 = $2,700\n\n**Option C (Community):**\n- Coordinator time: $2,000\n- Reviews: $1,000\n- **Total:** $3,000\n\n### Recommended Budget\n- Development: $17,000 (340 hours @ $50/hr)\n- Translation: $3,000 (AI + review)\n- **Total:** $20,000\n\n---\n\n## Timeline Summary\n\n| Phase | Duration | Deliverables |\n|-------|----------|--------------|\n| 1. Infrastructure | 2 weeks | i18n framework, extraction tools |\n| 2. Code Refactoring | 3 weeks | All code using `_()` function |\n| 3. File Generation | 1 week | .pot and .po files created |\n| 4. Translation (Tier 1) | 4 weeks | ES, FR, PT, DE, IT complete |\n| 5. Translation (Tier 2) | 4 weeks | ZH, JA complete |\n| 6. Translation (Tier 3) | 4 weeks | AR, HI complete |\n| 7. UI Adjustments | 2 weeks | Layout fixes, font support |\n| 8. Configuration | 1 week | Settings panel, config files |\n| 9. Testing & QA | 3 weeks | All locales tested |\n| **Total** | **24 weeks** | **Full localization support** |\n\n**Accelerated Timeline:** 16 weeks (with parallel work and larger team)\n\n---\n\n## Dependencies\n\n### Required Libraries\n- `babel>=2.14.0` - i18n tooling\n- `polib>=1.2.0` - .po file handling\n- Font packages for non-Latin scripts (Linux)\n\n### External Dependencies\n- Translation service or translators\n- Native speaker reviewers\n- Testing infrastructure\n\n---\n\n## References\n\n### Standards and Specifications\n- [GNU gettext](https://www.gnu.org/software/gettext/)\n- [Babel Documentation](http://babel.pocoo.org/)\n- [ISO 639-1 Language Codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)\n- [Unicode CLDR](https://cldr.unicode.org/)\n\n### Best Practices\n- [Python i18n/L10n Tutorial](https://docs.python.org/3/library/gettext.html)\n- [Tkinter Internationalization](https://tkdocs.com/tutorial/text.html)\n- [Translation Best Practices](https://www.w3.org/International/questions/qa-i18n)\n\n---\n\n## Appendix A: Example Translations\n\n### Common UI Strings\n\n| English | Spanish | French | German | Chinese |\n|---------|---------|--------|--------|---------|\n| Ready | Listo | Pr\u00eat | Bereit | \u51c6\u5907\u5c31\u7eea |\n| Loading | Cargando | Chargement | L\u00e4dt | \u52a0\u8f7d\u4e2d |\n| Error | Error | Erreur | Fehler | \u9519\u8bef |\n| Success | \u00c9xito | Succ\u00e8s | Erfolg | \u6210\u529f |\n| Cancel | Cancelar | Annuler | Abbrechen | \u53d6\u6d88 |\n| Save | Guardar | Enregistrer | Speichern | \u4fdd\u5b58 |\n| Load Brain | Cargar cerebro | Charger le cerveau | Gehirn laden | \u52a0\u8f7d\u5927\u8111 |\n| Export CSV | Exportar CSV | Exporter CSV | CSV exportieren | \u5bfc\u51faCSV |\n| Start Training | Iniciar entrenamiento | D\u00e9marrer l'entra\u00eenement | Training starten | \u5f00\u59cb\u8bad\u7ec3 |\n\n---\n\n## Appendix B: .po File Workflow\n\n### Creating New Language\n\n```bash\n# 1. Extract strings\npython scripts/extract_translations.py\n\n# 2. Initialize new language\npybabel init -i src/aios/locales/messages.pot \\\n             -d src/aios/locales \\\n             -l it_IT\n\n# 3. Edit translations\n# Open src/aios/locales/it_IT/LC_MESSAGES/messages.po\n# Fill in msgstr values\n\n# 4. Compile\npython scripts/compile_translations.py\n\n# 5. Test\nAIOS_LOCALE=it_IT aios gui\n```\n\n### Updating Existing Language\n\n```bash\n# 1. Extract new strings\npython scripts/extract_translations.py\n\n# 2. Update .po files\npybabel update -i src/aios/locales/messages.pot \\\n               -d src/aios/locales\n\n# 3. Translate new strings (marked with \"fuzzy\")\n\n# 4. Compile\npython scripts/compile_translations.py\n```\n\n---\n\n## Appendix C: Contribution Guide\n\n### For Translators\n\n**To contribute a translation:**\n\n1. Fork the AI-OS repository\n2. Install dependencies: `pip install babel polib`\n3. Check if language already initialized:\n   - If yes: Update existing .po file\n   - If no: Run `pybabel init` for your language\n4. Edit `.po` file with a text editor or Poedit\n5. Compile to test: `python scripts/compile_translations.py`\n6. Test in application: `AIOS_LOCALE=<your_locale> aios gui`\n7. Submit pull request with updated .po file\n\n**Translation Guidelines:**\n- Keep placeholders like `{0}`, `%s`, `{score}` intact\n- Maintain consistent technical terminology\n- Use appropriate formality level (usually formal for software)\n- Translate meaning, not word-for-word\n- Ask for context if unclear\n\n---\n\n**Last Updated:** November 9, 2025  \n**Document Version:** 1.0  \n**Status:** Ready for Review\n", "tags": ["cli", "datasets", "evaluation", "gui", "hrm", "mcp", "training"], "headings": [{"line": 0, "text": "Internationalization and Localization (i18n/L10n)"}, {"line": 10, "text": "Overview"}, {"line": 19, "text": "Motivation"}, {"line": 21, "text": "Business Case"}, {"line": 27, "text": "Target Language Justification"}, {"line": 40, "text": "Technical Scope"}, {"line": 42, "text": "Components Requiring Localization"}, {"line": 44, "text": "1. GUI (Tkinter Interface)"}, {"line": 75, "text": "2. CLI (Command-Line Interface)"}, {"line": 101, "text": "3. Documentation"}, {"line": 130, "text": "Implementation Plan"}, {"line": 132, "text": "Phase 1: Infrastructure Setup (Week 1-2)"}, {"line": 134, "text": "1.1 Choose i18n Framework"}, {"line": 152, "text": "1.2 Create Directory Structure"}, {"line": 185, "text": "1.3 Create i18n Utilities Module"}, {"line": 200, "text": "Default locale"}, {"line": 203, "text": "Supported locales"}, {"line": 217, "text": "Global translation function"}, {"line": 231, "text": "Try to detect system locale"}, {"line": 238, "text": "Validate locale"}, {"line": 244, "text": "Set up gettext"}, {"line": 256, "text": "Fallback to no-op translation"}, {"line": 300, "text": "1.4 Create Babel Configuration"}, {"line": 312, "text": "1.5 Create Extraction Script"}, {"line": 317, "text": "!/usr/bin/env python3"}, {"line": 331, "text": "Ensure locale directory exists"}, {"line": 334, "text": "Extract strings"}, {"line": 365, "text": "1.6 Create Compilation Script"}, {"line": 370, "text": "!/usr/bin/env python3"}, {"line": 385, "text": "Find all .po files"}, {"line": 419, "text": "Phase 2: Code Refactoring (Week 3-5)"}, {"line": 421, "text": "2.1 GUI Refactoring Strategy"}, {"line": 425, "text": "Before"}, {"line": 430, "text": "After"}, {"line": 453, "text": "2.2 CLI Refactoring Strategy"}, {"line": 457, "text": "Before"}, {"line": 465, "text": "After"}, {"line": 482, "text": "2.3 String Extraction Guidelines"}, {"line": 499, "text": "Translator comment for clarity"}, {"line": 500, "text": "Translators: This appears in the training progress dialog"}, {"line": 506, "text": "Phase 3: Translation File Generation (Week 6)"}, {"line": 508, "text": "3.1 Extract Strings"}, {"line": 515, "text": "3.2 Initialize Language Files"}, {"line": 527, "text": "... repeat for all languages"}, {"line": 532, "text": "3.3 Translation File Structure"}, {"line": 537, "text": "Spanish translations for AI-OS"}, {"line": 538, "text": "Copyright (C) 2025 Wulfic"}, {"line": 539, "text": "This file is distributed under the same license as the AI-OS package."}, {"line": 551, "text": ": src/aios/gui/app/ui_setup.py:63"}, {"line": 555, "text": ": src/aios/gui/app/ui_setup.py:64"}, {"line": 559, "text": ": src/aios/gui/app/ui_setup.py:65"}, {"line": 563, "text": ": src/aios/gui/app/ui_setup.py:66"}, {"line": 567, "text": ": src/aios/gui/components/brains_panel/panel_main.py:123"}, {"line": 571, "text": ": src/aios/gui/components/brains_panel/panel_main.py:145"}, {"line": 575, "text": ": src/aios/gui/components/brains_panel/panel_main.py:201"}, {"line": 582, "text": "Phase 4: Translation Work (Week 7-18)"}, {"line": 584, "text": "4.1 Translation Approaches"}, {"line": 609, "text": "4.2 Translation Priority Order"}, {"line": 626, "text": "4.3 Translation Quality Checklist"}, {"line": 638, "text": "4.4 Special Translation Considerations"}, {"line": 663, "text": "Phase 5: UI Layout Adjustments (Week 19-20)"}, {"line": 665, "text": "5.1 Dynamic Widget Sizing"}, {"line": 672, "text": "Before"}, {"line": 675, "text": "After"}, {"line": 680, "text": "5.2 Text Overflow Handling"}, {"line": 696, "text": "5.3 RTL Support (Arabic)"}, {"line": 705, "text": "5.4 Font Support"}, {"line": 711, "text": "For CJK"}, {"line": 714, "text": "For Arabic"}, {"line": 717, "text": "For Hindi"}, {"line": 723, "text": "Phase 6: Configuration and Runtime Selection (Week 21)"}, {"line": 725, "text": "6.1 Add Locale Configuration"}, {"line": 730, "text": "Internationalization settings"}, {"line": 732, "text": "Default locale (auto-detected if not set)"}, {"line": 735, "text": "Fallback locale if selected locale unavailable"}, {"line": 739, "text": "6.2 GUI Language Selector"}, {"line": 744, "text": "In src/aios/gui/components/settings_panel/panel_main.py"}, {"line": 750, "text": "... existing code ..."}, {"line": 752, "text": "Language selection"}, {"line": 780, "text": "Save to config"}, {"line": 781, "text": "... save logic ..."}, {"line": 783, "text": "Show restart message"}, {"line": 790, "text": "6.3 CLI Language Selection"}, {"line": 793, "text": "Set via environment variable"}, {"line": 797, "text": "Or via command line flag"}, {"line": 800, "text": "Or set in config file"}, {"line": 806, "text": "Phase 7: Testing and QA (Week 22-24)"}, {"line": 808, "text": "7.1 Automated Testing"}, {"line": 831, "text": "Assumes Spanish translation exists"}, {"line": 839, "text": "Should not raise exception"}, {"line": 849, "text": "Assuming translation exists"}, {"line": 854, "text": "7.2 Manual Testing Checklist"}, {"line": 881, "text": "7.3 Native Speaker Review"}, {"line": 899, "text": "Deployment Strategy"}, {"line": 901, "text": "Build Process Updates"}, {"line": 916, "text": "In build process (CI/CD)"}, {"line": 920, "text": "Packaging"}, {"line": 934, "text": "Maintenance Plan"}, {"line": 936, "text": "Ongoing Translation Updates"}, {"line": 945, "text": "Translation Contributors"}, {"line": 955, "text": "Success Metrics"}, {"line": 957, "text": "Quantitative Metrics"}, {"line": 964, "text": "Qualitative Metrics"}, {"line": 972, "text": "Risks and Mitigation"}, {"line": 974, "text": "Risk 1: Translation Quality"}, {"line": 978, "text": "Risk 2: Incomplete Translations"}, {"line": 982, "text": "Risk 3: UI Layout Breaks"}, {"line": 986, "text": "Risk 4: Maintenance Burden"}, {"line": 990, "text": "Risk 5: RTL Complexity (Arabic)"}, {"line": 996, "text": "Alternative Approaches Considered"}, {"line": 998, "text": "1. English-Only with External Translation Tools"}, {"line": 1003, "text": "2. Machine Translation at Runtime"}, {"line": 1008, "text": "3. Partial Localization (GUI only)"}, {"line": 1015, "text": "Budget Estimate"}, {"line": 1017, "text": "Development Time"}, {"line": 1025, "text": "Translation Costs"}, {"line": 1037, "text": "Recommended Budget"}, {"line": 1044, "text": "Timeline Summary"}, {"line": 1063, "text": "Dependencies"}, {"line": 1065, "text": "Required Libraries"}, {"line": 1070, "text": "External Dependencies"}, {"line": 1077, "text": "References"}, {"line": 1079, "text": "Standards and Specifications"}, {"line": 1085, "text": "Best Practices"}, {"line": 1092, "text": "Appendix A: Example Translations"}, {"line": 1094, "text": "Common UI Strings"}, {"line": 1110, "text": "Appendix B: .po File Workflow"}, {"line": 1112, "text": "Creating New Language"}, {"line": 1115, "text": "1. Extract strings"}, {"line": 1118, "text": "2. Initialize new language"}, {"line": 1123, "text": "3. Edit translations"}, {"line": 1124, "text": "Open src/aios/locales/it_IT/LC_MESSAGES/messages.po"}, {"line": 1125, "text": "Fill in msgstr values"}, {"line": 1127, "text": "4. Compile"}, {"line": 1130, "text": "5. Test"}, {"line": 1134, "text": "Updating Existing Language"}, {"line": 1137, "text": "1. Extract new strings"}, {"line": 1140, "text": "2. Update .po files"}, {"line": 1144, "text": "3. Translate new strings (marked with \"fuzzy\")"}, {"line": 1146, "text": "4. Compile"}, {"line": 1152, "text": "Appendix C: Contribution Guide"}, {"line": 1154, "text": "For Translators"}]}, {"path": "planned_features/LORA_OPTIMIZATION_ENHANCEMENT.md", "content": "# LoRA Parameter Optimization Enhancement\n\n**Date:** October 19, 2025  \n**Status:** Planned Enhancement  \n**Target:** Progressive Optimizer\n\n---\n\n## Problem Statement\n\nThe Progressive Optimizer currently tests LoRA ON/OFF but uses **hardcoded** parameters:\n- `lora_r = 16` (rank)\n- `lora_alpha = 32`\n- `lora_dropout = 0.05`\n- `lora_target_modules = \"q_proj,v_proj\"` (minimal)\n\nThis ignores the canonical guide **features/LORA_PEFT.md** which shows:\n- Different ranks (4, 8, 16, 32, 64) have massive impact on quality (85-99.5%)\n- Module combinations affect trainable params (1M to 24M)\n- VRAM requirements vary significantly (1-7 GB overhead)\n\n---\n\n## Goal\n\nEnhance Progressive Optimizer to intelligently test LoRA configurations based on:\n1. **Available VRAM** - Test appropriate ranks and module combinations\n2. **Dataset size** - Adjust dropout accordingly\n3. **Task complexity** - Use quality/efficiency trade-offs from matrix\n4. **Performance metrics** - Select based on throughput and quality\n\n---\n\n## Design\n\n### 1. LoRA Configuration Dataclass\n\n```python\n@dataclass\nclass LoRAConfig:\n    \"\"\"Configuration for a specific LoRA/PEFT setup.\"\"\"\n    enabled: bool = False\n    method: str = \"lora\"  # \"lora\", \"adalora\", \"ia3\"\n    rank: int = 16\n    alpha: int = 32\n    dropout: float = 0.05\n    target_modules: str = \"q_proj,v_proj\"\n    \n    @property\n    def estimated_params(self) -> int:\n        \"\"\"Estimate trainable parameters based on config.\"\"\"\n    # From canonical LoRA/PEFT guide decision rules\n        params_map = {\n            (\"minimal\", 4): 250_000,\n            (\"minimal\", 8): 500_000,\n            (\"minimal\", 16): 1_000_000,\n            (\"minimal\", 32): 2_000_000,\n            (\"minimal\", 64): 4_000_000,\n            (\"balanced\", 16): 2_000_000,\n            (\"balanced\", 32): 4_000_000,\n            (\"full\", 16): 6_000_000,\n            (\"full\", 32): 12_000_000,\n            (\"full\", 64): 24_000_000,\n        }\n        \n        # Determine module set\n        modules = set(self.target_modules.split(\",\"))\n        if modules <= {\"q_proj\", \"v_proj\"}:\n            module_set = \"minimal\"\n        elif modules <= {\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"}:\n            module_set = \"balanced\"\n        else:\n            module_set = \"full\"\n        \n        return params_map.get((module_set, self.rank), 1_000_000)\n    \n    @property\n    def estimated_vram_overhead_gb(self) -> float:\n    \"\"\"Estimate VRAM overhead (see canonical LoRA/PEFT guide).\"\"\"\n        vram_map = {\n            4: 1.0,\n            8: 1.5,\n            16: 2.5,\n            32: 4.0,\n            64: 7.0,\n        }\n        return vram_map.get(self.rank, 2.5)\n    \n    def to_cli_args(self) -> List[str]:\n        \"\"\"Convert to CLI arguments.\"\"\"\n        if not self.enabled:\n            return [\"--no-peft\"]\n        \n        args = [\n            \"--use-peft\",\n            \"--peft-method\", self.method,\n        ]\n        \n        if self.method in [\"lora\", \"adalora\"]:\n            args.extend([\n                \"--lora-r\", str(self.rank),\n                \"--lora-alpha\", str(self.alpha),\n                \"--lora-dropout\", str(self.dropout),\n            ])\n        \n        args.extend([\"--lora-target-modules\", self.target_modules])\n        \n        return args\n    \n    def __str__(self) -> str:\n        \"\"\"Human-readable description.\"\"\"\n        if not self.enabled:\n            return \"No PEFT\"\n        \n        if self.method == \"ia3\":\n            return f\"IA3 {self.target_modules}\"\n        \n        modules_short = \"minimal\" if \"q_proj,v_proj\" == self.target_modules else (\n            \"balanced\" if len(self.target_modules.split(\",\")) <= 4 else \"full\"\n        )\n        return f\"{self.method.upper()} r={self.rank} \u03b1={self.alpha} {modules_short}\"\n```\n\n---\n\n### 2. LoRA Configuration Factory\n\nBased on the canonical **features/LORA_PEFT.md** decision rules:\n\n```python\ndef create_lora_configs(\n    available_vram_gb: float,\n    dataset_size: Optional[int] = None,\n    task_complexity: str = \"medium\"\n) -> List[LoRAConfig]:\n    \"\"\"\n    Create LoRA configurations to test based on available resources.\n    \n    Uses decision rules from the canonical LoRA/PEFT guide\n    \n    Args:\n        available_vram_gb: Available VRAM in GB\n        dataset_size: Number of samples in dataset (for dropout tuning)\n        task_complexity: \"simple\", \"medium\", \"complex\", \"very_complex\"\n    \n    Returns:\n        List of LoRAConfig objects to test\n    \"\"\"\n    configs = []\n    \n    # Always test NO PEFT as baseline\n    configs.append(LoRAConfig(enabled=False))\n    \n    # Determine dropout based on dataset size\n    if dataset_size:\n        if dataset_size > 100_000:\n            dropout = 0.0\n        elif dataset_size > 10_000:\n            dropout = 0.05\n        elif dataset_size > 1_000:\n            dropout = 0.1\n        else:\n            dropout = 0.2\n    else:\n        dropout = 0.05  # Default\n    \n    # ====================================================================\n    # VRAM-Based Configuration Selection\n    # From canonical guide Quick Decision rules\n    # ====================================================================\n    \n    if available_vram_gb < 8:\n        # < 8 GB: IA3 Minimal or LoRA r=8 Minimal\n        configs.extend([\n            LoRAConfig(\n                enabled=True,\n                method=\"ia3\",\n                target_modules=\"q_proj,v_proj\"\n            ),\n            LoRAConfig(\n                enabled=True,\n                method=\"lora\",\n                rank=8,\n                alpha=16,\n                dropout=dropout,\n                target_modules=\"q_proj,v_proj\"\n            )\n        ])\n        \n    elif available_vram_gb < 10:\n        # 8-10 GB: LoRA r=8 or r=16 Minimal\n        configs.extend([\n            LoRAConfig(\n                enabled=True,\n                method=\"lora\",\n                rank=8,\n                alpha=16,\n                dropout=dropout,\n                target_modules=\"q_proj,v_proj\"\n            ),\n            LoRAConfig(\n                enabled=True,\n                method=\"lora\",\n                rank=16,\n                alpha=32,\n                dropout=dropout,\n                target_modules=\"q_proj,v_proj\"\n            )\n        ])\n        \n    elif available_vram_gb < 14:\n        # 10-14 GB: LoRA r=16 Minimal and Balanced\n        configs.extend([\n            LoRAConfig(\n                enabled=True,\n                method=\"lora\",\n                rank=16,\n                alpha=32,\n                dropout=dropout,\n                target_modules=\"q_proj,v_proj\"\n            ),\n            LoRAConfig(\n                enabled=True,\n                method=\"lora\",\n                rank=16,\n                alpha=32,\n                dropout=dropout,\n                target_modules=\"q_proj,k_proj,v_proj,o_proj\"\n            )\n        ])\n        \n    elif available_vram_gb < 20:\n        # 14-20 GB: LoRA r=16 Balanced and r=32 Balanced/Full\n        configs.extend([\n            LoRAConfig(\n                enabled=True,\n                method=\"lora\",\n                rank=16,\n                alpha=32,\n                dropout=dropout,\n                target_modules=\"q_proj,k_proj,v_proj,o_proj\"\n            ),\n            LoRAConfig(\n                enabled=True,\n                method=\"lora\",\n                rank=32,\n                alpha=64,\n                dropout=dropout,\n                target_modules=\"q_proj,k_proj,v_proj,o_proj\"\n            ),\n            LoRAConfig(\n                enabled=True,\n                method=\"lora\",\n                rank=32,\n                alpha=64,\n                dropout=dropout,\n                target_modules=\"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n            )\n        ])\n        \n    else:\n        # 20+ GB: LoRA r=32, r=64 Full, or consider full fine-tuning\n        configs.extend([\n            LoRAConfig(\n                enabled=True,\n                method=\"lora\",\n                rank=32,\n                alpha=64,\n                dropout=dropout,\n                target_modules=\"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n            ),\n            LoRAConfig(\n                enabled=True,\n                method=\"lora\",\n                rank=64,\n                alpha=128,\n                dropout=dropout,\n                target_modules=\"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n            )\n        ])\n        \n        # For research: test AdaLoRA\n        if task_complexity in [\"complex\", \"very_complex\"]:\n            configs.append(LoRAConfig(\n                enabled=True,\n                method=\"adalora\",\n                rank=32,\n                alpha=64,\n                dropout=dropout,\n                target_modules=\"q_proj,k_proj,v_proj,o_proj\"\n            ))\n    \n    return configs\n```\n\n---\n\n### 3. Enhanced OptimizationLevel\n\nModify existing `OptimizationLevel` to include LoRA config:\n\n```python\n@dataclass\nclass OptimizationLevel:\n    \"\"\"Represents a specific combination of optimizations to test.\"\"\"\n    \n    name: str\n    gradient_checkpointing: bool = True\n    amp: bool = False\n    flashattn2: bool = False\n    lora_config: Optional[LoRAConfig] = None  # \u2190 NEW\n    cpu_offload: bool = False\n    zero_stage: str = \"none\"\n    chunk_size: Optional[int] = None\n    \n    @property\n    def lora(self) -> bool:\n        \"\"\"Backward compatibility.\"\"\"\n        return self.lora_config is not None and self.lora_config.enabled\n    \n    def to_cli_args(self) -> List[str]:\n        \"\"\"Convert this optimization level to CLI arguments.\"\"\"\n        args = []\n        \n        # ... existing args ...\n        \n        # LoRA (via PEFT) - NOW CONFIGURABLE\n        if self.lora_config:\n            args.extend(self.lora_config.to_cli_args())\n        else:\n            args.append(\"--no-peft\")\n        \n        # ... rest of args ...\n        \n        return args\n    \n    def __str__(self) -> str:\n        \"\"\"Human-readable description.\"\"\"\n        parts = []\n        if self.gradient_checkpointing:\n            parts.append(\"GradCP\")\n        if self.amp:\n            parts.append(\"AMP\")\n        if self.lora_config:\n            parts.append(str(self.lora_config))\n        # ... rest ...\n        \n        return \" + \".join(parts) if parts else \"baseline\"\n```\n\n---\n\n### 4. Updated Level Factory\n\n```python\ndef create_optimization_levels(\n    config: \"OptimizationConfig\",\n    log_func=None\n) -> List[\"OptimizationLevel\"]:\n    \"\"\"Create the progression of optimization levels to test.\"\"\"\n    \n    # Estimate available VRAM (simplified - improve with actual detection)\n    available_vram_gb = estimate_available_vram()\n    \n    # Get LoRA configurations to test\n    lora_configs = create_lora_configs(\n        available_vram_gb=available_vram_gb,\n        dataset_size=None,  # Could parse from dataset file\n        task_complexity=\"medium\"  # Could be a config parameter\n    )\n    \n    levels = []\n    \n    # Level 1: Baseline (no PEFT)\n    base_config = {\n        \"name\": \"Baseline (No PEFT)\",\n        \"gradient_checkpointing\": True,\n        \"amp\": True,\n        \"lora_config\": LoRAConfig(enabled=False),\n        \"cpu_offload\": False,\n        \"zero_stage\": \"none\"\n    }\n    levels.append(OptimizationLevel(**base_config))\n    \n    # Level 2-N: Test each LoRA configuration\n    for idx, lora_cfg in enumerate(lora_configs):\n        if not lora_cfg.enabled:\n            continue  # Skip disabled config (already tested as baseline)\n        \n        level_config = {\n            \"name\": f\"Level {len(levels) + 1}: GradCP + AMP + {lora_cfg}\",\n            \"gradient_checkpointing\": True,\n            \"amp\": True,\n            \"lora_config\": lora_cfg,\n            \"cpu_offload\": False,\n            \"zero_stage\": \"none\"\n        }\n        levels.append(OptimizationLevel(**level_config))\n    \n    # Additional levels with CPU offload and ZeRO stages\n    # (only for the best performing LoRA config if needed)\n    \n    return levels\n```\n\n---\n\n### 5. Result Scoring\n\nEnhanced scoring to consider quality estimates from matrix:\n\n```python\ndef score_optimization_result(\n    level: OptimizationLevel,\n    batch_size: int,\n    throughput: float,\n    memory_percent: float\n) -> float:\n    \"\"\"\n    Score an optimization result considering:\n    - Throughput (steps/sec)\n    - Memory efficiency\n    - Expected quality (from canonical LoRA/PEFT guide)\n    \n    Returns:\n        Score (higher is better)\n    \"\"\"\n    \n    # Base score from throughput\n    throughput_score = throughput * 100\n    \n    # Memory efficiency bonus (prefer using GPU fully)\n    if 85 <= memory_percent <= 95:\n        memory_bonus = 50  # Sweet spot\n    elif memory_percent < 85:\n        memory_bonus = (memory_percent / 85) * 50  # Penalty for underutilization\n    else:\n        memory_bonus = 0  # Using too much memory\n    \n    # Quality estimate (see canonical LoRA/PEFT guide)\n    quality_score = 0\n    if level.lora_config and level.lora_config.enabled:\n        # Quality estimates from matrix\n        if level.lora_config.method == \"ia3\":\n            quality_score = 87.5  # 85-90%\n        elif level.lora_config.method == \"adalora\":\n            quality_score = 98.75  # 98-99.5%\n        else:  # LoRA\n            # Based on rank and modules\n            if level.lora_config.rank >= 64:\n                quality_score = 99.25  # 99%+\n            elif level.lora_config.rank >= 32:\n                if \"gate_proj\" in level.lora_config.target_modules:\n                    quality_score = 99.0  # Full modules\n                else:\n                    quality_score = 98.75  # Balanced\n            elif level.lora_config.rank >= 16:\n                if \"o_proj\" in level.lora_config.target_modules:\n                    quality_score = 98.0  # Balanced\n                else:\n                    quality_score = 96.5  # Minimal\n            else:  # r=8\n                quality_score = 93.5  # 92-95%\n    else:\n        quality_score = 100  # Full fine-tuning baseline\n    \n    # Combine scores\n    # Weight: 40% throughput, 20% memory, 40% quality\n    total_score = (\n        throughput_score * 0.4 +\n        memory_bonus * 0.2 +\n        quality_score * 0.4\n    )\n    \n    return total_score\n```\n\n---\n\n## Implementation Steps\n\n### Phase 1: Core Infrastructure (2-3 hours)\n1. \u2705 Create `LoRAConfig` dataclass\n2. \u2705 Create `create_lora_configs()` factory function\n3. \u2705 Update `OptimizationLevel` to include `lora_config`\n4. \u2705 Update `to_cli_args()` to use LoRA config\n\n### Phase 2: Level Generation (1-2 hours)\n1. \u2705 Update `create_optimization_levels()` to generate LoRA variants\n2. \u2705 Add VRAM detection helper\n3. \u2705 Add dataset size estimation helper\n4. \u2705 Test level generation logic\n\n### Phase 3: Scoring System (1 hour)\n1. \u2705 Implement `score_optimization_result()` function\n2. \u2705 Update result selection to use new scoring\n3. \u2705 Add quality estimates to result output\n\n### Phase 4: Testing (2-3 hours)\n1. \u2705 Test with low VRAM scenario (< 8 GB)\n2. \u2705 Test with medium VRAM scenario (10-14 GB)\n3. \u2705 Test with high VRAM scenario (20+ GB)\n4. \u2705 Verify CLI arg generation\n5. \u2705 Verify result scoring\n\n### Phase 5: Documentation (1 hour)\n1. \u2705 Update Progressive Optimizer docstrings\n2. \u2705 Add examples to features/LORA_PEFT.md\n3. \u2705 Update OPTIMIZER_SYSTEMS_EXPLAINED.md\n\n---\n\n## Expected Results\n\n### Before Enhancement\n```\nTesting Level 3: GradCP + AMP + LoRA\n- LoRA: r=16, alpha=32, dropout=0.05, modules=q_proj,v_proj\n- Result: 95-98% quality\n```\n\n### After Enhancement\n```\nTesting Level 2: GradCP + AMP + LoRA r=8 minimal\n- LoRA: r=8, alpha=16, dropout=0.05, modules=q_proj,v_proj\n- Result: 92-95% quality, Score: 85.2\n\nTesting Level 3: GradCP + AMP + LoRA r=16 minimal\n- LoRA: r=16, alpha=32, dropout=0.05, modules=q_proj,v_proj\n- Result: 95-98% quality, Score: 92.1\n\nTesting Level 4: GradCP + AMP + LoRA r=16 balanced\n- LoRA: r=16, alpha=32, dropout=0.05, modules=q_proj,k_proj,v_proj,o_proj\n- Result: 97-99% quality, Score: 96.8 \u2b50 BEST\n\nSelected: Level 4 (highest score considering throughput, memory, and quality)\n```\n\n---\n\n## Files to Modify\n\n1. `src/aios/gui/components/hrm_training/optimizer_progressive/models.py`\n   - Add `LoRAConfig` dataclass\n   - Update `OptimizationLevel` to include `lora_config`\n\n2. `src/aios/gui/components/hrm_training/optimizer_progressive/level_factory.py`\n   - Add `create_lora_configs()` function\n   - Update `create_optimization_levels()` to use LoRA configs\n   - Add VRAM detection helper\n\n3. `src/aios/gui/components/hrm_training/optimizer_progressive/optimizer.py`\n   - Update result scoring to use new `score_optimization_result()`\n   - Add quality estimates to output\n\n4. `docs/guide/features/LORA_PEFT.md`\n   - Add section on automatic optimization\n   - Add examples of optimizer output\n\n---\n\n## Future Enhancements\n\n### Adaptive Tuning\n- Monitor loss during optimization\n- If loss plateau, automatically try higher rank\n- If training unstable, automatically reduce alpha\n\n### Dataset Analysis\n- Parse dataset file to estimate size\n- Adjust dropout based on actual sample count\n- Detect task complexity from dataset content\n\n### Multi-Objective Optimization\n- Allow user to prioritize: speed vs quality vs memory\n- Use Pareto optimization to find trade-offs\n- Generate multiple recommendations\n\n---\n\n## References\n\n- **features/LORA_PEFT.md** - Parameter impact analysis and decision rules\n- **OPTIMIZER_SYSTEMS_EXPLAINED.md** - Overview of all optimization systems\n- `src/aios/gui/components/hrm_training/optimizer_progressive/` - Current implementation\n\n---\n\n**Status:** Ready for implementation  \n**Estimated Time:** 7-11 hours total  \n**Priority:** High (significantly improves training quality)\n", "tags": ["cli", "datasets", "gui", "training"], "headings": [{"line": 0, "text": "LoRA Parameter Optimization Enhancement"}, {"line": 8, "text": "Problem Statement"}, {"line": 23, "text": "Goal"}, {"line": 33, "text": "Design"}, {"line": 35, "text": "1. LoRA Configuration Dataclass"}, {"line": 51, "text": "From canonical LoRA/PEFT guide decision rules"}, {"line": 65, "text": "Determine module set"}, {"line": 125, "text": "2. LoRA Configuration Factory"}, {"line": 150, "text": "Always test NO PEFT as baseline"}, {"line": 153, "text": "Determine dropout based on dataset size"}, {"line": 166, "text": "===================================================================="}, {"line": 167, "text": "VRAM-Based Configuration Selection"}, {"line": 168, "text": "From canonical guide Quick Decision rules"}, {"line": 169, "text": "===================================================================="}, {"line": 172, "text": "< 8 GB: IA3 Minimal or LoRA r=8 Minimal"}, {"line": 190, "text": "8-10 GB: LoRA r=8 or r=16 Minimal"}, {"line": 211, "text": "10-14 GB: LoRA r=16 Minimal and Balanced"}, {"line": 232, "text": "14-20 GB: LoRA r=16 Balanced and r=32 Balanced/Full"}, {"line": 261, "text": "20+ GB: LoRA r=32, r=64 Full, or consider full fine-tuning"}, {"line": 281, "text": "For research: test AdaLoRA"}, {"line": 297, "text": "3. Enhanced OptimizationLevel"}, {"line": 324, "text": "... existing args ..."}, {"line": 326, "text": "LoRA (via PEFT) - NOW CONFIGURABLE"}, {"line": 332, "text": "... rest of args ..."}, {"line": 345, "text": "... rest ..."}, {"line": 352, "text": "4. Updated Level Factory"}, {"line": 361, "text": "Estimate available VRAM (simplified - improve with actual detection)"}, {"line": 364, "text": "Get LoRA configurations to test"}, {"line": 373, "text": "Level 1: Baseline (no PEFT)"}, {"line": 384, "text": "Level 2-N: Test each LoRA configuration"}, {"line": 399, "text": "Additional levels with CPU offload and ZeRO stages"}, {"line": 400, "text": "(only for the best performing LoRA config if needed)"}, {"line": 407, "text": "5. Result Scoring"}, {"line": 428, "text": "Base score from throughput"}, {"line": 431, "text": "Memory efficiency bonus (prefer using GPU fully)"}, {"line": 439, "text": "Quality estimate (see canonical LoRA/PEFT guide)"}, {"line": 442, "text": "Quality estimates from matrix"}, {"line": 448, "text": "Based on rank and modules"}, {"line": 466, "text": "Combine scores"}, {"line": 467, "text": "Weight: 40% throughput, 20% memory, 40% quality"}, {"line": 479, "text": "Implementation Steps"}, {"line": 481, "text": "Phase 1: Core Infrastructure (2-3 hours)"}, {"line": 487, "text": "Phase 2: Level Generation (1-2 hours)"}, {"line": 493, "text": "Phase 3: Scoring System (1 hour)"}, {"line": 498, "text": "Phase 4: Testing (2-3 hours)"}, {"line": 505, "text": "Phase 5: Documentation (1 hour)"}, {"line": 512, "text": "Expected Results"}, {"line": 514, "text": "Before Enhancement"}, {"line": 521, "text": "After Enhancement"}, {"line": 540, "text": "Files to Modify"}, {"line": 561, "text": "Future Enhancements"}, {"line": 563, "text": "Adaptive Tuning"}, {"line": 568, "text": "Dataset Analysis"}, {"line": 573, "text": "Multi-Objective Optimization"}, {"line": 580, "text": "References"}]}, {"path": "planned_features/MIXED_GPU_VENDOR_SUPPORT.md", "content": "# Mixed GPU Vendor Support for Parallel Training\n\n**Status:** \ud83d\udccb Planned  \n> Note: Any references to `docs/user_guide/*` are placeholders for future user-facing docs. For current information, use `docs/INDEX.md` and guides under `docs/guide/`.\n**Priority:** Medium  \n**Complexity:** Medium (2-4 hours implementation)  \n**Target Version:** Future Release  \n**Created:** 2025-10-18\n\n## Overview\n\nEnable parallel independent training to work with mixed GPU vendors in a single system. This allows users to utilize all available GPUs regardless of manufacturer for training, maximizing hardware utilization.\n\n## Use Cases\n\n### Primary Use Cases\n1. **Developers with mixed setups**: NVIDIA + Intel Arc development machines\n2. **Budget builds**: Using older NVIDIA + newer AMD GPUs together\n3. **Testing environments**: Multi-vendor CI/CD systems\n4. **Workstation upgrades**: Adding new GPU without removing old one\n\n### Example Configurations\n- NVIDIA RTX 3090 + Intel Arc A770\n- AMD RX 7900 XTX + NVIDIA RTX 2080 Ti\n- Intel Arc A380 + AMD RX 6600\n- NVIDIA RTX 4090 + AMD RX 7800 XT + Intel Arc A750 (triple vendor)\n\n## Current Limitations\n\n### What Works Now \u2705\n- **Same vendor, different models**: NVIDIA RTX 3090 + RTX 2080 Ti\n- **Automatic detection**: `--cuda-ids 0,1` works for NVIDIA-only\n\n### What Doesn't Work \u274c\n- **Mixed vendors**: Different GPU types not detected\n- **Backend selection**: Hardcoded to CUDA only\n- **Device enumeration**: Only scans NVIDIA GPUs\n- **VRAM checking**: Only checks CUDA devices\n\n## Technical Requirements\n\n### PyTorch Backend Support\n\n#### NVIDIA GPUs (CUDA)\n```python\nimport torch\nif torch.cuda.is_available():\n    device = torch.device('cuda:0')\n    count = torch.cuda.device_count()\n```\n\n#### AMD GPUs (ROCm)\n```python\nimport torch\n# Requires: pip install torch+rocm5.7\nif torch.cuda.is_available():  # ROCm pretends to be CUDA\n    device = torch.device('cuda:0')  # Still uses 'cuda' namespace\n```\n\n#### Intel Arc/Xe GPUs (XPU)\n```python\nimport intel_extension_for_pytorch as ipex\nif torch.xpu.is_available():\n    device = torch.device('xpu:0')\n    count = torch.xpu.device_count()\n```\n\n### Dependencies\n\n**Current:**\n- `torch>=2.0.0` with CUDA support\n\n**Needed for full support:**\n- `torch+rocm` (AMD support) - separate wheel\n- `intel-extension-for-pytorch` (Intel Arc support)\n- Optional: Auto-detect and install based on hardware\n\n## Implementation Plan\n\n### Phase 1: Detection & Enumeration (High Priority)\n\n**Goal:** Detect all available GPUs across vendors\n\n```python\ndef detect_all_gpus() -> list[dict]:\n    \"\"\"Detect GPUs from all supported vendors.\n    \n    Returns:\n        List of GPU info dicts:\n        {\n            'id': 0,\n            'backend': 'cuda' | 'xpu' | 'hip',\n            'device_id': 0,  # Backend-specific ID\n            'name': 'NVIDIA GeForce RTX 3090',\n            'vendor': 'NVIDIA' | 'AMD' | 'Intel',\n            'vram_total': 24 * 1024**3,  # bytes\n            'vram_available': 23.5 * 1024**3,\n            'compute_capability': '8.6',  # NVIDIA only\n        }\n    \"\"\"\n    gpus = []\n    global_id = 0\n    \n    # NVIDIA (CUDA)\n    try:\n        import torch\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                props = torch.cuda.get_device_properties(i)\n                gpus.append({\n                    'id': global_id,\n                    'backend': 'cuda',\n                    'device_id': i,\n                    'name': props.name,\n                    'vendor': 'NVIDIA',\n                    'vram_total': props.total_memory,\n                    'vram_available': torch.cuda.mem_get_info(i)[0],\n                    'compute_capability': f\"{props.major}.{props.minor}\",\n                })\n                global_id += 1\n    except Exception as e:\n        print(f\"CUDA detection failed: {e}\")\n    \n    # Intel Arc (XPU)\n    try:\n        import intel_extension_for_pytorch as ipex\n        if torch.xpu.is_available():\n            for i in range(torch.xpu.device_count()):\n                props = torch.xpu.get_device_properties(i)\n                gpus.append({\n                    'id': global_id,\n                    'backend': 'xpu',\n                    'device_id': i,\n                    'name': props.name,\n                    'vendor': 'Intel',\n                    'vram_total': props.total_memory,\n                    'vram_available': torch.xpu.mem_get_info(i)[0],\n                    'compute_capability': None,\n                })\n                global_id += 1\n    except ImportError:\n        pass  # Intel extension not installed\n    except Exception as e:\n        print(f\"XPU detection failed: {e}\")\n    \n    # AMD (ROCm/HIP) - tricky because it uses CUDA namespace\n    # Need to check GPU names to distinguish from NVIDIA\n    try:\n        import torch\n        if hasattr(torch, 'hip') and torch.hip.is_available():\n            for i in range(torch.hip.device_count()):\n                props = torch.hip.get_device_properties(i)\n                gpus.append({\n                    'id': global_id,\n                    'backend': 'hip',\n                    'device_id': i,\n                    'name': props.name,\n                    'vendor': 'AMD',\n                    'vram_total': props.total_memory,\n                    'vram_available': torch.hip.mem_get_info(i)[0],\n                    'compute_capability': None,\n                })\n                global_id += 1\n    except Exception as e:\n        print(f\"HIP detection failed: {e}\")\n    \n    return gpus\n```\n\n**Files to modify:**\n- `src/aios/cli/hrm_hf/parallel_independent_training.py`\n- New: `src/aios/cli/hrm_hf/gpu_detection.py`\n\n### Phase 2: Device Mapping (High Priority)\n\n**Goal:** Map user-specified GPU IDs to backend-specific devices\n\n```python\nclass GPUDevice:\n    \"\"\"Abstraction for different GPU backends.\"\"\"\n    \n    def __init__(self, backend: str, device_id: int, info: dict):\n        self.backend = backend\n        self.device_id = device_id\n        self.info = info\n        self._device = None\n    \n    @property\n    def device(self) -> torch.device:\n        \"\"\"Get PyTorch device object.\"\"\"\n        if self._device is None:\n            if self.backend == 'cuda':\n                self._device = torch.device(f'cuda:{self.device_id}')\n            elif self.backend == 'xpu':\n                self._device = torch.device(f'xpu:{self.device_id}')\n            elif self.backend == 'hip':\n                self._device = torch.device(f'cuda:{self.device_id}')  # HIP uses cuda namespace\n        return self._device\n    \n    def set_device(self):\n        \"\"\"Set this as the active device.\"\"\"\n        if self.backend == 'cuda':\n            torch.cuda.set_device(self.device)\n        elif self.backend == 'xpu':\n            torch.xpu.set_device(self.device)\n        elif self.backend == 'hip':\n            torch.cuda.set_device(self.device)\n    \n    def synchronize(self):\n        \"\"\"Synchronize device.\"\"\"\n        if self.backend == 'cuda':\n            torch.cuda.synchronize(self.device)\n        elif self.backend == 'xpu':\n            torch.xpu.synchronize(self.device)\n        elif self.backend == 'hip':\n            torch.cuda.synchronize(self.device)\n    \n    def create_stream(self):\n        \"\"\"Create backend-specific stream.\"\"\"\n        if self.backend == 'cuda':\n            return torch.cuda.Stream(device=self.device)\n        elif self.backend == 'xpu':\n            return torch.xpu.Stream(device=self.device)\n        elif self.backend == 'hip':\n            return torch.cuda.Stream(device=self.device)\n    \n    def get_memory_info(self) -> tuple[int, int]:\n        \"\"\"Get (free, total) memory in bytes.\"\"\"\n        if self.backend == 'cuda':\n            return torch.cuda.mem_get_info(self.device_id)\n        elif self.backend == 'xpu':\n            return torch.xpu.mem_get_info(self.device_id)\n        elif self.backend == 'hip':\n            return torch.cuda.mem_get_info(self.device_id)\n        return (0, 0)\n```\n\n### Phase 3: AMP Backend Support (Medium Priority)\n\n**Goal:** Handle vendor-specific AMP implementations\n\n```python\ndef create_scaler(backend: str, enabled: bool):\n    \"\"\"Create AMP scaler for specific backend.\"\"\"\n    if backend == 'cuda':\n        return torch.amp.GradScaler('cuda', enabled=enabled)\n    elif backend == 'xpu':\n        # Intel XPU uses different AMP API\n        return ipex.optimize(enable_auto_mixed_precision=enabled)\n    elif backend == 'hip':\n        return torch.amp.GradScaler('cuda', enabled=enabled)  # HIP uses CUDA namespace\n    return None\n\ndef autocast_context(backend: str, enabled: bool):\n    \"\"\"Get appropriate autocast context for backend.\"\"\"\n    if backend == 'cuda':\n        return torch.amp.autocast('cuda', enabled=enabled)\n    elif backend == 'xpu':\n        return torch.xpu.amp.autocast(enabled=enabled)\n    elif backend == 'hip':\n        return torch.amp.autocast('cuda', enabled=enabled)\n    return nullcontext()\n```\n\n### Phase 4: CLI Integration (Medium Priority)\n\n**New CLI interface:**\n\n```bash\n# Current (CUDA-only):\naios hrm-hf train-actv1 --cuda-ids 0,1\n\n# New (auto-detect all vendors):\naios hrm-hf train-actv1 --gpu-ids 0,1,2\n# Where: 0=NVIDIA, 1=Intel Arc, 2=AMD\n\n# Explicit vendor selection:\naios hrm-hf train-actv1 --gpu-ids cuda:0,xpu:0,hip:0\n\n# List available GPUs:\naios hrm-hf list-gpus\n# Output:\n# ID  Vendor   Model                      VRAM    Backend\n# 0   NVIDIA   GeForce RTX 3090          24 GB   cuda\n# 1   Intel    Arc A770                  16 GB   xpu\n# 2   AMD      Radeon RX 7900 XTX        24 GB   hip\n```\n\n**Files to modify:**\n- `src/aios/cli/hrm_hf_cli.py` - Add `--gpu-ids` parameter\n- `src/aios/cli/hrm_hf_cli.py` - Add `list-gpus` command\n\n### Phase 5: Load Balancing (Low Priority - Future)\n\n**Goal:** Assign work proportionally to GPU speed\n\n```python\ndef calculate_gpu_weights(gpus: list[GPUDevice]) -> list[float]:\n    \"\"\"Calculate relative performance weights for GPUs.\n    \n    Uses heuristics based on:\n    - VRAM size\n    - Vendor (NVIDIA > AMD > Intel for ML)\n    - Known performance tiers\n    \"\"\"\n    weights = []\n    for gpu in gpus:\n        # Base weight from VRAM\n        vram_gb = gpu.info['vram_total'] / (1024**3)\n        weight = vram_gb / 8  # Normalize to 8GB = 1.0\n        \n        # Vendor multiplier (rough performance hierarchy)\n        if gpu.vendor == 'NVIDIA':\n            weight *= 1.0\n        elif gpu.vendor == 'AMD':\n            weight *= 0.8  # ~20% slower on ML workloads\n        elif gpu.vendor == 'Intel':\n            weight *= 0.5  # ~50% slower (Arc is newer to ML)\n        \n        weights.append(weight)\n    \n    # Normalize to sum = 1.0\n    total = sum(weights)\n    return [w / total for w in weights]\n\n# Usage:\nweights = calculate_gpu_weights(gpus)\n# Assign data: GPU 0 gets 50%, GPU 1 gets 30%, GPU 2 gets 20%\n```\n\n## Testing Strategy\n\n### Unit Tests\n```python\ndef test_gpu_detection():\n    \"\"\"Test GPU detection works for available hardware.\"\"\"\n    gpus = detect_all_gpus()\n    assert len(gpus) > 0\n    assert all('backend' in gpu for gpu in gpus)\n\ndef test_device_abstraction():\n    \"\"\"Test GPUDevice works across backends.\"\"\"\n    gpus = detect_all_gpus()\n    for gpu_info in gpus:\n        device = GPUDevice(gpu_info['backend'], gpu_info['device_id'], gpu_info)\n        assert device.device is not None\n        device.set_device()\n        device.synchronize()\n\ndef test_mixed_training():\n    \"\"\"Test training works with mixed GPUs.\"\"\"\n    # Only runs if multiple vendor GPUs available\n    gpus = detect_all_gpus()\n    vendors = set(gpu['vendor'] for gpu in gpus)\n    if len(vendors) < 2:\n        pytest.skip(\"Mixed GPU hardware not available\")\n    \n    # Run short training\n    run_training(gpu_ids=[0, 1], steps=10)\n```\n\n### Manual Testing Checklist\n- [ ] NVIDIA + Intel Arc training completes\n- [ ] NVIDIA + AMD training completes\n- [ ] Intel Arc + AMD training completes\n- [ ] All three vendors simultaneously\n- [ ] VRAM checking works per vendor\n- [ ] AMP works on each vendor\n- [ ] Gradient checkpointing works on each vendor\n- [ ] Checkpoint merging produces valid model\n- [ ] Performance is reasonable (not worse than sequential)\n\n## Performance Considerations\n\n### Bottleneck Analysis\n\n**Scenario 1: Mixed High-End GPUs**\n- RTX 4090 (165 TFLOPS) + RX 7900 XTX (61 TFLOPS)\n- Bottleneck: AMD ~2.7x slower\n- Solution: Assign 73% data to RTX 4090, 27% to RX 7900 XTX\n- Expected speedup: ~1.6x vs single RTX 4090\n\n**Scenario 2: High-End + Low-End**\n- RTX 4090 (165 TFLOPS) + Arc A380 (8 TFLOPS)\n- Bottleneck: Arc ~20x slower\n- Solution: Don't use Arc, or give it <10% of data\n- Expected speedup: Minimal, possibly negative\n\n**Recommendation:**\n- Start without load balancing (equal distribution)\n- Add load balancing in Phase 5 if needed\n- Document performance expectations\n\n## Backwards Compatibility\n\n### Breaking Changes: None \u2705\n- `--cuda-ids` continues to work for NVIDIA-only setups\n- New `--gpu-ids` parameter is optional\n- Auto-detection falls back to CUDA if no other backends\n\n### Migration Path\n```python\n# Old code (still works):\nconfig.cuda_ids = \"0,1\"\n\n# New code (recommended):\nconfig.gpu_ids = \"0,1\"  # Auto-detects vendor\nconfig.gpu_ids = \"cuda:0,xpu:1\"  # Explicit\n```\n\n## Documentation Needed\n\n### User Documentation\n- Update `QUICK_START.md` with multi-vendor examples\n- Add (placeholder) `docs/user_guide/MIXED_GPU_TRAINING.md` (to be authored later; see docs/INDEX.md for current guidance)\n- Update `README.md` with mixed GPU capabilities\n\n### Developer Documentation\n- Add `docs/development/GPU_BACKEND_ARCHITECTURE.md`\n- Document `GPUDevice` abstraction layer\n- Add troubleshooting guide for backend issues\n\n## Known Limitations\n\n### Phase 1-4 Limitations\n1. **No automatic load balancing**: Slow GPU limits speed\n2. **No cross-GPU communication**: Can't implement DDP across vendors\n3. **Backend-specific quirks**: Some features may not work on all vendors\n4. **Installation complexity**: Users need correct PyTorch builds\n\n### Permanent Limitations\n1. **Performance**: Limited by slowest GPU\n2. **Memory**: Each GPU needs full model copy\n3. **Synchronization**: Barrier waits for all GPUs\n\n## Success Metrics\n\n### Must Have \u2705\n- [ ] Detection works for NVIDIA + Intel Arc\n- [ ] Detection works for NVIDIA + AMD\n- [ ] Training completes without errors\n- [ ] Checkpoints merge correctly\n- [ ] All existing NVIDIA-only functionality preserved\n\n### Nice to Have \ud83c\udfaf\n- [ ] Performance within 10% of theoretical maximum\n- [ ] Automatic load balancing implemented\n- [ ] User documentation complete\n- [ ] Zero user-facing configuration needed\n\n### Stretch Goals \ud83d\ude80\n- [ ] Intel Arc + AMD tested and working\n- [ ] Three-vendor training working\n- [ ] Automatic backend installation\n- [ ] GUI support for mixed GPUs\n\n## Future Enhancements\n\n### Post-Implementation\n1. **Dynamic scaling**: Adjust work distribution based on actual throughput\n2. **Health monitoring**: Detect slow/stuck GPUs and redistribute work\n3. **Power management**: Respect TDP limits per GPU\n4. **Cloud support**: Work with mixed instance types (A100 + V100)\n\n### Research Opportunities\n1. **Cross-vendor communication**: Investigate vendor-agnostic collective ops\n2. **Unified memory**: Explore cross-GPU memory pooling\n3. **Heterogeneous parallelism**: Different model parts on different vendors\n\n## References\n\n### PyTorch Backend Documentation\n- CUDA: https://pytorch.org/docs/stable/cuda.html\n- Intel XPU: https://intel.github.io/intel-extension-for-pytorch/\n- AMD ROCm: https://pytorch.org/docs/stable/notes/hip.html\n\n### Similar Implementations\n- TensorFlow multi-backend: https://www.tensorflow.org/guide/gpu\n- JAX multi-backend: https://jax.readthedocs.io/en/latest/jax.devices.html\n\n## Timeline Estimate\n\n**Aggressive (1 week):**\n- Day 1-2: Phase 1 (Detection)\n- Day 3-4: Phase 2 (Mapping)\n- Day 5: Phase 3 (AMP)\n- Day 6-7: Phase 4 (CLI) + Testing\n\n**Realistic (2 weeks):**\n- Week 1: Phases 1-3 + Initial testing\n- Week 2: Phase 4 + Documentation + Comprehensive testing\n\n**Conservative (1 month):**\n- Week 1-2: Implementation\n- Week 3: Testing on real mixed hardware\n- Week 4: Bug fixes + Documentation + Load balancing\n\n## Open Questions\n\n1. **AMD ROCm detection**: How to reliably distinguish from NVIDIA CUDA?\n2. **Error handling**: What if one GPU fails mid-training?\n3. **Checkpoint format**: Should we store which GPU/vendor trained each checkpoint?\n4. **GUI integration**: How to represent mixed GPUs in the GUI?\n5. **Package management**: Should we bundle all backends or make them optional?\n\n## Decision Log\n\n| Date | Decision | Rationale |\n|------|----------|-----------|\n| 2025-10-18 | Use abstraction layer (GPUDevice) | Cleaner than if/else everywhere |\n| 2025-10-18 | Skip load balancing in Phase 1 | Get basic functionality first |\n| 2025-10-18 | Keep `--cuda-ids` for backwards compat | Don't break existing usage |\n| TBD | Bundle backends or optional? | Pending: install size vs user friction |\n\n## Approval Status\n\n- [ ] Technical Lead Review\n- [ ] Architecture Review\n- [ ] Product Manager Approval\n- [ ] Implementation Started\n- [ ] Testing Complete\n- [ ] Documentation Complete\n- [ ] Released\n\n---\n\n**Last Updated:** 2025-10-18  \n**Next Review:** After Phase 1 implementation  \n**Owner:** @AI-OS-Team\n", "tags": ["gui", "training"], "headings": [{"line": 0, "text": "Mixed GPU Vendor Support for Parallel Training"}, {"line": 9, "text": "Overview"}, {"line": 13, "text": "Use Cases"}, {"line": 15, "text": "Primary Use Cases"}, {"line": 21, "text": "Example Configurations"}, {"line": 27, "text": "Current Limitations"}, {"line": 29, "text": "What Works Now \u2705"}, {"line": 33, "text": "What Doesn't Work \u274c"}, {"line": 39, "text": "Technical Requirements"}, {"line": 41, "text": "PyTorch Backend Support"}, {"line": 43, "text": "NVIDIA GPUs (CUDA)"}, {"line": 51, "text": "AMD GPUs (ROCm)"}, {"line": 54, "text": "Requires: pip install torch+rocm5.7"}, {"line": 59, "text": "Intel Arc/Xe GPUs (XPU)"}, {"line": 67, "text": "Dependencies"}, {"line": 77, "text": "Implementation Plan"}, {"line": 79, "text": "Phase 1: Detection & Enumeration (High Priority)"}, {"line": 103, "text": "NVIDIA (CUDA)"}, {"line": 123, "text": "Intel Arc (XPU)"}, {"line": 145, "text": "AMD (ROCm/HIP) - tricky because it uses CUDA namespace"}, {"line": 146, "text": "Need to check GPU names to distinguish from NVIDIA"}, {"line": 173, "text": "Phase 2: Device Mapping (High Priority)"}, {"line": 237, "text": "Phase 3: AMP Backend Support (Medium Priority)"}, {"line": 247, "text": "Intel XPU uses different AMP API"}, {"line": 264, "text": "Phase 4: CLI Integration (Medium Priority)"}, {"line": 269, "text": "Current (CUDA-only):"}, {"line": 272, "text": "New (auto-detect all vendors):"}, {"line": 274, "text": "Where: 0=NVIDIA, 1=Intel Arc, 2=AMD"}, {"line": 276, "text": "Explicit vendor selection:"}, {"line": 279, "text": "List available GPUs:"}, {"line": 281, "text": "Output:"}, {"line": 282, "text": "ID  Vendor   Model                      VRAM    Backend"}, {"line": 283, "text": "0   NVIDIA   GeForce RTX 3090          24 GB   cuda"}, {"line": 284, "text": "1   Intel    Arc A770                  16 GB   xpu"}, {"line": 285, "text": "2   AMD      Radeon RX 7900 XTX        24 GB   hip"}, {"line": 292, "text": "Phase 5: Load Balancing (Low Priority - Future)"}, {"line": 307, "text": "Base weight from VRAM"}, {"line": 311, "text": "Vendor multiplier (rough performance hierarchy)"}, {"line": 321, "text": "Normalize to sum = 1.0"}, {"line": 325, "text": "Usage:"}, {"line": 327, "text": "Assign data: GPU 0 gets 50%, GPU 1 gets 30%, GPU 2 gets 20%"}, {"line": 330, "text": "Testing Strategy"}, {"line": 332, "text": "Unit Tests"}, {"line": 351, "text": "Only runs if multiple vendor GPUs available"}, {"line": 357, "text": "Run short training"}, {"line": 361, "text": "Manual Testing Checklist"}, {"line": 372, "text": "Performance Considerations"}, {"line": 374, "text": "Bottleneck Analysis"}, {"line": 393, "text": "Backwards Compatibility"}, {"line": 395, "text": "Breaking Changes: None \u2705"}, {"line": 400, "text": "Migration Path"}, {"line": 402, "text": "Old code (still works):"}, {"line": 405, "text": "New code (recommended):"}, {"line": 410, "text": "Documentation Needed"}, {"line": 412, "text": "User Documentation"}, {"line": 417, "text": "Developer Documentation"}, {"line": 422, "text": "Known Limitations"}, {"line": 424, "text": "Phase 1-4 Limitations"}, {"line": 430, "text": "Permanent Limitations"}, {"line": 435, "text": "Success Metrics"}, {"line": 437, "text": "Must Have \u2705"}, {"line": 444, "text": "Nice to Have \ud83c\udfaf"}, {"line": 450, "text": "Stretch Goals \ud83d\ude80"}, {"line": 456, "text": "Future Enhancements"}, {"line": 458, "text": "Post-Implementation"}, {"line": 464, "text": "Research Opportunities"}, {"line": 469, "text": "References"}, {"line": 471, "text": "PyTorch Backend Documentation"}, {"line": 476, "text": "Similar Implementations"}, {"line": 480, "text": "Timeline Estimate"}, {"line": 497, "text": "Open Questions"}, {"line": 505, "text": "Decision Log"}, {"line": 514, "text": "Approval Status"}]}, {"path": "planned_features/MOE_LIGHTNING_INTEGRATION.md", "content": "# MoE-Lightning Integration Plan\n\n## Overview\n\nThis document outlines the plan for integrating **MoE-Lightning** into the AI-OS project to enable high-throughput Mixture-of-Experts (MoE) inference on memory-constrained GPUs. MoE-Lightning is a state-of-the-art system that achieves up to 10.3\u00d7 higher throughput than existing solutions through novel CPU-GPU-I/O pipeline scheduling and a Hierarchical Roofline Model for performance optimization.\n\n**Paper Reference**: [MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs](https://arxiv.org/html/2411.11217)\n\n**Created**: November 8, 2025\n**Status**: Planning Phase\n**Priority**: High\n**Complexity**: High\n\n---\n\n## Table of Contents\n\n1. [Motivation](#motivation)\n2. [Technical Background](#technical-background)\n3. [Core Components](#core-components)\n4. [Integration Architecture](#integration-architecture)\n5. [Implementation Phases](#implementation-phases)\n6. [Technical Requirements](#technical-requirements)\n7. [Performance Targets](#performance-targets)\n8. [Risk Assessment](#risk-assessment)\n9. [Testing Strategy](#testing-strategy)\n10. [Future Enhancements](#future-enhancements)\n11. [References](#references)\n\n---\n\n## Motivation\n\n### Problem Statement\n\nAI-OS currently faces significant challenges when running large Mixture-of-Experts models on memory-constrained hardware:\n\n1. **Limited GPU Memory**: Models like Mixtral 8x7B (~47GB) and Mixtral 8x22B (>256GB) cannot fit entirely in consumer-grade GPU memory (typically 16-24GB)\n2. **Poor Resource Utilization**: Existing offloading solutions (DeepSpeed-Inference, FlexGen) suffer from:\n   - GPU idle time while waiting for data transfers\n   - Inefficient overlap of computation and I/O\n   - Suboptimal batch size selection\n3. **Accessibility Gap**: High-end GPUs are expensive and unavailable to most users who want to experiment with large MoE models\n\n### Benefits of Integration\n\n1. **Dramatic Throughput Improvements**: 3.5-10.3\u00d7 higher throughput on single GPU compared to existing systems\n2. **Memory Efficiency**: Run models with 2-3\u00d7 less CPU memory while maintaining peak throughput\n3. **Better Hardware Utilization**: Efficiently utilize CPU, GPU, and memory bandwidth simultaneously\n4. **Democratization**: Enable more users to run large MoE models on consumer hardware\n5. **Super-linear Scaling**: 2.77-3.38\u00d7 throughput improvement when scaling from 2 to 4 GPUs\n6. **Compatibility**: Works with popular MoE models (Mixtral 8x7B, Mixtral 8x22B, DBRX)\n\n---\n\n## Technical Background\n\n### Mixture of Experts (MoE) Architecture\n\nMoE models use a gating mechanism to route inputs to specialized expert sub-networks:\n- Only a subset of experts are activated per token (sparse activation)\n- Provides better parameter efficiency than dense models\n- Significantly larger memory footprint due to multiple expert FFNs\n- Example: Mixtral 8x7B has 8 experts per layer, activates top-2\n\n### Key Innovations in MoE-Lightning\n\n#### 1. CGOPipe (CPU-GPU-I/O Pipeline Schedule)\n\n**Problem**: Traditional approaches transfer data sequentially, causing bubbles in the pipeline where resources sit idle.\n\n**Solution**: Fine-grained pipelining that overlaps:\n- GPU computation (post-attention, pre-attention tasks)\n- CPU computation (attention with softmax)\n- I/O transfers (weights, hidden states, KV cache)\n\n**Key Technique - Weights Paging**:\n- Chunk weights into `n` pages (where `n` = number of micro-batches)\n- Interleave weight transfers with intermediate result transfers\n- Enable parallel transfers in opposite directions (CPU\u2192GPU and GPU\u2192CPU)\n\n#### 2. HRM (Hierarchical Roofline Model)\n\n**Problem**: Existing performance models don't account for heterogeneous resources and cross-level data movement.\n\n**Solution**: Extended Roofline Model with multiple memory hierarchies:\n\n**Performance Equation**:\n```\nP_x^i = min(P_peak^i, B_peak^i \u00d7 I_x^i, B_peak^(j,i) \u00d7 I_x^j)\n```\n\nWhere:\n- `P_peak^i`: Peak compute at level i (GPU/CPU)\n- `B_peak^i`: Memory bandwidth at level i\n- `B_peak^(j,i)`: Bandwidth from level j to level i (e.g., CPU to GPU)\n- `I_x^i`: Operational intensity of computation x at level i\n\n**Turning Points**: The model identifies critical operational intensities that determine:\n- When to perform computation on CPU vs GPU\n- When the system is GPU memory-bound vs CPU-GPU bandwidth-bound\n- Optimal batch size and micro-batch size combinations\n\n**Balance Point**:\n```\nB_peak^i \u00d7 I_x^i = B_peak^(j,i) \u00d7 I_x^j\n```\nThis represents the optimal configuration where all resources are fully utilized.\n\n#### 3. Tensor Parallelism\n\nUnlike pipeline parallelism (scales with model depth), MoE-Lightning uses tensor parallelism:\n- Scales with layer size\n- Increases total GPU memory capacity linearly\n- Increases GPU memory bandwidth linearly\n- Achieves super-linear scaling in practice (3.38\u00d7 with 4 GPUs vs 2 GPUs)\n\n### Performance Analysis Insights\n\n#### Attention Block\n- Operational intensity independent of batch size\n- For context length 512 on L4 GPU: CPU attention is 3-4\u00d7 faster than KV cache transfer\n- CPU attention becomes bottleneck at large batch sizes and long context lengths\n\n#### MoE FFN Block\n- Operational intensity increases with batch size (more computation per weight access)\n- Memory-bound in decode stage for typical micro-batch sizes\n- Benefits most from weight offloading strategies\n\n---\n\n## Core Components\n\n### Component 1: CGOPipe Scheduler\n\n**Purpose**: Implement fine-grained CPU-GPU-I/O pipeline scheduling\n\n**Key Features**:\n```python\n# Pseudo-code for CGOPipe execution order\nfor decode_step in range(generation_length):\n    # Prologue (first 2 micro-batches)\n    for j in [1, 2]:\n        PreAttn(layer=1, microbatch=j)\n        OffloadQKV(layer=1, microbatch=j)\n        CPUAttn(layer=1, microbatch=j)\n        WeightsCPUtoPin(layer=2, microbatch=j)\n    \n    # Main pipeline (steady state)\n    for layer in range(1, num_layers):\n        for microbatch in range(1, num_microbatches + 1):\n            # Parallel execution\n            PostAttn(layer, microbatch)        # GPU\n            PreAttn(layer, microbatch+1)       # GPU\n            CPUAttn(layer, microbatch+1)       # CPU\n            WeightsPinToGPU(layer+1, page)     # I/O\n```\n\n**Implementation Requirements**:\n- Asynchronous task execution with CUDA streams\n- Synchronization primitives for data dependencies\n- Weight paging system with page table management\n- Dual-buffer for weight transfers (2\u00d7 layer weight size)\n\n### Component 2: HRM Performance Model\n\n**Purpose**: Find optimal execution policies based on hardware, model, and workload\n\n**Policy Search Space**:\n```python\n@dataclass\nclass InferencePolicy:\n    N: int              # Batch size\n    \u03bc: int              # Micro-batch size\n    A_g: bool           # Perform attention on GPU?\n    F_g: bool           # Perform FFN on GPU?\n    r_w: float          # Ratio of weights on GPU (0-1)\n    r_c: float          # Ratio of KV cache on GPU (0-1)\n```\n\n**Optimization Target**:\n```python\ndef optimize_policy(hardware, model, workload):\n    \"\"\"\n    Minimize per-layer latency while satisfying memory constraints\n    \n    T(M, H, W, P) = max(comm_cpu_to_gpu, T_cpu, T_gpu)\n    \n    where:\n    - T_cpu = T_attn_cpu + T_ffn_cpu\n    - T_gpu = T_attn_gpu + T_ffn_gpu\n    - comm_cpu_to_gpu = bytes_transferred / bandwidth_cpu_to_gpu\n    \n    Subject to:\n    - GPU_memory_used \u2264 GPU_memory_capacity\n    - CPU_memory_used \u2264 CPU_memory_capacity\n    \"\"\"\n    # Use MILP solver for policy search\n    # Takes <1 minute for offline optimization\n```\n\n**Model Configuration**:\n- Hardware: GPU/CPU memory, bandwidth, FLOPS\n- Model: Layers, dimensions, expert count, data types\n- Workload: Average prompt length, generation length\n\n### Component 3: Memory Management System\n\n**Weight Paging**:\n```python\nclass WeightPagingManager:\n    \"\"\"\n    Manages paged weight transfers with double buffering\n    \"\"\"\n    def __init__(self, layer_weight_size, num_pages):\n        # Allocate 2\u00d7 layer weight buffer on GPU\n        self.weight_buffer_size = 2 * layer_weight_size\n        self.num_pages = num_pages\n        self.page_size = layer_weight_size / num_pages\n        \n        # Page table for MoE expert routing\n        self.page_table = {}\n    \n    def transfer_page(self, layer, page_id, stream):\n        # CPU DRAM \u2192 CPU Pinned Memory\n        self.copy_to_pinned_async(layer, page_id, stream)\n        \n        # CPU Pinned Memory \u2192 GPU (overlapped)\n        self.copy_to_gpu_async(layer, page_id, stream)\n```\n\n**KV Cache Management**:\n- Store all KV cache on CPU after prefill stage\n- Transfer to GPU only for attention computation (if GPU attention is selected)\n- For CPU attention: keep on CPU, pass hidden states instead\n\n### Component 4: CPU Attention Kernels\n\n**Purpose**: High-performance Grouped Query Attention on CPU\n\n**Implementation**:\n- Based on Intel MKL library optimizations\n- SIMD vectorization for matrix operations\n- Cache-friendly memory access patterns\n- Multi-threaded execution\n\n**Performance Characteristics**:\n- 3-4\u00d7 faster than KV cache transfer on typical hardware\n- Becomes bottleneck at very large batch sizes (>256) or long contexts (>2048)\n\n### Component 5: Request Batching System\n\n**Purpose**: Handle variable-length prompts efficiently without padding\n\n**Algorithm**:\n```python\ndef balanced_batching(requests, num_microbatches, target_batch_size):\n    \"\"\"\n    Distribute requests across micro-batches to balance token counts\n    \n    Returns micro-batches with roughly equal total tokens\n    \"\"\"\n    # Sort requests by length (descending)\n    sorted_requests = sorted(requests, key=lambda r: r.length, reverse=True)\n    \n    microbatches = [[] for _ in range(num_microbatches)]\n    token_counts = [0] * num_microbatches\n    \n    # Greedy assignment to micro-batch with fewest tokens\n    for request in sorted_requests:\n        min_idx = token_counts.index(min(token_counts))\n        microbatches[min_idx].append(request)\n        token_counts[min_idx] += request.length\n    \n    return microbatches\n```\n\n---\n\n## Integration Architecture\n\n### System Architecture Diagram\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       AI-OS Core                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502         MoE-Lightning Integration Layer                \u2502  \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502\n\u2502  \u2502                                                         \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u2502\n\u2502  \u2502  \u2502   HRM Model  \u2502  \u2502  CGOPipe     \u2502  \u2502  Policy     \u2502 \u2502  \u2502\n\u2502  \u2502  \u2502  Optimizer   \u2502\u2190\u2192\u2502  Scheduler   \u2502\u2190\u2192\u2502  Cache      \u2502 \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  \u2502\n\u2502  \u2502         \u2193                  \u2193                           \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u2502\n\u2502  \u2502  \u2502   Weight     \u2502  \u2502  Request     \u2502  \u2502  CPU Attn   \u2502 \u2502  \u2502\n\u2502  \u2502  \u2502   Paging     \u2502  \u2502  Batching    \u2502  \u2502  Kernels    \u2502 \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                           \u2195                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502         Existing AI-OS Components                      \u2502  \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502\n\u2502  \u2502  \u2022 HuggingFace Model Loading                          \u2502  \u2502\n\u2502  \u2502  \u2022 vLLM/SGLang Integration                            \u2502  \u2502\n\u2502  \u2502  \u2022 Memory Estimation System                           \u2502  \u2502\n\u2502  \u2502  \u2022 Expert Manager                                     \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                           \u2195                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Hardware Abstraction Layer               \u2502  \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502\n\u2502  \u2502  GPU (CUDA)  \u2502  CPU (MKL)  \u2502  Memory (Pinned/Paged)  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Integration Points\n\n#### 1. Model Loading Layer\n- Extend existing HuggingFace model loading in `aios/brain.py`\n- Detect MoE architecture (Mixtral, DBRX, DeepSeek-MoE)\n- Configure weight storage strategy (GPU/CPU split based on policy)\n\n#### 2. Inference Engine Layer\n- New module: `aios/inference/moe_lightning/`\n- Interface with existing inference systems (vLLM, SGLang)\n- Provide unified API for MoE model inference\n\n#### 3. Memory Management Layer\n- Integrate with existing memory estimation (`artifacts/memory_estimation/`)\n- Extend GPU memory tracking\n- Add CPU memory and pinned memory tracking\n\n#### 4. CLI Integration\n- Add commands to `aios/cli/aios.py`:\n  ```bash\n  aios infer-moe --model mixtral-8x7b --strategy moe-lightning --batch-size auto\n  aios profile-moe --model mixtral-8x7b --hardware-config gpu_config.yaml\n  ```\n\n#### 5. Configuration Layer\n- New config file: `config/moe_lightning.yaml`\n- Hardware profiles for common GPU configurations (T4, L4, A100, etc.)\n- Model-specific optimization profiles\n\n---\n\n## Implementation Phases\n\n### Phase 1: Foundation & Research (Weeks 1-3)\n\n**Objectives**:\n- Deep dive into MoE-Lightning paper and codebase\n- Set up development environment\n- Implement basic prototype\n\n**Tasks**:\n1. **Code Analysis**\n   - Study MoE-Lightning reference implementation (if available)\n   - Analyze vLLM and SGLang MoE support\n   - Document API interfaces and extension points\n\n2. **Prototype Development**\n   - Implement basic HRM model for simple 2-level hierarchy (CPU/GPU)\n   - Create simplified weight paging mechanism\n   - Benchmark baseline performance with existing systems\n\n3. **Environment Setup**\n   - Configure test environments with various GPU configs (T4, L4)\n   - Set up profiling tools (NVIDIA Nsight, Intel VTune)\n   - Prepare test datasets (MTBench, HELM benchmarks)\n\n**Deliverables**:\n- Technical design document with architecture diagrams\n- Proof-of-concept code demonstrating HRM policy optimization\n- Baseline performance benchmarks\n\n**Success Criteria**:\n- HRM model correctly predicts bottleneck resources\n- Prototype shows measurable improvement over naive offloading\n- Development environment ready for full implementation\n\n---\n\n### Phase 2: Core Components Implementation (Weeks 4-8)\n\n**Objectives**:\n- Implement CGOPipe scheduler\n- Develop weight paging system\n- Create CPU attention kernels\n\n**Tasks**:\n\n#### 2.1 HRM Performance Model (Week 4-5)\n```python\n# aios/inference/moe_lightning/hrm/model.py\nclass HierarchicalRooflineModel:\n    \"\"\"\n    Performance model for heterogeneous MoE inference\n    \"\"\"\n    def __init__(self, hardware_config, model_config):\n        self.hw = hardware_config\n        self.model = model_config\n        \n    def estimate_latency(self, policy: InferencePolicy) -> float:\n        \"\"\"Estimate per-layer decode latency\"\"\"\n        T_comm = self._compute_communication_time(policy)\n        T_cpu = self._compute_cpu_time(policy)\n        T_gpu = self._compute_gpu_time(policy)\n        \n        return max(T_comm, T_cpu, T_gpu)\n    \n    def optimize_policy(self, workload_config) -> InferencePolicy:\n        \"\"\"Use MILP to find optimal policy\"\"\"\n        # Implement using scipy.optimize or CVXPY\n        pass\n```\n\n#### 2.2 CGOPipe Scheduler (Week 5-6)\n```python\n# aios/inference/moe_lightning/scheduler/cgopipe.py\nclass CGOPipeScheduler:\n    \"\"\"\n    CPU-GPU-I/O Pipeline Scheduler with weights paging\n    \"\"\"\n    def __init__(self, policy: InferencePolicy, model, device_manager):\n        self.policy = policy\n        self.model = model\n        self.dm = device_manager\n        \n        # Initialize CUDA streams\n        self.gpu_stream = torch.cuda.Stream()\n        self.transfer_stream = torch.cuda.Stream()\n        \n        # Initialize weight paging\n        self.weight_pager = WeightPagingManager(\n            layer_weight_size=model.layer_size,\n            num_pages=policy.\u03bc\n        )\n    \n    def execute_decode_step(self, microbatches):\n        \"\"\"Execute one decode step with pipelined scheduling\"\"\"\n        # Implement Algorithm 1 from paper\n        pass\n```\n\n#### 2.3 Weight Paging System (Week 6-7)\n```python\n# aios/inference/moe_lightning/memory/weight_paging.py\nclass WeightPagingManager:\n    \"\"\"\n    Manages paged transfers of model weights between CPU and GPU\n    \"\"\"\n    def __init__(self, layer_weight_size, num_pages):\n        self.page_size = layer_weight_size // num_pages\n        self.num_pages = num_pages\n        \n        # Allocate pinned memory buffer\n        self.pinned_buffer = self._allocate_pinned_buffer()\n        \n        # Page table for expert routing\n        self.page_table = PageTable()\n    \n    def prefetch_page(self, layer_id, page_id, stream):\n        \"\"\"Asynchronously prefetch weight page\"\"\"\n        # CPU DRAM \u2192 CPU Pinned (background thread)\n        self._copy_to_pinned_async(layer_id, page_id)\n        \n        # CPU Pinned \u2192 GPU (CUDA stream)\n        self._copy_to_gpu_async(layer_id, page_id, stream)\n```\n\n#### 2.4 CPU Attention Kernels (Week 7-8)\n```python\n# aios/inference/moe_lightning/kernels/cpu_attention.py\nimport intel_extension_for_pytorch as ipex\n\nclass CPUGroupedQueryAttention:\n    \"\"\"\n    Optimized CPU attention using Intel MKL\n    \"\"\"\n    def __init__(self, num_heads, num_kv_heads, head_dim):\n        self.num_heads = num_heads\n        self.num_kv_heads = num_kv_heads\n        self.head_dim = head_dim\n        \n        # Configure MKL threads\n        torch.set_num_threads(self._get_optimal_threads())\n    \n    @torch.jit.script\n    def forward(self, query, key_cache, value_cache, seq_lens):\n        \"\"\"\n        Compute attention on CPU with GQA optimization\n        \n        Args:\n            query: [batch, num_heads, head_dim]\n            key_cache: [batch, max_seq_len, num_kv_heads, head_dim]\n            value_cache: [batch, max_seq_len, num_kv_heads, head_dim]\n            seq_lens: [batch]\n        \"\"\"\n        # Implement optimized GQA with SIMD vectorization\n        pass\n```\n\n**Deliverables**:\n- Fully functional HRM model with policy optimizer\n- Working CGOPipe scheduler with async execution\n- CPU attention kernels matching or exceeding KV cache transfer speed\n- Weight paging system with double buffering\n\n**Success Criteria**:\n- HRM policy optimizer runs in <1 minute for typical configs\n- CGOPipe achieves >80% resource utilization (GPU, CPU, I/O)\n- CPU attention is 3-4\u00d7 faster than KV cache transfer\n- Weight paging reduces pipeline bubbles by >50%\n\n---\n\n### Phase 3: Integration & Optimization (Weeks 9-12)\n\n**Objectives**:\n- Integrate components into AI-OS\n- Implement request batching\n- Optimize end-to-end performance\n\n**Tasks**:\n\n#### 3.1 AI-OS Integration (Week 9-10)\n- Create `aios/inference/moe_lightning/` module structure\n- Implement unified inference API\n- Add MoE detection logic to model loading\n- Extend CLI with MoE-Lightning commands\n\n#### 3.2 Request Batching System (Week 10)\n```python\n# aios/inference/moe_lightning/batching/request_batcher.py\nclass VariableLengthBatcher:\n    \"\"\"\n    Batch variable-length requests without padding\n    \"\"\"\n    def create_microbatches(self, requests, policy):\n        \"\"\"\n        Implement Algorithm 2 from paper\n        Balances token distribution across micro-batches\n        \"\"\"\n        pass\n```\n\n#### 3.3 Tensor Parallelism Support (Week 11)\n```python\n# aios/inference/moe_lightning/distributed/tensor_parallel.py\nclass TensorParallelExecutor:\n    \"\"\"\n    Execute MoE inference with tensor parallelism across GPUs\n    \"\"\"\n    def __init__(self, num_gpus, policy):\n        self.num_gpus = num_gpus\n        self.device_mesh = self._create_device_mesh()\n        \n        # Scale policy parameters for multiple GPUs\n        self.adjusted_policy = self._adjust_policy_for_tp(policy)\n```\n\n#### 3.4 Performance Profiling & Optimization (Week 11-12)\n- Profile with NVIDIA Nsight Systems\n- Identify bottlenecks in data transfer and synchronization\n- Optimize kernel launch overhead\n- Tune thread counts for CPU operations\n- Implement caching for policy optimization results\n\n**Deliverables**:\n- Complete integration with AI-OS inference pipeline\n- Variable-length batching system\n- Tensor parallelism for multi-GPU setups\n- Performance optimization report\n\n**Success Criteria**:\n- API is compatible with existing AI-OS inference workflows\n- Variable-length batching provides 2-3\u00d7 memory savings vs padding\n- Tensor parallelism achieves >2.5\u00d7 speedup on 4 GPUs vs 2 GPUs\n- End-to-end latency overhead <5% compared to direct execution\n\n---\n\n### Phase 4: Testing & Validation (Weeks 13-15)\n\n**Objectives**:\n- Comprehensive testing across models and hardware\n- Validate performance claims\n- Ensure numerical correctness\n\n**Tasks**:\n\n#### 4.1 Correctness Testing (Week 13)\n- Compare outputs with reference implementations (vLLM, transformers)\n- Test with multiple MoE architectures (Mixtral, DBRX, DeepSeek-MoE)\n- Validate attention correctness (CPU vs GPU implementation)\n- Memory safety checks (no leaks, proper cleanup)\n\n#### 4.2 Performance Benchmarking (Week 14)\n```python\n# tests/benchmarks/test_moe_lightning_performance.py\nclass MoELightningBenchmarks:\n    \"\"\"\n    Comprehensive performance benchmarks\n    \"\"\"\n    def benchmark_throughput(self, model, hardware, workload):\n        \"\"\"\n        Measure end-to-end throughput for various configurations\n        \n        Compare against:\n        - FlexGen\n        - DeepSpeed-Inference\n        - vLLM (if fits in memory)\n        \"\"\"\n        pass\n    \n    def benchmark_memory_efficiency(self, model, hardware):\n        \"\"\"\n        Measure CPU/GPU memory usage at peak throughput\n        \"\"\"\n        pass\n    \n    def benchmark_scaling(self, model, num_gpus_list):\n        \"\"\"\n        Test tensor parallelism scaling efficiency\n        \"\"\"\n        pass\n```\n\n**Test Matrices**:\n\n| Model | Hardware | Workload | Expected Speedup |\n|-------|----------|----------|------------------|\n| Mixtral 8x7B | 1\u00d7T4 (16GB) | MTBench (gen_len=128) | 3.5\u00d7 vs FlexGen |\n| Mixtral 8x7B | 1\u00d7L4 (24GB) | HELM Reasoning | 5\u00d7 vs FlexGen |\n| Mixtral 8x22B | 2\u00d7T4 (32GB) | MTBench (gen_len=64) | 2.8\u00d7 vs FlexGen |\n| Mixtral 8x22B | 4\u00d7T4 (64GB) | MTBench (gen_len=64) | Super-linear scaling |\n| DBRX | 4\u00d7T4 (64GB) | MTBench (gen_len=128) | 2.1-2.8\u00d7 scaling |\n\n#### 4.3 Stress Testing (Week 15)\n- Long-running inference jobs (24+ hours)\n- Extreme batch sizes (pushing memory limits)\n- Error handling and recovery\n- Multi-user concurrent requests\n\n**Deliverables**:\n- Comprehensive test suite with >90% coverage\n- Benchmark results report comparing to baseline systems\n- Validated correctness across all supported models\n- Stress test results and reliability metrics\n\n**Success Criteria**:\n- All correctness tests pass with numerical differences <1e-5\n- Achieve paper's reported speedups (within 10% margin)\n- No memory leaks or crashes in 24-hour stress tests\n- Error recovery works for common failure modes\n\n---\n\n### Phase 5: Documentation & Deployment (Weeks 16-17)\n\n**Objectives**:\n- Create comprehensive documentation\n- Prepare for production deployment\n- Train users and gather feedback\n\n**Tasks**:\n\n#### 5.1 User Documentation (Week 16)\n```markdown\n# docs/guide/moe_lightning_quickstart.md\n## MoE-Lightning Quick Start\n\nLearn how to run large MoE models on consumer GPUs with MoE-Lightning.\n\n### Installation\n### Basic Usage\n### Configuration Guide\n### Performance Tuning\n### Troubleshooting\n```\n\n#### 5.2 Developer Documentation (Week 16)\n- API reference documentation\n- Architecture diagrams and design decisions\n- Contribution guidelines for MoE-Lightning components\n- Performance profiling guide\n\n#### 5.3 Example Notebooks (Week 17)\n```python\n# examples/moe_lightning_mixtral.ipynb\n\"\"\"\nRunning Mixtral 8x7B on a Single T4 GPU\n\nThis notebook demonstrates:\n1. Model loading and configuration\n2. Policy optimization for your hardware\n3. Running inference with MoE-Lightning\n4. Comparing performance to baseline systems\n\"\"\"\n```\n\n#### 5.4 Deployment Preparation (Week 17)\n- Docker images with optimized dependencies\n- Installation scripts for common platforms\n- Hardware compatibility matrix\n- Known issues and workarounds\n\n**Deliverables**:\n- Complete user and developer documentation\n- Example notebooks and tutorials\n- Deployment artifacts (Docker images, installers)\n- Performance tuning guide\n\n**Success Criteria**:\n- Documentation covers all use cases and configurations\n- New users can run first inference within 15 minutes\n- Examples run successfully on documented hardware\n- Docker deployment works on Ubuntu 20.04/22.04 and Windows 11\n\n---\n\n### Phase 6: Advanced Features & Extensions (Weeks 18-20)\n\n**Objectives**:\n- Add advanced optimizations\n- Support additional models and hardware\n- Integrate with AI-OS ecosystem\n\n**Tasks**:\n\n#### 6.1 Extended Hardware Support (Week 18)\n- AMD GPU support (ROCm)\n- Apple Silicon support (MPS backend)\n- Intel GPU support (oneAPI)\n- Multi-node distributed inference\n\n#### 6.2 Advanced Optimizations (Week 19)\n- KV cache quantization (INT4, INT8)\n- Sparse attention patterns\n- Expert caching for common routing patterns\n- Dynamic policy adjustment based on runtime metrics\n\n#### 6.3 AI-OS Ecosystem Integration (Week 20)\n- Integration with Expert Manager for MoE expert tracking\n- Memory estimation updates for MoE-Lightning\n- Dream system integration for synthetic data generation\n- CLI enhancements for interactive optimization\n\n**Deliverables**:\n- Multi-platform hardware support\n- Advanced optimization features\n- Deep integration with AI-OS features\n\n**Success Criteria**:\n- Works on at least 2 additional hardware platforms\n- Advanced optimizations provide additional 1.2-1.5\u00d7 speedup\n- Seamless integration with existing AI-OS workflows\n\n---\n\n## Technical Requirements\n\n### Hardware Requirements\n\n#### Minimum Configuration\n- **GPU**: NVIDIA T4 (16GB) or equivalent\n- **CPU**: 8-core, 2.0+ GHz\n- **RAM**: 64GB DDR4\n- **Storage**: 500GB SSD for model weights\n- **PCIe**: Gen3 x16 for optimal CPU-GPU bandwidth\n\n#### Recommended Configuration\n- **GPU**: NVIDIA L4 (24GB) or 2\u00d7 T4 (32GB total)\n- **CPU**: 16-core, 2.5+ GHz (e.g., Intel Xeon)\n- **RAM**: 128GB DDR4 or better\n- **Storage**: 1TB NVMe SSD\n- **PCIe**: Gen4 x16\n\n#### Optimal Configuration\n- **GPU**: 4\u00d7 NVIDIA T4 (64GB) or 2\u00d7 A100 (80GB each)\n- **CPU**: 32-core, 3.0+ GHz\n- **RAM**: 256GB+ DDR5\n- **Storage**: 2TB NVMe SSD RAID\n- **Network**: 100Gbps for multi-node (future)\n\n### Software Requirements\n\n#### Core Dependencies\n```python\n# pyproject.toml additions\n[project.dependencies]\ntorch = \">=2.1.0\"\nintel-extension-for-pytorch = \">=2.1.0\"  # For CPU kernels\nvllm = \">=0.2.0\"  # For MoE support\nsglang = \">=0.1.0\"  # For structured generation\nscipy = \">=1.10.0\"  # For optimization\ncvxpy = \">=1.4.0\"  # For MILP solver (optional)\n```\n\n#### System Requirements\n- **CUDA**: 12.1+ (for NVIDIA GPUs)\n- **cuDNN**: 8.9+\n- **Intel MKL**: 2023.0+ (for CPU operations)\n- **Python**: 3.10+\n- **OS**: Ubuntu 20.04+, Windows 11, or macOS 13+\n\n### Model Support\n\n#### Supported Architectures\n1. **Mixtral Family**\n   - Mixtral 8x7B (~47GB)\n   - Mixtral 8x22B (~141GB)\n   \n2. **DBRX**\n   - DBRX 132B (16 experts)\n   \n3. **DeepSeek-MoE**\n   - DeepSeek-MoE 16B\n   - DeepSeek-MoE 145B\n\n4. **Future Support**\n   - Custom MoE architectures\n   - Dense model fallback mode\n\n#### Model Format Support\n- HuggingFace Transformers format\n- SafeTensors format (preferred)\n- GGUF format (via conversion)\n\n---\n\n## Performance Targets\n\n### Throughput Targets\n\nBased on paper results, we target the following throughput improvements:\n\n#### Single GPU (T4 16GB)\n| Workload | Baseline (FlexGen) | Target (MoE-Lightning) | Speedup |\n|----------|-------------------|------------------------|---------|\n| MTBench (gen=32) | 6.5 tok/s | 22.8 tok/s | 3.5\u00d7 |\n| MTBench (gen=128) | 9.5 tok/s | 30.1 tok/s | 3.2\u00d7 |\n| HELM Reasoning | 16.9 tok/s | 26.3 tok/s | 1.6\u00d7 |\n| HELM Summarization | 2.6 tok/s | 4.5 tok/s | 1.7\u00d7 |\n\n#### Single GPU (L4 24GB)\n| Workload | Baseline (FlexGen) | Target (MoE-Lightning) | Speedup |\n|----------|-------------------|------------------------|---------|\n| MTBench (gen=128) | 20.7 tok/s | 105.3 tok/s | 5.1\u00d7 |\n| HELM Reasoning | 50.1 tok/s | 105.3 tok/s | 2.1\u00d7 |\n\n#### Multi-GPU (4\u00d7T4 64GB)\n| Model | 2\u00d7T4 | 4\u00d7T4 | Scaling Factor |\n|-------|------|------|----------------|\n| Mixtral 8x22B | 25.3 tok/s | 70.2 tok/s | 2.77\u00d7 |\n| DBRX | 22.1 tok/s | 58.3 tok/s | 2.64\u00d7 |\n\n### Memory Efficiency Targets\n\n| Metric | Target | Baseline |\n|--------|--------|----------|\n| CPU Memory at Peak Throughput | 100GB | 200GB+ |\n| GPU Memory Utilization | >85% | 60-70% |\n| I/O Bandwidth Utilization | >90% | 50-60% |\n| Pipeline Bubble Reduction | >50% | N/A |\n\n### Latency Targets\n\n| Phase | Target | Acceptable Range |\n|-------|--------|------------------|\n| Policy Optimization | <1 minute | <5 minutes |\n| Model Loading | <30 seconds | <60 seconds |\n| First Token Latency | <2 seconds | <5 seconds |\n| Per-token Latency (decode) | <100ms | <200ms |\n\n---\n\n## Risk Assessment\n\n### Technical Risks\n\n#### Risk 1: Performance Below Targets\n**Probability**: Medium  \n**Impact**: High\n\n**Description**: Achieved performance doesn't match paper's reported improvements\n\n**Mitigation**:\n- Start with exact replication of paper's test setup\n- Profile extensively to identify bottlenecks\n- Engage with paper authors for implementation guidance\n- Have fallback to incremental improvements (e.g., 2\u00d7 instead of 10\u00d7)\n\n#### Risk 2: Hardware Compatibility Issues\n**Probability**: Medium  \n**Impact**: Medium\n\n**Description**: CPU attention or weight paging doesn't work on all hardware\n\n**Mitigation**:\n- Test on multiple hardware configurations early\n- Implement fallback to GPU-only execution\n- Use platform-agnostic libraries where possible\n- Maintain compatibility matrix in documentation\n\n#### Risk 3: Memory Management Complexity\n**Probability**: High  \n**Impact**: High\n\n**Description**: Memory leaks or fragmentation under high load\n\n**Mitigation**:\n- Extensive memory profiling (valgrind, CUDA sanitizers)\n- Implement comprehensive cleanup logic\n- Use smart pointers and RAII patterns\n- Regular stress testing during development\n\n#### Risk 4: Integration Conflicts\n**Probability**: Medium  \n**Impact**: Medium\n\n**Description**: Conflicts with existing AI-OS inference systems\n\n**Mitigation**:\n- Design clean interface boundaries\n- Make integration opt-in initially\n- Comprehensive integration testing\n- Version compatibility testing\n\n### Project Risks\n\n#### Risk 5: Scope Creep\n**Probability**: High  \n**Impact**: Medium\n\n**Description**: Feature requests expand beyond core MoE-Lightning\n\n**Mitigation**:\n- Clearly define Phase 1-3 deliverables as MVP\n- Defer advanced features to Phase 6\n- Regular scope reviews with stakeholders\n- Maintain feature backlog for future work\n\n#### Risk 6: Resource Constraints\n**Probability**: Medium  \n**Impact**: High\n\n**Description**: Insufficient GPU resources for testing\n\n**Mitigation**:\n- Use cloud resources (GCP, AWS) for expensive tests\n- Prioritize tests on available hardware\n- Implement simulation mode for policy testing\n- Partner with organizations with GPU access\n\n#### Risk 7: Dependency Changes\n**Probability**: Medium  \n**Impact**: Medium\n\n**Description**: Breaking changes in PyTorch, vLLM, or other dependencies\n\n**Mitigation**:\n- Pin dependency versions initially\n- Monitor upstream changes\n- Contribute to upstream projects\n- Maintain compatibility layer\n\n---\n\n## Testing Strategy\n\n### Unit Testing\n\n**Coverage Target**: >90% for core components\n\n```python\n# tests/unit/test_hrm_model.py\nclass TestHierarchicalRooflineModel:\n    def test_compute_roofs(self):\n        \"\"\"Test compute and memory roof calculations\"\"\"\n        \n    def test_turning_points(self):\n        \"\"\"Test turning point identification\"\"\"\n        \n    def test_policy_optimization(self):\n        \"\"\"Test MILP policy search\"\"\"\n        \n    def test_memory_constraints(self):\n        \"\"\"Test policy respects memory limits\"\"\"\n\n# tests/unit/test_cgopipe.py\nclass TestCGOPipeScheduler:\n    def test_async_execution(self):\n        \"\"\"Test asynchronous task execution\"\"\"\n        \n    def test_synchronization(self):\n        \"\"\"Test data dependency enforcement\"\"\"\n        \n    def test_weight_paging(self):\n        \"\"\"Test weight page scheduling\"\"\"\n\n# tests/unit/test_cpu_attention.py\nclass TestCPUAttention:\n    def test_correctness(self):\n        \"\"\"Compare output with reference implementation\"\"\"\n        \n    def test_performance(self):\n        \"\"\"Verify speedup vs KV cache transfer\"\"\"\n```\n\n### Integration Testing\n\n```python\n# tests/integration/test_moe_lightning_inference.py\nclass TestMoELightningInference:\n    def test_mixtral_8x7b_single_gpu(self):\n        \"\"\"Test Mixtral 8x7B on single T4\"\"\"\n        \n    def test_mixtral_8x22b_multi_gpu(self):\n        \"\"\"Test Mixtral 8x22B on multiple GPUs\"\"\"\n        \n    def test_dbrx_inference(self):\n        \"\"\"Test DBRX model\"\"\"\n        \n    def test_variable_length_batching(self):\n        \"\"\"Test with mixed prompt lengths\"\"\"\n```\n\n### Performance Testing\n\n```python\n# tests/performance/test_throughput.py\nclass TestThroughput:\n    @pytest.mark.benchmark\n    def test_mtbench_t4(self):\n        \"\"\"Benchmark MTBench on T4 GPU\"\"\"\n        assert throughput > 22.8  # tokens/sec\n        \n    @pytest.mark.benchmark\n    def test_helm_reasoning_l4(self):\n        \"\"\"Benchmark HELM reasoning on L4\"\"\"\n        assert throughput > 105.3  # tokens/sec\n        \n    @pytest.mark.benchmark\n    def test_scaling_4xT4(self):\n        \"\"\"Test super-linear scaling\"\"\"\n        scaling_factor = throughput_4gpu / throughput_2gpu\n        assert scaling_factor > 2.5\n```\n\n### Correctness Testing\n\n```python\n# tests/correctness/test_numerical_accuracy.py\nclass TestNumericalAccuracy:\n    def test_cpu_vs_gpu_attention(self):\n        \"\"\"Verify CPU attention matches GPU\"\"\"\n        max_diff = compute_max_difference(cpu_output, gpu_output)\n        assert max_diff < 1e-5\n        \n    def test_vs_vllm_reference(self):\n        \"\"\"Compare outputs with vLLM\"\"\"\n        assert outputs_match(moe_lightning_output, vllm_output)\n```\n\n### Stress Testing\n\n```python\n# tests/stress/test_reliability.py\nclass TestReliability:\n    def test_24_hour_continuous_inference(self):\n        \"\"\"Run inference for 24 hours\"\"\"\n        \n    def test_memory_leak_detection(self):\n        \"\"\"Monitor memory usage over 1000 batches\"\"\"\n        \n    def test_concurrent_requests(self):\n        \"\"\"Handle 100 concurrent requests\"\"\"\n```\n\n---\n\n## Future Enhancements\n\n### Short-term (6 months)\n\n1. **Flash Attention Integration**\n   - Integrate Flash Attention 2/3 for GPU attention\n   - Further reduce memory footprint\n   - Improve attention performance\n\n2. **Quantization Support**\n   - INT8/INT4 weight quantization\n   - KV cache quantization\n   - GPTQ/AWQ integration\n\n3. **Speculative Decoding**\n   - Use smaller MoE model as draft\n   - Improve latency for interactive use cases\n\n4. **Expert Caching**\n   - Cache frequently activated experts on GPU\n   - Dynamic expert placement based on routing patterns\n\n### Mid-term (12 months)\n\n1. **Multi-Node Distributed Inference**\n   - Pipeline parallelism across nodes\n   - Expert parallelism\n   - Optimize for cluster environments\n\n2. **Continuous Batching**\n   - Orca-style continuous batching\n   - Improve throughput for serving workloads\n\n3. **Adaptive Policy Selection**\n   - Runtime policy adjustment\n   - Workload-aware optimization\n   - Reinforcement learning for policy search\n\n4. **AMD/Intel GPU Support**\n   - ROCm backend for AMD GPUs\n   - OneAPI backend for Intel GPUs\n   - Multi-vendor heterogeneous execution\n\n### Long-term (18+ months)\n\n1. **Automatic Model Parallelism**\n   - Automatic sharding for arbitrary MoE sizes\n   - Mixed expert and tensor parallelism\n   - Cost-aware placement optimization\n\n2. **Disk Offloading**\n   - NVMe SSD integration for very large models\n   - Intelligent prefetching\n   - Compression for disk storage\n\n3. **Custom CUDA Kernels**\n   - Fused MoE kernels\n   - Optimized expert routing\n   - Custom attention implementations\n\n4. **Neural Architecture Search for MoE**\n   - Automatic expert configuration\n   - Router optimization\n   - Efficient expert specialization\n\n---\n\n## Success Metrics\n\n### Technical Metrics\n\n| Metric | Target | Method |\n|--------|--------|--------|\n| Throughput vs FlexGen | 3.5-10\u00d7 | Benchmark comparison |\n| Memory efficiency | 2-3\u00d7 less CPU RAM | Memory profiling |\n| GPU utilization | >85% | NVIDIA profiler |\n| I/O utilization | >90% | Bandwidth monitoring |\n| Scaling efficiency (4 GPUs) | >2.5\u00d7 vs 2 GPUs | Multi-GPU benchmarks |\n| Policy search time | <1 minute | Timer measurement |\n| First token latency | <2 seconds | Latency profiling |\n\n### Project Metrics\n\n| Metric | Target | Method |\n|--------|--------|--------|\n| Code coverage | >90% | pytest-cov |\n| Documentation coverage | 100% of public APIs | Doc review |\n| User adoption | 50+ users in first month | Analytics |\n| Bug reports | <5 critical bugs | Issue tracking |\n| Performance regression | <5% | CI/CD benchmarks |\n| Community contributions | 5+ contributors | GitHub metrics |\n\n### User Experience Metrics\n\n| Metric | Target | Method |\n|--------|--------|--------|\n| Time to first inference | <15 minutes | User studies |\n| Setup success rate | >90% | Telemetry |\n| User satisfaction | >4/5 rating | Surveys |\n| Documentation clarity | >4/5 rating | Feedback forms |\n\n---\n\n## References\n\n### Primary Paper\n- **MoE-Lightning**: Shiyi Cao et al., \"MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs,\" arXiv:2411.11217, 2024. [Link](https://arxiv.org/html/2411.11217)\n\n### Related Papers\n\n#### MoE Architectures\n- **Mixtral**: \"Mixtral of Experts,\" Mistral AI, 2024\n- **DBRX**: \"Introducing DBRX,\" Databricks, 2024\n- **DeepSeek-MoE**: \"DeepSeekMoE: Towards Ultimate Expert Specialization,\" 2024\n- **GShard**: Lepikhin et al., \"GShard: Scaling Giant Models with Conditional Computation,\" 2020\n\n#### Performance Modeling\n- **Roofline Model**: Williams et al., \"Roofline: An Insightful Visual Performance Model,\" CACM 2009\n- **LLM Inference Analysis**: Yuan et al., \"LLM Inference Unveiled: Survey and Roofline Model Insights,\" 2024\n\n#### Inference Systems\n- **FlexGen**: Sheng et al., \"FlexGen: High-throughput Generative Inference,\" ICML 2023\n- **vLLM**: Kwon et al., \"Efficient Memory Management for LLM Serving with PagedAttention,\" SOSP 2023\n- **DeepSpeed-Inference**: Aminabadi et al., \"DeepSpeed-Inference: Enabling Efficient Inference,\" SC 2022\n- **FastDecode**: He & Zhai, \"FastDecode: High-throughput GPU-efficient LLM Serving,\" 2024\n\n#### Optimization Techniques\n- **Flash Attention**: Dao et al., \"FlashAttention: Fast and Memory-Efficient Exact Attention,\" NeurIPS 2022\n- **Flash Attention 2**: Dao, \"FlashAttention-2: Faster Attention with Better Parallelism,\" ICLR 2024\n- **Speculative Decoding**: Chen et al., \"Accelerating LLM Decoding with Speculative Sampling,\" 2023\n\n### Implementation References\n- PyTorch Documentation: https://pytorch.org/docs/stable/\n- vLLM GitHub: https://github.com/vllm-project/vllm\n- SGLang GitHub: https://github.com/sgl-project/sglang\n- Intel Extension for PyTorch: https://github.com/intel/intel-extension-for-pytorch\n- HuggingFace Transformers: https://github.com/huggingface/transformers\n\n### AI-OS Related\n- Existing AI-OS architecture documentation\n- Memory estimation system (`artifacts/memory_estimation/`)\n- Expert management system (`artifacts/experts/`)\n- HRM training integration (`aios/cli/aios.py` - HRM commands)\n\n---\n\n## Appendix\n\n### A. Glossary\n\n- **CGOPipe**: CPU-GPU-I/O Pipeline scheduling strategy\n- **HRM**: Hierarchical Roofline Model\n- **MoE**: Mixture of Experts\n- **GQA**: Grouped Query Attention\n- **FFN**: Feed-Forward Network\n- **Operational Intensity**: Ratio of FLOPs to bytes accessed (FLOPs/Byte)\n- **Roofline Model**: Performance model correlating compute and memory bandwidth\n- **Turning Point**: Critical operational intensity where bottleneck resource changes\n- **Balance Point**: Optimal configuration where all resources are fully utilized\n- **Micro-batch**: Subset of batch that fits in GPU memory for one kernel execution\n- **Weight Paging**: Technique of chunking and scheduling weight transfers\n\n### B. Hardware Specifications\n\n#### NVIDIA T4\n- Memory: 16GB GDDR6\n- Memory Bandwidth: 320 GB/s\n- Compute (FP16): 65 TFLOPS\n- TDP: 70W\n- Use Case: Cost-effective inference\n\n#### NVIDIA L4\n- Memory: 24GB GDDR6\n- Memory Bandwidth: 300 GB/s\n- Compute (FP16): 121 TFLOPS\n- TDP: 72W\n- Use Case: Balanced performance/cost\n\n#### NVIDIA A100\n- Memory: 40GB or 80GB HBM2e\n- Memory Bandwidth: 1.6 TB/s (40GB) / 2.0 TB/s (80GB)\n- Compute (FP16): 312 TFLOPS\n- TDP: 400W\n- Use Case: High-performance inference\n\n### C. Model Specifications\n\n#### Mixtral 8x7B\n- Total Parameters: 46.7B\n- Active Parameters: 12.9B per token\n- Experts: 8 per MoE layer\n- Top-K: 2\n- Hidden Dim: 4096\n- Intermediate Dim: 14336\n- Layers: 32\n- Memory (FP16): ~94GB\n\n#### Mixtral 8x22B\n- Total Parameters: 141B\n- Active Parameters: ~39B per token\n- Experts: 8 per MoE layer\n- Top-K: 2\n- Hidden Dim: 6144\n- Memory (FP16): ~282GB\n\n#### DBRX\n- Total Parameters: 132B\n- Active Parameters: 36B per token\n- Experts: 16 per MoE layer\n- Top-K: 4\n- Layers: 40\n- Memory (FP16): ~264GB\n\n### D. Configuration Examples\n\n#### config/moe_lightning.yaml\n```yaml\n# Hardware profiles\nhardware:\n  t4_single:\n    gpu_memory: 16384  # MB\n    cpu_memory: 65536  # MB\n    gpu_bandwidth: 320  # GB/s\n    cpu_bandwidth: 100  # GB/s\n    cpu_to_gpu_bandwidth: 16  # GB/s (PCIe Gen3 x16)\n    gpu_compute: 65  # TFLOPS (FP16)\n    cpu_compute: 1.6  # TFLOPS\n    \n  l4_single:\n    gpu_memory: 24576\n    cpu_memory: 65536\n    gpu_bandwidth: 300\n    cpu_bandwidth: 120\n    cpu_to_gpu_bandwidth: 16\n    gpu_compute: 121\n    cpu_compute: 1.6\n\n# Model configurations\nmodels:\n  mixtral-8x7b:\n    num_layers: 32\n    hidden_dim: 4096\n    intermediate_dim: 14336\n    num_experts: 8\n    top_k: 2\n    num_heads: 32\n    num_kv_heads: 8\n    \n# Default policies (auto-optimized if not specified)\npolicies:\n  mixtral-8x7b-t4:\n    batch_size: 36\n    micro_batch_size: 4\n    use_cpu_attention: true\n    use_gpu_ffn: true\n    weight_gpu_ratio: 0.0\n    kv_cache_gpu_ratio: 0.0\n```\n\n---\n\n## Contact & Collaboration\n\n**Project Lead**: [To be assigned]  \n**Technical Advisors**: [Paper authors - optional consultation]  \n**Discussion Forum**: GitHub Discussions in AI-OS repo  \n**Issue Tracking**: GitHub Issues with label `moe-lightning`\n\n**Collaboration Opportunities**:\n- Hardware vendors: Testing on diverse GPU configurations\n- Research institutions: Advanced optimization techniques\n- Open-source community: Code contributions and testing\n\n---\n\n**Document Version**: 1.0  \n**Last Updated**: November 8, 2025  \n**Next Review**: December 8, 2025\n", "tags": ["experts", "training"], "headings": [{"line": 0, "text": "MoE-Lightning Integration Plan"}, {"line": 2, "text": "Overview"}, {"line": 15, "text": "Table of Contents"}, {"line": 31, "text": "Motivation"}, {"line": 33, "text": "Problem Statement"}, {"line": 44, "text": "Benefits of Integration"}, {"line": 55, "text": "Technical Background"}, {"line": 57, "text": "Mixture of Experts (MoE) Architecture"}, {"line": 65, "text": "Key Innovations in MoE-Lightning"}, {"line": 67, "text": "1. CGOPipe (CPU-GPU-I/O Pipeline Schedule)"}, {"line": 81, "text": "2. HRM (Hierarchical Roofline Model)"}, {"line": 109, "text": "3. Tensor Parallelism"}, {"line": 117, "text": "Performance Analysis Insights"}, {"line": 119, "text": "Attention Block"}, {"line": 124, "text": "MoE FFN Block"}, {"line": 131, "text": "Core Components"}, {"line": 133, "text": "Component 1: CGOPipe Scheduler"}, {"line": 139, "text": "Pseudo-code for CGOPipe execution order"}, {"line": 141, "text": "Prologue (first 2 micro-batches)"}, {"line": 148, "text": "Main pipeline (steady state)"}, {"line": 151, "text": "Parallel execution"}, {"line": 164, "text": "Component 2: HRM Performance Model"}, {"line": 197, "text": "Use MILP solver for policy search"}, {"line": 198, "text": "Takes <1 minute for offline optimization"}, {"line": 206, "text": "Component 3: Memory Management System"}, {"line": 215, "text": "Allocate 2\u00d7 layer weight buffer on GPU"}, {"line": 220, "text": "Page table for MoE expert routing"}, {"line": 224, "text": "CPU DRAM \u2192 CPU Pinned Memory"}, {"line": 227, "text": "CPU Pinned Memory \u2192 GPU (overlapped)"}, {"line": 236, "text": "Component 4: CPU Attention Kernels"}, {"line": 250, "text": "Component 5: Request Batching System"}, {"line": 262, "text": "Sort requests by length (descending)"}, {"line": 268, "text": "Greedy assignment to micro-batch with fewest tokens"}, {"line": 279, "text": "Integration Architecture"}, {"line": 281, "text": "System Architecture Diagram"}, {"line": 320, "text": "Integration Points"}, {"line": 322, "text": "1. Model Loading Layer"}, {"line": 327, "text": "2. Inference Engine Layer"}, {"line": 332, "text": "3. Memory Management Layer"}, {"line": 337, "text": "4. CLI Integration"}, {"line": 344, "text": "5. Configuration Layer"}, {"line": 351, "text": "Implementation Phases"}, {"line": 353, "text": "Phase 1: Foundation & Research (Weeks 1-3)"}, {"line": 388, "text": "Phase 2: Core Components Implementation (Weeks 4-8)"}, {"line": 397, "text": "2.1 HRM Performance Model (Week 4-5)"}, {"line": 399, "text": "aios/inference/moe_lightning/hrm/model.py"}, {"line": 418, "text": "Implement using scipy.optimize or CVXPY"}, {"line": 422, "text": "2.2 CGOPipe Scheduler (Week 5-6)"}, {"line": 424, "text": "aios/inference/moe_lightning/scheduler/cgopipe.py"}, {"line": 434, "text": "Initialize CUDA streams"}, {"line": 438, "text": "Initialize weight paging"}, {"line": 446, "text": "Implement Algorithm 1 from paper"}, {"line": 450, "text": "2.3 Weight Paging System (Week 6-7)"}, {"line": 452, "text": "aios/inference/moe_lightning/memory/weight_paging.py"}, {"line": 461, "text": "Allocate pinned memory buffer"}, {"line": 464, "text": "Page table for expert routing"}, {"line": 469, "text": "CPU DRAM \u2192 CPU Pinned (background thread)"}, {"line": 472, "text": "CPU Pinned \u2192 GPU (CUDA stream)"}, {"line": 476, "text": "2.4 CPU Attention Kernels (Week 7-8)"}, {"line": 478, "text": "aios/inference/moe_lightning/kernels/cpu_attention.py"}, {"line": 490, "text": "Configure MKL threads"}, {"line": 504, "text": "Implement optimized GQA with SIMD vectorization"}, {"line": 522, "text": "Phase 3: Integration & Optimization (Weeks 9-12)"}, {"line": 531, "text": "3.1 AI-OS Integration (Week 9-10)"}, {"line": 537, "text": "3.2 Request Batching System (Week 10)"}, {"line": 539, "text": "aios/inference/moe_lightning/batching/request_batcher.py"}, {"line": 552, "text": "3.3 Tensor Parallelism Support (Week 11)"}, {"line": 554, "text": "aios/inference/moe_lightning/distributed/tensor_parallel.py"}, {"line": 563, "text": "Scale policy parameters for multiple GPUs"}, {"line": 567, "text": "3.4 Performance Profiling & Optimization (Week 11-12)"}, {"line": 588, "text": "Phase 4: Testing & Validation (Weeks 13-15)"}, {"line": 597, "text": "4.1 Correctness Testing (Week 13)"}, {"line": 603, "text": "4.2 Performance Benchmarking (Week 14)"}, {"line": 605, "text": "tests/benchmarks/test_moe_lightning_performance.py"}, {"line": 644, "text": "4.3 Stress Testing (Week 15)"}, {"line": 664, "text": "Phase 5: Documentation & Deployment (Weeks 16-17)"}, {"line": 673, "text": "5.1 User Documentation (Week 16)"}, {"line": 675, "text": "docs/guide/moe_lightning_quickstart.md"}, {"line": 676, "text": "MoE-Lightning Quick Start"}, {"line": 680, "text": "Installation"}, {"line": 681, "text": "Basic Usage"}, {"line": 682, "text": "Configuration Guide"}, {"line": 683, "text": "Performance Tuning"}, {"line": 684, "text": "Troubleshooting"}, {"line": 687, "text": "5.2 Developer Documentation (Week 16)"}, {"line": 693, "text": "5.3 Example Notebooks (Week 17)"}, {"line": 695, "text": "examples/moe_lightning_mixtral.ipynb"}, {"line": 707, "text": "5.4 Deployment Preparation (Week 17)"}, {"line": 727, "text": "Phase 6: Advanced Features & Extensions (Weeks 18-20)"}, {"line": 736, "text": "6.1 Extended Hardware Support (Week 18)"}, {"line": 742, "text": "6.2 Advanced Optimizations (Week 19)"}, {"line": 748, "text": "6.3 AI-OS Ecosystem Integration (Week 20)"}, {"line": 766, "text": "Technical Requirements"}, {"line": 768, "text": "Hardware Requirements"}, {"line": 770, "text": "Minimum Configuration"}, {"line": 777, "text": "Recommended Configuration"}, {"line": 784, "text": "Optimal Configuration"}, {"line": 791, "text": "Software Requirements"}, {"line": 793, "text": "Core Dependencies"}, {"line": 795, "text": "pyproject.toml additions"}, {"line": 805, "text": "System Requirements"}, {"line": 812, "text": "Model Support"}, {"line": 814, "text": "Supported Architectures"}, {"line": 830, "text": "Model Format Support"}, {"line": 837, "text": "Performance Targets"}, {"line": 839, "text": "Throughput Targets"}, {"line": 843, "text": "Single GPU (T4 16GB)"}, {"line": 851, "text": "Single GPU (L4 24GB)"}, {"line": 857, "text": "Multi-GPU (4\u00d7T4 64GB)"}, {"line": 863, "text": "Memory Efficiency Targets"}, {"line": 872, "text": "Latency Targets"}, {"line": 883, "text": "Risk Assessment"}, {"line": 885, "text": "Technical Risks"}, {"line": 887, "text": "Risk 1: Performance Below Targets"}, {"line": 899, "text": "Risk 2: Hardware Compatibility Issues"}, {"line": 911, "text": "Risk 3: Memory Management Complexity"}, {"line": 923, "text": "Risk 4: Integration Conflicts"}, {"line": 935, "text": "Project Risks"}, {"line": 937, "text": "Risk 5: Scope Creep"}, {"line": 949, "text": "Risk 6: Resource Constraints"}, {"line": 961, "text": "Risk 7: Dependency Changes"}, {"line": 975, "text": "Testing Strategy"}, {"line": 977, "text": "Unit Testing"}, {"line": 982, "text": "tests/unit/test_hrm_model.py"}, {"line": 996, "text": "tests/unit/test_cgopipe.py"}, {"line": 1007, "text": "tests/unit/test_cpu_attention.py"}, {"line": 1016, "text": "Integration Testing"}, {"line": 1019, "text": "tests/integration/test_moe_lightning_inference.py"}, {"line": 1034, "text": "Performance Testing"}, {"line": 1037, "text": "tests/performance/test_throughput.py"}, {"line": 1056, "text": "Correctness Testing"}, {"line": 1059, "text": "tests/correctness/test_numerical_accuracy.py"}, {"line": 1071, "text": "Stress Testing"}, {"line": 1074, "text": "tests/stress/test_reliability.py"}, {"line": 1088, "text": "Future Enhancements"}, {"line": 1090, "text": "Short-term (6 months)"}, {"line": 1110, "text": "Mid-term (12 months)"}, {"line": 1131, "text": "Long-term (18+ months)"}, {"line": 1155, "text": "Success Metrics"}, {"line": 1157, "text": "Technical Metrics"}, {"line": 1169, "text": "Project Metrics"}, {"line": 1180, "text": "User Experience Metrics"}, {"line": 1191, "text": "References"}, {"line": 1193, "text": "Primary Paper"}, {"line": 1196, "text": "Related Papers"}, {"line": 1198, "text": "MoE Architectures"}, {"line": 1204, "text": "Performance Modeling"}, {"line": 1208, "text": "Inference Systems"}, {"line": 1214, "text": "Optimization Techniques"}, {"line": 1219, "text": "Implementation References"}, {"line": 1226, "text": "AI-OS Related"}, {"line": 1234, "text": "Appendix"}, {"line": 1236, "text": "A. Glossary"}, {"line": 1250, "text": "B. Hardware Specifications"}, {"line": 1252, "text": "NVIDIA T4"}, {"line": 1259, "text": "NVIDIA L4"}, {"line": 1266, "text": "NVIDIA A100"}, {"line": 1273, "text": "C. Model Specifications"}, {"line": 1275, "text": "Mixtral 8x7B"}, {"line": 1285, "text": "Mixtral 8x22B"}, {"line": 1293, "text": "DBRX"}, {"line": 1301, "text": "D. Configuration Examples"}, {"line": 1303, "text": "config/moe_lightning.yaml"}, {"line": 1305, "text": "Hardware profiles"}, {"line": 1325, "text": "Model configurations"}, {"line": 1336, "text": "Default policies (auto-optimized if not specified)"}, {"line": 1349, "text": "Contact & Collaboration"}]}, {"path": "planned_features/MULTIMODAL_SPECIALIZED_TOKENIZERS.md", "content": "# \ud83c\udfa8 Multimodal & Specialized Tokenizers - Complete Collection\n\n> ## \u26a0\ufe0f **CRITICAL NOTICE - NOT YET IMPLEMENTED** \u26a0\ufe0f\n> \n> **STATUS**: \ud83d\udea7 **PLANNED FEATURE - NOT CURRENTLY AVAILABLE** \ud83d\udea7\n> \n> The multimodal and specialized tokenizers described in this document are **NOT YET IMPLEMENTED** in AI-OS. This document represents planned future functionality.\n> \n> **What works NOW**:\n> - \u2705 GPT-2, Qwen 2.5, Mistral 7B, Code Llama, DeepSeek-Coder V2, StarCoder2, Phi-3 Mini\n> \n> **What does NOT work yet**:\n> - \u274c CLIP, LLaVA, SigLIP (Vision/Multimodal)\n> - \u274c BioBERT, SciBERT, Legal-BERT, FinBERT (Specialized domains)\n> - \u274c Image processing pipeline\n> - \u274c Vision-language model support\n> \n> **Last Verified**: October 13, 2025\n> \n> For currently supported tokenizers, see: `docs/SUPPORTED_TOKENIZERS.md` (to be created)\n\n---\n\n**Date:** October 13, 2025  \n**Major Update:** Added Vision, Multimodal, and Domain-Specialized Tokenizers  \n**\u26a0\ufe0f Status:** DOCUMENTATION ONLY - IMPLEMENTATION PENDING\n\n---\n\n## \ud83d\ude80 What's New (PLANNED)\n\n### \ud83c\udfa8 Vision & Multimodal Tokenizers (3)\n\nTrain AI models that understand **images, videos, and vision-language tasks**:\n\n| Tokenizer | Vocab | Best For | Downloaded |\n|-----------|-------|----------|------------|\n| **CLIP (Vision-Language)** \ud83d\uddbc\ufe0f | 49,408 | Image-text understanding, zero-shot vision | \u2705 Yes |\n| **LLaVA 1.5 (Vision Chat)** \ud83d\udcac | 32,000 | Visual Q&A, image understanding, multimodal chat | \u2705 Yes |\n| **SigLIP** \ud83c\udfaf | 32,000 | Advanced vision-language, better than CLIP | \u2705 Yes |\n\n### \ud83d\udd2c Specialized Domain Tokenizers (4)\n\nTrain AI models for **specific professional domains**:\n\n| Tokenizer | Vocab | Best For | Downloaded |\n|-----------|-------|----------|------------|\n| **BioBERT** \ud83e\uddec | 28,996 | Biomedical, healthcare, medical research | \u2705 Yes |\n| **SciBERT** \ud83d\udd2c | 31,090 | Scientific papers, academic research | \u2705 Yes |\n| **Legal-BERT** \u2696\ufe0f | 30,522 | Legal documents, contracts, compliance | \u26a0\ufe0f No |\n| **FinBERT** \ud83d\udcb0 | 30,873 | Financial analysis, trading, markets | \u26a0\ufe0f No |\n\n---\n\n## \ud83d\udcca Complete Tokenizer Collection\n\n**Total: 16 Tokenizers** across all categories!\n\n### General Purpose (4)\n- \u2b50 Qwen 2.5 - 151K vocab (Best overall, 2025)\n- Llama 3 - 128K vocab (Requires auth)\n- Mistral 7B - 32K vocab\n- GPT-2 - 50K vocab (Legacy)\n\n### Code Specialized (3)\n- \u2b50 DeepSeek-Coder V2 - 100K vocab (Best for code, 2025)\n- StarCoder2 - 49K vocab (600+ languages)\n- Code Llama - 32K vocab (Stable)\n\n### Vision & Multimodal (3) \ud83c\udd95\n- **CLIP (Vision-Language)** - 49K vocab (Image-text)\n- **LLaVA 1.5** - 32K vocab (Visual chat)\n- **SigLIP** - 32K vocab (Advanced vision)\n\n### Specialized Domains (4) \ud83c\udd95\n- **BioBERT** - 29K vocab (Biomedical)\n- **SciBERT** - 31K vocab (Scientific)\n- **Legal-BERT** - 31K vocab (Legal)\n- **FinBERT** - 31K vocab (Financial)\n\n### Compact & Efficient (2)\n- Phi-3 Mini - 32K vocab\n- GPT-2 Base Model - 50K vocab (Backward compatible)\n\n---\n\n## \ud83c\udfa8 Vision & Multimodal Use Cases\n\n### CLIP (Vision-Language)\n\n**What it does:**\n- Understands relationships between images and text\n- Zero-shot image classification\n- Image-text retrieval\n- Image generation guidance\n\n**Training Examples:**\n- Image captioning datasets\n- Visual question answering\n- Image-text matching\n- Multimodal embeddings\n\n**Best for:**\n- Building image search engines\n- Visual understanding systems\n- Text-to-image generation\n- Multimodal AI assistants\n\n**Model Architecture:**\n```\nInput: Image + Text\n      \u2193\nCLIP Tokenizer (49,408 vocab)\n      \u2193\nDual Encoders (Vision + Language)\n      \u2193\nShared Embedding Space\n      \u2193\nOutput: Similarity Scores / Embeddings\n```\n\n### LLaVA 1.5 (Vision Chat)\n\n**What it does:**\n- Visual question answering\n- Image understanding and description\n- Multimodal conversation\n- Visual reasoning\n\n**Training Examples:**\n- VQA datasets (Visual Question Answering)\n- Image description pairs\n- Visual instruction following\n- Multimodal dialogue\n\n**Best for:**\n- Visual chatbots\n- Image analysis assistants\n- Accessibility tools (image description)\n- Educational AI tutors\n\n**Model Architecture:**\n```\nInput: Image + Question\n      \u2193\nVision Encoder (ViT) + LLaVA Tokenizer (32K)\n      \u2193\nMultimodal Fusion\n      \u2193\nLanguage Model Decoder\n      \u2193\nOutput: Text Answer\n```\n\n### SigLIP (Sigmoid Loss Vision)\n\n**What it does:**\n- Improved vision-language alignment\n- Better zero-shot classification\n- More efficient training (sigmoid loss vs softmax)\n- Enhanced image understanding\n\n**Training Examples:**\n- Large-scale image-text pairs\n- Visual classification tasks\n- Image retrieval datasets\n- Cross-modal learning\n\n**Best for:**\n- Advanced vision-language models\n- Efficient multimodal training\n- Zero-shot vision tasks\n- Production vision systems\n\n---\n\n## \ud83d\udd2c Specialized Domain Use Cases\n\n### BioBERT (Biomedical) \ud83e\uddec\n\n**What it does:**\n- Understanding medical terminology\n- Biomedical named entity recognition\n- Clinical text analysis\n- Drug-disease relationship extraction\n\n**Training Examples:**\n- PubMed abstracts\n- Clinical notes\n- Medical research papers\n- Drug interaction databases\n- Disease symptom datasets\n\n**Best for:**\n- Medical AI assistants\n- Clinical decision support\n- Biomedical research tools\n- Healthcare chatbots\n- Drug discovery AI\n\n**Pre-trained on:**\n- 4.5B words from PubMed abstracts\n- 13.5B words from PMC full-text articles\n\n### SciBERT (Scientific) \ud83d\udd2c\n\n**What it does:**\n- Understanding scientific terminology\n- Research paper analysis\n- Citation relationship extraction\n- Scientific entity recognition\n\n**Training Examples:**\n- Academic papers (1.14M papers)\n- Scientific abstracts\n- Research datasets\n- Technical documentation\n- Lab reports\n\n**Best for:**\n- Research paper summarization\n- Literature review automation\n- Scientific question answering\n- Academic writing assistance\n- Grant proposal analysis\n\n**Pre-trained on:**\n- 1.14M papers from Semantic Scholar\n- 18% computer science papers\n- 82% biomedical papers\n\n### Legal-BERT (Legal) \u2696\ufe0f\n\n**What it does:**\n- Understanding legal terminology\n- Contract analysis\n- Legal document classification\n- Case law reasoning\n\n**Training Examples:**\n- Legal contracts\n- Court opinions\n- Legislation text\n- Legal briefs\n- Regulatory documents\n\n**Best for:**\n- Contract review automation\n- Legal research assistants\n- Compliance checking\n- Document classification\n- Legal chatbots\n\n**Pre-trained on:**\n- Legal corpora\n- Case law databases\n- Legislation texts\n\n### FinBERT (Financial) \ud83d\udcb0\n\n**What it does:**\n- Financial sentiment analysis\n- Market trend prediction\n- Financial entity recognition\n- Risk assessment\n\n**Training Examples:**\n- Financial news articles\n- Earnings reports\n- Market analysis\n- Trading data\n- Economic indicators\n\n**Best for:**\n- Trading algorithms\n- Financial news analysis\n- Risk assessment tools\n- Investment research\n- Financial chatbots\n\n**Pre-trained on:**\n- Financial news and reports\n- Corporate filings\n- Market commentary\n\n---\n\n## \ud83c\udfaf Choosing the Right Tokenizer\n\n### For Image Understanding & Generation\n**Use: CLIP** \ud83d\uddbc\ufe0f\n- Large vocabulary (49K)\n- Industry-standard vision-language model\n- Zero-shot capabilities\n- Compatible with Stable Diffusion and other image models\n\n**Example Projects:**\n- Image search engines\n- Content moderation\n- Visual similarity matching\n- Text-to-image applications\n\n### For Visual Question Answering\n**Use: LLaVA 1.5** \ud83d\udcac\n- Optimized for visual chat\n- Instruction following\n- Multimodal conversation\n- Image understanding\n\n**Example Projects:**\n- Visual assistants\n- Image description tools\n- Educational tutors\n- Accessibility applications\n\n### For Biomedical AI\n**Use: BioBERT** \ud83e\uddec\n- Medical terminology expertise\n- Clinical text understanding\n- Biomedical entity recognition\n\n**Example Projects:**\n- Clinical decision support\n- Medical record analysis\n- Drug discovery\n- Healthcare chatbots\n\n### For Scientific Research\n**Use: SciBERT** \ud83d\udd2c\n- Scientific vocabulary\n- Research paper understanding\n- Technical documentation\n\n**Example Projects:**\n- Literature review tools\n- Research assistants\n- Paper summarization\n- Citation analysis\n\n### For Legal Applications\n**Use: Legal-BERT** \u2696\ufe0f\n- Legal terminology\n- Contract understanding\n- Case law analysis\n\n**Example Projects:**\n- Contract review\n- Legal research tools\n- Compliance checking\n- Document classification\n\n### For Financial Analysis\n**Use: FinBERT** \ud83d\udcb0\n- Financial terminology\n- Market sentiment\n- Economic understanding\n\n**Example Projects:**\n- Trading algorithms\n- Market analysis\n- Risk assessment\n- Financial news processing\n\n---\n\n## \ud83d\ude80 Quick Start Examples\n\n### Creating a Vision AI Brain\n\n```bash\n# 1. Download CLIP tokenizer\npython scripts/download_tokenizer.py clip-vit\n\n# 2. Open AI-OS GUI\n# 3. HRM Training \u2192 Create New\n\n# 4. Configure:\nPreset: 10M (or preferred size)\nName: vision-understanding-v1\nGoal: Understand and describe images, answer visual questions\nTokenizer: \u2713 CLIP (Vision-Language) (49,408 tokens)\n\n# 5. Create & Train on image-text datasets!\n```\n\n### Creating a Medical AI Brain\n\n```bash\n# 1. Download BioBERT tokenizer\npython scripts/download_tokenizer.py biobert\n\n# 2. Configure brain:\nName: medical-assistant-v1\nGoal: Analyze medical texts and provide clinical insights\nTokenizer: \u2713 BioBERT (Biomedical) (28,996 tokens)\n\n# 3. Train on medical datasets (PubMed, clinical notes, etc.)\n```\n\n### Creating a Scientific Research Brain\n\n```bash\n# 1. Download SciBERT tokenizer\npython scripts/download_tokenizer.py scibert\n\n# 2. Configure brain:\nName: research-assistant-v1\nGoal: Understand scientific papers and answer research questions\nTokenizer: \u2713 SciBERT (Scientific) (31,090 tokens)\n\n# 3. Train on scientific papers and technical docs\n```\n\n---\n\n## \ud83d\udcda Training Dataset Recommendations\n\n### For Vision Models (CLIP, LLaVA, SigLIP)\n\n**Public Datasets:**\n- **COCO** - Image captioning (330K images)\n- **Flickr30k** - Image descriptions (31K images)\n- **Visual Genome** - Dense annotations (108K images)\n- **Conceptual Captions** - 3.3M image-text pairs\n- **VQA v2** - Visual question answering (1M+ questions)\n\n**Custom Data:**\n- Product images + descriptions\n- Screenshots + instructions\n- Medical images + diagnoses\n- Satellite imagery + labels\n\n### For Biomedical (BioBERT)\n\n**Public Datasets:**\n- **PubMed Abstracts** - Medical research\n- **MIMIC-III** - Clinical notes (requires approval)\n- **PMC-OA** - Full-text biomedical articles\n- **ChEMBL** - Drug-target interactions\n- **UMLS** - Medical terminology\n\n### For Scientific (SciBERT)\n\n**Public Datasets:**\n- **ArXiv** - Scientific papers\n- **Semantic Scholar** - Academic papers\n- **PubMed Central** - Biomedical literature\n- **CiteSeer** - Computer science papers\n- **IEEE Xplore** - Engineering papers\n\n### For Legal (Legal-BERT)\n\n**Public Datasets:**\n- **EDGAR** - SEC filings\n- **Court Listener** - Court opinions\n- **EUR-Lex** - EU legal documents\n- **CaseLaw Access Project** - US case law\n\n### For Financial (FinBERT)\n\n**Public Datasets:**\n- **Financial PhraseBank** - Sentiment analysis\n- **StockTwits** - Market sentiment\n- **SEC Filings** - Corporate documents\n- **Reuters Financial News**\n- **Bloomberg News Archives**\n\n---\n\n## \ud83d\udd04 Multimodal Training Pipeline\n\n### Vision-Language Training Flow\n\n```\n1. Prepare Dataset\n   \u251c\u2500\u2500 Images (.jpg, .png)\n   \u251c\u2500\u2500 Captions/Labels (.txt, .json)\n   \u2514\u2500\u2500 Pair them correctly\n\n2. Choose Tokenizer\n   \u2514\u2500\u2500 CLIP, LLaVA, or SigLIP\n\n3. Create Brain\n   \u2514\u2500\u2500 Select vision tokenizer in GUI\n\n4. Configure Training\n   \u251c\u2500\u2500 Image preprocessing (resize, normalize)\n   \u251c\u2500\u2500 Text tokenization\n   \u2514\u2500\u2500 Batch pairing (image + text)\n\n5. Train Model\n   \u251c\u2500\u2500 Vision encoder learns image features\n   \u251c\u2500\u2500 Language encoder learns text features\n   \u2514\u2500\u2500 Contrastive loss aligns both spaces\n\n6. Evaluate\n   \u251c\u2500\u2500 Image retrieval accuracy\n   \u251c\u2500\u2500 Text retrieval accuracy\n   \u2514\u2500\u2500 Zero-shot classification\n```\n\n---\n\n## \ud83e\uddea Testing Results\n\n### All New Tokenizers Verified \u2705\n\n```\n\ud83d\udcca Vision & Multimodal\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2705 CLIP: Downloaded & Tested\n\u2705 LLaVA 1.5: Downloaded & Tested\n\u2705 SigLIP: Downloaded (verification warning)\n\n\ud83d\udcca Specialized Domains\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2705 BioBERT: Downloaded & Tested\n\u2705 SciBERT: Downloaded & Tested\n\u26a0\ufe0f Legal-BERT: Not yet downloaded\n\u26a0\ufe0f FinBERT: Not yet downloaded\n```\n\n### Test Brains Created\n\n| Brain Name | Tokenizer | Use Case |\n|------------|-----------|----------|\n| CLIP-(Vision-Language)-test | clip-vit | Image-text understanding |\n| LLaVA-1.5-(Vision-Chat)-test | llava-1.5 | Visual Q&A |\n| BioBERT-(Biomedical)-test | biobert | Medical text |\n| SciBERT-(Scientific)-test | scibert | Research papers |\n\n---\n\n## \ud83d\udca1 Advanced Use Cases\n\n### Robotics Training (Future)\n\nWhile we don't have RT-2 tokenizer yet, you can prepare for robotics:\n\n**Using Current Tokenizers:**\n1. **Code tokenizers** (DeepSeek-Coder V2, StarCoder2) for action sequences\n2. **Vision tokenizers** (CLIP, LLaVA) for visual perception\n3. **Qwen 2.5** for multimodal reasoning\n\n**Action Tokenization Concept:**\n```python\n# Represent robot actions as tokens\nrobot_actions = {\n    \"move_forward\": 1001,\n    \"move_backward\": 1002,\n    \"turn_left\": 1003,\n    \"turn_right\": 1004,\n    \"grasp\": 1005,\n    \"release\": 1006,\n    # ... etc\n}\n\n# Train on: Vision \u2192 Action Sequences\nInput: [image_tokens] + [instruction_tokens]\nOutput: [action_token_1, action_token_2, ..., action_token_n]\n```\n\n### Video Understanding\n\n**Using Vision Tokenizers:**\n- CLIP or LLaVA can process video frames\n- Tokenize each frame individually\n- Temporal modeling at model level\n\n**Workflow:**\n```\nVideo \u2192 Extract Frames (e.g., 1 fps)\n      \u2193\nEach Frame \u2192 Vision Tokenizer\n      \u2193\nSequence of Frame Tokens\n      \u2193\nTemporal Model (Transformer)\n      \u2193\nVideo Understanding\n```\n\n### Audio-Visual Models\n\n**Multimodal Combination:**\n```\nAudio: Use text tokenizer for transcribed speech\nVision: Use CLIP/LLaVA for visual frames\nFusion: Combine at embedding level\n\nInput: [audio_transcript_tokens] + [video_frame_tokens]\nOutput: Multimodal understanding\n```\n\n---\n\n## \ud83d\udce6 Installation Summary\n\n### Downloaded & Ready (13 Tokenizers)\n```bash\n\u2705 gpt2, gpt2-base-model\n\u2705 qwen2.5-7b (Best overall)\n\u2705 mistral-7b\n\u2705 deepseek-coder-v2 (Best for code)\n\u2705 starcoder2\n\u2705 codellama\n\u2705 phi3-mini\n\u2705 clip-vit (Vision-language)\n\u2705 llava-1.5 (Vision chat)\n\u2705 siglip (Advanced vision)\n\u2705 biobert (Biomedical)\n\u2705 scibert (Scientific)\n```\n\n### Available to Download (3 Tokenizers)\n```bash\n\u26a0\ufe0f llama3-8b (requires HF access approval)\n\u26a0\ufe0f legal-bert\n\u26a0\ufe0f finbert\n```\n\n---\n\n## \ud83c\udf93 Learning Resources\n\n### Vision-Language Models\n- **CLIP Paper:** https://arxiv.org/abs/2103.00020\n- **LLaVA Paper:** https://arxiv.org/abs/2304.08485\n- **SigLIP:** https://arxiv.org/abs/2303.15343\n\n### Domain-Specific Models\n- **BioBERT:** https://arxiv.org/abs/1901.08746\n- **SciBERT:** https://arxiv.org/abs/1903.10676\n- **Legal-BERT:** https://arxiv.org/abs/2010.02559\n- **FinBERT:** https://arxiv.org/abs/1908.10063\n\n### Robotics & Embodied AI\n- **RT-2:** https://robotics-transformer2.github.io/\n- **PaLM-E:** https://palm-e.github.io/\n\n---\n\n## \u2753 FAQ\n\n**Q: Can I train image generation models with CLIP?**  \nA: CLIP is for understanding, but can guide generation (like in Stable Diffusion). For generation, you'd need image decoder models.\n\n**Q: Do I need special hardware for vision models?**  \nA: Vision models need more VRAM. Recommend 8GB+ GPU for small models, 24GB+ for larger ones.\n\n**Q: Can I mix tokenizers (e.g., vision + text)?**  \nA: Not directly. Choose one tokenizer per brain. For multimodal, use tokenizers designed for both (CLIP, LLaVA).\n\n**Q: Are specialized tokenizers better than general ones?**  \nA: For their specific domains, YES! BioBERT understands medical terms much better than GPT-2.\n\n**Q: Can I train on video datasets?**  \nA: Yes! Extract frames, tokenize each frame, and model temporal relationships at the architecture level.\n\n**Q: How do I prepare medical/scientific datasets?**  \nA: Start with public datasets (PubMed, ArXiv), then add your own domain-specific data.\n\n**Q: Will you add RT-2 for robotics?**  \nA: RT-2 requires special action tokenization. We're exploring options for embodied AI support!\n\n---\n\n## \ud83c\udf89 Summary\n\nYou now have access to **16 specialized tokenizers** covering:\n\n- \u2705 **General AI** (Qwen 2.5, Llama 3, Mistral)\n- \u2705 **Code** (DeepSeek-Coder V2, StarCoder2, CodeLlama)\n- \u2705 **Vision & Multimodal** (CLIP, LLaVA, SigLIP) \ud83c\udd95\n- \u2705 **Biomedical** (BioBERT) \ud83c\udd95\n- \u2705 **Scientific** (SciBERT) \ud83c\udd95\n- \u2705 **Legal** (Legal-BERT - available) \ud83c\udd95\n- \u2705 **Financial** (FinBERT - available) \ud83c\udd95\n\n**Ready to train specialized AI for any domain!** \ud83d\ude80\n\n---\n\n*Last Updated: October 13, 2025*  \n*Vision and specialized tokenizers fully integrated*\n\n---\n\n## \ud83e\udded Planned Expansion: Image & Video Generation (Design Spec)\n\n> Status: PLANNED \u2013 Architecture and UX defined below. No runtime support in main yet.\n\nThis addendum extends the tokenizer roadmap to full multimodal generation and scientific-output workflows. It defines the architecture, data contracts, and UI spec required to support:\n\n- Text-to-image, image-to-image, inpainting/outpainting, ControlNet-guided image generation\n- Image-to-video and text-to-video (short clips) generation\n- Scientific outputs in chat: rich plots, tables, LaTeX, HTML reports, downloadable files\n- A new Chat Window v2 that can render most multimodal outputs\n\n### High-level Phases\n\n1) Foundations: artifact store, rich message schema, viewers (images/tables/plots/latex)  \n2) Image generation (inference-first): SDXL/LCM, ControlNet, prompt/seed reproducibility  \n3) Video generation (inference-first): SVD (image\u2192video), lightweight text\u2192video integration  \n4) Scientific outputs: plot/table/LaTeX/HTML/file attachments, export flows  \n5) Optional training: dreambooth/LoRA for images; fine-tuning where feasible for video\n\n---\n\n## \ud83d\uddbc\ufe0f Image Generation (Planned)\n\nSupported (Planned) pipelines and modalities:\n\n- Text\u2192Image: SD 1.5 / SDXL via Diffusers; optional LCM for fast steps\n- Image\u2192Image: style transfer/variations with strength control\n- Inpainting/Outpainting: masked editing\n- Conditioning: ControlNet (canny, depth, pose), IP-Adapter (face/style), LoRA adapters\n\nImplementation notes:\n- Runtime: Hugging Face Diffusers (+ transformers, accelerate, safetensors, xformers)\n- Precision: fp16/bf16 on GPU; CPU fallback (slow) for dev\n- Reproducibility: seed, guidance_scale, scheduler stored with artifact metadata\n- Safety filter hooks: optional NSFW classifier toggle (user-controlled)\n\nCLI (planned examples):\n```\naios gen image --model sdxl --prompt \"a red fox in a misty forest, photorealistic\" \\\n      --steps 20 --seed 42 --size 1024x1024 --guidance-scale 7.5 \\\n      --out artifacts/outputs/images/\n\naios gen image2image --model sd15 --init-image path/to/img.png --strength 0.6 \\\n      --prompt \"studio portrait, soft lighting\" --lora face_finetune.safetensors\n```\n\n---\n\n## \ud83c\udf9e\ufe0f Video Generation (Planned)\n\nInitial scope focuses on inference and short clips:\n\n- Image\u2192Video: Stable Video Diffusion (SVD) 14-24 fps, 2-4s clips\n- Text\u2192Image\u2192Video: generate keyframe with SDXL then animate via SVD\n- Text\u2192Video (experimental): integrate an open model (e.g., CogVideoX small) when stable\n\nKey controls:\n- fps (e.g., 8\u201324), duration (seconds), resolution (e.g., 576p/720p), motion strength\n- Seed and scheduler captured for reproducibility\n\nCLI (planned examples):\n```\naios gen video --from-image artifacts/outputs/images/fox.jpg --model svd --fps 14 --seconds 3\n\naios gen video --prompt \"drone shot over snowy mountains at sunrise\" \\\n      --pipeline txt2img+svd --fps 12 --seconds 4 --seed 123\n```\n\nDataset prep (future training): frame extraction to frames/, JSON pairs with timing; ffmpeg helpers.\n\n---\n\n## \ud83e\uddea Scientific Output Interfaces (Planned)\n\nThe assistant will produce structured scientific artifacts alongside text:\n\n- Plots: Plotly JSON spec (preferred for interactivity) or Matplotlib PNG fallback\n- Tables: column schema + row data; optional CSV/Parquet attachment for download\n- Equations: LaTeX rendered with KaTeX in UI\n- Reports: minimal HTML (sanitized) and Markdown export\n- Files: CSV/JSON/NetCDF/HDF5/ZIP as downloadable artifacts\n- 3D/Geo (future): GLB/PLY previews; GeoJSON/tiles via map component (opt-in)\n\nServer responsibilities:\n- Validate payload sizes and types; store artifacts with content hash\n- Generate preview thumbnails/posters where applicable\n- Attach metadata (units, provenance, seeds, code refs)\n\n---\n\n## \ud83e\udde9 Chat Window v2 \u2013 Rich Message Schema (Planned)\n\nThe chat UI will display multi-part messages. Each message may contain text plus a list of parts. Parts reference inline payloads or stored artifacts.\n\nMessage (concept):\n```json\n{\n      \"id\": \"msg_01\",\n      \"role\": \"assistant\",\n      \"parts\": [\n            { \"type\": \"text\", \"text\": \"Here is the generated image.\" },\n            { \"type\": \"image\", \"source\": { \"type\": \"artifact\", \"id\": \"art_abc\" }, \"alt\": \"red fox\" },\n            { \"type\": \"plot\", \"spec\": { \"version\": 2, \"data\": [], \"layout\": {} } },\n            { \"type\": \"table\", \"columns\": [{\"name\":\"time\",\"type\":\"number\"}], \"rows\": [[0],[1]],\n                  \"download\": { \"artifactId\": \"art_tbl\" } },\n            { \"type\": \"latex\", \"code\": \"E=mc^2\" },\n            { \"type\": \"file\", \"artifactId\": \"art_zip\", \"name\": \"results.zip\" }\n      ]\n}\n```\n\nArtifact (concept):\n```json\n{\n      \"id\": \"art_abc\",\n      \"kind\": \"image\",\n      \"mime\": \"image/png\",\n      \"path\": \"artifacts/outputs/images/fox.png\",\n      \"sha256\": \"...\",\n      \"width\": 1024,\n      \"height\": 1024,\n      \"createdAt\": \"2025-10-15T12:34:56Z\",\n      \"metadata\": {\n            \"prompt\": \"a red fox in a misty forest\",\n            \"seed\": 42,\n            \"steps\": 20,\n            \"guidance_scale\": 7.5,\n            \"scheduler\": \"EulerA\"\n      }\n}\n```\n\nUI Behaviors:\n- Text streams first; media parts render when artifacts are ready\n- Image/video gallery with zoom; video player with poster/thumb\n- Plotly interactive viewer; CSV download from table; LaTeX inline via KaTeX\n- Side \u201cArtifacts\u201d panel lists recent outputs with search and filters\n- Tool-run logs collapsible; copy-to-clipboard for prompts/configs\n\nSecurity & Limits:\n- Sanitize HTML; disallow inline scripts; CSP headers\n- Size limits per part and per message; chunked uploads for large files\n\n---\n\n## \ud83c\udfd7\ufe0f Architecture Addendum (Planned)\n\nLogical components:\n\n1) Generation Runtimes\n       - Diffusers pipelines: SD 1.5, SDXL, SVD (+ ControlNet, IP-Adapter, LoRA)\n       - Scheduler registry; VRAM-aware autotune\n2) Artifact Store\n       - Path: `artifacts/outputs/{images|videos|plots|tables|files}/` (content-hashed)\n       - Metadata JSON sidecars; thumbnail/poster derivation jobs\n3) Messaging API\n       - Returns messages with parts; emits events: text-delta, artifact-pending, artifact-ready\n4) Chat UI v2\n       - React components: Text, Image, Video, Plotly, Table, LaTeX, File\n       - View toggles: compact/threaded/gallery\n5) CLI/HRM Integrations\n       - `aios gen image|video` commands; HRM tool to attach outputs to runs\n\nDeployment considerations:\n- Windows, Linux GPU; optional CPU fallback for dev  \n- CUDA/cuDNN versions pinned; xformers optional  \n- Model weights cached in `training_data/hf_cache/`\n\n---\n\n## \ud83d\udcd0 Data Contracts (Planned)\n\nMessagePart types:\n- text, image, video, audio (future), plot, table, latex, html (sanitized), file\n\nCommon fields:\n- `type`, `source` (inline|artifact), `mime` (when applicable), `metadata`\n\nTable contract (sketch):\n```json\n{\n      \"type\": \"table\",\n      \"columns\": [ {\"name\":\"col\",\"type\":\"string\",\"unit\":\"\"} ],\n      \"rows\": [ [\"value\"] ],\n      \"download\": { \"artifactId\": \"art_csv\" }\n}\n```\n\nPlotly contract: store full `spec` and optional `screenshot` thumbnail.\n\n---\n\n## \ud83e\uddea Acceptance Criteria (Milestones)\n\nM1 \u2013 Foundations\n- Render images, tables (CSV download), Plotly, LaTeX in Chat v2\n- Artifact store with hashing + metadata; events wired\n\nM2 \u2013 Image Generation (Inference)\n- SDXL text\u2192image end-to-end via CLI and Chat tool\n- Reproducible artifacts (seed/scheduler stored); ControlNet optional\n\nM3 \u2013 Video Generation (Inference)\n- SVD image\u2192video; keyframe path (txt\u2192img\u2192vid) exposed in CLI and Chat\n- Video player in UI with fps/duration metadata\n\nM4 \u2013 Scientific Outputs\n- Plot/table/latex produced by tools; downloadable CSV/ZIP\n- Simple HTML report (sanitized) preview and export\n\nM5 \u2013 UX Polish & Reliability\n- Gallery view, artifact panel, retries, error toasts, rate limits\n\n---\n\n## \ud83d\udee0\ufe0f Dependencies (Planned)\n\nPython\n- diffusers, transformers, accelerate, torch, safetensors, xformers (optional)\n- pillow, opencv-python, ffmpeg-python (or system ffmpeg)\n\nFrontend\n- KaTeX, Plotly.js, React Player (or video element), MIME renderers\n\n---\n\n## \u26a0\ufe0f Risks & Considerations\n\n- VRAM constraints: add auto-downscale and low-VRAM schedulers\n- Large artifacts: background upload and streaming; disk quotas\n- Content safety: optional NSFW filters; user control and transparency\n- Licensing: check weights and dataset licenses; document restrictions\n\n---\n\n## \ud83e\uddea Try-it (Future, subject to change)\n\n```\n# Text \u2192 Image\naios gen image --model sdxl --prompt \"scientific diagram of DNA helix, vector style\" --size 1024x1024\n\n# Image \u2192 Video\naios gen video --from-image artifacts/outputs/images/diagram.png --model svd --fps 12 --seconds 3\n\n# Scientific Plot in Chat (tool)\naios tools plot --spec path/to/plotly.json --attach\n```\n\n---\n\nThis specification updates the planned feature set to include multimodal generation and scientific outputs with a concrete data and UI contract, without claiming availability today.\n", "tags": ["cli", "training"], "headings": [{"line": 0, "text": "\ud83c\udfa8 Multimodal & Specialized Tokenizers - Complete Collection"}, {"line": 29, "text": "\ud83d\ude80 What's New (PLANNED)"}, {"line": 31, "text": "\ud83c\udfa8 Vision & Multimodal Tokenizers (3)"}, {"line": 41, "text": "\ud83d\udd2c Specialized Domain Tokenizers (4)"}, {"line": 54, "text": "\ud83d\udcca Complete Tokenizer Collection"}, {"line": 58, "text": "General Purpose (4)"}, {"line": 64, "text": "Code Specialized (3)"}, {"line": 69, "text": "Vision & Multimodal (3) \ud83c\udd95"}, {"line": 74, "text": "Specialized Domains (4) \ud83c\udd95"}, {"line": 80, "text": "Compact & Efficient (2)"}, {"line": 86, "text": "\ud83c\udfa8 Vision & Multimodal Use Cases"}, {"line": 88, "text": "CLIP (Vision-Language)"}, {"line": 121, "text": "LLaVA 1.5 (Vision Chat)"}, {"line": 154, "text": "SigLIP (Sigmoid Loss Vision)"}, {"line": 176, "text": "\ud83d\udd2c Specialized Domain Use Cases"}, {"line": 178, "text": "BioBERT (Biomedical) \ud83e\uddec"}, {"line": 204, "text": "SciBERT (Scientific) \ud83d\udd2c"}, {"line": 231, "text": "Legal-BERT (Legal) \u2696\ufe0f"}, {"line": 258, "text": "FinBERT (Financial) \ud83d\udcb0"}, {"line": 287, "text": "\ud83c\udfaf Choosing the Right Tokenizer"}, {"line": 289, "text": "For Image Understanding & Generation"}, {"line": 302, "text": "For Visual Question Answering"}, {"line": 315, "text": "For Biomedical AI"}, {"line": 327, "text": "For Scientific Research"}, {"line": 339, "text": "For Legal Applications"}, {"line": 351, "text": "For Financial Analysis"}, {"line": 365, "text": "\ud83d\ude80 Quick Start Examples"}, {"line": 367, "text": "Creating a Vision AI Brain"}, {"line": 370, "text": "1. Download CLIP tokenizer"}, {"line": 373, "text": "2. Open AI-OS GUI"}, {"line": 374, "text": "3. HRM Training \u2192 Create New"}, {"line": 376, "text": "4. Configure:"}, {"line": 382, "text": "5. Create & Train on image-text datasets!"}, {"line": 385, "text": "Creating a Medical AI Brain"}, {"line": 388, "text": "1. Download BioBERT tokenizer"}, {"line": 391, "text": "2. Configure brain:"}, {"line": 396, "text": "3. Train on medical datasets (PubMed, clinical notes, etc.)"}, {"line": 399, "text": "Creating a Scientific Research Brain"}, {"line": 402, "text": "1. Download SciBERT tokenizer"}, {"line": 405, "text": "2. Configure brain:"}, {"line": 410, "text": "3. Train on scientific papers and technical docs"}, {"line": 415, "text": "\ud83d\udcda Training Dataset Recommendations"}, {"line": 417, "text": "For Vision Models (CLIP, LLaVA, SigLIP)"}, {"line": 432, "text": "For Biomedical (BioBERT)"}, {"line": 441, "text": "For Scientific (SciBERT)"}, {"line": 450, "text": "For Legal (Legal-BERT)"}, {"line": 458, "text": "For Financial (FinBERT)"}, {"line": 469, "text": "\ud83d\udd04 Multimodal Training Pipeline"}, {"line": 471, "text": "Vision-Language Training Flow"}, {"line": 503, "text": "\ud83e\uddea Testing Results"}, {"line": 505, "text": "All New Tokenizers Verified \u2705"}, {"line": 522, "text": "Test Brains Created"}, {"line": 533, "text": "\ud83d\udca1 Advanced Use Cases"}, {"line": 535, "text": "Robotics Training (Future)"}, {"line": 546, "text": "Represent robot actions as tokens"}, {"line": 554, "text": "... etc"}, {"line": 557, "text": "Train on: Vision \u2192 Action Sequences"}, {"line": 562, "text": "Video Understanding"}, {"line": 582, "text": "Audio-Visual Models"}, {"line": 596, "text": "\ud83d\udce6 Installation Summary"}, {"line": 598, "text": "Downloaded & Ready (13 Tokenizers)"}, {"line": 614, "text": "Available to Download (3 Tokenizers)"}, {"line": 623, "text": "\ud83c\udf93 Learning Resources"}, {"line": 625, "text": "Vision-Language Models"}, {"line": 630, "text": "Domain-Specific Models"}, {"line": 636, "text": "Robotics & Embodied AI"}, {"line": 642, "text": "\u2753 FAQ"}, {"line": 667, "text": "\ud83c\udf89 Summary"}, {"line": 688, "text": "\ud83e\udded Planned Expansion: Image & Video Generation (Design Spec)"}, {"line": 699, "text": "High-level Phases"}, {"line": 709, "text": "\ud83d\uddbc\ufe0f Image Generation (Planned)"}, {"line": 736, "text": "\ud83c\udf9e\ufe0f Video Generation (Planned)"}, {"line": 760, "text": "\ud83e\uddea Scientific Output Interfaces (Planned)"}, {"line": 778, "text": "\ud83e\udde9 Chat Window v2 \u2013 Rich Message Schema (Planned)"}, {"line": 833, "text": "\ud83c\udfd7\ufe0f Architecture Addendum (Planned)"}, {"line": 858, "text": "\ud83d\udcd0 Data Contracts (Planned)"}, {"line": 880, "text": "\ud83e\uddea Acceptance Criteria (Milestones)"}, {"line": 903, "text": "\ud83d\udee0\ufe0f Dependencies (Planned)"}, {"line": 914, "text": "\u26a0\ufe0f Risks & Considerations"}, {"line": 923, "text": "\ud83e\uddea Try-it (Future, subject to change)"}, {"line": 926, "text": "Text \u2192 Image"}, {"line": 929, "text": "Image \u2192 Video"}, {"line": 932, "text": "Scientific Plot in Chat (tool)"}]}, {"path": "planned_features/presidio-pii-redaction.md", "content": "## PF-006: Presidio PII redaction for datasets and logs\n\n### Summary\n\nAdd optional PII redaction using Microsoft Presidio across dataset ingestion, evaluation samples, and JSONL metrics logging. Expose simple flags and a YAML policy to control which fields and entity types to redact. Provide a small local GUI preview to tune policies.\n\n### Why this matters\n\n- Reduce risk of inadvertently storing PII in training artifacts and logs.\n- Help teams comply with stricter data policies without blocking iteration.\n- Keep defaults safe and opt-in to minimize performance overhead.\n\n---\n\n## What ships in PF-006\n\n- Utility module: `src/aios/safety/presidio_redactor.py` (Analyzer + Anonymizer pipeline)\n- Config file: `config/presidio.yaml` (with `config/presidio.yaml.example` scaffold)\n- CLI flags:\n\t- `--redact-inputs/--no-redact-inputs`\n\t- `--redact-logs/--no-redact-logs`\n\t- `--presidio-config PATH`\n- Dataset preprocessor CLI: `aios datasets-redact` to create a redacted copy under `training_data/redacted/...`\n- Logging hook: optional redaction inside the JSONL logger for sensitive payload fields\n- Optional GUI preview (Streamlit/Gradio) to interactively test redaction rules on sample text\n\n---\n\n## Architecture overview\n\nData paths affected:\n\n1) Ingestion path (training/eval data)\n\t - If `--redact-inputs` is enabled, wrap the lines loader and apply redaction per line.\n\t - Write redacted datasets with `aios datasets-redact` when needed for offline inspection.\n\n2) Logging path (metrics JSONL)\n\t - If `--redact-logs` is enabled, wrap the JSONL writer and redact whitelisted keys: `text`, `sample`, `prompt`, `completion`, `generated`, `context`, and any configured custom keys.\n\n3) Preview GUI (optional)\n\t - Small app to paste text, toggle entity types, see the anonymized output and the recognized entities before committing policy changes.\n\nCore components:\n\n- PresidioRedactor (utility): constructs AnalyzerEngine + AnonymizerEngine; exposes `redact_text` and `redact_json`.\n- YAML policy: selects entities to target, anonymizer strategies, field-level overrides, and performance knobs.\n- Typer flags: propagate redaction options and config path into training/eval CLIs and dataset tools.\n\n---\n\n## Dependencies and setup\n\nRequired packages:\n- `presidio-analyzer`\n- `presidio-anonymizer`\n- `spacy`\n- spaCy model: `en_core_web_lg` (preferred) or `en_core_web_sm` (lighter, fewer entities)\n\nWindows/PowerShell install (example):\n\n```powershell\n# Activate your venv first if needed\n# python -m venv .venv; . .\\.venv\\Scripts\\Activate.ps1\n\npip install presidio-analyzer presidio-anonymizer spacy\npython -m spacy download en_core_web_lg\n```\n\nNotes:\n- For airgapped setups, pre-download wheels and the spaCy model; update `nlp_engine` in YAML accordingly.\n- Performance: Presidio adds CPU-bound overhead; consider using `--no-redact-logs` for long runs and enabling only on CI or releases.\n\n---\n\n## Configuration schema (`config/presidio.yaml`)\n\nMinimal keys (all optional; safe defaults apply when file missing):\n\n- `enabled: true|false` \u2014 master switch for the utility (CLI flags still gate behavior per path)\n- `entities: [\"PHONE_NUMBER\", \"EMAIL_ADDRESS\", \"US_SSN\", ...]` \u2014 target entity types\n- `anonymizers:` map of entity \u2192 strategy, e.g.:\n\t- `EMAIL_ADDRESS: { type: \"replace\", new_value: \"<EMAIL>\" }`\n\t- `PHONE_NUMBER: { type: \"mask\", masking_char: \"*\", chars_to_mask: 0, from_end: true }`\n- `default_anonymizer: { type: \"replace\", new_value: \"<PII>\" }`\n- `fields:` field-level behavior for JSON/records, e.g.:\n\t- `text: { redact: true }`\n\t- `prompt: { redact: true, entities: [\"EMAIL_ADDRESS\", \"PHONE_NUMBER\"] }`\n\t- `user: { redact: false }`\n- `custom_recognizers:` list of lightweight recognizers (regex + score + name), optional.\n- `nlp_engine:` spaCy model name and language, e.g. `model: en_core_web_lg`, `lang: en`.\n- `performance:` batch size, max_workers, timeouts.\n\nSee `config/presidio.yaml.example` for a concrete, commented template.\n\n---\n\n## Utility API design (`src/aios/safety/presidio_redactor.py`)\n\nContract:\n- Inputs: `text: str` or `record: dict` with configured fields\n- Outputs: redacted text/record plus optional matches metadata\n- Error modes: if Presidio is not installed or model missing, log a warning and operate as pass-through (no redaction)\n\nAPI sketch:\n- `class PresidioRedactor:`\n\t- `__init__(config_path: Optional[str] = None, overrides: Optional[dict] = None)`\n\t- `redact_text(text: str, *, entities: Optional[list[str]] = None) -> tuple[str, list[dict]]`\n\t- `redact_json(payload: dict, *, fields: Optional[list[str]] = None) -> tuple[dict, dict]`  # (redacted_copy, matches_by_field)\n\t- Internal: lazy init AnalyzerEngine/AnonymizerEngine; caching of compiled regex; thread-safe\n\nImplementation notes:\n- Use Presidio `AnalyzerEngine` and `AnonymizerEngine` with spaCy NLP engine.\n- Apply field-level overrides from YAML (`fields.*`).\n- For JSONL payloads, only redact whitelisted keys; never mutate the original dict before writing to disk.\n\n---\n\n## CLI design (Typer)\n\n1) Training CLI: `hrm-hf train-actv1` (`src/aios/cli/hrm_hf_cli.py`)\n\nAdd options:\n- `--redact-inputs/--no-redact-inputs` (default: no-redact)\n- `--redact-logs/--no-redact-logs` (default: no-redact)\n- `--presidio-config PATH` (default: `config/presidio.yaml` if exists)\n\nPlumbing:\n- Extend `TrainingConfig` with `redact_inputs: bool`, `redact_logs: bool`, `presidio_config: Optional[str]`.\n- Pass flags through to `train_actv1_impl` (file: `src/aios/cli/hrm_hf/train_actv1.py`).\n- In `train_actv1_impl`, create `PresidioRedactor` early (rank0 only if distributed, broadcast minimal toggles) and:\n\t- Wrap line ingestion (`get_training_lines`): if `redact_inputs`, map `redact_text` over lines.\n\t- Wrap `_write_jsonl_helper`: if `redact_logs`, first redact whitelisted payload keys.\n\n2) Datasets CLI: new command `datasets-redact` (`src/aios/cli/datasets_cli.py` -> register)\n\nCommand:\n\n```powershell\naios datasets-redact `\n\t--input training_data/curated_datasets/test_sample.txt `\n\t--output training_data/redacted/test_sample.txt `\n\t--presidio-config config/presidio.yaml `\n\t--format text `\n\t--json-field text\n```\n\nBehavior:\n- Reads input (plain text or JSONL), redacts line-by-line, writes output.\n- For JSONL, use `--json-field` to pick which key to redact (default: `text`).\n- Prints summary with number of lines, redaction hits by entity type.\n\n3) Optional GUI preview: `aios redaction-preview` (future)\n\n- Minimal Streamlit/Gradio app to paste sample text and see redaction live.\n- Launch via: `aios redaction-preview --presidio-config config/presidio.yaml`.\n- Out of scope for initial PR; spec retained here for future follow-up.\n\n---\n\n## Integration details (where to add hooks)\n\n- File: `src/aios/cli/hrm_hf/train_actv1.py`\n\t- Initialize redactor once, considering distributed rank.\n\t- Wrap `_write_jsonl_helper` via local closure `_write_jsonl` to redact payload keys when `config.redact_logs`.\n\t- During ingestion, before tokenization: if `config.redact_inputs`, map lines through `redact_text`.\n\n- File: `src/aios/cli/hrm_hf/data.py`\n\t- In `get_training_lines(...)`, inject optional redactor callable to transform lines when `redact_inputs` is enabled.\n\n- File: `src/aios/cli/datasets_cli.py`\n\t- Register new command `datasets-redact` implemented in `src/aios/cli/datasets/redact_cmd.py`.\n\nField keys to consider for log redaction (configurable):\n- `text`, `sample`, `prompt`, `completion`, `generated`, `context`, `input`, `output`\n\n---\n\n## Testing and acceptance criteria\n\nUnit tests:\n- `tests/test_presidio_redaction.py`:\n\t- Email and phone in a sentence \u2192 redacted with placeholders `<EMAIL>`, masked phone.\n\t- JSON payload: only configured fields are redacted; other keys unchanged.\n- `tests/test_logging_redaction.py`:\n\t- Wrap `_write_jsonl_helper` with redaction; ensure logged file contains redacted content.\n\nIntegration checks:\n- `aios hrm-hf train-actv1 --redact-logs --log-file artifacts/brains/actv1/metrics.jsonl` produces logs without raw PII.\n- `aios datasets-redact --input ... --output ...` writes a redacted dataset copy and prints a summary.\n\nAcceptance:\n- When enabled, no raw emails/phones appear in logs or redacted datasets (verified via regex search).\n- Redaction disabled by default; enabling adds measurable but acceptable overhead on CPU-only systems.\n\n---\n\n## Risks and mitigations\n\n- False positives/negatives: document scope and provide allow/deny lists; expose `custom_recognizers`.\n- Performance overhead: keep redaction opt-in; allow selecting `en_core_web_sm`; support batch processing in the utility.\n- Internationalization: default to English model; allow switching NLP model via YAML.\n\n---\n\n## Rollout plan\n\n1) M1 (1 day): Utility + unit tests + example config\n\t - Implement `PresidioRedactor` with text and JSON helpers.\n\t - Create `config/presidio.yaml.example` and docs.\n\t - Add basic unit tests for core behaviors.\n\n2) M2 (1 day): Hooks + CLI + docs\n\t - Add CLI flags and wire into training + dataset commands.\n\t - Implement `datasets-redact` command.\n\t - Expand docs with troubleshooting and examples.\n\nOptional (follow-ups):\n- Streamlit/Gradio preview app; VS Code task to launch it.\n- Presidio recognizer registry loader from YAML (advanced patterns).\n\n---\n\n## Troubleshooting\n\n- spaCy model error: run `python -m spacy download en_core_web_lg` (or use `en_core_web_sm`).\n- Presidio not installed: utility logs a warning and passes text through unchanged.\n- Slow runs: disable `--redact-logs` for long training; keep redaction for CI or releases.\n- Entity not redacted: verify it\u2019s in `entities` and not excluded by field-level overrides.\n\n---\n\n## Quickstart (Windows/PowerShell)\n\n```powershell\n# 1) Install deps\npip install presidio-analyzer presidio-anonymizer spacy\npython -m spacy download en_core_web_lg\n\n# 2) Copy example config and tweak\nCopy-Item config/presidio.yaml.example config/presidio.yaml\n\n# 3) Redact a dataset copy (text)\naios datasets-redact `\n\t--input training_data/curated_datasets/test_sample.txt `\n\t--output training_data/redacted/test_sample.txt `\n\t--presidio-config config/presidio.yaml `\n\t--format text\n\n# 4) Run training with log redaction\naios hrm-hf train-actv1 `\n\t--model gpt2 `\n\t--dataset-file training_data/curated_datasets/test_sample.txt `\n\t--steps 10 --batch-size 2 `\n\t--redact-logs `\n\t--presidio-config config/presidio.yaml `\n\t--log-file artifacts/brains/actv1/metrics.jsonl\n```\n\n---\n\n## Developer checklist (end-to-end)\n\n- [ ] Add `src/aios/safety/presidio_redactor.py` (utility class + lazy init)\n- [ ] Extend `TrainingConfig` with `redact_inputs`, `redact_logs`, `presidio_config`\n- [ ] Wire flags in `src/aios/cli/hrm_hf_cli.py` and plumb to `train_actv1_impl`\n- [ ] In `train_actv1_impl`, wrap `_write_jsonl_helper` with redaction when enabled\n- [ ] In `get_training_lines` and/or callsites, map redaction over lines when enabled\n- [ ] New CLI `src/aios/cli/datasets/redact_cmd.py` + register in `datasets_cli.py`\n- [ ] Add unit tests (`tests/test_presidio_redaction.py`, `tests/test_logging_redaction.py`)\n- [ ] Add `config/presidio.yaml.example`\n- [ ] Update docs and add Examples section\n\n---\n\n## Operator checklist (before enabling in prod)\n\n- [ ] Verify Presidio + spaCy installed; run the Quickstart to confirm\n- [ ] Copy `config/presidio.yaml.example` \u2192 `config/presidio.yaml` and tailor entities/placeholders\n- [ ] Dry-run `datasets-redact` on a small sample; inspect outputs\n- [ ] Enable `--redact-logs` on a short training run; inspect `metrics.jsonl`\n- [ ] Monitor performance; consider `en_core_web_sm` if needed\n\n", "tags": ["cli", "datasets", "evaluation", "gui", "training"], "headings": [{"line": 0, "text": "PF-006: Presidio PII redaction for datasets and logs"}, {"line": 2, "text": "Summary"}, {"line": 6, "text": "Why this matters"}, {"line": 14, "text": "What ships in PF-006"}, {"line": 28, "text": "Architecture overview"}, {"line": 50, "text": "Dependencies and setup"}, {"line": 61, "text": "Activate your venv first if needed"}, {"line": 62, "text": "python -m venv .venv; . .\\.venv\\Scripts\\Activate.ps1"}, {"line": 74, "text": "Configuration schema (`config/presidio.yaml`)"}, {"line": 96, "text": "Utility API design (`src/aios/safety/presidio_redactor.py`)"}, {"line": 117, "text": "CLI design (Typer)"}, {"line": 159, "text": "Integration details (where to add hooks)"}, {"line": 177, "text": "Testing and acceptance criteria"}, {"line": 196, "text": "Risks and mitigations"}, {"line": 204, "text": "Rollout plan"}, {"line": 222, "text": "Troubleshooting"}, {"line": 231, "text": "Quickstart (Windows/PowerShell)"}, {"line": 234, "text": "1) Install deps"}, {"line": 238, "text": "2) Copy example config and tweak"}, {"line": 241, "text": "3) Redact a dataset copy (text)"}, {"line": 248, "text": "4) Run training with log redaction"}, {"line": 260, "text": "Developer checklist (end-to-end)"}, {"line": 274, "text": "Operator checklist (before enabling in prod)"}]}, {"path": "planned_features/SEAL_INTEGRATION_PLAN.md", "content": "# SEAL Integration Implementation Plan\n**AI-OS Enhancement: Self-Adapting Language Models**\n\n**Document Version**: 1.0  \n**Created**: January 15, 2025  \n**Status**: Planning Phase  \n**Owner**: AI-OS Development Team  \n**Priority**: HIGH (Phase 1), MEDIUM (Phase 2), OPTIONAL (Phase 3)\n\n---\n\n> Note: References to `docs/user_guide/*` in this planning document are placeholders for future user-facing docs. For current information, use `docs/INDEX.md` and guides under `docs/guide/`.\n\n\n## \ud83d\udccb Table of Contents\n\n1. [Executive Summary](#executive-summary)\n2. [Phase 1: Self-Edit Data Generation](#phase-1-self-edit-data-generation-weeks-1-4)\n3. [Phase 2: RL Optimization](#phase-2-rl-optimization-weeks-5-14)\n4. [Phase 3: Advanced Meta-Learning](#phase-3-advanced-meta-learning-weeks-15)\n5. [Dependencies & Prerequisites](#dependencies--prerequisites)\n6. [Success Metrics](#success-metrics)\n7. [Risk Management](#risk-management)\n8. [Testing Strategy](#testing-strategy)\n9. [Documentation Requirements](#documentation-requirements)\n10. [Rollout Plan](#rollout-plan)\n\n---\n\n## Executive Summary\n\n**Goal**: Integrate SEAL (Self-Adapting Language Models) framework into AI-OS to enable autonomous expert training with synthetic data generation and reinforcement learning optimization.\n\n**Expected Benefits**:\n- Phase 1: 10-20% improvement in expert quality\n- Phase 2: 15-25% improvement with RL optimization\n- Phase 3: Full meta-learning and self-improvement capabilities\n\n**Total Estimated Effort**: 200-300 hours across 14-20 weeks\n\n**Key Deliverables**:\n- Self-edit generation system\n- RL optimization framework (optional)\n- Enhanced AutoTrainingOrchestrator\n- Comprehensive testing suite\n- User documentation\n\n---\n\n## Phase 1: Self-Edit Data Generation (Weeks 1-4)\n\n**Objective**: Implement SEAL's synthetic data generation to augment expert training datasets\n\n**Effort**: 40-60 hours  \n**Risk Level**: LOW  \n**Priority**: HIGH  \n**Dependencies**: None (can start immediately)\n\n---\n\n### Week 1: Foundation & Core Classes\n\n#### 1.1 Project Setup\n**Duration**: 2-3 hours\n\n- [ ] **Create directory structure**\n  ```\n  src/aios/core/seal/\n  \u251c\u2500\u2500 __init__.py\n  \u251c\u2500\u2500 self_edit_generator.py\n  \u251c\u2500\u2500 prompts.py\n  \u251c\u2500\u2500 strategies.py\n  \u2514\u2500\u2500 cache.py\n  \n  tests/test_seal/\n  \u251c\u2500\u2500 __init__.py\n  \u251c\u2500\u2500 test_self_edit_generator.py\n  \u251c\u2500\u2500 test_strategies.py\n  \u2514\u2500\u2500 fixtures/\n      \u2514\u2500\u2500 sample_passages.txt\n  ```\n\n- [ ] **Add SEAL dependencies to requirements**\n  - File: `pyproject.toml`\n  - Add dependencies:\n    ```toml\n    # SEAL Integration\n    \"sentence-transformers>=2.2.0\",  # For quality filtering\n    \"rouge-score>=0.1.2\",             # For diversity metrics\n    ```\n\n- [ ] **Update configuration schema**\n  - File: `config/default.yaml`\n  - Add SEAL configuration section (see below)\n\n- [ ] **Create feature flag**\n  - File: `src/aios/core/config/feature_flags.py`\n  - Add `ENABLE_SEAL_SELF_EDITS = True`\n\n#### 1.2 Prompts Module\n**Duration**: 3-4 hours  \n**File**: `src/aios/core/seal/prompts.py`\n\n- [ ] **Define prompt templates**\n  ```python\n  # Implement these prompt strategies from SEAL paper:\n  - IMPLICATIONS_PROMPT\n  - IMPLICATIONS_LONG_PROMPT\n  - IMPLICATIONS_VERY_LONG_PROMPT\n  - REWRITE_PROMPT\n  - SELF_QA_PROMPT\n  - IMPLICATIONS_COT_PROMPT\n  ```\n\n- [ ] **Create PromptBuilder class**\n  - [ ] Method: `build_prompt(passage: str, strategy: str) -> str`\n  - [ ] Method: `format_system_message() -> str`\n  - [ ] Method: `format_few_shot_examples() -> List[Dict]`\n  - [ ] Validation for unknown strategies\n\n- [ ] **Add prompt customization**\n  - [ ] Support for user-defined prompts\n  - [ ] Prompt template variables (passage, domain, etc.)\n  - [ ] Load prompts from config file\n\n- [ ] **Write unit tests**\n  - [ ] Test all 6 prompt strategies\n  - [ ] Test prompt formatting\n  - [ ] Test edge cases (empty passage, very long passage)\n\n#### 1.3 Strategies Module\n**Duration**: 4-5 hours  \n**File**: `src/aios/core/seal/strategies.py`\n\n- [ ] **Define SelfEditStrategy base class**\n  ```python\n  class SelfEditStrategy(ABC):\n      @abstractmethod\n      def generate(self, passage: str, model, tokenizer, **kwargs) -> List[str]:\n          pass\n      \n      @abstractmethod\n      def parse_output(self, raw_output: str) -> List[str]:\n          pass\n  ```\n\n- [ ] **Implement ImplicationsStrategy**\n  - [ ] Generate method with temperature sampling\n  - [ ] Parse output by newlines\n  - [ ] Filter empty/invalid implications\n  - [ ] Deduplication logic\n\n- [ ] **Implement RewriteStrategy**\n  - [ ] Multiple rewrite variants\n  - [ ] Parse by delimiter\n  - [ ] Ensure rewrites are different from original\n\n- [ ] **Implement SelfQAStrategy**\n  - [ ] Generate Q&A pairs\n  - [ ] Parse questions and answers separately\n  - [ ] Format as training pairs\n  - [ ] Validate Q&A structure\n\n- [ ] **Add strategy registry**\n  - [ ] Registry dict: strategy_name -> Strategy class\n  - [ ] Factory method: `get_strategy(name: str) -> SelfEditStrategy`\n\n- [ ] **Write unit tests**\n  - [ ] Test each strategy independently\n  - [ ] Mock model generation\n  - [ ] Test parsing with various formats\n  - [ ] Test edge cases\n\n#### 1.4 Core Generator Class\n**Duration**: 6-8 hours  \n**File**: `src/aios/core/seal/self_edit_generator.py`\n\n- [ ] **Implement SelfEditGenerator class**\n  ```python\n  class SelfEditGenerator:\n      def __init__(\n          self,\n          model: nn.Module,\n          tokenizer: PreTrainedTokenizer,\n          strategy: str = \"implications\",\n          num_edits: int = 5,\n          temperature: float = 1.0,\n          max_length: int = 1024,\n          cache_dir: Optional[str] = None,\n      ):\n          # Implementation\n  ```\n\n- [ ] **Core generation method**\n  - [ ] `generate_self_edits(passage: str, **kwargs) -> List[str]`\n  - [ ] Batched generation for efficiency\n  - [ ] Error handling for generation failures\n  - [ ] Timeout handling (max 30s per edit)\n  - [ ] GPU memory management\n\n- [ ] **Quality filtering**\n  - [ ] Filter by minimum length (20 tokens)\n  - [ ] Filter by maximum length (2048 tokens)\n  - [ ] Detect and remove duplicates\n  - [ ] Detect and remove low-quality outputs (gibberish)\n  - [ ] Optional: Semantic similarity filtering\n\n- [ ] **Caching mechanism**\n  - [ ] Cache key: hash(passage + strategy + model_name)\n  - [ ] Save to disk: `{cache_dir}/{hash}.json`\n  - [ ] Load from cache if available\n  - [ ] Cache invalidation strategy\n  - [ ] Max cache size limit (10GB default)\n\n- [ ] **Logging and telemetry**\n  - [ ] Log generation time per passage\n  - [ ] Log cache hit/miss rates\n  - [ ] Log quality filter statistics\n  - [ ] Track total tokens generated\n\n- [ ] **Write unit tests**\n  - [ ] Test with mock model\n  - [ ] Test caching behavior\n  - [ ] Test quality filtering\n  - [ ] Test error handling\n  - [ ] Test with real small model (optional)\n\n#### 1.5 Configuration Integration\n**Duration**: 2-3 hours  \n**File**: `config/default.yaml`\n\n- [ ] **Add SEAL configuration section**\n  ```yaml\n  seal:\n    # Self-Edit Generation\n    enabled: true\n    \n    # Strategy Selection\n    strategy: \"implications\"  # implications, rewrite, self-qa, implications-long\n    num_edits_per_passage: 5\n    temperature: 1.0\n    max_length: 1024\n    \n    # Quality Filtering\n    min_length: 20\n    max_length: 2048\n    enable_deduplication: true\n    enable_quality_filter: true\n    similarity_threshold: 0.9  # For deduplication\n    \n    # Caching\n    enable_cache: true\n    cache_dir: \"artifacts/seal_cache\"\n    max_cache_size_gb: 10\n    \n    # Performance\n    batch_size: 4\n    max_generation_time: 30  # seconds per edit\n    \n    # Advanced (Phase 2)\n    enable_rl_optimization: false\n    restem_iterations: 2\n    restem_batch_size: 50\n    restem_candidates_per_passage: 5\n  ```\n\n- [ ] **Validation schema**\n  - File: `src/aios/core/config/schemas.py`\n  - Add SEAL config validation\n  - Type checking for all fields\n  - Range validation (e.g., temperature 0.1-2.0)\n\n---\n\n### Week 2: Integration with AutoTrainingOrchestrator\n\n#### 2.1 Refactor AutoTrainingOrchestrator\n**Duration**: 6-8 hours  \n**File**: `src/aios/core/training/auto_training_orchestrator.py`\n\n- [ ] **Add SelfEditGenerator to orchestrator**\n  ```python\n  class AutoTrainingOrchestrator:\n      def __init__(self, config):\n          # ... existing code ...\n          \n          if config.seal.enabled:\n              self.self_edit_generator = SelfEditGenerator(\n                  model=self.base_model,\n                  tokenizer=self.tokenizer,\n                  strategy=config.seal.strategy,\n                  num_edits=config.seal.num_edits_per_passage,\n                  cache_dir=config.seal.cache_dir,\n              )\n          else:\n              self.self_edit_generator = None\n  ```\n\n- [ ] **Modify dataset preparation pipeline**\n  - [ ] Existing method: `prepare_training_data(dataset_path: str)`\n  - [ ] Add parameter: `augment_with_self_edits: bool = True`\n  - [ ] Load original passages\n  - [ ] Generate self-edits for each passage\n  - [ ] Combine original + self-edits\n  - [ ] Return augmented dataset\n\n- [ ] **Add progress tracking**\n  - [ ] Progress bar for self-edit generation\n  - [ ] ETA calculation\n  - [ ] Intermediate saving (every 100 passages)\n  - [ ] Resume capability if interrupted\n\n- [ ] **Error handling**\n  - [ ] Graceful fallback if generation fails\n  - [ ] Retry logic (max 3 retries per passage)\n  - [ ] Log failures for debugging\n  - [ ] Continue with original data if all retries fail\n\n- [ ] **Write unit tests**\n  - [ ] Test augmentation pipeline\n  - [ ] Test with SEAL enabled/disabled\n  - [ ] Test error handling\n  - [ ] Test progress tracking\n\n#### 2.2 Update Expert Training Pipeline\n**Duration**: 4-5 hours  \n**File**: `src/aios/core/training/expert_trainer.py`\n\n- [ ] **Modify train_expert() function**\n  - [ ] Add parameter: `use_self_edits: bool = True`\n  - [ ] Integrate self-edit generation before training\n  - [ ] Log augmented dataset statistics\n  - [ ] Save augmented dataset for inspection\n\n- [ ] **Dataset statistics logging**\n  ```python\n  def log_dataset_stats(original, augmented):\n      # Original dataset size\n      # Augmented dataset size\n      # Augmentation ratio\n      # Average self-edits per passage\n      # Total tokens (before/after)\n  ```\n\n- [ ] **A/B testing support**\n  - [ ] Train two experts: with/without self-edits\n  - [ ] Compare validation metrics\n  - [ ] Generate comparison report\n  - [ ] Save results to `artifacts/seal_experiments/`\n\n- [ ] **Write integration tests**\n  - [ ] End-to-end training with self-edits\n  - [ ] Verify augmented dataset format\n  - [ ] Verify training completes successfully\n  - [ ] Test A/B comparison workflow\n\n#### 2.3 CLI Integration\n**Duration**: 3-4 hours  \n**File**: `src/aios/cli/aios.py`\n\n- [ ] **Add SEAL commands**\n  ```bash\n  # Generate self-edits for a dataset (standalone)\n  aios seal generate --input dataset.txt --output augmented.txt --strategy implications\n  \n  # Preview self-edits (no saving)\n  aios seal preview --input dataset.txt --num-samples 5\n  \n  # Cache management\n  aios seal cache clear\n  aios seal cache stats\n  aios seal cache prune --max-size 5GB\n  ```\n\n- [ ] **Extend train command**\n  ```bash\n  # Train expert with self-edits (default)\n  aios hrm-hf train-expert --expert-id abc123 --dataset data.txt\n  \n  # Train expert without self-edits\n  aios hrm-hf train-expert --expert-id abc123 --dataset data.txt --no-seal\n  \n  # Train with specific strategy\n  aios hrm-hf train-expert --expert-id abc123 --dataset data.txt --seal-strategy rewrite\n  ```\n\n- [ ] **Add verbose logging option**\n  - [ ] `--seal-verbose` flag\n  - [ ] Show generation progress\n  - [ ] Show quality filter statistics\n  - [ ] Show cache hit rates\n\n- [ ] **Write CLI tests**\n  - [ ] Test all new commands\n  - [ ] Test flag combinations\n  - [ ] Test error messages\n\n---\n\n### Week 3: Testing & Validation\n\n#### 3.1 Unit Testing\n**Duration**: 6-8 hours\n\n- [ ] **Test coverage targets**\n  - [ ] SelfEditGenerator: 90%+ coverage\n  - [ ] Strategies: 85%+ coverage\n  - [ ] Prompts: 80%+ coverage\n  - [ ] Integration: 75%+ coverage\n\n- [ ] **Specific test cases**\n  - [ ] Test each prompt strategy\n  - [ ] Test with various passage lengths\n  - [ ] Test with special characters/unicode\n  - [ ] Test with edge cases (empty, very long)\n  - [ ] Test caching behavior\n  - [ ] Test quality filtering\n  - [ ] Test error handling\n  - [ ] Test memory management\n\n- [ ] **Mock generation testing**\n  - File: `tests/test_seal/test_self_edit_generator.py`\n  - Mock model.generate() to return predictable outputs\n  - Verify parsing logic\n  - Verify filtering logic\n  - Verify caching logic\n\n- [ ] **Performance testing**\n  - [ ] Measure generation time per passage\n  - [ ] Measure memory usage\n  - [ ] Test batch generation efficiency\n  - [ ] Profile bottlenecks\n\n#### 3.2 Integration Testing\n**Duration**: 6-8 hours\n\n- [ ] **End-to-end workflow tests**\n  - File: `tests/integration/test_seal_workflow.py`\n  - [ ] Test 1: Generate self-edits \u2192 Train expert \u2192 Validate\n  - [ ] Test 2: Train with/without SEAL \u2192 Compare results\n  - [ ] Test 3: Cache persistence across runs\n  - [ ] Test 4: Error recovery and retries\n\n- [ ] **Real model testing (optional but recommended)**\n  - [ ] Use small model (e.g., GPT-2 124M)\n  - [ ] Generate real self-edits\n  - [ ] Verify output quality\n  - [ ] Compare to manual inspection\n\n- [ ] **Training pipeline integration**\n  - [ ] Verify expert training completes\n  - [ ] Verify no regressions in base model\n  - [ ] Verify augmented dataset format\n  - [ ] Verify checkpoint saving\n\n#### 3.3 A/B Experiment Setup\n**Duration**: 4-5 hours\n\n- [ ] **Design experiment**\n  - [ ] Select small test dataset (100 passages)\n  - [ ] Split into train/validation\n  - [ ] Define success metrics (see below)\n\n- [ ] **Implement comparison script**\n  - File: `scripts/seal_ab_test.py`\n  ```python\n  # Train two experts:\n  # Control: Standard training\n  # Treatment: Training with SEAL self-edits\n  \n  # Compare:\n  # - Validation loss\n  # - Perplexity\n  # - Training time\n  # - Memory usage\n  ```\n\n- [ ] **Create experiment config**\n  - File: `experiments/seal_phase1_experiment.yaml`\n  - Define hyperparameters\n  - Define evaluation metrics\n  - Define comparison criteria\n\n- [ ] **Run pilot experiment**\n  - [ ] Train control expert (no SEAL)\n  - [ ] Train treatment expert (with SEAL)\n  - [ ] Compare results\n  - [ ] Document findings\n\n---\n\n### Week 4: Documentation & Refinement\n\n#### 4.1 User Documentation\n**Duration**: 4-5 hours\n\n- [ ] **Create user guide**\n  - File: `docs/user_guide/SEAL_SELF_EDITS.md` (placeholder; to be created \u2014 see `docs/INDEX.md` and `docs/guide/` for current docs)\n  - [ ] What is SEAL?\n  - [ ] How does self-edit generation work?\n  - [ ] When to use self-edits?\n  - [ ] Configuration options explained\n  - [ ] CLI usage examples\n  - [ ] Troubleshooting common issues\n\n- [ ] **Update existing docs**\n  - [ ] Update `docs/QUICK_START.md` with SEAL mention\n  - [ ] Update `docs/user_guide/TRAINING.md` with self-edit section (placeholder target; user guide not yet authored)\n  - [ ] Update `config/default.yaml` with inline comments\n  - [ ] Update `README.md` with SEAL feature highlight\n\n- [ ] **Create tutorial**\n  - File: `docs/tutorials/SEAL_FIRST_EXPERT.md` (placeholder; to be created)\n  - [ ] Step-by-step: Train your first expert with SEAL\n  - [ ] Expected results and interpretation\n  - [ ] Comparison: with vs without SEAL\n\n#### 4.2 API Documentation\n**Duration**: 3-4 hours\n\n- [ ] **Generate API docs**\n  - [ ] Add docstrings to all public methods\n  - [ ] Use Google/NumPy docstring format\n  - [ ] Include usage examples in docstrings\n  - [ ] Generate Sphinx/MkDocs API reference\n\n- [ ] **Code examples**\n  - File: `docs/api/seal_examples.md` (placeholder; to be created)\n  ```python\n  # Example 1: Basic usage\n  # Example 2: Custom strategy\n  # Example 3: Advanced configuration\n  # Example 4: Programmatic access\n  ```\n\n#### 4.3 Performance Optimization\n**Duration**: 4-6 hours\n\n- [ ] **Profile generation pipeline**\n  - [ ] Identify bottlenecks\n  - [ ] Optimize batching\n  - [ ] Optimize tokenization\n  - [ ] Optimize caching\n\n- [ ] **Memory optimization**\n  - [ ] Reduce peak memory usage\n  - [ ] Implement streaming for large datasets\n  - [ ] Add memory usage monitoring\n  - [ ] Test on large datasets (10K+ passages)\n\n- [ ] **Caching optimization**\n  - [ ] Implement cache warming\n  - [ ] Add cache preloading\n  - [ ] Optimize cache lookup\n  - [ ] Test cache performance\n\n#### 4.4 Final Validation\n**Duration**: 2-3 hours\n\n- [ ] **Run full test suite**\n  - [ ] All unit tests pass\n  - [ ] All integration tests pass\n  - [ ] Code coverage meets targets\n  - [ ] No critical linting errors\n\n- [ ] **Manual testing checklist**\n  - [ ] Install on fresh environment\n  - [ ] Run through tutorial\n  - [ ] Test all CLI commands\n  - [ ] Generate self-edits for sample dataset\n  - [ ] Train sample expert\n  - [ ] Verify improvement vs baseline\n\n- [ ] **Performance benchmarks**\n  - [ ] Generation time per passage\n  - [ ] Memory usage during generation\n  - [ ] Cache hit rate\n  - [ ] Training time comparison\n\n- [ ] **Documentation review**\n  - [ ] All docs are accurate\n  - [ ] All examples work\n  - [ ] No broken links\n  - [ ] Clear and easy to follow\n\n---\n\n### Phase 1 Checklist Summary\n\n**Before Starting**:\n- [ ] Review SEAL paper thoroughly\n- [ ] Review SEAL GitHub code\n- [ ] Understand AI-OS codebase\n- [ ] Set up development environment\n- [ ] Create feature branch: `feature/seal-phase1`\n\n**Week 1 Complete**:\n- [ ] Project structure created\n- [ ] Dependencies added\n- [ ] Prompts module implemented and tested\n- [ ] Strategies module implemented and tested\n- [ ] SelfEditGenerator class implemented and tested\n- [ ] Configuration integrated\n\n**Week 2 Complete**:\n- [ ] AutoTrainingOrchestrator updated\n- [ ] Expert training pipeline updated\n- [ ] CLI commands added\n- [ ] Integration tests written\n\n**Week 3 Complete**:\n- [ ] All unit tests pass (90%+ coverage)\n- [ ] All integration tests pass\n- [ ] A/B experiment completed\n- [ ] Results documented\n\n**Week 4 Complete**:\n- [ ] User documentation written\n- [ ] API documentation generated\n- [ ] Performance optimized\n- [ ] Final validation passed\n- [ ] Ready for merge to main\n\n**Phase 1 Acceptance Criteria**:\n- [ ] Self-edit generation works for all strategies\n- [ ] Expert training with SEAL completes successfully\n- [ ] 10-15% improvement in validation loss vs baseline\n- [ ] No regressions in base model or existing features\n- [ ] Documentation is complete and accurate\n- [ ] Code coverage >85%\n- [ ] Performance within acceptable limits (<30s per passage)\n\n---\n\n## Phase 2: RL Optimization (Weeks 5-14)\n\n**Objective**: Implement ReSTEM reinforcement learning to optimize self-edit quality\n\n**Effort**: 80-120 hours  \n**Risk Level**: MEDIUM  \n**Priority**: MEDIUM  \n**Dependencies**: Phase 1 complete and validated\n\n**Note**: This phase is OPTIONAL. Only proceed if Phase 1 shows promising results and you have compute budget for RL training.\n\n---\n\n### Week 5-6: ReSTEM Foundation\n\n#### 5.1 Research & Planning\n**Duration**: 6-8 hours\n\n- [ ] **Deep dive into ReSTEM**\n  - [ ] Read SEAL paper Section 3.1 thoroughly\n  - [ ] Study SEAL GitHub implementation\n  - [ ] Understand E-step (sampling) and M-step (SFT)\n  - [ ] Understand binary reward computation\n  - [ ] Review hyperparameters from paper\n\n- [ ] **Design AI-OS adaptation**\n  - [ ] Define reward function for AI-OS\n  - [ ] Design inner loop (TTT)\n  - [ ] Design outer loop (policy update)\n  - [ ] Plan checkpoint management\n  - [ ] Plan GPU resource allocation\n\n- [ ] **Create design document**\n  - File: `docs/development/SEAL_RESTEM_DESIGN.md` (placeholder; to be created)\n  - [ ] Architecture diagram\n  - [ ] Algorithm pseudocode\n  - [ ] Resource requirements\n  - [ ] Risk analysis\n\n#### 5.2 Reward Function Implementation\n**Duration**: 6-8 hours  \n**File**: `src/aios/core/seal/reward.py`\n\n- [ ] **Define RewardFunction base class**\n  ```python\n  class RewardFunction(ABC):\n      @abstractmethod\n      def compute_reward(\n          self,\n          expert: nn.Module,\n          validation_dataset: Dataset,\n          threshold: float,\n      ) -> float:\n          \"\"\"\n          Returns:\n              1.0 if expert improves over threshold\n              0.0 otherwise\n          \"\"\"\n          pass\n  ```\n\n- [ ] **Implement ValidationLossReward**\n  - [ ] Evaluate expert on validation set\n  - [ ] Compute average loss\n  - [ ] Return 1 if loss < threshold, 0 otherwise\n  - [ ] Cache validation results\n\n- [ ] **Implement PerplexityReward**\n  - [ ] Compute perplexity on validation set\n  - [ ] Return binary reward based on threshold\n\n- [ ] **Implement AccuracyReward (optional)**\n  - [ ] For classification/QA tasks\n  - [ ] Compute accuracy\n  - [ ] Return binary reward\n\n- [ ] **Add reward computation utilities**\n  - [ ] Efficient validation batching\n  - [ ] GPU memory management\n  - [ ] Timeout handling\n  - [ ] Result caching\n\n- [ ] **Write unit tests**\n  - [ ] Test each reward function\n  - [ ] Test with mock expert\n  - [ ] Test edge cases\n\n#### 5.3 Test-Time Training (TTT) Module\n**Duration**: 8-10 hours  \n**File**: `src/aios/core/seal/test_time_training.py`\n\n- [ ] **Implement TTTTrainer class**\n  ```python\n  class TTTTrainer:\n      def train_temporary_expert(\n          self,\n          base_expert: nn.Module,\n          self_edit: str,\n          epochs: int = 5,\n          learning_rate: float = 1e-4,\n      ) -> Tuple[nn.Module, Dict]:\n          \"\"\"\n          Train expert on self-edit, return trained expert + metrics.\n          \"\"\"\n  ```\n\n- [ ] **LoRA integration**\n  - [ ] Use PEFT library for efficient adapters\n  - [ ] Configure LoRA rank and alpha\n  - [ ] Apply LoRA to expert module\n  - [ ] Train only LoRA parameters\n\n- [ ] **Training loop**\n  - [ ] Freeze base expert\n  - [ ] Train LoRA adapter\n  - [ ] Monitor loss\n  - [ ] Early stopping if loss plateaus\n  - [ ] Save temporary checkpoint\n\n- [ ] **Memory management**\n  - [ ] Create expert copy for TTT\n  - [ ] Clean up after training\n  - [ ] Garbage collection\n  - [ ] GPU memory clearing\n\n- [ ] **Write unit tests**\n  - [ ] Test TTT with mock expert\n  - [ ] Test LoRA application\n  - [ ] Test memory cleanup\n  - [ ] Test training convergence\n\n---\n\n### Week 7-8: ReSTEM Optimizer Core\n\n#### 7.1 ReSTEM Optimizer Implementation\n**Duration**: 10-12 hours  \n**File**: `src/aios/core/seal/restem_optimizer.py`\n\n- [ ] **Implement ReSTEMOptimizer class**\n  ```python\n  class ReSTEMOptimizer:\n      def __init__(\n          self,\n          self_edit_generator: SelfEditGenerator,\n          ttt_trainer: TTTTrainer,\n          reward_function: RewardFunction,\n          num_iterations: int = 2,\n          batch_size: int = 50,\n          candidates_per_passage: int = 5,\n      ):\n          # Implementation\n  ```\n\n- [ ] **E-Step: Sample self-edits**\n  - [ ] Generate N candidates per passage\n  - [ ] Use current generator policy\n  - [ ] Store candidates with metadata\n  - [ ] Log sampling statistics\n\n- [ ] **Inner Loop: TTT evaluation**\n  - [ ] For each candidate self-edit:\n    - [ ] Train temporary expert via TTT\n    - [ ] Evaluate on validation set\n    - [ ] Compute reward\n    - [ ] Store (candidate, reward) pair\n  - [ ] Parallel processing (if multiple GPUs)\n  - [ ] Progress tracking\n  - [ ] Checkpointing\n\n- [ ] **M-Step: Policy update**\n  - [ ] Filter candidates by reward (keep reward=1)\n  - [ ] Fine-tune generator on good self-edits\n  - [ ] Use standard SFT (AdamW optimizer)\n  - [ ] Monitor training metrics\n  - [ ] Save updated generator checkpoint\n\n- [ ] **Iteration management**\n  - [ ] Run E-M loop for N iterations\n  - [ ] Track metrics across iterations\n  - [ ] Early stopping if no improvement\n  - [ ] Save best generator checkpoint\n\n- [ ] **Logging and telemetry**\n  - [ ] Log per-iteration statistics\n  - [ ] Log reward distribution\n  - [ ] Log generator improvement\n  - [ ] Save to JSONL file\n\n#### 7.2 Integration with Training Pipeline\n**Duration**: 6-8 hours  \n**File**: `src/aios/core/training/auto_training_orchestrator.py`\n\n- [ ] **Add ReSTEM mode to orchestrator**\n  ```python\n  def train_expert_with_restem(\n      self,\n      expert_id: str,\n      dataset_path: str,\n      validation_split: float = 0.2,\n  ) -> Dict:\n      # Split dataset\n      # Initialize ReSTEM optimizer\n      # Run optimization loop\n      # Train final expert with optimized generator\n      # Return metrics\n  ```\n\n- [ ] **Resource management**\n  - [ ] Estimate GPU memory requirements\n  - [ ] Check available resources\n  - [ ] Allocate GPUs for TTT\n  - [ ] Handle out-of-memory errors\n\n- [ ] **Checkpoint management**\n  - [ ] Save intermediate generator checkpoints\n  - [ ] Save reward statistics\n  - [ ] Save best self-edits\n  - [ ] Resume capability from checkpoint\n\n- [ ] **Write integration tests**\n  - [ ] Test ReSTEM workflow end-to-end\n  - [ ] Test with small mock dataset\n  - [ ] Verify checkpoint saving/loading\n  - [ ] Test error handling\n\n---\n\n### Week 9-10: Testing & Optimization\n\n#### 9.1 Comprehensive Testing\n**Duration**: 8-10 hours\n\n- [ ] **Unit tests**\n  - [ ] RewardFunction implementations\n  - [ ] TTTTrainer methods\n  - [ ] ReSTEMOptimizer E-step\n  - [ ] ReSTEMOptimizer M-step\n  - [ ] Edge cases and error handling\n\n- [ ] **Integration tests**\n  - [ ] Full ReSTEM loop with mock data\n  - [ ] Multi-GPU TTT if available\n  - [ ] Checkpoint save/load\n  - [ ] Memory cleanup\n\n- [ ] **Performance tests**\n  - [ ] Measure TTT time per candidate\n  - [ ] Measure total ReSTEM time\n  - [ ] Measure memory usage\n  - [ ] Identify bottlenecks\n\n#### 9.2 Hyperparameter Tuning\n**Duration**: 6-8 hours\n\n- [ ] **Create tuning script**\n  - File: `scripts/tune_restem_hyperparams.py`\n  - [ ] Grid search over:\n    - Learning rate: [1e-5, 3e-5, 1e-4]\n    - Reward threshold: [auto, fixed values]\n    - LoRA rank: [8, 16, 32]\n    - TTT epochs: [3, 5, 10]\n\n- [ ] **Run tuning experiments**\n  - [ ] Use small dataset for speed\n  - [ ] Track validation improvement\n  - [ ] Document best hyperparameters\n  - [ ] Update default config\n\n#### 9.3 Validation Experiment\n**Duration**: 8-10 hours\n\n- [ ] **Design experiment**\n  - [ ] Select test domain (e.g., Python programming)\n  - [ ] Prepare dataset (100-200 passages)\n  - [ ] Define success criteria\n\n- [ ] **Run comparison**\n  - [ ] Baseline: Phase 1 (self-edits without RL)\n  - [ ] Treatment: Phase 2 (ReSTEM optimization)\n  - [ ] Measure:\n    - Validation loss improvement\n    - Training time\n    - Compute cost\n    - Expert quality\n\n- [ ] **Document results**\n  - File: `experiments/seal_phase2_results.md`\n  - [ ] Quantitative metrics\n  - [ ] Qualitative assessment\n  - [ ] Cost-benefit analysis\n  - [ ] Recommendation for production\n\n---\n\n### Week 11-12: CLI, GUI, and Usability\n\n#### 11.1 CLI Commands\n**Duration**: 4-5 hours\n\n- [ ] **Add ReSTEM commands**\n  ```bash\n  # Run ReSTEM optimization\n  aios seal restem optimize \\\n      --dataset data.txt \\\n      --output-generator generator.pt \\\n      --iterations 2 \\\n      --batch-size 50\n  \n  # Use optimized generator for training\n  aios hrm-hf train-expert \\\n      --expert-id abc123 \\\n      --dataset data.txt \\\n      --seal-generator path/to/optimized_generator.pt\n  ```\n\n- [ ] **Add monitoring commands**\n  ```bash\n  # View ReSTEM progress\n  aios seal restem status\n  \n  # View reward statistics\n  aios seal restem rewards --run-id xyz\n  \n  # Compare generators\n  aios seal restem compare --baseline gen1.pt --optimized gen2.pt\n  ```\n\n#### 11.2 GUI Integration (Optional)\n**Duration**: 6-8 hours\n\n- [ ] **Add ReSTEM panel to Subbrains Manager**\n  - Show optimization progress\n  - Display reward statistics\n  - Show best self-edits\n  - Allow starting/stopping optimization\n\n- [ ] **Visualization**\n  - Reward distribution over iterations\n  - Generator improvement metrics\n  - Training progress for TTT\n  - Resource usage graphs\n\n#### 11.3 Documentation\n**Duration**: 6-8 hours\n\n- [ ] **Create ReSTEM guide**\n  - File: `docs/user_guide/SEAL_RESTEM.md` (placeholder; to be created)\n  - [ ] What is ReSTEM?\n  - [ ] When to use it?\n  - [ ] How to configure?\n  - [ ] Interpreting results\n  - [ ] Troubleshooting\n\n- [ ] **Update existing docs**\n  - [ ] Update SEAL overview\n  - [ ] Update training guide\n  - [ ] Add cost estimates\n  - [ ] Add best practices\n\n- [ ] **Create tutorial**\n  - File: `docs/tutorials/SEAL_RESTEM_TUTORIAL.md` (placeholder; to be created)\n  - [ ] Step-by-step ReSTEM optimization\n  - [ ] Expected results\n  - [ ] Analysis and interpretation\n\n---\n\n### Week 13-14: Production Readiness\n\n#### 13.1 Performance Optimization\n**Duration**: 8-10 hours\n\n- [ ] **Profile ReSTEM pipeline**\n  - [ ] Identify bottlenecks\n  - [ ] Optimize TTT batching\n  - [ ] Optimize reward computation\n  - [ ] Reduce checkpoint I/O\n\n- [ ] **Memory optimization**\n  - [ ] Reduce peak GPU memory\n  - [ ] Implement gradient accumulation\n  - [ ] Add CPU offloading\n  - [ ] Test with limited resources\n\n- [ ] **Parallelization**\n  - [ ] Multi-GPU TTT\n  - [ ] Parallel candidate evaluation\n  - [ ] Async reward computation\n  - [ ] Test scaling behavior\n\n#### 13.2 Cost Analysis\n**Duration**: 3-4 hours\n\n- [ ] **Compute cost estimation**\n  - [ ] Calculate GPU hours for typical run\n  - [ ] Estimate cloud costs\n  - [ ] Compare to Phase 1 baseline\n  - [ ] Document cost-benefit tradeoff\n\n- [ ] **Create cost calculator**\n  - File: `scripts/estimate_restem_cost.py`\n  - [ ] Input: dataset size, iterations, GPU type\n  - [ ] Output: time and cost estimates\n  - [ ] Recommendations for budget\n\n#### 13.3 Final Validation\n**Duration**: 4-5 hours\n\n- [ ] **Full test suite**\n  - [ ] All unit tests pass\n  - [ ] All integration tests pass\n  - [ ] Performance tests pass\n  - [ ] Code coverage >85%\n\n- [ ] **End-to-end validation**\n  - [ ] Fresh environment install\n  - [ ] Run ReSTEM optimization\n  - [ ] Train expert with optimized generator\n  - [ ] Verify improvement vs Phase 1\n  - [ ] Document results\n\n- [ ] **Production readiness checklist**\n  - [ ] Error handling complete\n  - [ ] Logging comprehensive\n  - [ ] Monitoring in place\n  - [ ] Documentation complete\n  - [ ] Performance acceptable\n  - [ ] Costs documented\n\n---\n\n### Phase 2 Checklist Summary\n\n**Before Starting**:\n- [ ] Phase 1 complete and validated\n- [ ] Results show 10-15% improvement\n- [ ] Compute budget approved (2xH100 for 20-40 hours)\n- [ ] Create feature branch: `feature/seal-phase2`\n\n**Week 5-6 Complete**:\n- [ ] ReSTEM research and design complete\n- [ ] Reward functions implemented and tested\n- [ ] TTT module implemented and tested\n\n**Week 7-8 Complete**:\n- [ ] ReSTEM optimizer core implemented\n- [ ] Integration with training pipeline complete\n- [ ] Basic tests passing\n\n**Week 9-10 Complete**:\n- [ ] Comprehensive testing complete\n- [ ] Hyperparameters tuned\n- [ ] Validation experiment complete\n\n**Week 11-12 Complete**:\n- [ ] CLI commands added\n- [ ] GUI integration complete (optional)\n- [ ] Documentation written\n\n**Week 13-14 Complete**:\n- [ ] Performance optimized\n- [ ] Cost analysis complete\n- [ ] Final validation passed\n- [ ] Ready for merge\n\n**Phase 2 Acceptance Criteria**:\n- [ ] ReSTEM optimization completes successfully\n- [ ] 15-20% improvement over Phase 1 (total 25-35% over baseline)\n- [ ] Cost is reasonable (<$100 per expert on cloud)\n- [ ] No critical bugs or crashes\n- [ ] Documentation is complete\n- [ ] Code coverage >85%\n- [ ] Performance within acceptable limits\n\n---\n\n## Phase 3: Advanced Meta-Learning (Weeks 15+)\n\n**Objective**: Implement advanced SEAL features for full meta-learning capabilities\n\n**Effort**: 100-150 hours  \n**Risk Level**: HIGH  \n**Priority**: OPTIONAL (Research Phase)  \n**Dependencies**: Phase 2 complete and providing significant value\n\n**Note**: This phase is HIGHLY OPTIONAL and research-oriented. Only proceed if you have specific use cases and research goals.\n\n---\n\n### Week 15-16: Test-Time Adaptation\n\n#### 15.1 Context-Aware Fine-Tuning\n**Duration**: 10-12 hours  \n**File**: `src/aios/core/seal/test_time_adaptation.py`\n\n- [ ] **Implement TTAModule**\n  ```python\n  class TestTimeAdaptation:\n      def adapt_to_context(\n          self,\n          expert: nn.Module,\n          conversation_history: List[str],\n          adaptation_steps: int = 5,\n      ) -> nn.Module:\n          \"\"\"\n          Fine-tune expert on recent conversation for better context awareness.\n          \"\"\"\n  ```\n\n- [ ] **Context extraction**\n  - [ ] Extract relevant passages from conversation\n  - [ ] Generate self-edits from context\n  - [ ] Create mini training set\n\n- [ ] **Rapid adaptation**\n  - [ ] Use small LoRA (rank=4)\n  - [ ] Very low learning rate (1e-5)\n  - [ ] Few gradient steps (5-10)\n  - [ ] Minimal memory overhead\n\n- [ ] **Integration with chat**\n  - [ ] Adapt expert after every N messages\n  - [ ] Track adaptation history\n  - [ ] Option to disable TTA\n  - [ ] Performance monitoring\n\n- [ ] **Write tests**\n  - [ ] Test adaptation logic\n  - [ ] Test memory management\n  - [ ] Test performance impact\n\n#### 15.2 Few-Shot Learning (ARC-Style)\n**Duration**: 10-12 hours  \n**File**: `src/aios/core/seal/few_shot_learning.py`\n\n- [ ] **Implement FewShotLearner**\n  - [ ] Based on SEAL's ARC experiments\n  - [ ] Support data augmentation tools\n  - [ ] Support hyperparameter selection\n  - [ ] Generate self-edits with tool invocations\n\n- [ ] **Tool framework**\n  - [ ] Define tool interface\n  - [ ] Implement rotation/flip augmentations\n  - [ ] Implement learning rate selection\n  - [ ] Implement epoch selection\n\n- [ ] **Integration**\n  - [ ] Add to expert training pipeline\n  - [ ] Enable for specific tasks\n  - [ ] Track success rates\n\n- [ ] **Write tests**\n  - [ ] Test tool invocation\n  - [ ] Test augmentation strategies\n  - [ ] Test few-shot scenarios\n\n---\n\n### Week 17-18: Cross-Model Transfer\n\n#### 17.1 Expert Portability\n**Duration**: 8-10 hours\n\n- [ ] **Export/import experts**\n  - [ ] Export expert to HuggingFace format\n  - [ ] Include metadata and training history\n  - [ ] Include self-edit generator configuration\n  - [ ] Package as distributable artifact\n\n- [ ] **Cross-model adaptation**\n  - [ ] Load expert trained on Model A\n  - [ ] Adapt to Model B architecture\n  - [ ] Fine-tune adapter layer\n  - [ ] Validate performance\n\n- [ ] **Expert marketplace (concept)**\n  - [ ] Design sharing protocol\n  - [ ] Define quality standards\n  - [ ] Create browsing interface\n  - [ ] Implement download/install\n\n#### 17.2 Generator Transfer\n**Duration**: 6-8 hours\n\n- [ ] **Transfer learned strategies**\n  - [ ] Export optimized generator\n  - [ ] Import to new domain\n  - [ ] Fine-tune for domain\n  - [ ] Compare to from-scratch training\n\n- [ ] **Multi-domain optimization**\n  - [ ] Train generator on multiple domains\n  - [ ] Test generalization\n  - [ ] Measure transfer learning benefit\n\n---\n\n### Week 19-20: Research Features\n\n#### 19.1 Advanced Routing\n**Duration**: 8-10 hours\n\n- [ ] **SEAL-aware routing**\n  - [ ] Route based on self-edit quality\n  - [ ] Track which self-edits led to activations\n  - [ ] Use as routing signal\n\n- [ ] **Meta-routing**\n  - [ ] Learn routing from SEAL training\n  - [ ] Optimize routing for self-improvement\n  - [ ] Experiment with routing strategies\n\n#### 19.2 Continual ReSTEM\n**Duration**: 8-10 hours\n\n- [ ] **Background optimization**\n  - [ ] Run ReSTEM during idle time\n  - [ ] Incrementally improve generators\n  - [ ] Track long-term improvements\n\n- [ ] **Online learning**\n  - [ ] Update generators from user feedback\n  - [ ] Incorporate conversation quality signals\n  - [ ] Avoid catastrophic forgetting\n\n#### 19.3 Documentation & Publication\n**Duration**: 8-10 hours\n\n- [ ] **Research documentation**\n  - File: `docs/research/SEAL_METALEARNING.md`\n  - [ ] Document novel contributions\n  - [ ] Document experimental results\n  - [ ] Comparison to SEAL paper\n\n- [ ] **Potential paper/blog post**\n  - [ ] \"SEAL Integration in HRM Architecture\"\n  - [ ] Document challenges and solutions\n  - [ ] Share lessons learned\n  - [ ] Release findings to community\n\n---\n\n### Phase 3 Checklist Summary\n\n**Before Starting**:\n- [ ] Phase 2 showing strong results (20-30% improvement)\n- [ ] Research goals clearly defined\n- [ ] Team has bandwidth for experiments\n- [ ] Create feature branch: `feature/seal-phase3`\n\n**Week 15-16 Complete**:\n- [ ] Test-time adaptation implemented\n- [ ] Few-shot learning implemented\n- [ ] Initial experiments complete\n\n**Week 17-18 Complete**:\n- [ ] Expert portability working\n- [ ] Cross-model transfer validated\n- [ ] Marketplace concept designed\n\n**Week 19-20 Complete**:\n- [ ] Advanced routing experimented\n- [ ] Continual learning explored\n- [ ] Research documentation written\n\n**Phase 3 Acceptance Criteria**:\n- [ ] Novel features implemented and tested\n- [ ] Experimental results documented\n- [ ] Research contributions identified\n- [ ] Community-shareable findings\n- [ ] Decision made on production features\n\n---\n\n## Dependencies & Prerequisites\n\n### System Requirements\n\n**Phase 1**:\n- [ ] Python 3.10+\n- [ ] PyTorch 2.0+\n- [ ] Transformers 4.30+\n- [ ] 1x GPU with 16GB+ VRAM (RTX 4090, A100, etc.)\n- [ ] 32GB+ RAM\n- [ ] 50GB+ disk space\n\n**Phase 2** (additional):\n- [ ] 2x GPU with 24GB+ VRAM (A100, H100)\n- [ ] 64GB+ RAM\n- [ ] 200GB+ disk space\n- [ ] High-speed GPU interconnect (NVLink preferred)\n\n**Phase 3** (additional):\n- [ ] Research compute budget\n- [ ] Multi-GPU cluster (optional)\n- [ ] Extended storage (1TB+)\n\n### Software Dependencies\n\n**Phase 1**:\n```toml\n[project.dependencies]\n# Existing\ntorch = \">=2.0.0\"\ntransformers = \">=4.30.0\"\n# New for SEAL\nsentence-transformers = \">=2.2.0\"\nrouge-score = \">=0.1.2\"\n```\n\n**Phase 2** (additional):\n```toml\npeft = \">=0.5.0\"  # LoRA\naccelerate = \">=0.21.0\"  # Multi-GPU\nwandb = \">=0.15.0\"  # Optional: experiment tracking\n```\n\n**Phase 3** (additional):\n```toml\ndatasets = \">=2.14.0\"  # HuggingFace datasets\nhuggingface-hub = \">=0.16.0\"  # Model/expert sharing\n```\n\n### Knowledge Prerequisites\n\n**Phase 1**:\n- [ ] Understanding of AI-OS codebase\n- [ ] Familiarity with PyTorch and Transformers\n- [ ] Experience with text generation\n- [ ] Understanding of prompt engineering\n\n**Phase 2**:\n- [ ] Understanding of reinforcement learning basics\n- [ ] Familiarity with policy gradient methods\n- [ ] Experience with multi-GPU training\n- [ ] Understanding of LoRA and PEFT\n\n**Phase 3**:\n- [ ] Advanced RL knowledge\n- [ ] Meta-learning concepts\n- [ ] Transfer learning experience\n- [ ] Research methodology\n\n---\n\n## Success Metrics\n\n### Phase 1 Metrics\n\n**Primary Metrics**:\n- [ ] **Expert Validation Loss**: 10-15% improvement over baseline\n- [ ] **Expert Perplexity**: Corresponding improvement\n- [ ] **Training Time**: <2x overhead for augmentation\n- [ ] **Generation Time**: <30s per passage per edit\n\n**Secondary Metrics**:\n- [ ] **Cache Hit Rate**: >50% on repeated datasets\n- [ ] **Quality Filter Rate**: <20% filtered outputs\n- [ ] **User Satisfaction**: Qualitative assessment\n- [ ] **Code Coverage**: >85%\n\n**Success Criteria**:\n- [ ] At least 10% improvement in expert quality\n- [ ] No regressions in existing features\n- [ ] Generation time acceptable (<30s/edit)\n- [ ] System stable and reliable\n\n### Phase 2 Metrics\n\n**Primary Metrics**:\n- [ ] **Expert Validation Loss**: 15-25% improvement over Phase 1\n- [ ] **Cumulative Improvement**: 25-35% over original baseline\n- [ ] **ReSTEM Time**: <40 GPU-hours per expert\n- [ ] **Reward Convergence**: Improvement within 2 iterations\n\n**Secondary Metrics**:\n- [ ] **Generator Quality**: Surpass GPT-4.1 synthetic data\n- [ ] **Cost per Expert**: <$100 on cloud\n- [ ] **Iteration Efficiency**: >20% rewards=1 after iteration 1\n- [ ] **Generalization**: Strategy transfers to new domains\n\n**Success Criteria**:\n- [ ] At least 15% improvement over Phase 1\n- [ ] Compute cost is reasonable\n- [ ] Generator quality measurably improves\n- [ ] Strategy generalizes across domains\n\n### Phase 3 Metrics\n\n**Research Metrics**:\n- [ ] **Novel Contributions**: At least 2 novel techniques\n- [ ] **Experimental Validation**: Positive results in 3+ experiments\n- [ ] **Transfer Learning**: >10% benefit from cross-domain transfer\n- [ ] **Adaptation Speed**: <5 steps for context adaptation\n\n**Success Criteria**:\n- [ ] At least one feature ready for production\n- [ ] Research findings documented and shareable\n- [ ] Clear roadmap for future work\n- [ ] Community interest and feedback\n\n---\n\n## Risk Management\n\n### Phase 1 Risks\n\n| Risk | Probability | Impact | Mitigation |\n|------|------------|--------|------------|\n| **Self-edits are low quality** | Medium | High | Use SEAL's proven prompts, add quality filtering, A/B test |\n| **Performance overhead too high** | Low | Medium | Optimize batching, use caching, make optional |\n| **Integration breaks existing features** | Low | High | Comprehensive testing, feature flags, gradual rollout |\n| **Limited improvement (<10%)** | Medium | Medium | Try multiple strategies, tune hyperparameters, document honestly |\n\n### Phase 2 Risks\n\n| Risk | Probability | Impact | Mitigation |\n|------|------------|--------|------------|\n| **RL doesn't converge** | Medium | High | Use SEAL hyperparameters, extensive tuning, fallback to Phase 1 |\n| **Compute cost too high** | Medium | Medium | Estimate upfront, optimize efficiency, set budget limits |\n| **Memory issues** | Medium | High | Test on smaller scales, implement gradient accumulation, CPU offload |\n| **No improvement over Phase 1** | Low | Medium | Validate Phase 1 first, careful experiment design |\n\n### Phase 3 Risks\n\n| Risk | Probability | Impact | Mitigation |\n|------|------------|--------|------------|\n| **Features too experimental** | High | Low | Clear research goals, document failures, learn from experiments |\n| **No clear production value** | Medium | Medium | Focus on specific use cases, validate with users |\n| **Resource drain** | Medium | Medium | Set time/budget limits, periodic review |\n\n---\n\n## Testing Strategy\n\n### Unit Testing\n**Target Coverage**: 85-90%\n\n**Phase 1**:\n- [ ] Test each prompt strategy\n- [ ] Test self-edit parsing\n- [ ] Test quality filtering\n- [ ] Test caching logic\n- [ ] Test error handling\n- [ ] Mock model generation\n\n**Phase 2**:\n- [ ] Test reward computation\n- [ ] Test TTT training\n- [ ] Test ReSTEM E-step\n- [ ] Test ReSTEM M-step\n- [ ] Test checkpoint management\n\n**Phase 3**:\n- [ ] Test adaptation logic\n- [ ] Test transfer learning\n- [ ] Test advanced features\n\n### Integration Testing\n\n**Phase 1**:\n- [ ] End-to-end generation pipeline\n- [ ] Training with augmented data\n- [ ] Cache persistence\n- [ ] CLI commands\n\n**Phase 2**:\n- [ ] Full ReSTEM optimization\n- [ ] Multi-GPU coordination\n- [ ] Checkpoint save/load\n- [ ] Resource cleanup\n\n**Phase 3**:\n- [ ] Cross-model transfer\n- [ ] Online adaptation\n- [ ] Complex workflows\n\n### Performance Testing\n\n**Benchmarks**:\n- [ ] Generation time per passage\n- [ ] Memory usage during generation\n- [ ] TTT time per candidate (Phase 2)\n- [ ] End-to-end training time\n- [ ] GPU utilization\n\n**Targets**:\n- [ ] Phase 1: <30s per edit\n- [ ] Phase 2: <40 GPU-hours per expert\n- [ ] Memory: <90% GPU utilization\n- [ ] No memory leaks over long runs\n\n### Manual Testing\n\n**Phase 1**:\n- [ ] Generate self-edits for sample data\n- [ ] Inspect output quality\n- [ ] Train sample expert\n- [ ] Verify improvement\n- [ ] Test all CLI commands\n\n**Phase 2**:\n- [ ] Run ReSTEM on small dataset\n- [ ] Monitor GPU usage\n- [ ] Verify improvement over Phase 1\n- [ ] Test checkpoint recovery\n\n**Phase 3**:\n- [ ] Test experimental features\n- [ ] Validate research hypotheses\n- [ ] User testing (if applicable)\n\n---\n\n## Documentation Requirements\n\n### User Documentation\n\n**Phase 1**:\n - [ ] `docs/user_guide/SEAL_OVERVIEW.md` - What is SEAL? (placeholder)\n - [ ] `docs/user_guide/SEAL_SELF_EDITS.md` - How to use self-edits (placeholder)\n - [ ] `docs/tutorials/SEAL_FIRST_EXPERT.md` - Tutorial (placeholder)\n- [ ] `config/default.yaml` - Inline comments for SEAL config\n- [ ] `README.md` - Feature highlight\n\n**Phase 2**:\n - [ ] `docs/user_guide/SEAL_RESTEM.md` - ReSTEM guide (placeholder)\n - [ ] `docs/tutorials/SEAL_RESTEM_TUTORIAL.md` - Step-by-step (placeholder)\n - [ ] `docs/user_guide/SEAL_COST_ANALYSIS.md` - Cost estimation (placeholder)\n- [ ] Update existing docs with Phase 2 info\n\n**Phase 3**:\n - [ ] `docs/research/SEAL_METALEARNING.md` - Research documentation (placeholder)\n - [ ] `docs/advanced/SEAL_ADVANCED_FEATURES.md` - Advanced usage (placeholder)\n- [ ] Case studies and examples\n\n### API Documentation\n\n**All Phases**:\n- [ ] Comprehensive docstrings (Google format)\n- [ ] Type annotations\n- [ ] Usage examples in docstrings\n- [ ] API reference (Sphinx/MkDocs)\n- [ ] Code examples in `docs/api/`\n\n### Developer Documentation\n\n**Phase 1**:\n - [ ] `docs/development/SEAL_ARCHITECTURE.md` - Architecture overview (placeholder)\n - [ ] `docs/development/SEAL_CONTRIBUTING.md` - How to contribute (placeholder)\n- [ ] Inline code comments for complex logic\n\n**Phase 2**:\n - [ ] `docs/development/SEAL_RESTEM_DESIGN.md` - ReSTEM design doc (placeholder)\n - [ ] `docs/development/SEAL_TESTING.md` - Testing guide (placeholder)\n- [ ] Performance optimization notes\n\n**Phase 3**:\n- [ ] Research methodology\n- [ ] Experiment protocols\n- [ ] Lessons learned\n\n---\n\n## Rollout Plan\n\n### Phase 1 Rollout\n\n**Stage 1: Internal Testing (Week 4)**\n- [ ] Merge to `develop` branch\n- [ ] Deploy to staging environment\n- [ ] Internal team testing\n- [ ] Bug fixes and refinements\n\n**Stage 2: Beta Release (Week 5)**\n- [ ] Release as beta feature\n- [ ] Feature flag: `seal.enabled = false` (opt-in)\n- [ ] Gather user feedback\n- [ ] Monitor performance and errors\n\n**Stage 3: General Availability (Week 6)**\n- [ ] Enable by default: `seal.enabled = true`\n- [ ] Announce in release notes\n- [ ] Promote in documentation\n- [ ] Monitor adoption and satisfaction\n\n### Phase 2 Rollout\n\n**Stage 1: Internal Validation (Week 13-14)**\n- [ ] Validate on internal datasets\n- [ ] Compare costs and benefits\n- [ ] Document best practices\n- [ ] Create cost calculator\n\n**Stage 2: Opt-In Release (Week 15)**\n- [ ] Release as advanced feature\n- [ ] Require explicit opt-in\n- [ ] Provide cost estimates\n- [ ] Gather feedback from power users\n\n**Stage 3: Production (Week 16+)**\n- [ ] Enable for users who need it\n- [ ] Provide clear documentation\n- [ ] Ongoing monitoring and optimization\n\n### Phase 3 Rollout\n\n**Research Preview**:\n- [ ] Release as experimental features\n- [ ] Clear \"research preview\" labeling\n- [ ] Gather feedback and data\n- [ ] Decide which features to productionize\n\n---\n\n## Ongoing Maintenance\n\n### Post-Release Tasks\n\n**Phase 1**:\n- [ ] Monitor cache usage and performance\n- [ ] Collect user feedback\n- [ ] Track success metrics\n- [ ] Fix bugs and issues\n- [ ] Add new prompt strategies based on feedback\n\n**Phase 2**:\n- [ ] Monitor ReSTEM usage and costs\n- [ ] Optimize performance based on usage patterns\n- [ ] Tune hyperparameters for common domains\n- [ ] Update documentation with best practices\n\n**Phase 3**:\n- [ ] Evaluate research features\n- [ ] Productionize successful experiments\n- [ ] Deprecate failed experiments\n- [ ] Plan future research directions\n\n### Continuous Improvement\n\n- [ ] Regular performance profiling\n- [ ] Dependency updates\n- [ ] Security patches\n- [ ] Documentation updates\n- [ ] Community engagement\n\n---\n\n## Appendices\n\n### A. Useful Commands\n\n```bash\n# Phase 1: Generate self-edits\naios seal generate --input data.txt --output augmented.txt --strategy implications\n\n# Phase 1: Train expert with SEAL\naios hrm-hf train-expert --expert-id abc123 --dataset data.txt\n\n# Phase 1: A/B test\npython scripts/seal_ab_test.py --dataset data.txt --output results.json\n\n# Phase 2: Run ReSTEM\naios seal restem optimize --dataset data.txt --iterations 2\n\n# Phase 2: Monitor progress\naios seal restem status\n\n# Cache management\naios seal cache stats\naios seal cache clear\naios seal cache prune --max-size 5GB\n```\n\n### B. Key Files\n\n**Phase 1**:\n- `src/aios/core/seal/self_edit_generator.py`\n- `src/aios/core/seal/prompts.py`\n- `src/aios/core/seal/strategies.py`\n- `src/aios/core/training/auto_training_orchestrator.py`\n- `config/default.yaml` (SEAL section)\n\n**Phase 2**:\n- `src/aios/core/seal/restem_optimizer.py`\n- `src/aios/core/seal/reward.py`\n- `src/aios/core/seal/test_time_training.py`\n\n**Phase 3**:\n- `src/aios/core/seal/test_time_adaptation.py`\n- `src/aios/core/seal/few_shot_learning.py`\n- `src/aios/core/seal/transfer_learning.py`\n\n### C. References\n\n- **SEAL Paper**: https://arxiv.org/html/2506.10943v2\n- **SEAL GitHub**: https://github.com/Continual-Intelligence/SEAL\n- **AI-OS Docs**: `docs/INDEX.md`\n- **HRM Paper**: https://arxiv.org/html/2506.21734v3\n- **Dynamic Subbrains**: `docs/DYNAMIC_SUBBRAINS_ARCHITECTURE.md` (placeholder; doc not yet created)\n\n---\n\n## Revision History\n\n| Version | Date | Author | Changes |\n|---------|------|--------|---------|\n| 1.0 | January 15, 2025 | AI-OS Team | Initial detailed implementation plan |\n\n---\n\n## Sign-Off\n\n**Phase 1 Ready to Start**: \u2610 Yes \u2610 No  \n**Phase 2 Approved**: \u2610 Yes \u2610 No \u2610 Pending Phase 1 Results  \n**Phase 3 Approved**: \u2610 Yes \u2610 No \u2610 Research Only  \n\n**Project Lead Signature**: _______________  \n**Date**: _______________\n\n---\n\n**END OF DOCUMENT**\n\nThis plan is a living document. Update as implementation progresses and new insights are gained.\n", "tags": ["datasets", "experts", "gui", "training"], "headings": [{"line": 0, "text": "SEAL Integration Implementation Plan"}, {"line": 14, "text": "\ud83d\udccb Table of Contents"}, {"line": 29, "text": "Executive Summary"}, {"line": 49, "text": "Phase 1: Self-Edit Data Generation (Weeks 1-4)"}, {"line": 60, "text": "Week 1: Foundation & Core Classes"}, {"line": 62, "text": "1.1 Project Setup"}, {"line": 86, "text": "SEAL Integration"}, {"line": 99, "text": "1.2 Prompts Module"}, {"line": 105, "text": "Implement these prompt strategies from SEAL paper:"}, {"line": 130, "text": "1.3 Strategies Module"}, {"line": 173, "text": "1.4 Core Generator Class"}, {"line": 190, "text": "Implementation"}, {"line": 227, "text": "1.5 Configuration Integration"}, {"line": 234, "text": "Self-Edit Generation"}, {"line": 237, "text": "Strategy Selection"}, {"line": 243, "text": "Quality Filtering"}, {"line": 250, "text": "Caching"}, {"line": 255, "text": "Performance"}, {"line": 259, "text": "Advanced (Phase 2)"}, {"line": 274, "text": "Week 2: Integration with AutoTrainingOrchestrator"}, {"line": 276, "text": "2.1 Refactor AutoTrainingOrchestrator"}, {"line": 284, "text": "... existing code ..."}, {"line": 324, "text": "2.2 Update Expert Training Pipeline"}, {"line": 337, "text": "Original dataset size"}, {"line": 338, "text": "Augmented dataset size"}, {"line": 339, "text": "Augmentation ratio"}, {"line": 340, "text": "Average self-edits per passage"}, {"line": 341, "text": "Total tokens (before/after)"}, {"line": 356, "text": "2.3 CLI Integration"}, {"line": 362, "text": "Generate self-edits for a dataset (standalone)"}, {"line": 365, "text": "Preview self-edits (no saving)"}, {"line": 368, "text": "Cache management"}, {"line": 376, "text": "Train expert with self-edits (default)"}, {"line": 379, "text": "Train expert without self-edits"}, {"line": 382, "text": "Train with specific strategy"}, {"line": 399, "text": "Week 3: Testing & Validation"}, {"line": 401, "text": "3.1 Unit Testing"}, {"line": 433, "text": "3.2 Integration Testing"}, {"line": 455, "text": "3.3 A/B Experiment Setup"}, {"line": 466, "text": "Train two experts:"}, {"line": 467, "text": "Control: Standard training"}, {"line": 468, "text": "Treatment: Training with SEAL self-edits"}, {"line": 470, "text": "Compare:"}, {"line": 471, "text": "- Validation loss"}, {"line": 472, "text": "- Perplexity"}, {"line": 473, "text": "- Training time"}, {"line": 474, "text": "- Memory usage"}, {"line": 491, "text": "Week 4: Documentation & Refinement"}, {"line": 493, "text": "4.1 User Documentation"}, {"line": 517, "text": "4.2 API Documentation"}, {"line": 529, "text": "Example 1: Basic usage"}, {"line": 530, "text": "Example 2: Custom strategy"}, {"line": 531, "text": "Example 3: Advanced configuration"}, {"line": 532, "text": "Example 4: Programmatic access"}, {"line": 535, "text": "4.3 Performance Optimization"}, {"line": 556, "text": "4.4 Final Validation"}, {"line": 587, "text": "Phase 1 Checklist Summary"}, {"line": 634, "text": "Phase 2: RL Optimization (Weeks 5-14)"}, {"line": 647, "text": "Week 5-6: ReSTEM Foundation"}, {"line": 649, "text": "5.1 Research & Planning"}, {"line": 673, "text": "5.2 Reward Function Implementation"}, {"line": 721, "text": "5.3 Test-Time Training (TTT) Module"}, {"line": 767, "text": "Week 7-8: ReSTEM Optimizer Core"}, {"line": 769, "text": "7.1 ReSTEM Optimizer Implementation"}, {"line": 785, "text": "Implementation"}, {"line": 823, "text": "7.2 Integration with Training Pipeline"}, {"line": 835, "text": "Split dataset"}, {"line": 836, "text": "Initialize ReSTEM optimizer"}, {"line": 837, "text": "Run optimization loop"}, {"line": 838, "text": "Train final expert with optimized generator"}, {"line": 839, "text": "Return metrics"}, {"line": 862, "text": "Week 9-10: Testing & Optimization"}, {"line": 864, "text": "9.1 Comprehensive Testing"}, {"line": 886, "text": "9.2 Hyperparameter Tuning"}, {"line": 903, "text": "9.3 Validation Experiment"}, {"line": 929, "text": "Week 11-12: CLI, GUI, and Usability"}, {"line": 931, "text": "11.1 CLI Commands"}, {"line": 936, "text": "Run ReSTEM optimization"}, {"line": 943, "text": "Use optimized generator for training"}, {"line": 952, "text": "View ReSTEM progress"}, {"line": 955, "text": "View reward statistics"}, {"line": 958, "text": "Compare generators"}, {"line": 962, "text": "11.2 GUI Integration (Optional)"}, {"line": 977, "text": "11.3 Documentation"}, {"line": 1002, "text": "Week 13-14: Production Readiness"}, {"line": 1004, "text": "13.1 Performance Optimization"}, {"line": 1025, "text": "13.2 Cost Analysis"}, {"line": 1040, "text": "13.3 Final Validation"}, {"line": 1066, "text": "Phase 2 Checklist Summary"}, {"line": 1111, "text": "Phase 3: Advanced Meta-Learning (Weeks 15+)"}, {"line": 1124, "text": "Week 15-16: Test-Time Adaptation"}, {"line": 1126, "text": "15.1 Context-Aware Fine-Tuning"}, {"line": 1166, "text": "15.2 Few-Shot Learning (ARC-Style)"}, {"line": 1194, "text": "Week 17-18: Cross-Model Transfer"}, {"line": 1196, "text": "17.1 Expert Portability"}, {"line": 1217, "text": "17.2 Generator Transfer"}, {"line": 1233, "text": "Week 19-20: Research Features"}, {"line": 1235, "text": "19.1 Advanced Routing"}, {"line": 1248, "text": "19.2 Continual ReSTEM"}, {"line": 1261, "text": "19.3 Documentation & Publication"}, {"line": 1278, "text": "Phase 3 Checklist Summary"}, {"line": 1310, "text": "Dependencies & Prerequisites"}, {"line": 1312, "text": "System Requirements"}, {"line": 1333, "text": "Software Dependencies"}, {"line": 1338, "text": "Existing"}, {"line": 1341, "text": "New for SEAL"}, {"line": 1359, "text": "Knowledge Prerequisites"}, {"line": 1381, "text": "Success Metrics"}, {"line": 1383, "text": "Phase 1 Metrics"}, {"line": 1403, "text": "Phase 2 Metrics"}, {"line": 1423, "text": "Phase 3 Metrics"}, {"line": 1439, "text": "Risk Management"}, {"line": 1441, "text": "Phase 1 Risks"}, {"line": 1450, "text": "Phase 2 Risks"}, {"line": 1459, "text": "Phase 3 Risks"}, {"line": 1469, "text": "Testing Strategy"}, {"line": 1471, "text": "Unit Testing"}, {"line": 1494, "text": "Integration Testing"}, {"line": 1513, "text": "Performance Testing"}, {"line": 1528, "text": "Manual Testing"}, {"line": 1550, "text": "Documentation Requirements"}, {"line": 1552, "text": "User Documentation"}, {"line": 1572, "text": "API Documentation"}, {"line": 1581, "text": "Developer Documentation"}, {"line": 1600, "text": "Rollout Plan"}, {"line": 1602, "text": "Phase 1 Rollout"}, {"line": 1622, "text": "Phase 2 Rollout"}, {"line": 1641, "text": "Phase 3 Rollout"}, {"line": 1651, "text": "Ongoing Maintenance"}, {"line": 1653, "text": "Post-Release Tasks"}, {"line": 1674, "text": "Continuous Improvement"}, {"line": 1684, "text": "Appendices"}, {"line": 1686, "text": "A. Useful Commands"}, {"line": 1689, "text": "Phase 1: Generate self-edits"}, {"line": 1692, "text": "Phase 1: Train expert with SEAL"}, {"line": 1695, "text": "Phase 1: A/B test"}, {"line": 1698, "text": "Phase 2: Run ReSTEM"}, {"line": 1701, "text": "Phase 2: Monitor progress"}, {"line": 1704, "text": "Cache management"}, {"line": 1710, "text": "B. Key Files"}, {"line": 1729, "text": "C. References"}, {"line": 1739, "text": "Revision History"}, {"line": 1747, "text": "Sign-Off"}]}, {"path": "planned_features/TOON_INTEGRATION.md", "content": "## PF-007: TOON Format Integration for Token-Efficient LLM Data Exchange\n\n### Summary\n\nIntegrate Token-Oriented Object Notation (TOON) as an optional serialization format for AI-OS to reduce token consumption when passing structured data to LLMs. TOON achieves 30-60% token savings over JSON for uniform tabular data while maintaining human readability and LLM-friendliness. This integration will support encoding/decoding in training pipelines, evaluation outputs, and LLM prompts.\n\n### Why this matters\n\n- **Token cost reduction**: Training and inference with large context windows can save 30-60% tokens on structured data, reducing API costs and improving throughput.\n- **LLM-friendly structure**: Explicit length markers `[N]` and field headers `{field1,field2}` help LLMs validate and generate structured output more reliably.\n- **Flexible serialization**: Drop-in replacement for JSON in data paths where uniform arrays dominate (logs, metrics, evaluation results, datasets).\n- **Python ecosystem alignment**: Multiple Python implementations available; official `toon_format` package in active development.\n\n---\n\n## What ships in PF-007\n\n- **Core utility module**: `src/aios/formats/toon_codec.py` (encoder/decoder with fallback to JSON)\n- **CLI integration**: \n  - `aios toon encode` - Convert JSON to TOON\n  - `aios toon decode` - Convert TOON to JSON\n  - Flags for existing commands: `--output-format toon|json` for metrics/evaluation outputs\n- **Training/Eval hooks**: Optional TOON encoding for JSONL metrics and evaluation results\n- **Config support**: YAML settings for TOON preferences (delimiter, indent, length markers)\n- **Documentation**: Examples, benchmarks, and best practices for when to use TOON vs JSON\n\n---\n\n## Architecture overview\n\n### Data paths affected\n\n1. **Metrics logging** (`artifacts/brains/actv1/metrics.jsonl`)\n   - Optional TOON encoding for training metrics with `--metrics-format toon`\n   - Particularly beneficial for batch eval results with uniform structure\n\n2. **Evaluation outputs** (`artifacts/evaluation/`)\n   - Encode evaluation results in TOON format for token-efficient LLM analysis\n   - Support both formats side-by-side for compatibility\n\n3. **Dataset exports** (`training_data/`)\n   - Convert curated datasets to TOON format for reduced storage and faster LLM ingestion\n   - Command: `aios datasets export --format toon`\n\n4. **LLM prompt payloads** (inline usage)\n   - Utility functions to encode context data in TOON before inserting into prompts\n   - Automatic format detection when decoding LLM outputs\n\n### Core components\n\n- **ToonCodec** (utility class): Wraps TOON encoder/decoder with graceful fallback\n  - `encode(data: Any, *, delimiter: str = ',', indent: int = 2, length_marker: bool = False) -> str`\n  - `decode(toon_str: str, *, strict: bool = True) -> Any`\n  - `is_toon_available() -> bool` (checks if TOON library installed)\n  \n- **Format negotiation**: Auto-detect format based on content or extension\n  - `.toon` extension for TOON files\n  - `.json` or `.jsonl` for JSON files\n  - Content sniffing: look for TOON patterns like `[N]{fields}:` headers\n\n- **Config schema** (`config/default.yaml`):\n  ```yaml\n  toon:\n    enabled: false  # Master switch\n    default_format: json  # json | toon\n    delimiter: ','  # ',' | '\\t' | '|'\n    indent: 2\n    length_marker: false  # Add # prefix to lengths\n    metrics: false  # Use TOON for metrics logging\n    evaluation: false  # Use TOON for evaluation outputs\n  ```\n\n---\n\n## Dependencies and setup\n\n### Python implementation options\n\n**Option 1: Official implementation (recommended)**\n```powershell\npip install toon-format  # When officially released\n```\n\n**Option 2: Community implementation (current)**\n```powershell\npip install python-toon  # https://github.com/xaviviro/python-toon\n```\n\n**Option 3: Custom lightweight implementation**\n- Implement minimal TOON encoder/decoder following [TOON spec v1.4](https://github.com/toon-format/spec)\n- Use conformance tests from spec repo to validate\n- Fallback option if no stable Python package exists\n\n### Installation strategy\n\nMake TOON optional dependency:\n```toml\n# pyproject.toml\n[project.optional-dependencies]\ntoon = [\n    \"python-toon>=0.1.0\",  # Or official package when available\n]\n```\n\nGraceful degradation: If TOON not installed, log info message and fall back to JSON.\n\n---\n\n## Implementation details\n\n### File: `src/aios/formats/toon_codec.py`\n\n```python\n\"\"\"TOON (Token-Oriented Object Notation) codec with fallback to JSON.\"\"\"\n\nfrom typing import Any, Literal, Optional\nimport json\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass ToonCodec:\n    \"\"\"Encode/decode TOON format with graceful JSON fallback.\"\"\"\n    \n    def __init__(self):\n        self._toon_available = self._check_toon_available()\n        if self._toon_available:\n            try:\n                from toon_format import encode as toon_encode\n                from toon_format import decode as toon_decode\n                self._encode_fn = toon_encode\n                self._decode_fn = toon_decode\n            except ImportError:\n                # Try alternative package name\n                from toon import encode as toon_encode\n                from toon import decode as toon_decode\n                self._encode_fn = toon_encode\n                self._decode_fn = toon_decode\n    \n    def _check_toon_available(self) -> bool:\n        \"\"\"Check if TOON library is installed.\"\"\"\n        try:\n            import toon_format\n            return True\n        except ImportError:\n            try:\n                import toon\n                return True\n            except ImportError:\n                return False\n    \n    def is_available(self) -> bool:\n        \"\"\"Return True if TOON encoding is available.\"\"\"\n        return self._toon_available\n    \n    def encode(\n        self,\n        data: Any,\n        *,\n        delimiter: Literal[',', '\\t', '|'] = ',',\n        indent: int = 2,\n        length_marker: bool = False,\n    ) -> str:\n        \"\"\"Encode data to TOON format, fallback to JSON.\"\"\"\n        if not self._toon_available:\n            logger.debug(\"TOON not available, falling back to JSON\")\n            return json.dumps(data, indent=indent)\n        \n        try:\n            options = {\n                'delimiter': delimiter,\n                'indent': indent,\n            }\n            if length_marker:\n                options['length_marker'] = '#'\n            \n            return self._encode_fn(data, **options)\n        except Exception as e:\n            logger.warning(f\"TOON encoding failed: {e}, falling back to JSON\")\n            return json.dumps(data, indent=indent)\n    \n    def decode(self, content: str, *, strict: bool = True) -> Any:\n        \"\"\"Decode TOON or JSON format (auto-detect).\"\"\"\n        # Try TOON first if available and content looks like TOON\n        if self._toon_available and self._looks_like_toon(content):\n            try:\n                return self._decode_fn(content, strict=strict)\n            except Exception as e:\n                logger.debug(f\"TOON decoding failed: {e}, trying JSON\")\n        \n        # Fallback to JSON\n        return json.loads(content)\n    \n    def _looks_like_toon(self, content: str) -> bool:\n        \"\"\"Heuristic check if content is TOON format.\"\"\"\n        # Look for TOON patterns: array headers like [N], [N]{fields}:\n        lines = content.strip().split('\\n')[:5]  # Check first few lines\n        for line in lines:\n            if '[' in line and ']:' in line:\n                return True\n            if '[' in line and ']{' in line and '}:' in line:\n                return True\n        return False\n\n# Global singleton\n_codec = ToonCodec()\n\ndef encode_toon(data: Any, **options) -> str:\n    \"\"\"Convenience function for encoding.\"\"\"\n    return _codec.encode(data, **options)\n\ndef decode_toon(content: str, **options) -> Any:\n    \"\"\"Convenience function for decoding.\"\"\"\n    return _codec.decode(content, **options)\n\ndef is_toon_available() -> bool:\n    \"\"\"Check if TOON encoding is available.\"\"\"\n    return _codec.is_available()\n```\n\n### File: `src/aios/cli/toon_cli.py`\n\n```python\n\"\"\"CLI commands for TOON format conversion.\"\"\"\n\nimport typer\nfrom pathlib import Path\nfrom typing import Optional, Literal\nimport json\n\nfrom aios.formats.toon_codec import ToonCodec, is_toon_available\n\napp = typer.Typer(help=\"Convert between JSON and TOON formats\")\n\n@app.command()\ndef encode(\n    input_path: Path = typer.Argument(..., help=\"Input JSON file or - for stdin\"),\n    output_path: Optional[Path] = typer.Option(None, \"-o\", \"--output\", help=\"Output TOON file (stdout if omitted)\"),\n    delimiter: Literal[',', '\\t', '|'] = typer.Option(',', \"--delimiter\", help=\"Array delimiter\"),\n    indent: int = typer.Option(2, \"--indent\", help=\"Indentation spaces\"),\n    length_marker: bool = typer.Option(False, \"--length-marker\", help=\"Add # prefix to array lengths\"),\n    stats: bool = typer.Option(False, \"--stats\", help=\"Show token count estimates\"),\n):\n    \"\"\"Convert JSON to TOON format.\"\"\"\n    if not is_toon_available():\n        typer.echo(\"Error: TOON library not installed. Install with: pip install python-toon\", err=True)\n        raise typer.Exit(1)\n    \n    # Read input\n    if str(input_path) == '-':\n        import sys\n        data = json.load(sys.stdin)\n    else:\n        with open(input_path) as f:\n            data = json.load(f)\n    \n    # Encode\n    codec = ToonCodec()\n    toon_output = codec.encode(\n        data,\n        delimiter=delimiter,\n        indent=indent,\n        length_marker=length_marker,\n    )\n    \n    # Show stats if requested\n    if stats:\n        json_output = json.dumps(data, indent=indent)\n        json_tokens = estimate_tokens(json_output)\n        toon_tokens = estimate_tokens(toon_output)\n        savings = (1 - toon_tokens / json_tokens) * 100\n        typer.echo(f\"\\n\ud83d\udcca Token Comparison:\", err=True)\n        typer.echo(f\"  JSON:    {json_tokens:,} tokens\", err=True)\n        typer.echo(f\"  TOON:    {toon_tokens:,} tokens\", err=True)\n        typer.echo(f\"  Savings: {savings:.1f}%\\n\", err=True)\n    \n    # Write output\n    if output_path:\n        output_path.write_text(toon_output)\n        typer.echo(f\"\u2713 Encoded to {output_path}\")\n    else:\n        typer.echo(toon_output)\n\n@app.command()\ndef decode(\n    input_path: Path = typer.Argument(..., help=\"Input TOON file or - for stdin\"),\n    output_path: Optional[Path] = typer.Option(None, \"-o\", \"--output\", help=\"Output JSON file (stdout if omitted)\"),\n    indent: int = typer.Option(2, \"--indent\", help=\"JSON indentation spaces\"),\n    strict: bool = typer.Option(True, \"--strict/--no-strict\", help=\"Enable strict validation\"),\n):\n    \"\"\"Convert TOON to JSON format.\"\"\"\n    if not is_toon_available():\n        typer.echo(\"Error: TOON library not installed. Install with: pip install python-toon\", err=True)\n        raise typer.Exit(1)\n    \n    # Read input\n    if str(input_path) == '-':\n        import sys\n        content = sys.stdin.read()\n    else:\n        content = input_path.read_text()\n    \n    # Decode\n    codec = ToonCodec()\n    data = codec.decode(content, strict=strict)\n    \n    # Write output\n    json_output = json.dumps(data, indent=indent)\n    if output_path:\n        output_path.write_text(json_output)\n        typer.echo(f\"\u2713 Decoded to {output_path}\")\n    else:\n        typer.echo(json_output)\n\ndef estimate_tokens(text: str) -> int:\n    \"\"\"Rough token estimate (4 chars = 1 token for English).\"\"\"\n    return len(text) // 4\n```\n\n### Integration with training pipeline\n\n**File: `src/aios/cli/hrm_hf/train_actv1.py`**\n\nAdd TOON support to metrics logging:\n\n```python\ndef _write_jsonl_helper(\n    log_file: Path,\n    payload: dict,\n    *,\n    format: Literal['json', 'toon'] = 'json',\n    toon_options: Optional[dict] = None,\n):\n    \"\"\"Write metrics in JSON or TOON format.\"\"\"\n    if format == 'toon':\n        from aios.formats.toon_codec import is_toon_available, encode_toon\n        if is_toon_available():\n            toon_options = toon_options or {}\n            line = encode_toon(payload, **toon_options)\n        else:\n            logger.warning(\"TOON not available, falling back to JSON\")\n            line = json.dumps(payload)\n    else:\n        line = json.dumps(payload)\n    \n    with open(log_file, 'a', encoding='utf-8') as f:\n        f.write(line + '\\n')\n```\n\n**CLI flags in `src/aios/cli/hrm_hf_cli.py`**:\n\n```python\n@app.command()\ndef train_actv1(\n    # ... existing params ...\n    metrics_format: Literal['json', 'toon'] = typer.Option(\n        'json',\n        '--metrics-format',\n        help='Output format for metrics (json or toon)',\n    ),\n    toon_delimiter: Literal[',', '\\t', '|'] = typer.Option(\n        ',',\n        '--toon-delimiter',\n        help='Delimiter for TOON format (comma, tab, or pipe)',\n    ),\n):\n    \"\"\"Train with optional TOON metrics output.\"\"\"\n    # ... pass through to training function\n```\n\n---\n\n## CLI design\n\n### New command group: `aios toon`\n\n```powershell\n# Encode JSON to TOON\naios toon encode input.json -o output.toon\n\n# Decode TOON to JSON\naios toon decode data.toon -o output.json\n\n# Pipe operations\ncat data.json | aios toon encode --stats\n\n# Tab-delimited for better compression\naios toon encode input.json --delimiter \"\\t\" --stats\n```\n\n### Integration with existing commands\n\n```powershell\n# Training with TOON metrics\naios hrm-hf train-actv1 \\\n    --model gpt2 \\\n    --dataset-file training_data/curated_datasets/test_sample.txt \\\n    --metrics-format toon \\\n    --toon-delimiter \"\\t\" \\\n    --log-file artifacts/brains/actv1/metrics.toon\n\n# Export evaluation in TOON format\naios evaluation export \\\n    --format toon \\\n    --output artifacts/evaluation/results.toon\n```\n\n---\n\n## Testing and acceptance criteria\n\n### Unit tests: `tests/test_toon_integration.py`\n\n```python\ndef test_toon_codec_encode_simple():\n    \"\"\"Test encoding simple objects.\"\"\"\n    data = {\"id\": 123, \"name\": \"Alice\", \"active\": True}\n    codec = ToonCodec()\n    output = codec.encode(data)\n    assert \"id:\" in output\n    assert \"Alice\" in output\n\ndef test_toon_codec_encode_tabular():\n    \"\"\"Test encoding arrays of objects (tabular format).\"\"\"\n    data = {\n        \"items\": [\n            {\"sku\": \"A1\", \"qty\": 2, \"price\": 9.99},\n            {\"sku\": \"B2\", \"qty\": 1, \"price\": 14.5},\n        ]\n    }\n    codec = ToonCodec()\n    output = codec.encode(data)\n    assert \"items[2]{sku,qty,price}:\" in output\n    assert \"A1,2,9.99\" in output\n\ndef test_toon_codec_roundtrip():\n    \"\"\"Test encode -> decode roundtrip.\"\"\"\n    original = {\n        \"users\": [\n            {\"id\": 1, \"name\": \"Alice\", \"role\": \"admin\"},\n            {\"id\": 2, \"name\": \"Bob\", \"role\": \"user\"},\n        ]\n    }\n    codec = ToonCodec()\n    encoded = codec.encode(original)\n    decoded = codec.decode(encoded)\n    assert decoded == original\n\ndef test_toon_fallback_when_unavailable():\n    \"\"\"Test graceful fallback to JSON when TOON not installed.\"\"\"\n    # Mock TOON unavailable\n    # Verify JSON output returned instead\n```\n\n### Integration tests\n\n```powershell\n# Test metrics logging with TOON\naios hrm-hf train-actv1 \\\n    --model gpt2 \\\n    --dataset-file training_data/curated_datasets/test_sample.txt \\\n    --steps 1 --batch-size 2 \\\n    --metrics-format toon \\\n    --log-file artifacts/test_toon_metrics.toon\n\n# Verify output is valid TOON\naios toon decode artifacts/test_toon_metrics.toon\n```\n\n### Acceptance criteria\n\n- \u2705 TOON encoding reduces token count by 30-60% for tabular metrics data\n- \u2705 CLI can convert between JSON and TOON formats bidirectionally\n- \u2705 Training pipeline can output metrics in TOON format with flag\n- \u2705 Graceful fallback to JSON when TOON not installed (no crashes)\n- \u2705 Documentation includes benchmarks and when to use TOON vs JSON\n\n---\n\n## Use cases and recommendations\n\n### When to use TOON\n\n\u2705 **Excellent candidates:**\n- Training metrics with uniform structure (loss, accuracy, step numbers)\n- Evaluation results with consistent fields across samples\n- Large tabular datasets for LLM ingestion\n- Configuration exports with repeated structures\n- Batch prediction outputs\n\n\u2705 **Benefits:**\n- 30-60% token savings on uniform tabular data\n- Better than JSON for arrays of objects\n- Comparable to CSV but with nested object support\n\n### When to stick with JSON\n\n\u274c **Keep using JSON for:**\n- Deeply nested configurations with varied structures\n- Non-uniform data with inconsistent fields\n- Legacy compatibility requirements\n- Data consumed by non-TOON-aware tools\n\n### Benchmark expectations\n\nBased on TOON project benchmarks:\n\n| Data Type | TOON vs JSON | TOON vs JSON (compact) |\n|-----------|--------------|------------------------|\n| Uniform employee records | -60.7% | -36.8% |\n| E-commerce orders (mixed) | -33.1% | +5.5% |\n| Event logs (semi-uniform) | -15.0% | +19.9% |\n| Deeply nested config | -31.3% | +11.9% |\n\n*Note: TOON typically uses more tokens than minified JSON for deeply nested data, but offers better structure for LLMs.*\n\n---\n\n## Risks and mitigations\n\n### Risk: Python package stability\n- **Mitigation**: Use optional dependency + graceful fallback; consider custom implementation\n- **Status**: Official `toon_format` package in development; community implementations available\n\n### Risk: Format adoption\n- **Mitigation**: Keep JSON as default; TOON opt-in via flags; support both formats side-by-side\n- **Status**: TOON gaining traction in LLM community; 11.3k GitHub stars\n\n### Risk: Compatibility with existing tools\n- **Mitigation**: Provide conversion utilities; document migration path\n- **Status**: CLI tools make conversion trivial\n\n### Risk: LLM understanding\n- **Mitigation**: Include format examples in prompts; test with multiple models\n- **Status**: Benchmarks show 68.7% retrieval accuracy (vs JSON 65.7%) with token savings\n\n---\n\n## Rollout plan\n\n### Phase 1: Foundation (2 days)\n- [ ] Implement `ToonCodec` utility with fallback\n- [ ] Add optional dependency to `pyproject.toml`\n- [ ] Create unit tests for encode/decode\n- [ ] Add config schema to `config/default.yaml`\n\n### Phase 2: CLI tools (1 day)\n- [ ] Implement `aios toon encode/decode` commands\n- [ ] Add `--stats` flag for token comparison\n- [ ] Test conversion workflows\n\n### Phase 3: Training integration (1 day)\n- [ ] Add `--metrics-format toon` flag to training CLI\n- [ ] Implement TOON output in `_write_jsonl_helper`\n- [ ] Test training runs with TOON metrics\n\n### Phase 4: Documentation & examples (1 day)\n- [ ] Write user guide with examples\n- [ ] Add benchmark results\n- [ ] Create migration guide from JSON to TOON\n- [ ] Document when to use TOON vs JSON\n\n### Phase 5: Optional enhancements (future)\n- [ ] TOON support in evaluation exports\n- [ ] Dataset conversion utilities\n- [ ] VS Code extension for TOON syntax highlighting\n- [ ] Custom TOON implementation if official package delays\n\n---\n\n## Documentation outline\n\n### User guide sections\n\n1. **What is TOON?**\n   - Overview and benefits\n   - Token efficiency benchmarks\n   - LLM-friendly structure\n\n2. **Installation**\n   ```powershell\n   pip install \"aios[toon]\"\n   ```\n\n3. **Quick start**\n   - Converting files\n   - Using in training\n   - Reading TOON output\n\n4. **When to use TOON**\n   - Decision matrix\n   - Use case examples\n   - Performance expectations\n\n5. **Format reference**\n   - Syntax examples\n   - Object encoding\n   - Array encoding (tabular)\n   - Delimiter options\n\n6. **Troubleshooting**\n   - Package not installed\n   - Format detection issues\n   - Encoding errors\n\n### Examples\n\n```powershell\n# Example 1: Compare token usage\naios toon encode training_metrics.json --stats\n\n# Example 2: Convert existing metrics to TOON\nGet-ChildItem artifacts/brains/actv1/*.jsonl | ForEach-Object {\n    $output = $_.FullName -replace '\\.jsonl$', '.toon'\n    aios toon encode $_.FullName -o $output --delimiter \"\\t\" --stats\n}\n\n# Example 3: Training with TOON output\naios hrm-hf train-actv1 \\\n    --model artifacts/hf_implant/gpt2 \\\n    --dataset-file training_data/curated_datasets/test_sample.txt \\\n    --steps 100 --batch-size 4 \\\n    --metrics-format toon \\\n    --toon-delimiter \"\\t\" \\\n    --log-file artifacts/brains/actv1/metrics.toon\n\n# Example 4: Analyze TOON metrics with LLM\n$metrics = Get-Content artifacts/brains/actv1/metrics.toon -Raw\n# Pass to LLM with prompt: \"Analyze these training metrics in TOON format...\"\n```\n\n---\n\n## References\n\n- TOON official spec: https://github.com/toon-format/spec (v1.4)\n- TOON TypeScript implementation: https://github.com/toon-format/toon\n- TOON website: https://toonformat.dev/\n- Community Python implementation: https://github.com/xaviviro/python-toon\n- Official Python package (in dev): https://github.com/toon-format/toon-python\n- Format playground: https://www.curiouslychase.com/playground/format-tokenization-exploration\n\n### Key insights from TOON project\n\n1. **Token efficiency**: 30-60% savings on uniform data\n2. **LLM accuracy**: 68.7% vs JSON's 65.7% on retrieval tasks\n3. **Best use case**: Arrays of objects with identical primitive fields\n4. **Delimiter impact**: Tab delimiter often more efficient than comma\n5. **Length markers**: `[#N]` notation helps LLMs validate structure\n\n---\n\n## Future enhancements\n\n### Post-MVP features\n\n1. **Streaming TOON parser**\n   - Process large TOON files line-by-line\n   - Memory-efficient for big datasets\n\n2. **TOON compression analysis**\n   - Automated tool to analyze JSON \u2192 TOON savings potential\n   - Recommend TOON for files above threshold\n\n3. **VS Code integration**\n   - Syntax highlighting for `.toon` files\n   - Hover tooltips with field info\n   - Convert commands in context menu\n\n4. **Custom TOON recognizers for AI-OS**\n   - Domain-specific patterns (model names, metrics)\n   - Optimized for AI-OS data structures\n\n5. **TOON-aware log viewer**\n   - GUI tool to browse TOON metrics\n   - Side-by-side comparison with JSON\n\n---\n\n## Developer checklist\n\n### Implementation\n- [ ] Create `src/aios/formats/toon_codec.py`\n- [ ] Create `src/aios/cli/toon_cli.py`\n- [ ] Add TOON config to `config/default.yaml`\n- [ ] Update `pyproject.toml` with optional dependency\n- [ ] Integrate into training pipeline\n- [ ] Add CLI flags to `hrm_hf_cli.py`\n\n### Testing\n- [ ] Unit tests for `ToonCodec`\n- [ ] CLI command tests\n- [ ] Integration test with training\n- [ ] Roundtrip tests (encode \u2192 decode)\n- [ ] Fallback behavior tests\n\n### Documentation\n- [ ] User guide in `docs/guide/toon_integration.md`\n- [ ] Update training docs with TOON flags\n- [ ] Add examples to `docs/examples/`\n- [ ] Update README with TOON mention\n\n### Validation\n- [ ] Benchmark token savings on actual AI-OS metrics\n- [ ] Test LLM comprehension with TOON metrics\n- [ ] Verify graceful degradation without TOON package\n- [ ] Performance testing on large datasets\n\n---\n\n## Operator checklist\n\n### Pre-deployment\n- [ ] Install TOON package: `pip install python-toon`\n- [ ] Test conversion: `aios toon encode <sample.json> --stats`\n- [ ] Verify token savings meet expectations (>20% for tabular data)\n\n### Migration\n- [ ] Convert existing metrics: `aios toon encode <metrics.jsonl> -o <metrics.toon>`\n- [ ] Update analysis scripts to handle TOON format\n- [ ] Train team on TOON syntax and when to use it\n\n### Production use\n- [ ] Add `--metrics-format toon` to training scripts for long runs\n- [ ] Use tab delimiter for maximum compression: `--toon-delimiter \"\\t\"`\n- [ ] Monitor file sizes and token usage\n- [ ] Keep JSON format for compatibility where needed\n\n---\n\n## Success metrics\n\n### Quantitative\n- \u2705 30-60% token reduction on uniform training metrics\n- \u2705 No performance regression in training throughput\n- \u2705 100% roundtrip accuracy (encode \u2192 decode)\n- \u2705 Graceful fallback when TOON unavailable\n\n### Qualitative\n- \u2705 Users can easily convert between formats\n- \u2705 LLM analysis of TOON metrics is as good or better than JSON\n- \u2705 Documentation is clear and includes decision matrix\n- \u2705 Integration feels natural (opt-in, not forced)\n\n---\n\n## Quickstart (Windows/PowerShell)\n\n```powershell\n# 1) Install TOON support\npip install \"aios[toon]\"\n\n# 2) Test conversion on existing file\naios toon encode artifacts/brains/actv1/metrics.jsonl --stats\n\n# 3) Run training with TOON output\naios hrm-hf train-actv1 `\n    --model gpt2 `\n    --dataset-file training_data/curated_datasets/test_sample.txt `\n    --steps 100 --batch-size 4 `\n    --metrics-format toon `\n    --toon-delimiter \"\\t\" `\n    --log-file artifacts/brains/actv1/metrics.toon\n\n# 4) Convert back to JSON for analysis\naios toon decode artifacts/brains/actv1/metrics.toon -o metrics_decoded.json\n\n# 5) Compare file sizes\n(Get-Item artifacts/brains/actv1/metrics.toon).Length\n(Get-Item metrics_decoded.json).Length\n```\n\n---\n\n## Summary\n\nTOON integration provides a token-efficient alternative to JSON for AI-OS data serialization, particularly beneficial for training metrics, evaluation outputs, and LLM prompt payloads. The implementation follows AI-OS patterns with optional dependencies, graceful fallbacks, and clear documentation. By adopting TOON for uniform tabular data, operators can reduce token costs by 30-60% while maintaining or improving LLM comprehension.\n", "tags": ["cli", "datasets", "evaluation", "training"], "headings": [{"line": 0, "text": "PF-007: TOON Format Integration for Token-Efficient LLM Data Exchange"}, {"line": 2, "text": "Summary"}, {"line": 6, "text": "Why this matters"}, {"line": 15, "text": "What ships in PF-007"}, {"line": 28, "text": "Architecture overview"}, {"line": 30, "text": "Data paths affected"}, {"line": 48, "text": "Core components"}, {"line": 74, "text": "Dependencies and setup"}, {"line": 76, "text": "Python implementation options"}, {"line": 93, "text": "Installation strategy"}, {"line": 97, "text": "pyproject.toml"}, {"line": 108, "text": "Implementation details"}, {"line": 110, "text": "File: `src/aios/formats/toon_codec.py`"}, {"line": 133, "text": "Try alternative package name"}, {"line": 183, "text": "Try TOON first if available and content looks like TOON"}, {"line": 190, "text": "Fallback to JSON"}, {"line": 195, "text": "Look for TOON patterns: array headers like [N], [N]{fields}:"}, {"line": 204, "text": "Global singleton"}, {"line": 220, "text": "File: `src/aios/cli/toon_cli.py`"}, {"line": 248, "text": "Read input"}, {"line": 256, "text": "Encode"}, {"line": 265, "text": "Show stats if requested"}, {"line": 276, "text": "Write output"}, {"line": 295, "text": "Read input"}, {"line": 302, "text": "Decode"}, {"line": 306, "text": "Write output"}, {"line": 319, "text": "Integration with training pipeline"}, {"line": 354, "text": "... existing params ..."}, {"line": 367, "text": "... pass through to training function"}, {"line": 372, "text": "CLI design"}, {"line": 374, "text": "New command group: `aios toon`"}, {"line": 377, "text": "Encode JSON to TOON"}, {"line": 380, "text": "Decode TOON to JSON"}, {"line": 383, "text": "Pipe operations"}, {"line": 386, "text": "Tab-delimited for better compression"}, {"line": 390, "text": "Integration with existing commands"}, {"line": 393, "text": "Training with TOON metrics"}, {"line": 401, "text": "Export evaluation in TOON format"}, {"line": 409, "text": "Testing and acceptance criteria"}, {"line": 411, "text": "Unit tests: `tests/test_toon_integration.py`"}, {"line": 450, "text": "Mock TOON unavailable"}, {"line": 451, "text": "Verify JSON output returned instead"}, {"line": 454, "text": "Integration tests"}, {"line": 457, "text": "Test metrics logging with TOON"}, {"line": 465, "text": "Verify output is valid TOON"}, {"line": 469, "text": "Acceptance criteria"}, {"line": 479, "text": "Use cases and recommendations"}, {"line": 481, "text": "When to use TOON"}, {"line": 495, "text": "When to stick with JSON"}, {"line": 503, "text": "Benchmark expectations"}, {"line": 518, "text": "Risks and mitigations"}, {"line": 520, "text": "Risk: Python package stability"}, {"line": 524, "text": "Risk: Format adoption"}, {"line": 528, "text": "Risk: Compatibility with existing tools"}, {"line": 532, "text": "Risk: LLM understanding"}, {"line": 538, "text": "Rollout plan"}, {"line": 540, "text": "Phase 1: Foundation (2 days)"}, {"line": 546, "text": "Phase 2: CLI tools (1 day)"}, {"line": 551, "text": "Phase 3: Training integration (1 day)"}, {"line": 556, "text": "Phase 4: Documentation & examples (1 day)"}, {"line": 562, "text": "Phase 5: Optional enhancements (future)"}, {"line": 570, "text": "Documentation outline"}, {"line": 572, "text": "User guide sections"}, {"line": 605, "text": "Examples"}, {"line": 608, "text": "Example 1: Compare token usage"}, {"line": 611, "text": "Example 2: Convert existing metrics to TOON"}, {"line": 617, "text": "Example 3: Training with TOON output"}, {"line": 626, "text": "Example 4: Analyze TOON metrics with LLM"}, {"line": 628, "text": "Pass to LLM with prompt: \"Analyze these training metrics in TOON format...\""}, {"line": 633, "text": "References"}, {"line": 642, "text": "Key insights from TOON project"}, {"line": 652, "text": "Future enhancements"}, {"line": 654, "text": "Post-MVP features"}, {"line": 679, "text": "Developer checklist"}, {"line": 681, "text": "Implementation"}, {"line": 689, "text": "Testing"}, {"line": 696, "text": "Documentation"}, {"line": 702, "text": "Validation"}, {"line": 710, "text": "Operator checklist"}, {"line": 712, "text": "Pre-deployment"}, {"line": 717, "text": "Migration"}, {"line": 722, "text": "Production use"}, {"line": 730, "text": "Success metrics"}, {"line": 732, "text": "Quantitative"}, {"line": 738, "text": "Qualitative"}, {"line": 746, "text": "Quickstart (Windows/PowerShell)"}, {"line": 749, "text": "1) Install TOON support"}, {"line": 752, "text": "2) Test conversion on existing file"}, {"line": 755, "text": "3) Run training with TOON output"}, {"line": 764, "text": "4) Convert back to JSON for analysis"}, {"line": 767, "text": "5) Compare file sizes"}, {"line": 774, "text": "Summary"}]}, {"path": "planned_features/UNLIMIFORMER_INTEGRATION.md", "content": "## Unlimiformer integration for unlimited-length inputs\n\nThis planned feature proposes integrating Unlimiformer (NeurIPS 2023) into AI\u2011OS to enable effectively unlimited-length inputs for supported Hugging Face models during evaluation/inference and optionally during training.\n\nReferences\n- Paper: \u201cUnlimiformer: Long-Range Transformers with Unlimited Length Input\u201d (Bertsch et al., 2023)\n- Code: https://github.com/abertsch72/unlimiformer (MIT License)\n\n\n### Why this matters\n- Handle very long prompts (books, entire project repos, long transcripts) without truncation.\n- Keep base attention unchanged; add retrieval-based attention above a chosen layer (\u201clayer_begin\u201d).\n- Works with existing pretrained models (e.g., BART/T5 encoder\u2013decoder; decoder-only models such as Llama\u20112 according to the repo README) and can be used purely at evaluation time or for specialized training regimes.\n\n\n## Scope and success criteria\n\nIn scope (Phase 1)\n- Evaluation/inference integration for HF-backed \u201cbrains\u201d (aios.core.hrm_models.hf_adapter) with decoder-only models (LLaMA family) using Unlimiformer\u2019s retrieval at generation time.\n- Early-stopping evaluation using Unlimiformer on long validation inputs.\n- Configuration flags in AI\u2011OS to toggle Unlimiformer without breaking existing flows.\n\nIn scope (Phase 2)\n- Training-time modes: \u201crandom-encoded\u201d and \u201cretrieval\u201d training; alternating schedule as in the paper.\n- Encoder\u2013decoder support (e.g., BART/T5) on summarization-like datasets.\n\nOut of scope (initially)\n- Direct integration into the custom ACTV1 HRM model (non-HF architecture). This would require substantial porting of the attention hook path and is a potential Phase 3 exploration.\n\nSuccess criteria\n- P0: For a HF LLaMA\u2011family model configured in AI\u2011OS, users can enable Unlimiformer and successfully generate from inputs > base context (e.g., \u2265 100k tokens) with stable memory via FAISS datastore; outputs match expectations on long-doc summarization prompts.\n- P1: Automated test(s) verify that enabling Unlimiformer leaves standard generation unchanged on short inputs (parity within tolerance) and does not regress normal inference when disabled.\n\n\n## High-level design\n\nConcept\n- Unlimiformer augments a HF model with a retrieval path over the full input. Layers below `layer_begin` attend as usual over the last `context_window_size` tokens; from `layer_begin` upwards, cross/retrieval attention uses K\u2011NN over a datastore of hidden states (optionally via FAISS) to bring in relevant tokens from the entire (long) input.\n\nIntegration strategy in AI\u2011OS\n1) HF integration layer\n   - Extend or wrap `HFCausalLM_HRMAdapter` to optionally enable Unlimiformer for decoder-only models.\n   - Provide a thin compatibility shim that mirrors the Unlimiformer repo\u2019s expected arguments (e.g., `test_unlimiformer`, `layer_begin`, `use_datastore`, `gpu_index`, `gpu_datastore`, `index_devices`, `datastore_device`, `knn`).\n   - Keep Unlimiformer disabled by default.\n\n2) Vendor minimal Unlimiformer components\n   - Add `aios.integrations.unlimiformer` module containing the minimal files required from the upstream `src/` (per MIT license), focusing on inference paths: model wrapper, indexing/datastore, and argument schema.\n   - Avoid forking the training script initially; rely on our Typer CLI and config system.\n\n3) Configuration plumbing\n   - Add optional config keys under `brains.trainer_overrides` (HF mode) to toggle Unlimiformer and pass-through parameters.\n   - Example keys (all optional):\n     - `unlimiformer.enabled: false`\n     - `unlimiformer.layer_begin: 16`\n     - `unlimiformer.knn: true`\n     - `unlimiformer.use_datastore: true`\n     - `unlimiformer.gpu_datastore: true`\n     - `unlimiformer.gpu_index: true`\n     - `unlimiformer.index_devices: [1]`\n     - `unlimiformer.datastore_device: 1`\n     - `unlimiformer.context_window_size: 4096` (fallback sliding window for lower layers)\n     - `unlimiformer.eval_max_source_length: 999999`\n   - Defaults keep feature off, requiring no extra dependencies.\n\n4) Dependency management\n   - Add optional extra `[unlimiformer]` in `pyproject.toml` including: `faiss-cpu` (Windows-friendly), `faiss-gpu` (Linux/CUDA environments, optional), `transformers>=4.33`, and numpy/scipy as needed.\n   - Detect platform; default to FAISS\u2011CPU on Windows; allow GPU index/datastore where supported.\n\n5) Memory/runtimes\n   - For very long inputs, use FAISS datastore (CPU by default) to keep GPU memory stable.\n   - Provide flags to move index/datastore to GPU when memory allows.\n\n6) UX\n   - CLI switches added to relevant commands (e.g., `aios hrm-hf chat` or evaluation flows) to toggle Unlimiformer.\n   - GUI toggle in HRM/HF panels: \u201cUnlimited context (Unlimiformer)\u201d with tooltips and safe defaults.\n\n\n## Detailed design and APIs\n\n### Module layout\n- `src/aios/integrations/unlimiformer/`\n  - `__init__.py`: feature gate and version checks\n  - `adapter.py`: small wrapper class that:\n    - Accepts a HF model and tokenizer\n    - Builds/attaches Unlimiformer components\n    - Exposes a `.generate(...)` and `.prepare_inputs_for_long_context(prefix, prompt, suffix)` helper\n  - `datastore.py`: thin wrapper over FAISS index/datastore setup (CPU/GPU), device routing, and persistence\n  - `args.py`: shared dataclass/TypedDict mapping AI\u2011OS config to Unlimiformer expected settings\n  - `compat.py`: utilities to resolve `layer_begin` based on model depth and provide recommended defaults\n\n\n### HF adapter extension points\nLocation: `src/aios/core/hrm_models/hf_adapter.py`\n\nAdd optional Unlimiformer activation path:\n- New optional constructor parameter `unlimiformer: Optional[UnlimiformerConfigLike] = None`.\n- When provided and `unlimiformer.enabled` is true, wrap the underlying `self.model` with the Unlimiformer augmentation via `aios.integrations.unlimiformer.adapter.enable(self.model, tokenizer, cfg)`.\n- Ensure `forward(...)` and `generate(...)` continue to route through HF model seamlessly; Unlimiformer alters attention inside the model graph post-`layer_begin`.\n\nDatastore/index devices\n- Respect `datastore_device` and `index_devices` (GPU ids) when available; otherwise default to CPU FAISS.\n- Expose graceful fallbacks on Windows to CPU FAISS.\n\n\n### Configuration surface (proposed)\nUnder `brains.trainer_overrides` when `kind: hf`:\n\n```\nunlimiformer:\n  enabled: false\n  layer_begin: null          # null \u2192 auto (> 1/2 of total layers)\n  context_window_size: 4096  # sliding window for lower layers\n  knn: true\n  use_datastore: true\n  gpu_datastore: false       # default false on Windows\n  gpu_index: false\n  datastore_device: 1        # optional\n  index_devices: [1]         # optional list\n  eval_max_source_length: 999999\n```\n\nCLI flags (Typer) mirror the above and default to disabled.\n\n\n### Training integration (Phase 2)\n- Add a new command for HF fine\u2011tuning with long inputs using Unlimiformer:\n  - `aios hrm-hf train-hf-unlimiformer` (or integrate into `training_cli.py`)\n  - Modes: `--unlimiformer-training`, `--random-unlimiformer-training`, `--alternating` as in the repo\n- Datasets: begin with summarization datasets (GovReport, SummScreen, BookSum) to mirror paper experiments.\n- Early stopping: Evaluate with `test_unlimiformer` enabled to select checkpoints.\n\n\n## Dependencies and compatibility\n- Unlimiformer repo is MIT; vendoring minimal files is permissible with attribution.\n- FAISS:\n  - `faiss-cpu` works on Windows; `faiss-gpu` is Linux/CUDA\u2011oriented. Provide runtime detection and degrade to CPU index/datastore on unsupported platforms.\n- Transformers version pinning: test with the current repo baseline; maintain a gated extra if newer transformers are required by Unlimiformer.\n\n\n## Risks, constraints, and mitigations\n- Windows GPU FAISS availability: default to CPU FAISS; add clear logs; allow GPU usage on Linux/CUDA.\n- Model coverage: initial focus on LLaMA\u2011family decoder\u2011only; expand to encoder\u2013decoder later.\n- Performance variance: `layer_begin` is critical; provide auto\u2011heuristic (> half of total layers) and expose as a user setting.\n- Memory pressure: ensure `use_datastore` defaults to true for very long inputs; include telemetry in logs.\n- Maintenance: vendor minimal code only; isolate under `aios.integrations.unlimiformer` to avoid invasive changes.\n\n\n## Test plan\nAutomated\n- Unit test: enabling Unlimiformer on a short prompt yields outputs close to baseline generation (within tolerance on logits/perplexity or token parity for deterministic settings).\n- E2E smoke: long input (> 64k tokens) generates without OOM, with FAISS CPU datastore, and token streaming stays responsive.\n\nManual\n- LLaMA\u20112 chat model: summarize a 100k\u2011token text with `unlimiformer.enabled=true`; compare quality and latency with and without GPU index/datastore.\n\n\n## Milestones and timeline\n- P0 (1\u20132 days)\n  - Vendor minimal Unlimiformer inference code and feature\u2011gate in HF adapter; add config flags; CPU FAISS only; LLaMA\u2011family support.\n  - Add docs and CLI flags; write basic unit/E2E smoke tests.\n- P1 (3\u20135 days)\n  - GPU datastore/index support on Linux/CUDA; GUI toggle; early\u2011stopping evaluation integration.\n- P2 (1\u20132 weeks)\n  - Training modes (random\u2011encoded, retrieval, alternating) for encoder\u2013decoder tasks; sample recipes.\n- P3 (exploratory)\n  - Investigate feasibility of HRM (ACTV1) architecture adaptation.\n\n\n## Rollout and observability\n- Feature flag: off by default.\n- Structured logs: write Unlimiformer settings and device placements; record datastore/index memory footprint and retrieval latency.\n- Add a troubleshooting section in docs (common FAISS issues, Windows notes).\n\n\n## Next actions (Phase 1)\n1) Add optional dependency group `[unlimiformer]` to `pyproject.toml` (faiss-cpu; platform\u2011specific notes for faiss\u2011gpu).\n2) Create `aios.integrations.unlimiformer` module and vendor minimal code.\n3) Extend `HFCausalLM_HRMAdapter` to accept an optional `unlimiformer` config and wrap the model.\n4) Add CLI and config flags; wire through `brains.trainer_overrides`.\n5) Add unit and smoke tests; document usage and caveats.\n\n\n---\nMaintainers: Please comment on feasibility, preferred model targets for Phase 1, and any CI/platform constraints (especially Windows FAISS).\n\n\n## Phase breakdown and checklists\n\nBelow are execution-ready checklists per phase with acceptance criteria and exit gates.\n\n### Phase 0 \u2014 Repo readiness and scaffolding (0.5\u20131 day)\n\nChecklist\n- [ ] Add optional dependency extra `[unlimiformer]` (faiss-cpu; pin transformers range if needed)\n- [ ] Create `src/aios/integrations/unlimiformer/` with MIT attribution NOTICE\n- [ ] Define `UnlimiformerConfig` TypedDict and feature-gate helpers\n- [ ] Add config block (disabled) under `brains.trainer_overrides.unlimiformer`\n- [ ] Wire no-op read/validation in HF adapter (no behavior change yet)\n\nAcceptance criteria\n- [ ] App starts unchanged with default config\n- [ ] CI/lint/tests remain green\n\nExit gate\n- [ ] Feature is fully hidden behind `enabled: false`\n\n\n### Phase 1 \u2014 Inference PoC (decoder-only, Windows-friendly) (1\u20132 days)\n\nChecklist\n- [ ] Vendor minimal Unlimiformer inference components (index/datastore + model hook)\n- [ ] Implement `enable_on_model(model, tokenizer, cfg)` to attach retrieval attention from `layer_begin`\n- [ ] Default to FAISS-CPU datastore/index on Windows; detect and log device placement\n- [ ] Add CLI flags to evaluation/generation path to toggle Unlimiformer\n- [ ] Provide an example: summarize a long text (\u2265100k tokens) with LLaMA\u2011family model\n- [ ] Unit test: enabling Unlimiformer on short inputs \u2248 baseline outputs\n- [ ] E2E smoke: long input completes without OOM using FAISS-CPU; stream tokens\n\nAcceptance criteria\n- [ ] Long-input generation succeeds on Windows with FAISS-CPU\n- [ ] Short-input parity within tolerance when Unlimiformer is enabled\n- [ ] Clear logs for index/datastore size and latency\n\nExit gate\n- [ ] Docs updated with Windows usage and limitations\n\n\n### Phase 1.1 \u2014 Linux/CUDA GPU index/datastore (1\u20132 days)\n\nChecklist\n- [ ] Detect CUDA and allow `gpu_index`/`gpu_datastore`\n- [ ] Validate FAISS-GPU install path with helpful errors/fallbacks\n- [ ] Benchmark basic throughput vs CPU on the same prompt\n\nAcceptance criteria\n- [ ] GPU index/datastore operates with visible latency reduction vs CPU on eligible hardware\n- [ ] Graceful fallback to CPU with warning logs when unavailable\n\nExit gate\n- [ ] Docs include CUDA/GPU prerequisites and troubleshooting\n\n\n### Phase 1.2 \u2014 GUI toggle + early-stopping evaluation (1\u20132 days)\n\nChecklist\n- [ ] Add GUI switch \u201cUnlimited context (Unlimiformer)\u201d under HF trainer settings\n- [ ] Wire values: enabled, layer_begin, datastore/index devices\n- [ ] Integrate Unlimiformer during evaluation used for early stopping (if configured)\n\nAcceptance criteria\n- [ ] GUI toggle persists to config and is honored by evaluation\n- [ ] Early-stopping can leverage long-context evaluation without regressions\n\nExit gate\n- [ ] UX doc and tooltip coverage for each setting\n\n\n### Phase 2 \u2014 Training modes and encoder\u2013decoder support (1\u20132 weeks)\n\nChecklist\n- [ ] Add CLI to enable training modes: `--unlimiformer-training`, `--random-unlimiformer-training`, `--alternating`\n- [ ] Implement training-time hooks (sampling from full input, random-encoded mode)\n- [ ] Add dataset recipes for GovReport/SummScreen/BookSum (encoder\u2013decoder models)\n- [ ] Early-stopping/eval configured with `test_unlimiformer`\n- [ ] Add metrics logging for retrieval hits/latency during training\n\nAcceptance criteria\n- [ ] Reproduce representative improvements similar to paper on at least one dataset (relative, not exact)\n- [ ] Training is stable with recommended defaults; resource use documented\n\nExit gate\n- [ ] Docs include full training command examples and cost guidance\n\n\n### Phase 3 \u2014 ACTV1 HRM exploration (exploratory, timeboxed)\n\nChecklist\n- [ ] Spike: map HRM attention blocks to identify hook points analogous to Unlimiformer\n- [ ] Prototype minimal retrieval layer that augments HRM without changing base attention math\n- [ ] Measure memory/perf impact on long sequences\n\nAcceptance criteria\n- [ ] Decision doc: feasible/not feasible with outline of required changes and risks\n\nExit gate\n- [ ] If feasible, produce a separate follow-on feature spec; otherwise close spike with rationale\n\n\n### Phase 4 \u2014 Hardening, docs, and demos (2\u20134 days)\n\nChecklist\n- [ ] Consolidate structured logs and counters: index size, retrieval QPS/latency, device placement\n- [ ] Expand troubleshooting guide (FAISS install, GPU issues, Windows notes)\n- [ ] Create repeatable demos (scripts + sample prompts) for long\u2011doc summarization and QA\n- [ ] Finalize API stability and mark feature as beta (still default off)\n\nAcceptance criteria\n- [ ] Demo scripts run on Windows (CPU FAISS) and Linux/CUDA (GPU FAISS)\n- [ ] Users can follow docs to reproduce results without support\n\nExit gate\n- [ ] Sign-off from maintainers to publish feature docs\n\n\n## Testing matrix (summary)\n\n- Platforms\n  - [ ] Windows 11 + CUDA GPU present \u2192 default CPU FAISS works; GPU path explicitly disabled by default\n  - [ ] Linux + CUDA 12.x \u2192 CPU and GPU FAISS paths\n  - [ ] macOS (CPU only) \u2192 CPU FAISS path\n- Models\n  - [ ] LLaMA\u2011family (decoder\u2011only) \u2014 Phase 1\n  - [ ] BART/T5 (encoder\u2013decoder) \u2014 Phase 2\n- Inputs\n  - [ ] Short (\u2264 2k tokens) parity tests\n  - [ ] Long (\u2265 100k tokens) smoke + latency/throughput sampling\n- Modes\n  - [ ] Inference only\n  - [ ] Training: random\u2011encoded, retrieval, alternating (Phase 2)\n\n", "tags": ["datasets", "evaluation", "hrm", "training"], "headings": [{"line": 0, "text": "Unlimiformer integration for unlimited-length inputs"}, {"line": 9, "text": "Why this matters"}, {"line": 15, "text": "Scope and success criteria"}, {"line": 34, "text": "High-level design"}, {"line": 77, "text": "Detailed design and APIs"}, {"line": 79, "text": "Module layout"}, {"line": 91, "text": "HF adapter extension points"}, {"line": 104, "text": "Configuration surface (proposed)"}, {"line": 124, "text": "Training integration (Phase 2)"}, {"line": 132, "text": "Dependencies and compatibility"}, {"line": 139, "text": "Risks, constraints, and mitigations"}, {"line": 147, "text": "Test plan"}, {"line": 156, "text": "Milestones and timeline"}, {"line": 168, "text": "Rollout and observability"}, {"line": 174, "text": "Next actions (Phase 1)"}, {"line": 186, "text": "Phase breakdown and checklists"}, {"line": 190, "text": "Phase 0 \u2014 Repo readiness and scaffolding (0.5\u20131 day)"}, {"line": 207, "text": "Phase 1 \u2014 Inference PoC (decoder-only, Windows-friendly) (1\u20132 days)"}, {"line": 227, "text": "Phase 1.1 \u2014 Linux/CUDA GPU index/datastore (1\u20132 days)"}, {"line": 242, "text": "Phase 1.2 \u2014 GUI toggle + early-stopping evaluation (1\u20132 days)"}, {"line": 257, "text": "Phase 2 \u2014 Training modes and encoder\u2013decoder support (1\u20132 weeks)"}, {"line": 274, "text": "Phase 3 \u2014 ACTV1 HRM exploration (exploratory, timeboxed)"}, {"line": 288, "text": "Phase 4 \u2014 Hardening, docs, and demos (2\u20134 days)"}, {"line": 304, "text": "Testing matrix (summary)"}]}, {"path": "planned_features/ZERO_INFERENCE_INTEGRATION.md", "content": "# DeepSpeed ZeRO-Inference Integration Plan\n\n## Executive Summary\n\n**Goal**: Integrate DeepSpeed ZeRO-Inference to enable inference of massive models (100B+ parameters) on consumer hardware with limited GPU memory by offloading model weights to CPU/NVMe while streaming layers to GPU for computation.\n\n**Current State**: AI-OS supports standard inference that requires the entire model to fit in GPU memory. Large models (>10B params) require multiple high-end GPUs or cannot run at all on consumer hardware.\n\n**Target State**: Full ZeRO-Inference support enabling:\n- Single GPU inference of models up to 15 trillion parameters (with NVMe)\n- Throughput-oriented inference with large batch sizes\n- CPU and NVMe offloading with automatic layer streaming\n- Intelligent prefetching and multi-GPU parallelization\n\n**Impact**: \n- Run 100B+ parameter models on a single consumer GPU (11-24GB)\n- Democratize access to SOTA models (GPT-3, BLOOM, OPT-175B, etc.)\n- Enable inference on models 1000x larger than GPU memory allows\n- Trade ~20-40% speed for 90%+ memory savings\n\n---\n\n## Background\n\n### What is ZeRO-Inference?\n\nZeRO-Inference is DeepSpeed's technology for **massive model inference on limited GPU hardware** by:\n\n1. **Offloading model weights** to CPU DRAM or NVMe storage\n2. **Streaming layers** into GPU memory just-in-time for computation\n3. **Using GPU memory** primarily for activations and large batch sizes\n4. **Prefetching layers** ahead of time to hide transfer latency\n5. **Parallelizing fetches** across multiple GPUs for better bandwidth\n\n**Key Innovation**: Keep only 1-2 model layers in GPU memory at a time (~1% of model), use the freed memory for large batches to maximize throughput and hide transfer latency.\n\n### Memory Hierarchy\n\n```\nGPU Memory (32GB)\n\u251c\u2500 Active Layer Weights: ~1GB (1-2 layers of 100B model)\n\u251c\u2500 Activations: ~20GB (batch_size=64, seq_len=2048)\n\u2514\u2500 KV Cache: ~10GB (for text generation)\n\nCPU Memory (64GB) or NVMe (2TB)\n\u2514\u2500 Full Model Weights: ~200GB (100B params \u00d7 2 bytes fp16)\n```\n\n### Current AI-OS Capabilities\n\n\u2705 **Already Implemented**:\n- Standard inference (model fully in GPU memory)\n- Multi-GPU inference via tensor parallelism\n- 8-bit/4-bit quantization for compression\n- Flash Attention for memory-efficient attention\n- Dynamic batching for throughput\n\n\u274c **Missing for ZeRO-Inference**:\n- Weight offloading to CPU/NVMe\n- Layer-by-layer streaming to GPU\n- Prefetching pipeline for hiding latency\n- Multi-GPU fetch parallelization\n- Throughput-oriented batch size optimization\n- Integration with HuggingFace generation pipeline\n- Memory-aware batch size auto-tuning\n\n---\n\n## Technical Architecture\n\n### Inference Flow with ZeRO-Inference\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             Token Generation with ZeRO-Inference              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                \u2502\n\u2502  Phase 1: Prompt Processing (Process input prompt)            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 For each layer (0 to N):                             \u2502   \u2502\n\u2502  \u2502   1. Prefetch layer[i+1] from CPU/NVMe (async)       \u2502   \u2502\n\u2502  \u2502   2. Load layer[i] to GPU (if not prefetched)        \u2502   \u2502\n\u2502  \u2502   3. Compute activations on GPU                      \u2502   \u2502\n\u2502  \u2502   4. Free layer[i] weights from GPU                  \u2502   \u2502\n\u2502  \u2502   5. Keep activations for next layer                 \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                \u2502\n\u2502  Phase 2: Token Generation (Generate output tokens)           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 For each output token (0 to max_tokens):             \u2502   \u2502\n\u2502  \u2502   For each layer (0 to N):                           \u2502   \u2502\n\u2502  \u2502     1. Prefetch layer[i+1] (async)                   \u2502   \u2502\n\u2502  \u2502     2. Load layer[i] to GPU                          \u2502   \u2502\n\u2502  \u2502     3. Compute next token logits                     \u2502   \u2502\n\u2502  \u2502     4. Update KV cache                               \u2502   \u2502\n\u2502  \u2502     5. Free layer[i] weights                         \u2502   \u2502\n\u2502  \u2502   Sample next token from logits                      \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Multi-GPU Fetch Parallelization\n\nWhen using multiple GPUs, ZeRO-Inference parallelizes layer fetching:\n\n```\nSingle GPU (PCIe 3.0 @ 15 GB/s):\n\u251c\u2500 Fetch Layer: 6GB \u00f7 15 GB/s = 400ms\n\u2514\u2500 Compute Layer: batch_size=64 \u2192 300ms\n    Total: 700ms per layer (fetch dominates!)\n\n4x GPUs (4\u00d7 PCIe 3.0 @ 60 GB/s aggregate):\n\u251c\u2500 Each GPU fetches 1.5GB \u00f7 15 GB/s = 100ms\n\u251c\u2500 All-gather across GPUs (NVLink @ 300 GB/s): ~20ms\n\u2514\u2500 Compute Layer: 300ms\n    Total: 420ms per layer (40% faster!)\n```\n\n### Prefetching Optimization\n\n```\nWithout Prefetching:\n[Fetch L0] \u2192 [Compute L0] \u2192 [Fetch L1] \u2192 [Compute L1] \u2192 ...\n \u2191400ms\u2191     \u2191300ms\u2191       \u2191400ms\u2191      \u2191300ms\u2191\nTotal: 700ms \u00d7 N layers\n\nWith Prefetching:\n[Fetch L0] \u2192 [Compute L0] \u2192 [Compute L1] \u2192 [Compute L2] \u2192 ...\n \u2191400ms\u2191     [Fetch L1]\u2191   [Fetch L2]\u2191\n             \u2191300ms\u2191        \u2191300ms\u2191\nTotal: 400ms + (300ms \u00d7 N layers)  \u2190 Hides fetch latency!\n```\n\n---\n\n## Implementation Plan\n\n### Phase 1: Core Infrastructure (Week 1-2)\n\n#### 1.1 Configuration Schema\n\n**File**: `config/deepspeed_zero_inference.json`\n\n```json\n{\n  \"zero_optimization\": {\n    \"stage\": 3,\n    \n    \"offload_param\": {\n      \"device\": \"cpu\",\n      \"pin_memory\": true,\n      \"buffer_count\": 5,\n      \"buffer_size\": 100000000,\n      \"max_in_cpu\": 1000000000\n    }\n  },\n  \n  \"aio\": {\n    \"block_size\": 1048576,\n    \"queue_depth\": 8,\n    \"thread_count\": 1,\n    \"single_submit\": false,\n    \"overlap_events\": true\n  },\n  \n  \"fp16\": {\n    \"enabled\": true\n  },\n  \n  \"zero_inference\": {\n    \"enabled\": true,\n    \"offload_device\": \"cpu\",\n    \"prefetch_layers\": 2,\n    \"pin_memory\": true,\n    \"parallel_fetch\": true\n  }\n}\n```\n\n**File**: `config/deepspeed_zero_inference_nvme.json`\n\n```json\n{\n  \"zero_optimization\": {\n    \"stage\": 3,\n    \n    \"offload_param\": {\n      \"device\": \"nvme\",\n      \"nvme_path\": \"/tmp/deepspeed_inference_offload\",\n      \"pin_memory\": true,\n      \"buffer_count\": 5,\n      \"buffer_size\": 100000000,\n      \"max_in_cpu\": 1000000000\n    }\n  },\n  \n  \"aio\": {\n    \"block_size\": 1048576,\n    \"queue_depth\": 16,\n    \"thread_count\": 2,\n    \"single_submit\": false,\n    \"overlap_events\": true\n  },\n  \n  \"fp16\": {\n    \"enabled\": true\n  },\n  \n  \"zero_inference\": {\n    \"enabled\": true,\n    \"offload_device\": \"nvme\",\n    \"prefetch_layers\": 3,\n    \"pin_memory\": true,\n    \"parallel_fetch\": true\n  }\n}\n```\n\n#### 1.2 Inference Configuration Fields\n\n**File**: `src/aios/core/inference/inference_config.py` (new)\n\n```python\n\"\"\"Configuration for model inference with ZeRO-Inference support.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Literal, Optional\n\n\n@dataclass\nclass InferenceConfig:\n    \"\"\"Configuration for running inference with optional ZeRO-Inference.\"\"\"\n    \n    # Model configuration\n    model_path: str\n    \"\"\"Path to model checkpoint or HuggingFace model ID.\"\"\"\n    \n    device: str = \"cuda\"\n    \"\"\"Device to run inference on: 'cuda', 'cpu', or 'cuda:0'.\"\"\"\n    \n    dtype: Literal[\"fp16\", \"bf16\", \"fp32\"] = \"fp16\"\n    \"\"\"Data type for model weights and computation.\"\"\"\n    \n    # ZeRO-Inference settings\n    use_zero_inference: bool = False\n    \"\"\"Enable ZeRO-Inference for massive models.\n    \n    When enabled, model weights are offloaded to CPU or NVMe and streamed\n    to GPU layer-by-layer. Essential for models that don't fit in GPU memory.\n    \n    Use Cases:\n    - Models >10B params on single GPU\n    - Models >100B params on multi-GPU\n    - Maximizing batch size for throughput\n    \n    Trade-offs:\n    - Memory: 90%+ GPU memory freed\n    - Speed: ~20-40% slower (depends on offload device)\n    - Latency: Higher per-token latency (not suitable for real-time)\n    \n    Default: False (standard inference)\n    \"\"\"\n    \n    offload_device: Literal[\"cpu\", \"nvme\"] = \"cpu\"\n    \"\"\"Device to offload model weights to.\n    \n    Options:\n    - 'cpu': Offload to CPU DRAM (faster, ~20% slowdown)\n    - 'nvme': Offload to NVMe storage (slower, ~40% slowdown, unlimited capacity)\n    \n    CPU Offload:\n    - Speed: ~20% slower than GPU-only\n    - Capacity: Limited by RAM (~64-256GB typical)\n    - Best for: Models up to ~100B params\n    \n    NVMe Offload:\n    - Speed: ~40% slower than GPU-only\n    - Capacity: Limited by SSD (~1-4TB typical)\n    - Best for: Models 100B-15T params\n    \n    Requires: Fast NVMe SSD (>2 GB/s) for reasonable performance\n    \n    Default: 'cpu'\n    \"\"\"\n    \n    nvme_offload_path: str = \"/tmp/deepspeed_inference_offload\"\n    \"\"\"Path to NVMe directory for weight offloading.\n    \n    Only used when offload_device='nvme'. Must have sufficient free space:\n    - FP16: model_params \u00d7 2 bytes\n    - FP32: model_params \u00d7 4 bytes\n    \n    Example space requirements:\n    - 10B params (fp16): ~20GB\n    - 100B params (fp16): ~200GB\n    - 1T params (fp16): ~2TB\n    \n    Recommended: Fast NVMe SSD on PCIe 3.0+ with >2 GB/s write speed\n    \n    Default: '/tmp/deepspeed_inference_offload'\n    \"\"\"\n    \n    prefetch_layers: int = 2\n    \"\"\"Number of layers to prefetch ahead of computation.\n    \n    Prefetching overlaps layer transfer with computation to hide latency.\n    Higher values reduce stalls but use more CPU/NVMe bandwidth.\n    \n    Recommended values:\n    - 1: Minimal prefetch, lowest memory\n    - 2: Balanced (default)\n    - 3-4: Aggressive prefetch for fast NVMe\n    \n    Effect on performance:\n    - prefetch=0: ~40% slower (no overlap)\n    - prefetch=2: ~20% slower (good overlap)\n    - prefetch=4: ~15% slower (maximum overlap)\n    \n    Default: 2\n    \"\"\"\n    \n    pin_memory: bool = True\n    \"\"\"Use pinned (page-locked) CPU memory for faster GPU transfers.\n    \n    Pinned memory enables DMA transfers without CPU involvement,\n    reducing latency for CPU\u2192GPU transfers by ~30%.\n    \n    Impact:\n    - Speed: ~30% faster transfers\n    - Memory: Uses non-swappable RAM\n    - Stability: May fail with low RAM (<16GB)\n    \n    Disable if: System has <16GB RAM or memory errors occur\n    \n    Default: True\n    \"\"\"\n    \n    parallel_fetch: bool = True\n    \"\"\"Parallelize layer fetching across multiple GPUs.\n    \n    When using multiple GPUs, each GPU fetches a portion of the layer,\n    then assembles the full layer via all-gather. This parallelizes PCIe\n    bandwidth across GPUs.\n    \n    Example (4 GPUs):\n    - Serial: 1 GPU fetches 4GB \u2192 4GB \u00f7 15 GB/s = 267ms\n    - Parallel: 4 GPUs fetch 1GB each \u2192 1GB \u00f7 15 GB/s + all-gather = 90ms\n    \n    Speedup: 4\u00d7 PCIe bandwidth \u2192 ~3\u00d7 faster layer loading\n    \n    Requires: Multiple GPUs with high-speed interconnect (NVLink/PCIe)\n    \n    Default: True (auto-disabled if single GPU)\n    \"\"\"\n    \n    # Generation settings\n    max_batch_size: Optional[int] = None\n    \"\"\"Maximum batch size for inference.\n    \n    ZeRO-Inference performs best with large batches to hide transfer latency.\n    If None, automatically determined based on available GPU memory.\n    \n    Typical values:\n    - Standard inference: 8-32\n    - ZeRO-Inference (CPU): 64-256\n    - ZeRO-Inference (NVMe): 128-512\n    \n    Larger batches \u2192 higher throughput but higher latency\n    \n    Default: None (auto-tuned)\n    \"\"\"\n    \n    max_tokens: int = 128\n    \"\"\"Maximum number of tokens to generate.\"\"\"\n    \n    temperature: float = 1.0\n    \"\"\"Sampling temperature for generation.\"\"\"\n    \n    top_p: float = 0.9\n    \"\"\"Top-p (nucleus) sampling parameter.\"\"\"\n    \n    top_k: int = 50\n    \"\"\"Top-k sampling parameter.\"\"\"\n    \n    # Performance tuning\n    aio_block_size: int = 1048576\n    \"\"\"Async I/O block size for NVMe operations (bytes).\n    \n    Only used when offload_device='nvme'.\n    \n    Recommended:\n    - 1048576 (1MB): Balanced (default)\n    - 2097152 (2MB): High throughput NVMe\n    - 524288 (512KB): Lower latency\n    \n    Default: 1048576\n    \"\"\"\n    \n    aio_queue_depth: int = 8\n    \"\"\"Async I/O queue depth for NVMe operations.\n    \n    Only used when offload_device='nvme'.\n    \n    Recommended:\n    - 8: Balanced (default)\n    - 16: High-performance NVMe\n    - 32: Server-grade NVMe\n    \n    Default: 8\n    \"\"\"\n    \n    use_flash_attention: bool = True\n    \"\"\"Use Flash Attention for memory-efficient attention computation.\n    \n    Flash Attention reduces memory usage and improves speed for long contexts.\n    Compatible with ZeRO-Inference.\n    \n    Default: True (if available)\n    \"\"\"\n    \n    compile_model: bool = False\n    \"\"\"Use torch.compile() for faster computation.\n    \n    May improve compute speed by ~20-30% but increases startup time.\n    \n    Compatibility: Works with ZeRO-Inference but limited benefit\n    (bottleneck is transfer, not compute).\n    \n    Default: False\n    \"\"\"\n```\n\n#### 1.3 Layer Streaming Engine\n\n**File**: `src/aios/core/inference/layer_streaming.py` (new)\n\n```python\n\"\"\"Layer streaming engine for ZeRO-Inference.\"\"\"\n\nfrom __future__ import annotations\nimport torch\nimport asyncio\nfrom typing import Dict, List, Optional, Any\nfrom collections import deque\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass LayerCache:\n    \"\"\"Cache for prefetched model layers.\"\"\"\n    \n    def __init__(\n        self,\n        max_size: int = 3,\n        pin_memory: bool = True,\n        device: torch.device = torch.device(\"cuda\")\n    ):\n        self.max_size = max_size\n        self.pin_memory = pin_memory\n        self.device = device\n        self.cache: Dict[int, Dict[str, torch.Tensor]] = {}\n        self.queue: deque[int] = deque(maxlen=max_size)\n    \n    def put(self, layer_idx: int, layer_weights: Dict[str, torch.Tensor]) -> None:\n        \"\"\"Add layer weights to cache.\"\"\"\n        if len(self.cache) >= self.max_size:\n            # Evict oldest layer\n            if self.queue:\n                evict_idx = self.queue.popleft()\n                if evict_idx in self.cache:\n                    del self.cache[evict_idx]\n        \n        self.cache[layer_idx] = layer_weights\n        if layer_idx not in self.queue:\n            self.queue.append(layer_idx)\n    \n    def get(self, layer_idx: int) -> Optional[Dict[str, torch.Tensor]]:\n        \"\"\"Retrieve layer weights from cache.\"\"\"\n        return self.cache.get(layer_idx)\n    \n    def has(self, layer_idx: int) -> bool:\n        \"\"\"Check if layer is in cache.\"\"\"\n        return layer_idx in self.cache\n    \n    def clear(self) -> None:\n        \"\"\"Clear entire cache.\"\"\"\n        self.cache.clear()\n        self.queue.clear()\n\n\nclass LayerStreamer:\n    \"\"\"Streams model layers from CPU/NVMe to GPU for inference.\"\"\"\n    \n    def __init__(\n        self,\n        model: torch.nn.Module,\n        config: \"InferenceConfig\",\n        offload_manager: \"OffloadManager\",\n    ):\n        self.model = model\n        self.config = config\n        self.offload_manager = offload_manager\n        self.device = torch.device(config.device)\n        \n        # Layer cache for prefetching\n        self.cache = LayerCache(\n            max_size=config.prefetch_layers + 1,\n            pin_memory=config.pin_memory,\n            device=self.device,\n        )\n        \n        # Track which layers are currently on GPU\n        self.gpu_layers: set[int] = set()\n        \n        # Prefetch queue\n        self.prefetch_queue: asyncio.Queue = asyncio.Queue()\n        self.prefetch_task: Optional[asyncio.Task] = None\n    \n    async def prefetch_layer(self, layer_idx: int) -> None:\n        \"\"\"Prefetch a layer from CPU/NVMe to cache.\"\"\"\n        if self.cache.has(layer_idx):\n            return\n        \n        logger.debug(f\"Prefetching layer {layer_idx}\")\n        \n        # Load layer weights from offload manager\n        layer_weights = await self.offload_manager.load_layer(layer_idx)\n        \n        # Store in cache\n        self.cache.put(layer_idx, layer_weights)\n        \n        logger.debug(f\"Layer {layer_idx} prefetched successfully\")\n    \n    async def load_layer_to_gpu(self, layer_idx: int) -> None:\n        \"\"\"Load a layer to GPU memory.\"\"\"\n        # Check cache first\n        if self.cache.has(layer_idx):\n            layer_weights = self.cache.get(layer_idx)\n        else:\n            # Not in cache, load directly (blocking)\n            logger.warning(f\"Cache miss for layer {layer_idx}, loading directly\")\n            layer_weights = await self.offload_manager.load_layer(layer_idx)\n        \n        # Transfer to GPU\n        layer_module = self._get_layer_module(layer_idx)\n        \n        for name, param in layer_module.named_parameters():\n            if name in layer_weights:\n                param.data = layer_weights[name].to(\n                    self.device,\n                    non_blocking=self.config.pin_memory\n                )\n        \n        self.gpu_layers.add(layer_idx)\n        logger.debug(f\"Layer {layer_idx} loaded to GPU\")\n    \n    def unload_layer_from_gpu(self, layer_idx: int) -> None:\n        \"\"\"Free layer weights from GPU memory.\"\"\"\n        if layer_idx not in self.gpu_layers:\n            return\n        \n        layer_module = self._get_layer_module(layer_idx)\n        \n        # Move parameters to empty tensors (free GPU memory)\n        for param in layer_module.parameters():\n            param.data = torch.empty(0, device=self.device)\n        \n        self.gpu_layers.remove(layer_idx)\n        logger.debug(f\"Layer {layer_idx} unloaded from GPU\")\n    \n    def _get_layer_module(self, layer_idx: int) -> torch.nn.Module:\n        \"\"\"Get the layer module by index.\"\"\"\n        # Assumes model has .layers attribute (common for transformers)\n        # May need to adapt for different architectures\n        if hasattr(self.model, \"layers\"):\n            return self.model.layers[layer_idx]\n        elif hasattr(self.model, \"transformer\") and hasattr(self.model.transformer, \"h\"):\n            return self.model.transformer.h[layer_idx]\n        elif hasattr(self.model, \"model\") and hasattr(self.model.model, \"layers\"):\n            return self.model.model.layers[layer_idx]\n        else:\n            raise AttributeError(f\"Cannot find layer {layer_idx} in model\")\n    \n    async def forward_with_streaming(\n        self,\n        input_ids: torch.Tensor,\n        num_layers: int,\n    ) -> torch.Tensor:\n        \"\"\"Run forward pass with layer streaming.\"\"\"\n        # Get number of layers\n        if num_layers is None:\n            num_layers = self._get_num_layers()\n        \n        # Initial embedding\n        hidden_states = self._embed_tokens(input_ids)\n        \n        # Process each layer\n        for layer_idx in range(num_layers):\n            # Prefetch next layers\n            for prefetch_offset in range(1, self.config.prefetch_layers + 1):\n                next_idx = layer_idx + prefetch_offset\n                if next_idx < num_layers:\n                    asyncio.create_task(self.prefetch_layer(next_idx))\n            \n            # Load current layer to GPU\n            await self.load_layer_to_gpu(layer_idx)\n            \n            # Forward pass through layer\n            layer_module = self._get_layer_module(layer_idx)\n            hidden_states = layer_module(hidden_states)\n            \n            # Unload previous layer if not needed\n            if layer_idx > 0:\n                self.unload_layer_from_gpu(layer_idx - 1)\n        \n        return hidden_states\n    \n    def _get_num_layers(self) -> int:\n        \"\"\"Get number of layers in model.\"\"\"\n        if hasattr(self.model, \"config\"):\n            return getattr(self.model.config, \"num_hidden_layers\", \n                          getattr(self.model.config, \"n_layer\", 12))\n        return 12  # Default fallback\n    \n    def _embed_tokens(self, input_ids: torch.Tensor) -> torch.Tensor:\n        \"\"\"Get initial token embeddings.\"\"\"\n        if hasattr(self.model, \"embed_tokens\"):\n            return self.model.embed_tokens(input_ids)\n        elif hasattr(self.model, \"transformer\") and hasattr(self.model.transformer, \"wte\"):\n            return self.model.transformer.wte(input_ids)\n        elif hasattr(self.model, \"model\") and hasattr(self.model.model, \"embed_tokens\"):\n            return self.model.model.embed_tokens(input_ids)\n        else:\n            raise AttributeError(\"Cannot find embedding layer in model\")\n```\n\n#### 1.4 Offload Manager\n\n**File**: `src/aios/core/inference/offload_manager.py` (new)\n\n```python\n\"\"\"Manager for offloading model weights to CPU/NVMe.\"\"\"\n\nfrom __future__ import annotations\nimport torch\nimport asyncio\nimport pickle\nfrom pathlib import Path\nfrom typing import Dict, Optional, Literal\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass OffloadManager:\n    \"\"\"Manages offloading and loading of model weights.\"\"\"\n    \n    def __init__(\n        self,\n        model: torch.nn.Module,\n        device: Literal[\"cpu\", \"nvme\"],\n        nvme_path: Optional[str] = None,\n        pin_memory: bool = True,\n    ):\n        self.model = model\n        self.device = device\n        self.nvme_path = Path(nvme_path) if nvme_path else None\n        self.pin_memory = pin_memory\n        \n        # Storage for offloaded weights\n        self.cpu_storage: Dict[int, Dict[str, torch.Tensor]] = {}\n        self.nvme_files: Dict[int, Path] = {}\n        \n        if device == \"nvme\" and self.nvme_path:\n            self.nvme_path.mkdir(parents=True, exist_ok=True)\n    \n    async def offload_all_layers(self, num_layers: int) -> None:\n        \"\"\"Offload all model layers to CPU or NVMe.\"\"\"\n        logger.info(f\"Offloading {num_layers} layers to {self.device}\")\n        \n        for layer_idx in range(num_layers):\n            await self.offload_layer(layer_idx)\n        \n        logger.info(f\"All layers offloaded to {self.device}\")\n    \n    async def offload_layer(self, layer_idx: int) -> None:\n        \"\"\"Offload a single layer to CPU or NVMe.\"\"\"\n        layer_module = self._get_layer_module(layer_idx)\n        \n        # Extract weights\n        layer_weights = {}\n        for name, param in layer_module.named_parameters():\n            if self.device == \"cpu\":\n                # Move to CPU with optional pinning\n                cpu_tensor = param.data.cpu()\n                if self.pin_memory:\n                    cpu_tensor = cpu_tensor.pin_memory()\n                layer_weights[name] = cpu_tensor\n            else:\n                # Save to NVMe\n                layer_weights[name] = param.data.cpu()\n        \n        if self.device == \"cpu\":\n            self.cpu_storage[layer_idx] = layer_weights\n        else:\n            # Save to disk\n            file_path = self.nvme_path / f\"layer_{layer_idx}.pt\"\n            torch.save(layer_weights, file_path)\n            self.nvme_files[layer_idx] = file_path\n        \n        # Free GPU memory\n        for param in layer_module.parameters():\n            param.data = torch.empty(0)\n        \n        logger.debug(f\"Layer {layer_idx} offloaded to {self.device}\")\n    \n    async def load_layer(self, layer_idx: int) -> Dict[str, torch.Tensor]:\n        \"\"\"Load layer weights from CPU or NVMe.\"\"\"\n        if self.device == \"cpu\":\n            return self.cpu_storage.get(layer_idx, {})\n        else:\n            # Load from NVMe\n            file_path = self.nvme_files.get(layer_idx)\n            if file_path and file_path.exists():\n                # Use asyncio to avoid blocking\n                loop = asyncio.get_event_loop()\n                return await loop.run_in_executor(\n                    None,\n                    torch.load,\n                    file_path\n                )\n            return {}\n    \n    def _get_layer_module(self, layer_idx: int) -> torch.nn.Module:\n        \"\"\"Get layer module by index.\"\"\"\n        # Same logic as LayerStreamer\n        if hasattr(self.model, \"layers\"):\n            return self.model.layers[layer_idx]\n        elif hasattr(self.model, \"transformer\") and hasattr(self.model.transformer, \"h\"):\n            return self.model.transformer.h[layer_idx]\n        elif hasattr(self.model, \"model\") and hasattr(self.model.model, \"layers\"):\n            return self.model.model.layers[layer_idx]\n        else:\n            raise AttributeError(f\"Cannot find layer {layer_idx} in model\")\n    \n    def cleanup(self) -> None:\n        \"\"\"Clean up offloaded weights.\"\"\"\n        self.cpu_storage.clear()\n        \n        if self.device == \"nvme\" and self.nvme_path:\n            # Delete NVMe files\n            for file_path in self.nvme_files.values():\n                if file_path.exists():\n                    file_path.unlink()\n            self.nvme_files.clear()\n```\n\n---\n\n### Phase 2: HuggingFace Integration (Week 2-3)\n\n#### 2.1 Generation Pipeline Wrapper\n\n**File**: `src/aios/core/inference/zero_inference_pipeline.py` (new)\n\n```python\n\"\"\"HuggingFace generation pipeline with ZeRO-Inference support.\"\"\"\n\nfrom __future__ import annotations\nimport torch\nfrom transformers import GenerationMixin\nfrom typing import Optional, Dict, Any, List\nimport logging\n\nfrom .layer_streaming import LayerStreamer\nfrom .offload_manager import OffloadManager\nfrom .inference_config import InferenceConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass ZeroInferencePipeline:\n    \"\"\"Text generation pipeline with ZeRO-Inference.\"\"\"\n    \n    def __init__(\n        self,\n        model: torch.nn.Module,\n        tokenizer: Any,\n        config: InferenceConfig,\n    ):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.config = config\n        self.device = torch.device(config.device)\n        \n        # Initialize offload manager\n        self.offload_manager = OffloadManager(\n            model=model,\n            device=config.offload_device,\n            nvme_path=config.nvme_offload_path if config.offload_device == \"nvme\" else None,\n            pin_memory=config.pin_memory,\n        )\n        \n        # Initialize layer streamer\n        self.streamer = LayerStreamer(\n            model=model,\n            config=config,\n            offload_manager=self.offload_manager,\n        )\n        \n        # Determine num layers\n        self.num_layers = self._get_num_layers()\n        \n        # Offload all layers initially\n        import asyncio\n        asyncio.run(self.offload_manager.offload_all_layers(self.num_layers))\n        \n        logger.info(\n            f\"ZeroInferencePipeline initialized: \"\n            f\"{self.num_layers} layers offloaded to {config.offload_device}\"\n        )\n    \n    def generate(\n        self,\n        prompts: List[str],\n        max_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        top_k: Optional[int] = None,\n        **kwargs\n    ) -> List[str]:\n        \"\"\"Generate text from prompts using ZeRO-Inference.\"\"\"\n        max_tokens = max_tokens or self.config.max_tokens\n        temperature = temperature or self.config.temperature\n        top_p = top_p or self.config.top_p\n        top_k = top_k or self.config.top_k\n        \n        # Tokenize inputs\n        inputs = self.tokenizer(\n            prompts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n        ).to(self.device)\n        \n        # Generate with streaming\n        import asyncio\n        outputs = asyncio.run(self._generate_async(\n            input_ids=inputs.input_ids,\n            attention_mask=inputs.attention_mask,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n        ))\n        \n        # Decode outputs\n        generated_texts = self.tokenizer.batch_decode(\n            outputs,\n            skip_special_tokens=True\n        )\n        \n        return generated_texts\n    \n    async def _generate_async(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        max_tokens: int,\n        temperature: float,\n        top_p: float,\n        top_k: int,\n    ) -> torch.Tensor:\n        \"\"\"Async generation with layer streaming.\"\"\"\n        batch_size = input_ids.shape[0]\n        \n        # Phase 1: Process prompt (prefill)\n        hidden_states = await self.streamer.forward_with_streaming(\n            input_ids=input_ids,\n            num_layers=self.num_layers,\n        )\n        \n        # Get initial logits\n        logits = self._get_logits(hidden_states)\n        next_tokens = self._sample_tokens(logits[:, -1, :], temperature, top_p, top_k)\n        \n        # Initialize output with prompt + first token\n        output_ids = torch.cat([input_ids, next_tokens.unsqueeze(1)], dim=1)\n        \n        # Phase 2: Auto-regressive generation\n        for _ in range(max_tokens - 1):\n            # Process only the new token through all layers\n            new_token_ids = next_tokens.unsqueeze(1)\n            hidden_states = await self.streamer.forward_with_streaming(\n                input_ids=new_token_ids,\n                num_layers=self.num_layers,\n            )\n            \n            # Get next token\n            logits = self._get_logits(hidden_states)\n            next_tokens = self._sample_tokens(logits[:, -1, :], temperature, top_p, top_k)\n            \n            # Append to output\n            output_ids = torch.cat([output_ids, next_tokens.unsqueeze(1)], dim=1)\n            \n            # Check for EOS\n            if (next_tokens == self.tokenizer.eos_token_id).all():\n                break\n        \n        return output_ids\n    \n    def _get_logits(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        \"\"\"Get logits from hidden states.\"\"\"\n        if hasattr(self.model, \"lm_head\"):\n            return self.model.lm_head(hidden_states)\n        elif hasattr(self.model, \"score\"):\n            return self.model.score(hidden_states)\n        else:\n            raise AttributeError(\"Cannot find output head in model\")\n    \n    def _sample_tokens(\n        self,\n        logits: torch.Tensor,\n        temperature: float,\n        top_p: float,\n        top_k: int,\n    ) -> torch.Tensor:\n        \"\"\"Sample next tokens from logits.\"\"\"\n        # Apply temperature\n        logits = logits / temperature\n        \n        # Top-k filtering\n        if top_k > 0:\n            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n            logits[indices_to_remove] = float('-inf')\n        \n        # Top-p filtering\n        if top_p < 1.0:\n            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n            \n            sorted_indices_to_remove = cumulative_probs > top_p\n            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n            sorted_indices_to_remove[..., 0] = False\n            \n            indices_to_remove = sorted_indices_to_remove.scatter(\n                1, sorted_indices, sorted_indices_to_remove\n            )\n            logits[indices_to_remove] = float('-inf')\n        \n        # Sample\n        probs = torch.softmax(logits, dim=-1)\n        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n        \n        return next_tokens\n    \n    def _get_num_layers(self) -> int:\n        \"\"\"Get number of layers.\"\"\"\n        if hasattr(self.model, \"config\"):\n            return getattr(\n                self.model.config,\n                \"num_hidden_layers\",\n                getattr(self.model.config, \"n_layer\", 12)\n            )\n        return 12\n    \n    def cleanup(self) -> None:\n        \"\"\"Cleanup resources.\"\"\"\n        self.offload_manager.cleanup()\n        self.streamer.cache.clear()\n```\n\n#### 2.2 CLI Integration\n\n**File**: `src/aios/cli/inference_cli.py` (new)\n\n```python\n\"\"\"CLI for running inference with ZeRO-Inference support.\"\"\"\n\nimport typer\nfrom pathlib import Path\nfrom typing import Optional, List\nimport logging\n\napp = typer.Typer(help=\"Run model inference with ZeRO-Inference\")\nlogger = logging.getLogger(__name__)\n\n\n@app.command()\ndef generate(\n    model: str = typer.Argument(..., help=\"Model path or HuggingFace ID\"),\n    prompts: List[str] = typer.Option(..., \"--prompt\", \"-p\", help=\"Input prompts\"),\n    \n    # Output\n    output_file: Optional[Path] = typer.Option(\n        None, \"--output\", \"-o\",\n        help=\"Save generated text to file\"\n    ),\n    \n    # Generation params\n    max_tokens: int = typer.Option(128, \"--max-tokens\", help=\"Max tokens to generate\"),\n    temperature: float = typer.Option(1.0, \"--temperature\", help=\"Sampling temperature\"),\n    top_p: float = typer.Option(0.9, \"--top-p\", help=\"Top-p sampling\"),\n    top_k: int = typer.Option(50, \"--top-k\", help=\"Top-k sampling\"),\n    \n    # ZeRO-Inference\n    use_zero_inference: bool = typer.Option(\n        False, \"--zero-inference/--standard\",\n        help=\"Enable ZeRO-Inference for massive models\"\n    ),\n    offload_device: str = typer.Option(\n        \"cpu\", \"--offload-device\",\n        help=\"Offload device: 'cpu' or 'nvme'\"\n    ),\n    nvme_path: str = typer.Option(\n        \"/tmp/deepspeed_inference_offload\",\n        \"--nvme-path\",\n        help=\"NVMe offload directory\"\n    ),\n    prefetch_layers: int = typer.Option(\n        2, \"--prefetch-layers\",\n        help=\"Number of layers to prefetch\"\n    ),\n    \n    # Device\n    device: str = typer.Option(\"cuda\", \"--device\", help=\"Device: 'cuda' or 'cpu'\"),\n    dtype: str = typer.Option(\"fp16\", \"--dtype\", help=\"Data type: 'fp16', 'bf16', 'fp32'\"),\n):\n    \"\"\"Generate text from prompts using ZeRO-Inference.\"\"\"\n    \n    from aios.core.inference.inference_config import InferenceConfig\n    from aios.core.inference.zero_inference_pipeline import ZeroInferencePipeline\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n    import torch\n    \n    # Load model and tokenizer\n    typer.echo(f\"Loading model: {model}\")\n    tokenizer = AutoTokenizer.from_pretrained(model)\n    \n    if use_zero_inference:\n        # Load model to CPU first\n        model_obj = AutoModelForCausalLM.from_pretrained(\n            model,\n            torch_dtype=getattr(torch, dtype),\n            low_cpu_mem_usage=True,\n        )\n        \n        # Create config\n        config = InferenceConfig(\n            model_path=model,\n            device=device,\n            dtype=dtype,\n            use_zero_inference=True,\n            offload_device=offload_device,\n            nvme_offload_path=nvme_path,\n            prefetch_layers=prefetch_layers,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n        )\n        \n        # Create pipeline\n        typer.echo(f\"Initializing ZeRO-Inference (offload to {offload_device})\")\n        pipeline = ZeroInferencePipeline(\n            model=model_obj,\n            tokenizer=tokenizer,\n            config=config,\n        )\n        \n        # Generate\n        typer.echo(\"Generating...\")\n        outputs = pipeline.generate(prompts)\n        \n    else:\n        # Standard inference\n        model_obj = AutoModelForCausalLM.from_pretrained(\n            model,\n            torch_dtype=getattr(torch, dtype),\n        ).to(device)\n        \n        typer.echo(\"Generating (standard inference)...\")\n        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n        outputs_ids = model_obj.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n        )\n        outputs = tokenizer.batch_decode(outputs_ids, skip_special_tokens=True)\n    \n    # Print outputs\n    typer.echo(\"\\n\" + \"=\" * 80)\n    for i, output in enumerate(outputs):\n        typer.echo(f\"\\nPrompt {i+1}: {prompts[i]}\")\n        typer.echo(f\"Generated: {output}\")\n        typer.echo(\"=\" * 80)\n    \n    # Save to file\n    if output_file:\n        output_file.write_text(\"\\n\\n\".join(outputs))\n        typer.echo(f\"\\nSaved to {output_file}\")\n\n\nif __name__ == \"__main__\":\n    app()\n```\n\n---\n\n### Phase 3: Performance Optimization (Week 3-4)\n\n#### 3.1 Batch Size Auto-Tuning\n\n**File**: `src/aios/core/inference/batch_size_tuner.py` (new)\n\n```python\n\"\"\"Auto-tune batch size for ZeRO-Inference.\"\"\"\n\nimport torch\nfrom typing import Tuple\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\ndef estimate_max_batch_size(\n    model: torch.nn.Module,\n    seq_length: int,\n    available_vram_gb: float,\n    use_zero_inference: bool = True,\n    safety_margin: float = 0.9,\n) -> int:\n    \"\"\"Estimate maximum batch size given constraints.\"\"\"\n    \n    # Get model size\n    num_params = sum(p.numel() for p in model.parameters())\n    \n    if use_zero_inference:\n        # With ZeRO-Inference, only 1-2 layers in VRAM\n        model_vram_gb = (num_params * 2) / (1024**3) * 0.05  # ~5% of model\n    else:\n        # Standard inference: full model in VRAM\n        model_vram_gb = (num_params * 2) / (1024**3)  # FP16\n    \n    # Calculate per-sample memory\n    hidden_size = getattr(model.config, \"hidden_size\", 768)\n    num_layers = getattr(model.config, \"num_hidden_layers\", 12)\n    \n    # Activations per sample\n    activation_gb_per_sample = (\n        seq_length * hidden_size * num_layers * 4  # FP32 activations\n    ) / (1024**3)\n    \n    # KV cache per sample (for generation)\n    kv_cache_gb_per_sample = (\n        2 * num_layers * seq_length * hidden_size * 2  # FP16 K+V cache\n    ) / (1024**3)\n    \n    total_per_sample = activation_gb_per_sample + kv_cache_gb_per_sample\n    \n    # Calculate max batch size\n    available_for_batch = (available_vram_gb - model_vram_gb) * safety_margin\n    max_batch_size = int(available_for_batch / total_per_sample)\n    \n    # Clamp to reasonable range\n    max_batch_size = max(1, min(max_batch_size, 512))\n    \n    logger.info(\n        f\"Estimated max batch size: {max_batch_size} \"\n        f\"(seq_len={seq_length}, VRAM={available_vram_gb:.1f}GB)\"\n    )\n    \n    return max_batch_size\n\n\ndef find_optimal_batch_size(\n    model: torch.nn.Module,\n    tokenizer: Any,\n    device: torch.device,\n    use_zero_inference: bool = True,\n    max_seq_length: int = 2048,\n) -> int:\n    \"\"\"Binary search to find optimal batch size.\"\"\"\n    \n    # Get available VRAM\n    if device.type == \"cuda\":\n        torch.cuda.empty_cache()\n        available_vram_gb = torch.cuda.get_device_properties(device).total_memory / (1024**3)\n    else:\n        available_vram_gb = 16.0  # Default for CPU\n    \n    # Estimate starting point\n    estimated_max = estimate_max_batch_size(\n        model=model,\n        seq_length=max_seq_length,\n        available_vram_gb=available_vram_gb,\n        use_zero_inference=use_zero_inference,\n    )\n    \n    # Binary search for actual max\n    low, high = 1, estimated_max\n    optimal = 1\n    \n    while low <= high:\n        mid = (low + high) // 2\n        \n        # Try this batch size\n        try:\n            test_input = torch.randint(\n                0, 1000,\n                (mid, max_seq_length // 4),  # Test with 1/4 seq len\n                device=device\n            )\n            \n            with torch.no_grad():\n                _ = model(test_input)\n            \n            # Success! Try larger\n            optimal = mid\n            low = mid + 1\n            \n            # Clear memory\n            del test_input\n            torch.cuda.empty_cache()\n            \n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                # OOM, try smaller\n                high = mid - 1\n                torch.cuda.empty_cache()\n            else:\n                raise\n    \n    logger.info(f\"Optimal batch size found: {optimal}\")\n    return optimal\n```\n\n#### 3.2 Performance Monitoring\n\n**File**: `src/aios/core/inference/performance_monitor.py` (new)\n\n```python\n\"\"\"Monitor and log ZeRO-Inference performance metrics.\"\"\"\n\nimport time\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass PerformanceMetrics:\n    \"\"\"Performance metrics for inference.\"\"\"\n    \n    total_tokens: int\n    total_time_s: float\n    tokens_per_second: float\n    \n    # Layer streaming metrics\n    avg_layer_load_time_ms: float\n    avg_layer_compute_time_ms: float\n    cache_hit_rate: float\n    \n    # Memory metrics\n    peak_gpu_memory_gb: float\n    avg_gpu_memory_gb: float\n    \n    def __str__(self) -> str:\n        return (\n            f\"Performance Metrics:\\n\"\n            f\"  Throughput: {self.tokens_per_second:.2f} tokens/s\\n\"\n            f\"  Total Tokens: {self.total_tokens}\\n\"\n            f\"  Total Time: {self.total_time_s:.2f}s\\n\"\n            f\"  Layer Load Time: {self.avg_layer_load_time_ms:.2f}ms\\n\"\n            f\"  Layer Compute Time: {self.avg_layer_compute_time_ms:.2f}ms\\n\"\n            f\"  Cache Hit Rate: {self.cache_hit_rate:.1%}\\n\"\n            f\"  Peak GPU Memory: {self.peak_gpu_memory_gb:.2f}GB\\n\"\n            f\"  Avg GPU Memory: {self.avg_gpu_memory_gb:.2f}GB\"\n        )\n\n\nclass PerformanceMonitor:\n    \"\"\"Monitor ZeRO-Inference performance.\"\"\"\n    \n    def __init__(self, device: str = \"cuda\"):\n        self.device = device\n        self.reset()\n    \n    def reset(self) -> None:\n        \"\"\"Reset all metrics.\"\"\"\n        self.layer_load_times: List[float] = []\n        self.layer_compute_times: List[float] = []\n        self.cache_hits: int = 0\n        self.cache_misses: int = 0\n        self.gpu_memory_samples: List[float] = []\n        self.start_time: Optional[float] = None\n        self.end_time: Optional[float] = None\n        self.total_tokens: int = 0\n    \n    def start(self) -> None:\n        \"\"\"Start timing.\"\"\"\n        self.start_time = time.time()\n    \n    def end(self) -> None:\n        \"\"\"End timing.\"\"\"\n        self.end_time = time.time()\n    \n    def record_layer_load(self, duration_ms: float) -> None:\n        \"\"\"Record layer load time.\"\"\"\n        self.layer_load_times.append(duration_ms)\n    \n    def record_layer_compute(self, duration_ms: float) -> None:\n        \"\"\"Record layer compute time.\"\"\"\n        self.layer_compute_times.append(duration_ms)\n    \n    def record_cache_hit(self) -> None:\n        \"\"\"Record cache hit.\"\"\"\n        self.cache_hits += 1\n    \n    def record_cache_miss(self) -> None:\n        \"\"\"Record cache miss.\"\"\"\n        self.cache_misses += 1\n    \n    def sample_gpu_memory(self) -> None:\n        \"\"\"Sample current GPU memory usage.\"\"\"\n        if self.device == \"cuda\":\n            import torch\n            memory_gb = torch.cuda.memory_allocated() / (1024**3)\n            self.gpu_memory_samples.append(memory_gb)\n    \n    def get_metrics(self) -> PerformanceMetrics:\n        \"\"\"Get current metrics.\"\"\"\n        total_time = (self.end_time or time.time()) - (self.start_time or 0)\n        tokens_per_second = self.total_tokens / total_time if total_time > 0 else 0\n        \n        avg_load_time = (\n            sum(self.layer_load_times) / len(self.layer_load_times)\n            if self.layer_load_times else 0\n        )\n        \n        avg_compute_time = (\n            sum(self.layer_compute_times) / len(self.layer_compute_times)\n            if self.layer_compute_times else 0\n        )\n        \n        cache_hit_rate = (\n            self.cache_hits / (self.cache_hits + self.cache_misses)\n            if (self.cache_hits + self.cache_misses) > 0 else 0\n        )\n        \n        peak_memory = max(self.gpu_memory_samples) if self.gpu_memory_samples else 0\n        avg_memory = (\n            sum(self.gpu_memory_samples) / len(self.gpu_memory_samples)\n            if self.gpu_memory_samples else 0\n        )\n        \n        return PerformanceMetrics(\n            total_tokens=self.total_tokens,\n            total_time_s=total_time,\n            tokens_per_second=tokens_per_second,\n            avg_layer_load_time_ms=avg_load_time,\n            avg_layer_compute_time_ms=avg_compute_time,\n            cache_hit_rate=cache_hit_rate,\n            peak_gpu_memory_gb=peak_memory,\n            avg_gpu_memory_gb=avg_memory,\n        )\n```\n\n---\n\n### Phase 4: Testing & Validation (Week 4-5)\n\n#### 4.1 Unit Tests\n\n**File**: `tests/test_zero_inference.py`\n\n```python\n\"\"\"Unit tests for ZeRO-Inference components.\"\"\"\n\nimport pytest\nimport torch\nfrom aios.core.inference.layer_streaming import LayerCache, LayerStreamer\nfrom aios.core.inference.offload_manager import OffloadManager\nfrom aios.core.inference.inference_config import InferenceConfig\n\n\ndef test_layer_cache():\n    \"\"\"Test layer cache functionality.\"\"\"\n    cache = LayerCache(max_size=3, device=torch.device(\"cpu\"))\n    \n    # Add layers\n    layer0 = {\"weight\": torch.randn(10, 10)}\n    layer1 = {\"weight\": torch.randn(10, 10)}\n    layer2 = {\"weight\": torch.randn(10, 10)}\n    \n    cache.put(0, layer0)\n    cache.put(1, layer1)\n    cache.put(2, layer2)\n    \n    assert cache.has(0)\n    assert cache.has(1)\n    assert cache.has(2)\n    \n    # Eviction test\n    layer3 = {\"weight\": torch.randn(10, 10)}\n    cache.put(3, layer3)\n    \n    assert not cache.has(0)  # Evicted\n    assert cache.has(3)\n\n\ndef test_offload_manager_cpu():\n    \"\"\"Test CPU offload manager.\"\"\"\n    # Create simple model\n    model = torch.nn.Sequential(\n        torch.nn.Linear(10, 10),\n        torch.nn.Linear(10, 10),\n    )\n    \n    manager = OffloadManager(\n        model=model,\n        device=\"cpu\",\n        pin_memory=False,\n    )\n    \n    # This test would need async handling\n    # Skipping full implementation for brevity\n\n\ndef test_inference_config():\n    \"\"\"Test inference config creation.\"\"\"\n    config = InferenceConfig(\n        model_path=\"gpt2\",\n        use_zero_inference=True,\n        offload_device=\"cpu\",\n        prefetch_layers=2,\n    )\n    \n    assert config.use_zero_inference\n    assert config.offload_device == \"cpu\"\n    assert config.prefetch_layers == 2\n```\n\n#### 4.2 Integration Tests\n\n**File**: `tests/integration/test_zero_inference_e2e.py`\n\n```python\n\"\"\"End-to-end tests for ZeRO-Inference.\"\"\"\n\nimport pytest\nimport torch\nfrom pathlib import Path\n\n\n@pytest.mark.skipif(\n    not torch.cuda.is_available(),\n    reason=\"CUDA not available\"\n)\ndef test_zero_inference_small_model():\n    \"\"\"Test ZeRO-Inference on GPT-2.\"\"\"\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n    from aios.core.inference.zero_inference_pipeline import ZeroInferencePipeline\n    from aios.core.inference.inference_config import InferenceConfig\n    \n    # Load model\n    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n    \n    # Create config\n    config = InferenceConfig(\n        model_path=\"gpt2\",\n        device=\"cuda\",\n        use_zero_inference=True,\n        offload_device=\"cpu\",\n        max_tokens=10,\n    )\n    \n    # Create pipeline\n    pipeline = ZeroInferencePipeline(\n        model=model,\n        tokenizer=tokenizer,\n        config=config,\n    )\n    \n    # Generate\n    outputs = pipeline.generate([\"Hello, world!\"])\n    \n    assert len(outputs) == 1\n    assert len(outputs[0]) > 0\n    \n    # Cleanup\n    pipeline.cleanup()\n```\n\n---\n\n### Phase 5: Documentation (Week 5)\n\n#### 5.1 User Guide\n\n**File**: `docs/guide/zero_inference_guide.md`\n\nContent covering:\n- What is ZeRO-Inference\n- When to use it (model size thresholds)\n- Hardware requirements\n- Setup instructions\n- CLI examples\n- Performance tuning\n- Troubleshooting\n\n#### 5.2 API Documentation\n\n- Comprehensive docstrings for all classes\n- Usage examples in code\n- Integration guides for custom models\n\n---\n\n## Success Metrics\n\n### Functional Requirements\n1. \u2705 Run 100B+ param models on single 24GB GPU\n2. \u2705 CPU and NVMe offload working\n3. \u2705 Layer prefetching reduces latency\n4. \u2705 Multi-GPU fetch parallelization\n5. \u2705 Integration with HuggingFace models\n\n### Performance Requirements\n1. \u2705 CPU offload: <30% slowdown vs GPU-only\n2. \u2705 NVMe offload: <50% slowdown vs GPU-only\n3. \u2705 Prefetching: >20% speedup vs no prefetch\n4. \u2705 Multi-GPU: Near-linear scaling (2x GPUs \u2192 ~1.8x faster)\n5. \u2705 Throughput: >10 tokens/s for 100B model on 1x 24GB GPU\n\n### Quality Requirements\n1. \u2705 Comprehensive error handling\n2. \u2705 Performance monitoring\n3. \u2705 Auto-tuned batch sizes\n4. \u2705 Unit test coverage >80%\n5. \u2705 Complete documentation\n\n---\n\n## Risks & Mitigations\n\n### Risk 1: HuggingFace API Compatibility\n**Impact**: Different models have different layer access patterns\n\n**Mitigation**:\n- Abstract layer access via adapter pattern\n- Support common architectures (GPT, LLaMA, BLOOM, OPT)\n- Provide custom layer accessor API\n\n### Risk 2: Transfer Latency Dominates\n**Impact**: Even with prefetching, may be too slow\n\n**Mitigation**:\n- Optimize batch sizes (larger = better amortization)\n- Use pinned memory\n- Profile and optimize transfer paths\n- Consider compression (future)\n\n### Risk 3: Memory Fragmentation\n**Impact**: GPU memory fragmentation causes OOM\n\n**Mitigation**:\n- Use memory pools\n- Aggressive cache clearing\n- Monitor fragmentation\n- Restart recommendation if severe\n\n---\n\n## Timeline\n\n- **Week 1-2**: Phase 1 (Core Infrastructure)\n- **Week 2-3**: Phase 2 (HuggingFace Integration)\n- **Week 3-4**: Phase 3 (Performance Optimization)\n- **Week 4-5**: Phase 4 (Testing)\n- **Week 5**: Phase 5 (Documentation)\n- **Total**: ~5 weeks\n\n---\n\n## Dependencies\n\n### Required Packages\n```toml\n[project.optional-dependencies]\ninference = [\n    \"deepspeed>=0.6.6\",\n    \"transformers>=4.30.0\",\n    \"accelerate>=0.20.0\",\n]\n```\n\n### Hardware Requirements\n\n**Minimum**:\n- 1x NVIDIA GPU (11GB+)\n- 32GB RAM\n- CUDA 11.0+\n\n**Recommended**:\n- 1-2x NVIDIA GPU (24GB+)\n- 64GB RAM\n- NVMe SSD (for >100B models)\n\n---\n\n## References\n\n- [DeepSpeed ZeRO-Inference Blog](https://www.deepspeed.ai/2022/09/09/zero-inference.html)\n- [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed)\n- [HuggingFace Accelerate](https://github.com/huggingface/accelerate)\n\n---\n\n## Conclusion\n\nZeRO-Inference integration will democratize access to massive models by enabling inference on consumer hardware. Users with a single 24GB GPU will be able to run models previously requiring 8+ high-end GPUs.\n\n**Key Benefits**:\n- Run 100B+ models on single GPU\n- 90%+ memory savings\n- Reasonable performance (<30% slowdown with CPU offload)\n- Full HuggingFace compatibility\n- Automatic optimization\n\n**Recommendation**: Proceed with implementation, focusing on CPU offload first, then NVMe support.\n", "tags": [], "headings": [{"line": 0, "text": "DeepSpeed ZeRO-Inference Integration Plan"}, {"line": 2, "text": "Executive Summary"}, {"line": 22, "text": "Background"}, {"line": 24, "text": "What is ZeRO-Inference?"}, {"line": 36, "text": "Memory Hierarchy"}, {"line": 48, "text": "Current AI-OS Capabilities"}, {"line": 68, "text": "Technical Architecture"}, {"line": 70, "text": "Inference Flow with ZeRO-Inference"}, {"line": 102, "text": "Multi-GPU Fetch Parallelization"}, {"line": 119, "text": "Prefetching Optimization"}, {"line": 136, "text": "Implementation Plan"}, {"line": 138, "text": "Phase 1: Core Infrastructure (Week 1-2)"}, {"line": 140, "text": "1.1 Configuration Schema"}, {"line": 219, "text": "1.2 Inference Configuration Fields"}, {"line": 234, "text": "Model configuration"}, {"line": 244, "text": "ZeRO-Inference settings"}, {"line": 356, "text": "Generation settings"}, {"line": 385, "text": "Performance tuning"}, {"line": 433, "text": "1.3 Layer Streaming Engine"}, {"line": 468, "text": "Evict oldest layer"}, {"line": 506, "text": "Layer cache for prefetching"}, {"line": 513, "text": "Track which layers are currently on GPU"}, {"line": 516, "text": "Prefetch queue"}, {"line": 527, "text": "Load layer weights from offload manager"}, {"line": 530, "text": "Store in cache"}, {"line": 537, "text": "Check cache first"}, {"line": 541, "text": "Not in cache, load directly (blocking)"}, {"line": 545, "text": "Transfer to GPU"}, {"line": 565, "text": "Move parameters to empty tensors (free GPU memory)"}, {"line": 574, "text": "Assumes model has .layers attribute (common for transformers)"}, {"line": 575, "text": "May need to adapt for different architectures"}, {"line": 591, "text": "Get number of layers"}, {"line": 595, "text": "Initial embedding"}, {"line": 598, "text": "Process each layer"}, {"line": 600, "text": "Prefetch next layers"}, {"line": 606, "text": "Load current layer to GPU"}, {"line": 609, "text": "Forward pass through layer"}, {"line": 613, "text": "Unload previous layer if not needed"}, {"line": 638, "text": "1.4 Offload Manager"}, {"line": 671, "text": "Storage for offloaded weights"}, {"line": 691, "text": "Extract weights"}, {"line": 695, "text": "Move to CPU with optional pinning"}, {"line": 701, "text": "Save to NVMe"}, {"line": 707, "text": "Save to disk"}, {"line": 712, "text": "Free GPU memory"}, {"line": 723, "text": "Load from NVMe"}, {"line": 726, "text": "Use asyncio to avoid blocking"}, {"line": 737, "text": "Same logic as LayerStreamer"}, {"line": 752, "text": "Delete NVMe files"}, {"line": 761, "text": "Phase 2: HuggingFace Integration (Week 2-3)"}, {"line": 763, "text": "2.1 Generation Pipeline Wrapper"}, {"line": 797, "text": "Initialize offload manager"}, {"line": 805, "text": "Initialize layer streamer"}, {"line": 812, "text": "Determine num layers"}, {"line": 815, "text": "Offload all layers initially"}, {"line": 839, "text": "Tokenize inputs"}, {"line": 847, "text": "Generate with streaming"}, {"line": 858, "text": "Decode outputs"}, {"line": 878, "text": "Phase 1: Process prompt (prefill)"}, {"line": 884, "text": "Get initial logits"}, {"line": 888, "text": "Initialize output with prompt + first token"}, {"line": 891, "text": "Phase 2: Auto-regressive generation"}, {"line": 893, "text": "Process only the new token through all layers"}, {"line": 900, "text": "Get next token"}, {"line": 904, "text": "Append to output"}, {"line": 907, "text": "Check for EOS"}, {"line": 930, "text": "Apply temperature"}, {"line": 933, "text": "Top-k filtering"}, {"line": 938, "text": "Top-p filtering"}, {"line": 952, "text": "Sample"}, {"line": 974, "text": "2.2 CLI Integration"}, {"line": 995, "text": "Output"}, {"line": 1001, "text": "Generation params"}, {"line": 1007, "text": "ZeRO-Inference"}, {"line": 1026, "text": "Device"}, {"line": 1037, "text": "Load model and tokenizer"}, {"line": 1042, "text": "Load model to CPU first"}, {"line": 1049, "text": "Create config"}, {"line": 1064, "text": "Create pipeline"}, {"line": 1072, "text": "Generate"}, {"line": 1077, "text": "Standard inference"}, {"line": 1094, "text": "Print outputs"}, {"line": 1101, "text": "Save to file"}, {"line": 1113, "text": "Phase 3: Performance Optimization (Week 3-4)"}, {"line": 1115, "text": "3.1 Batch Size Auto-Tuning"}, {"line": 1138, "text": "Get model size"}, {"line": 1142, "text": "With ZeRO-Inference, only 1-2 layers in VRAM"}, {"line": 1145, "text": "Standard inference: full model in VRAM"}, {"line": 1148, "text": "Calculate per-sample memory"}, {"line": 1152, "text": "Activations per sample"}, {"line": 1157, "text": "KV cache per sample (for generation)"}, {"line": 1164, "text": "Calculate max batch size"}, {"line": 1168, "text": "Clamp to reasonable range"}, {"line": 1188, "text": "Get available VRAM"}, {"line": 1195, "text": "Estimate starting point"}, {"line": 1203, "text": "Binary search for actual max"}, {"line": 1210, "text": "Try this batch size"}, {"line": 1221, "text": "Success! Try larger"}, {"line": 1225, "text": "Clear memory"}, {"line": 1231, "text": "OOM, try smaller"}, {"line": 1241, "text": "3.2 Performance Monitoring"}, {"line": 1264, "text": "Layer streaming metrics"}, {"line": 1269, "text": "Memory metrics"}, {"line": 1376, "text": "Phase 4: Testing & Validation (Week 4-5)"}, {"line": 1378, "text": "4.1 Unit Tests"}, {"line": 1396, "text": "Add layers"}, {"line": 1409, "text": "Eviction test"}, {"line": 1419, "text": "Create simple model"}, {"line": 1431, "text": "This test would need async handling"}, {"line": 1432, "text": "Skipping full implementation for brevity"}, {"line": 1449, "text": "4.2 Integration Tests"}, {"line": 1471, "text": "Load model"}, {"line": 1475, "text": "Create config"}, {"line": 1484, "text": "Create pipeline"}, {"line": 1491, "text": "Generate"}, {"line": 1497, "text": "Cleanup"}, {"line": 1503, "text": "Phase 5: Documentation (Week 5)"}, {"line": 1505, "text": "5.1 User Guide"}, {"line": 1518, "text": "5.2 API Documentation"}, {"line": 1526, "text": "Success Metrics"}, {"line": 1528, "text": "Functional Requirements"}, {"line": 1535, "text": "Performance Requirements"}, {"line": 1542, "text": "Quality Requirements"}, {"line": 1551, "text": "Risks & Mitigations"}, {"line": 1553, "text": "Risk 1: HuggingFace API Compatibility"}, {"line": 1561, "text": "Risk 2: Transfer Latency Dominates"}, {"line": 1570, "text": "Risk 3: Memory Fragmentation"}, {"line": 1581, "text": "Timeline"}, {"line": 1592, "text": "Dependencies"}, {"line": 1594, "text": "Required Packages"}, {"line": 1604, "text": "Hardware Requirements"}, {"line": 1618, "text": "References"}, {"line": 1626, "text": "Conclusion"}]}]